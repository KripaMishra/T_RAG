{
  "id": 1,
  "title": "Library",
  "type": "library",
  "children": [
    {
      "id": 2,
      "title": "Book 1",
      "type": "book",
      "children": [
        {
          "id": 3,
          "title": "2.1      Architecture",
          "type": "section",
          "children": [
            {
              "id": 4,
              "title": "2.1      Architecture - Chunk 1",
              "type": "chunk",
              "children": [],
              "content": "2.1      Architecture\nture  \n \n  BART      uses      the      standard      sequence-to-sequence      Trans-  \n \n  former      architecture      from      (Vaswani      et      al.,      2017),      ex-  \n \n  cept,      following      GPT,      that      we      modify      ReLU      activa-  \n \n  tion      functions      to      GeLUs      (Hendrycks   \n \n&      Gimpel,      2016)  \n \n  and      initialise      parameters      from      A/(0,0.02)."
            },
            {
              "id": 5,
              "title": "2.1      Architecture - Chunk 2",
              "type": "chunk",
              "children": [],
              "content": "For      our  \n \n  base      model,      we      use   \n \n6      layers      in      the      encoder      and      de-  \n \n  coder,      and      for      our      large      model      we      use      12      layers      in  \n \n  each."
            },
            {
              "id": 6,
              "title": "2.1      Architecture - Chunk 3",
              "type": "chunk",
              "children": [],
              "content": "The      architecture      is      closely      related      to      that      used      in  \n \n  BERT,      with      the      following      differences:      (1)      each      layer      of  \n \n  the      decoder      additionally      performs      cross-attention      over  \n \n  the      final      hidden      layer      of      the      encoder      (as      in      the      trans-  \n \n  former      sequence-to-sequence      model);      and      (2)      BERT  \n \n  uses      an      additional      feed-forward      network      before      word-  \n \n  prediction,      which      BART      does      not."
            },
            {
              "id": 7,
              "title": "2.1      Architecture - Chunk 4",
              "type": "chunk",
              "children": [],
              "content": "In      total,      BART      con-  \n \n  tains      roughly      10%      more      parameters      than      the      equiva-  \n \n  lently      sized      BERT      model."
            }
          ],
          "content": ""
        },
        {
          "id": 8,
          "title": "2.2      Pre-training      BART",
          "type": "section",
          "children": [
            {
              "id": 9,
              "title": "2.2      Pre-training      BART - Chunk 1",
              "type": "chunk",
              "children": [],
              "content": "2.2      Pre-training      BART\nBART  \n \n  BART      is      trained      by      corrupting      documents      and      then      op-  \n \n  timizing   \n \na      reconstruction      loss—the      cross-entropy      be-  \n \n  tween      the      decoder’s      output      and      the      original      document."
            },
            {
              "id": 10,
              "title": "2.2      Pre-training      BART - Chunk 2",
              "type": "chunk",
              "children": [],
              "content": "Unlike      existing      denoising      autoencoders,      which      are      tai-  \n \n  lored      to      specific      noising      schemes,      BART      allows      us      to  \n \n  apply      any      type      of      document      corruption. In      the      extreme  \n \n  case,      where      all      information      about      the      source      is      lost,  \n \n  BART      is      equivalent      to   \n \na      language      model."
            },
            {
              "id": 11,
              "title": "2.2      Pre-training      BART - Chunk 3",
              "type": "chunk",
              "children": [],
              "content": "We      experiment      with      several      previously      proposed      and  \n \n  novel      transformations,      but      we      believe      there      is   \n \na      sig-  \n \n  nificant      potential      for      development      of      other      new      alter-  \n \n  natives. The      transformations      we      used      are      summarized  \n \n  below,      and      examples      are      shown      in      Figure      2."
            },
            {
              "id": 12,
              "title": "2.2      Pre-training      BART - Chunk 4",
              "type": "chunk",
              "children": [],
              "content": "Token      Masking      Following      BERT      (Devlin      et      al.,  \n \n  2019),      random      tokens      are      sampled      and      replaced      with  \n \n  [MASK]      elements. Token      Deletion      Random      tokens      are      deleted      from      the  \n \n  input. In      contrast      to      token      masking,      the      model      must  \n \n  decide      which      positions      are      missing      inputs. Token      Masking"
            }
          ],
          "content": ""
        },
        {
          "id": 13,
          "title": "Token      Masking",
          "type": "section",
          "children": [
            {
              "id": 14,
              "title": "Token      Masking - Chunk 1",
              "type": "chunk",
              "children": [],
              "content": "Token      Masking"
            }
          ],
          "content": ""
        },
        {
          "id": 15,
          "title": "C.DE.AB",
          "type": "section",
          "children": [
            {
              "id": 16,
              "title": "C.DE.AB - Chunk 1",
              "type": "chunk",
              "children": [],
              "content": "C.DE.AB\nToken      Deletion Text      Infilling\n \n \n  Text      Infilling   \n \nA      number      of      text      spans      are      sampled,  \n \n  with      span      lengths      drawn      from   \n \na      Poisson      distribution  \n \n  (A   \n \n=      3). Each      span      is      replaced      with   \n \na      single      [MASK]  \n \n  token. O-length      spans      correspond      to      the      insertion      of  \n \n  [MASK]      tokens."
            },
            {
              "id": 17,
              "title": "C.DE.AB - Chunk 2",
              "type": "chunk",
              "children": [],
              "content": "Text      infilling      is      inspired      by      Span-  \n \n  BERT      (Joshi      et      al.,      2019),      but      SpanBERT      samples  \n \n  span      lengths      from   \n \na      different      (clamped      geometric)      dis-  \n \n  tribution,      and      replaces      each      span      with   \n \na      sequence      of  \n \n  [MASK]      tokens      of      exactly      the      same      length."
            },
            {
              "id": 18,
              "title": "C.DE.AB - Chunk 3",
              "type": "chunk",
              "children": [],
              "content": "Text      infill-  \n \n  ing      teaches      the      model      to      predict      how      many      tokens      are  \n \n  missing      from   \n \na      span. Sentence      Permutation   \n \nA      document      is      divided      into  \n \n  sentences      based      on      full      stops,      and      these      sentences      are  \n \n  shuffled      in   \n \na      random      order."
            },
            {
              "id": 19,
              "title": "C.DE.AB - Chunk 4",
              "type": "chunk",
              "children": [],
              "content": "Document      Rotation   \n \nA      token      is      chosen      uniformly      at  \n \n  random,      and      the      document      is      rotated      so      that      it      begins  \n \n  with      that      token. This      task      trains      the      model      to      identify  \n \n  the      start      of      the      document."
            },
            {
              "id": 20,
              "title": "C.DE.AB - Chunk 5",
              "type": "chunk",
              "children": [],
              "content": "3\n \n   Fine-tuning      BART The      representations      produced      by      BART      can      be      used      in  \n \n  several      ways      for      downstream      applications."
            },
            {
              "id": 21,
              "title": "C.DE.AB - Chunk 6",
              "type": "chunk",
              "children": [],
              "content": "3.1      Sequence      Classification      Tasks\nasks  \n \n  For      sequence      classification      tasks,      the      same      input      is      fed  \n \n  into      the      encoder      and      decoder,      and      the      final      hidden      state  \n \n  of      the      final      decoder      token      is      fed      into      new      multi-class  \n \n  linear      classifier."
            },
            {
              "id": 22,
              "title": "C.DE.AB - Chunk 7",
              "type": "chunk",
              "children": [],
              "content": "This      approach      is      related      to      the      CLS  \n \n  token      in      BERT;      however      we      add      the      additional      token  \n \n  to      the      end      so      that      representation      for      the      token      in      the  \n \n  decoder      can      attend      to      decoder      states      from      the      complete  \n \n  input      (Figure      3a). 3.2."
            },
            {
              "id": 23,
              "title": "C.DE.AB - Chunk 8",
              "type": "chunk",
              "children": [],
              "content": "Token      Classification      Tasks\nasks  \n \n  For      token      classification      tasks,      such      as      answer      endpoint  \n \n  classification      for      SQUAD,      we      feed      the      complete      doc-  \n \n  ument      into      the      encoder      and      decoder,      and      use      the      top  \n \n  hidden      state      of      the      decoder      as   \n \na      representation      for      each  \n \n  word."
            },
            {
              "id": 24,
              "title": "C.DE.AB - Chunk 9",
              "type": "chunk",
              "children": [],
              "content": "This      representation      is      used      to      classify      the      token. 3.3      Sequence      Generation      Tasks\nasks  \n \n  Because      BART      has      an      autoregressive      decoder,      it      can      be  \n \n  directly      fine      tuned      for      sequence      generation      tasks      such  \n \n  as      abstractive      question      answering      and      summarization."
            },
            {
              "id": 25,
              "title": "C.DE.AB - Chunk 10",
              "type": "chunk",
              "children": [],
              "content": "In      both      of      these      tasks,      information      is      copied      from      the  \n \n  input      but      manipulated,      which      is      closely      related      to      the  \n \n  denoising      pre-training      objective. Here,      the      encoder      in-  \n \n  put      is      the      input      sequence,      and      the      decoder      generates  \n \n  outputs      autoregressively."
            },
            {
              "id": 26,
              "title": "C.DE.AB - Chunk 11",
              "type": "chunk",
              "children": [],
              "content": "3.4      Machine      Translation\ntion  \n \n  We      also      explore      using      BART      to      improve      machine      trans-  \n \n  lation      decoders      for      translating      into      English. Previous  \n \n  work      Edunov      et      al."
            },
            {
              "id": 27,
              "title": "C.DE.AB - Chunk 12",
              "type": "chunk",
              "children": [],
              "content": "(2019)      has      shown      that      models      can  \n \n  be      improved      by      incorporating      pre-trained      encoders,      but  \n \n  gains      from      using      pre-trained      language      models      in      de-  \n \n  coders      have      been      limited."
            },
            {
              "id": 28,
              "title": "C.DE.AB - Chunk 13",
              "type": "chunk",
              "children": [],
              "content": "We      show      that      it      is      possible  \n \n  to      use      the      entire      BART      model      (both      encoder      and      de-  \n \n  coder)      as   \n \na      single      pretrained      decoder      for      machine      trans-  \n \n  lation,      by      adding   \n \na      new      set      of      encoder      parameters      that  \n \n  are      learned      from      bitext      (see      Figure      3b)."
            },
            {
              "id": 29,
              "title": "C.DE.AB - Chunk 14",
              "type": "chunk",
              "children": [],
              "content": "More      precisely,      we      replace      BART’s      encoder      embed-  \n \n  ding      layer      with   \n \na      new      randomly      initialized      encoder. The      model      is      trained      end-to-end,      which      trains      the      new       encoder      to      map      foreign      words      into      an      input      that      BART  \n \n  can      de-noise      to      English."
            },
            {
              "id": 30,
              "title": "C.DE.AB - Chunk 15",
              "type": "chunk",
              "children": [],
              "content": "The      new      encoder      can      use   \n \na       separate      vocabulary      from      the      original      BART      model. We      train      the      source      encoder      in      two      steps,      in      both  \n \n  cases      backpropagating      the      cross-entropy      loss      from      the  \n \n  output      of      the      BART      model."
            },
            {
              "id": 31,
              "title": "C.DE.AB - Chunk 16",
              "type": "chunk",
              "children": [],
              "content": "In      the      first      step,      we      freeze  \n \n  most      of      BART      parameters      and      only      update      the      ran-  \n \n  domly      initialized      source      encoder,      the      BART      positional  \n \n  embeddings,      and      the      self-attention      input      projection      ma-  \n \n  trix      of      BART’s      encoder      first      layer."
            },
            {
              "id": 32,
              "title": "C.DE.AB - Chunk 17",
              "type": "chunk",
              "children": [],
              "content": "In      the      second      step,  \n \n  we      train      all      model      parameters      for   \n \na      small      number      of  \n \n  iterations. 4\n \n   Comparing      Pre-training      Objectives  \n \n  BART      supports   \n \na      much      wider      range      of      noising      schemes  \n \n  during      pre-training      than      previous      work."
            },
            {
              "id": 33,
              "title": "C.DE.AB - Chunk 18",
              "type": "chunk",
              "children": [],
              "content": "We      compare   \n \na       range      of      options      using      base-size      models      (6      encoder      and  \n \n  6      decoder      layers,      with   \n \na      hidden      size      of      768),      evaluated  \n \n  on   \n \na      representative      subset      of      the      tasks      we      will      consider  \n \n  for      the      full      large      scale      experiments      in      85."
            },
            {
              "id": 34,
              "title": "C.DE.AB - Chunk 19",
              "type": "chunk",
              "children": [],
              "content": "4.1      Comparison      Objectives\nives  \n \n  While      many      pre-training      objectives      have      been      pro-  \n \n  posed,      fair      comparisons      between      these      have      been      dif-  \n \n  ficult      to      perform,      at      least      in      part      due      to      differences      in  \n \n  training      data,      training      resources,      architectural      differ-  \n \n  ences      between      models,      and      fine-tuning      procedures."
            },
            {
              "id": 35,
              "title": "C.DE.AB - Chunk 20",
              "type": "chunk",
              "children": [],
              "content": "We  \n \n  ABCDE  \n \n  weee  \n \n  Pre-trained      Pre-trained  \n \n  Craters      Joo      rezapes cise)      Te      TrET Pre-trained  \n \n  Encoder Pre-trained  \n \n  Decoder aa      AB  \n \n  trrrt  \n \n  tr      ttt  \n \n  <s>A      BCD  \n \n  Randomly  \n \n  Initialized      Encoder  \n \n  TFT  \n \n  aBpByoe sentation      from      the      final      output      is      used."
            },
            {
              "id": 36,
              "title": "C.DE.AB - Chunk 21",
              "type": "chunk",
              "children": [],
              "content": "re-implement      strong      pre-training      approaches      recently  \n \n  proposed      for      discriminative      and      generation      tasks. We  \n \n  aim,      as      much      as      possible,      to      control      for      differences      un-  \n \n  related      to      the      pre-training      objective."
            },
            {
              "id": 37,
              "title": "C.DE.AB - Chunk 22",
              "type": "chunk",
              "children": [],
              "content": "However,      we      do  \n \n  make      minor      changes      to      the      learning      rate      and      usage      of  \n \n  layer      normalisation      in      order      to      improve      performance  \n \n  (tuning      these      separately      for      each      objective)."
            },
            {
              "id": 38,
              "title": "C.DE.AB - Chunk 23",
              "type": "chunk",
              "children": [],
              "content": "For      refer-  \n \n  ence,      we      compare      our      implementations      with      published  \n \n  numbers      from      BERT,      which      was      also      trained      for      1M  \n \n  steps      on   \n \na      combination      of      books      and      Wikipedia      data."
            },
            {
              "id": 39,
              "title": "C.DE.AB - Chunk 24",
              "type": "chunk",
              "children": [],
              "content": "We      compare      the      following      approaches:\n \n \n  Language      Model      Similarly      to      GPT      (Radford      et      al.,  \n \n  2018),      we      train   \n \na      left-to-right      Transformer      language  \n \n  model. This      model      is      equivalent      to      the      BART      decoder,  \n \n  without      cross-attention."
            },
            {
              "id": 40,
              "title": "C.DE.AB - Chunk 25",
              "type": "chunk",
              "children": [],
              "content": "Permuted      Language      Model      Based      on      XLNet      (Yang  \n \n  et      al.,      2019),      we      sample      1/6      of      the      tokens,      and      gener-  \n \n  ate      them      in   \n \na      random      order      autoregressively."
            },
            {
              "id": 41,
              "title": "C.DE.AB - Chunk 26",
              "type": "chunk",
              "children": [],
              "content": "For      con-  \n \n  sistency      with      other      models,      we      do      not      implement      the  \n \n  relative      positional      embeddings      or      attention      across      seg-  \n \n  ments      from      XLNet."
            },
            {
              "id": 42,
              "title": "C.DE.AB - Chunk 27",
              "type": "chunk",
              "children": [],
              "content": "Masked      Language      Model      Following      BERT      (Devlin  \n \n  et      al.,      2019),      we      replace      15%      of      tokens      with      [MASK]  \n \n  symbols,      and      train      the      model      to      independently      predict  \n \n  the      original      tokens."
            },
            {
              "id": 43,
              "title": "C.DE.AB - Chunk 28",
              "type": "chunk",
              "children": [],
              "content": "Multitask      Masked      Language      Model      As      in      UniLM  \n \n  (Dong      et      al.,      2019),      we      train   \n \na      Masked      Language  \n \n  Model      with      additional      self-attention      masks."
            },
            {
              "id": 44,
              "title": "C.DE.AB - Chunk 29",
              "type": "chunk",
              "children": [],
              "content": "Self      at-  \n \n  tention      masks      are      chosen      randomly      in      with      the      follow  \n \n  proportions:      1/6      left-to-right,      1/6      right-to-left,      1/3      un-  \n \n  masked,      and      1/3      with      the      first      50%      of      tokens      unmasked  \n \n  and   \n \na      left-to-right      mask      for      the      remainder."
            },
            {
              "id": 45,
              "title": "C.DE.AB - Chunk 30",
              "type": "chunk",
              "children": [],
              "content": "Masked      Seq-to-Seq      Inspired      by      MASS      (Song      et      al.,  \n \n  2019),      we      mask   \n \na      span      containing      50%      of      tokens,  \n \n  and      train   \n \na      sequence      to      sequence      model      to      predict      the  \n \n  masked      tokens."
            },
            {
              "id": 46,
              "title": "C.DE.AB - Chunk 31",
              "type": "chunk",
              "children": [],
              "content": "For      the      Permuted      LM,      Masked      LM      and      Multitask  \n \n  Masked      LM,      we      use      two-stream      attention      (Yang      et      al.,  \n \n  2019)      to      efficiently      compute      likelihoods      of      the      output  \n \n  part      of      the      sequence      (using   \n \na      diagonal      self-attention  \n \n  mask      on      the      output      to      predict      words      left-to-right)."
            },
            {
              "id": 47,
              "title": "C.DE.AB - Chunk 32",
              "type": "chunk",
              "children": [],
              "content": "We      experiment      with      (1)      treating      the      task      as   \n \na      stan-  \n \n  dard      sequence-to-sequence      problem,      where      the      source  \n \n  input      to      the      encoder      and      the      target      is      the      decoder      out-  \n \n  put,      or      (2)      adding      the      source      as      prefix      to      the      target      in  \n \n  the      decoder,      with   \n \na      loss      only      on      the      target      part      of      the  \n \n  sequence."
            },
            {
              "id": 48,
              "title": "C.DE.AB - Chunk 33",
              "type": "chunk",
              "children": [],
              "content": "We      find      the      former      works      better      for      BART  \n \n  models,      and      the      latter      for      other      models. To      most      directly      compare      our      models      on      their      ability  \n \n  to      model      their      fine-tuning      objective      (the      log      likelihood  \n \n  of      the      human      text),      we      report      perplexity      in      Table      1."
            },
            {
              "id": 49,
              "title": "C.DE.AB - Chunk 34",
              "type": "chunk",
              "children": [],
              "content": "4.2      Tasks\nasks  \n \n  SQuAD  \n \n  (Rajpurkar      et      al.,      2016)a      an      extractive      ques-  \n \n  tion      answering      task      on      Wikipedia      paragraphs. Answers  \n \n  are      text      spans      extracted      from   \n \na      given      document      context."
            },
            {
              "id": 50,
              "title": "C.DE.AB - Chunk 35",
              "type": "chunk",
              "children": [],
              "content": "Similar      to      BERT      (Devlin      et      al.,      2019),      we      use      concate-  \n \n  nated      question      and      context      as      input      to      the      encoder      of  \n \n  BART,      and      additionally      pass      them      to      the      decoder. The  \n \n  model      includes      classifiers      to      predict      the      start      and      end  \n \n  indices      of      each      token."
            },
            {
              "id": 51,
              "title": "C.DE.AB - Chunk 36",
              "type": "chunk",
              "children": [],
              "content": "MNLI      (Williams      et      al.,      2017),   \n \na      bitext      classification  \n \n  task      to      predict      whether      one      sentence      entails      another. The      fine-tuned      model      concatenates      the      two      sentences  \n \n  with      appended      an      EOS      token,      and      passes      them      to      both  \n \n  the      BART      encoder      and      decoder."
            },
            {
              "id": 52,
              "title": "C.DE.AB - Chunk 37",
              "type": "chunk",
              "children": [],
              "content": "In      contrast      to      BERT,  \n \n  the      representation      of      the      EOS      token      is      used      to      classify  \n \n  the      sentences      relations. ELIS      (Fanetal.,      2019),   \n \na      long-form      abstractive      ques-  \n \n  tion      answering      dataset. Models      generate      answers      con-  \n \n  ditioned      on      the      concatenation      of   \n \na      question      and      sup-  \n \n  porting      documents."
            },
            {
              "id": 53,
              "title": "C.DE.AB - Chunk 38",
              "type": "chunk",
              "children": [],
              "content": "XSum_      (Narayan      et      al.,      2018),   \n \na      news      summarization  \n \n  dataset      with      highly      abstractive      summaries. ConvAI2_      (Dinan      et      al.,      2019),   \n \na      dialogue      response  \n \n  generation      task,      conditioned      on      context      and   \n \na      persona. CNN/DM_      (Hermann      et      al.,      2015),   \n \na      news      summa-  \n \n  rization      dataset."
            },
            {
              "id": 54,
              "title": "C.DE.AB - Chunk 39",
              "type": "chunk",
              "children": [],
              "content": "Summaries      here      are      typically      closely  \n \n  related      to      source      sentences. 4.3      Results\nults Results      are      shown      in      Table      1."
            },
            {
              "id": 55,
              "title": "C.DE.AB - Chunk 40",
              "type": "chunk",
              "children": [],
              "content": "Several      trends      are      clear:\nModel      SQuAD      1.1      MNLI      ELIS      XSum   \n \n_      ConvAI2      CNN/DM  \n \n  Fl      Acc      PPL      PPL      PPL      PPL BERT      Base      (Devlin      et      al.,      2019)      88.5      84.3   \n \n-   \n \n-   \n \n-   \n \n-       Masked      Language      Model      90.0      83.5      24.77      7.87      12.59      7.06  \n \n  Masked      Seq2seq      87.0      82.1      23.40      6.80      11.43      6.19  \n \n  Language      Model      76.7      80.1      21.40      7.00      11.51      6.56  \n \n  Permuted      Language      Model      89.1      83.7      24.03      7.69      12.23      6.96  \n \n  Multitask      Masked      Language      Model      89.2      82.4      23.73      7.50      12.39      6.74  \n \n  BART      Base  \n \n  w/      Token      Masking      90.4      84.1      25.05      7.08      11.73      6.10  \n \n  w/      Token      Deletion      90.4      84.1      24.61      6.90      11.46      5.87  \n \n  w/      Text      Infilling      90.8      84.0      24.26      6.61      11.05      5.83  \n \n  w/      Document      Rotation      77.2      75.3      53.69      17.14      19.87      10.59  \n \n  w/      Sentence      Shuffling      85.4      81.5      41.87      10.93      16.67      7.89  \n \n  w/      Text      Infilling   \n \n+      Sentence      Shuffling      90.8      83.8      24.17      6.62      11.12      5.41\nconsistently      strong      performance."
            },
            {
              "id": 56,
              "title": "C.DE.AB - Chunk 41",
              "type": "chunk",
              "children": [],
              "content": "Performance      of      pre-training      methods      varies      signifi-  \n \n  cantly      across      tasks      The      effectiveness      of      pre-training  \n \n  methods      is      highly      dependent      on      the      task. For      exam-  \n \n  ple,   \n \na      simple      language      model      achieves      the      best      ELIS  \n \n  performance,      but      the      worst      SQUAD      results."
            },
            {
              "id": 57,
              "title": "C.DE.AB - Chunk 42",
              "type": "chunk",
              "children": [],
              "content": "Token      masking      is      crucial      Pre-training      objectives  \n \n  based      on      rotating      documents      or      permuting      sentences  \n \n  perform      poorly      in      isolation. The      successful      methods  \n \n  either      use      token      deletion      or      masking,      or      self-attention  \n \n  masks. Deletion      appears      to      outperform      masking      on  \n \n  generation      tasks."
            },
            {
              "id": 58,
              "title": "C.DE.AB - Chunk 43",
              "type": "chunk",
              "children": [],
              "content": "Left-to-right      pre-training      improves      generation  \n \n  The      Masked      Language      Model      and      the      Permuted  \n \n  Language      Model      perform      less      well      than      others      on  \n \n  generation,      and      are      the      only      models      we      consider      that  \n \n  do      not      include      left-to-right      auto-regressive      language  \n \n  modelling      during      pre-training."
            },
            {
              "id": 59,
              "title": "C.DE.AB - Chunk 44",
              "type": "chunk",
              "children": [],
              "content": "Bidirectional      encoders      are      crucial      for      SQUAD      As  \n \n  noted      in      previous      work      (Devlin      et      al.,      2019),      just  \n \n  left-to-right      decoder      performs      poorly      on      SQuAD,      be-  \n \n  cause      future      context      is      crucial      in      classification      deci-  \n \n  sions."
            },
            {
              "id": 60,
              "title": "C.DE.AB - Chunk 45",
              "type": "chunk",
              "children": [],
              "content": "However,      BART      achieves      similar      performance  \n \n  with      only      half      the      number      of      bidirectional      layers. The      pre-training      objective      is      not      the      only      important  \n \n  factor      Our      Permuted      Language      Model      performs      less  \n \n  well      than      XLNet      (Yang      et      al.,      2019)."
            },
            {
              "id": 61,
              "title": "C.DE.AB - Chunk 46",
              "type": "chunk",
              "children": [],
              "content": "Some      of      this      dif-  \n \n  ference      is      likely      due      to      not      including      other      architectural  \n \n  improvements,      such      as      relative-position      embeddings      or  \n \n  segment-level      recurrence."
            },
            {
              "id": 62,
              "title": "C.DE.AB - Chunk 47",
              "type": "chunk",
              "children": [],
              "content": "Pure      language      models      perform      best      on      ELIS      The  \n \n  ELIS      dataset      is      an      outlier,      with      much      higher      perplex-  \n \n  ities      than      other      tasks,      and      is      the      only      generation      task  \n \n  where      other      models      outperform      BART."
            },
            {
              "id": 63,
              "title": "C.DE.AB - Chunk 48",
              "type": "chunk",
              "children": [],
              "content": "A      pure      lan-  \n \n  guage      model      performs      best,      suggesting      that      BART      is  \n \n  less      effective      when      the      output      is      only      loosely      con-  \n \n  strained      by      the      input. BART      achieves      the      most      consistently      strong      perfor-  \n \n  mance."
            },
            {
              "id": 64,
              "title": "C.DE.AB - Chunk 49",
              "type": "chunk",
              "children": [],
              "content": "With      the      exception      of      ELI5,      BART      models  \n \n  using      text-infilling      perform      well      on      all      tasks."
            },
            {
              "id": 65,
              "title": "C.DE.AB - Chunk 50",
              "type": "chunk",
              "children": [],
              "content": "5\n \n   Large-scale      Pre-training      Experiments  \n \n  Recent      work      has      shown      that      downstream      performance  \n \n  can      dramatically      improve      when      pre-training      is      scaled  \n \n  to      large      batch      sizes      (Yang      et      al.,      2019;      Liu      et      al.,      2019)  \n \n  and      corpora."
            },
            {
              "id": 66,
              "title": "C.DE.AB - Chunk 51",
              "type": "chunk",
              "children": [],
              "content": "To      test      how      well      BART      performs      in      this  \n \n  regime,      and      to      create   \n \na      useful      model      for      downstream  \n \n  tasks,      we      trained      BART      using      the      same      scale      as      the  \n \n  RoBERTa      model."
            },
            {
              "id": 67,
              "title": "C.DE.AB - Chunk 52",
              "type": "chunk",
              "children": [],
              "content": "5.1      Experimental      Setup\netup  \n \n  We      pre-train   \n \na      large      model      with      12      layers      in      each      of      the  \n \n  encoder      and      decoder,      and   \n \na      hidden      size      of      1024. Fol-  \n \n  lowing      RoBERTa      (Liu      et      al.,      2019),      we      use   \n \na      batch      size  \n \n  of      8000,      and      train      the      model      for      500000      steps."
            },
            {
              "id": 68,
              "title": "C.DE.AB - Chunk 53",
              "type": "chunk",
              "children": [],
              "content": "Docu-  \n \n  ments      are      tokenized      with      the      same      byte-pair      encoding  \n \n  as      GPT-2      (Radford      et      al.,      2019). Based      on      the      results      in  \n \n  Section      §4,      we      use   \n \na      combination      of      text      infilling      and  \n \n  sentence      permutation. We      mask      30%      of      tokens      in      each  \n \n  document,      and      permute      all      sentences."
            },
            {
              "id": 69,
              "title": "C.DE.AB - Chunk 54",
              "type": "chunk",
              "children": [],
              "content": "Although      sen-  \n \n  tence      permutation      only      shows      significant      additive      gains SQuAD      1.1      SQuAD      2.0      MNLI      SST      QQP      QNLI      STS-B      RTE      MRPC      CoLA  \n \n  EM/F1      EM/F1      m/mm      Acc      Acc      Acc      Acc      Acc      Acc      Mcc\n \n \n  BERT      84.1/90.9      79.0/81.8      86.6/-      93.2      91.3      92.3      90.0      70.4      88.0      60.6  \n \n  UniLM      -/-      80.5/83.4      87.0/85.9      94.5   \n \n-      92.7   \n \n-      70.9   \n \n-      61.1  \n \n  XLNet      89.0/94.5      86.1/88.8      89.8/-      95.6      91.8      93.9      91.8      83.8      89.2      63.6  \n \n  RoBERTa      88.9/94.6      86.5/89.4      90.2/90.2      96.4      92.2      94.7      92.4      86.6      90.9      68.0  \n \n  BART      88.8/94.6      86.1/89.2      89.9/90.1      96.6      92.5      94.9      91.2      87.0      90.4      62.8 Table      2:      Results      for      large      models      on      SQUAD      and      GLUE      tasks."
            },
            {
              "id": 70,
              "title": "C.DE.AB - Chunk 55",
              "type": "chunk",
              "children": [],
              "content": "BART      performs      comparably      to      ROBERTa      and  \n \n  XLNet,      suggesting      that      BART’s      uni-directional      decoder      layers      do      not      reduce      performance      on      discriminative      tasks."
            },
            {
              "id": 71,
              "title": "C.DE.AB - Chunk 56",
              "type": "chunk",
              "children": [],
              "content": "CNN/DailyMail      XSum       RI      R2      RL      RI      R2      RL\n \n \n  Lead-3      40.42      17.62      36.67      16.30      1.60      11.95  \n \n  PTGEN      (See      et      al.,      2017)      36.44      15.66      33.42      29.70      9.21      23.24  \n \n  PTGEN+COV      (See      et      al.,      2017)      39.53      17.28      36.38      28.10      8.02      21.72  \n \n  UniLM      43.33      20.21      40.51   \n \n-   \n \n-   \n \n-       BERTSUMABS      (Liu   \n \n&      Lapata,      2019)      41.72      19.39      38.76      38.76      16.33      31.15  \n \n  BERTSUMEXTABS      (Liu   \n \n&      Lapata,      2019)      42.13      19.60      39.18      38.81      16.50      31.27 BART      44.16      21.28      40.90      45.14      22.27      37.25 Table      3:      Results      on      two      standard      summarization      datasets."
            },
            {
              "id": 72,
              "title": "C.DE.AB - Chunk 57",
              "type": "chunk",
              "children": [],
              "content": "BART      outperforms      previous      work      on      summarization      on\n \n \n  on      the      CNN/DM      summarization      dataset,      we      hypothe-      ConvAI2  \n \n  sised      that      larger      pre-trained      models      may      be      better      able      Valid      F1      Valid      PPL  \n \n  to   \n \n|      fi      this      task."
            },
            {
              "id": 73,
              "title": "C.DE.AB - Chunk 58",
              "type": "chunk",
              "children": [],
              "content": "To      help      th      del      better      fit      th  \n \n  RE      aa      ae      Pai      Cae      Seq2Seq+      Attention      16.02      35.07  \n \n  data,      we      disabled      dropout      for      the      final      10%      of      training   \n \n. . Best      System      19.09      17.51  \n \n  steps. We      use      the      same      pre-training      data      as      Liu      et      al."
            },
            {
              "id": 74,
              "title": "C.DE.AB - Chunk 59",
              "type": "chunk",
              "children": [],
              "content": "(2019),      consisting      of      160Gb      of      news,      books,      stories,  \n \n  and      web      text."
            },
            {
              "id": 75,
              "title": "C.DE.AB - Chunk 60",
              "type": "chunk",
              "children": [],
              "content": "BART      20.72      11.85 5.2      Discriminative      Tasks  \n \n  Table   \n \n2      compares      the      performance      of      BART      with      sev-  \n \n  eral      recent      approaches      on      the      well-studied      SQUAD      and  \n \n  GLUE      tasks      (Warstadt      et      al.,      2018;      Socher      et      al.,      2013;  \n \n  Dolan   \n \n&      Brockett,      2005;      Agirre      et      al.,      2007;      Williams  \n \n  et      al.,      2018;      Dagan      et      al.,      2006;      Levesque      et      al.,      2011)."
            },
            {
              "id": 76,
              "title": "C.DE.AB - Chunk 61",
              "type": "chunk",
              "children": [],
              "content": "Table      4:      BART      outperforms      previous      work      on      conver-  \n \n  sational      response      generation. Perplexities      are      renor-  \n \n  malized      based      on      official      tokenizer      for      ConvAI2. The      most      directly      comparable      baseline      is      ROBERTa,  \n \n  which      was      pre-trained      with      the      same      resources,      but  \n \n  a      different      objective."
            },
            {
              "id": 77,
              "title": "C.DE.AB - Chunk 62",
              "type": "chunk",
              "children": [],
              "content": "Overall,      BART      performs      simi-  \n \n  larly,      with      only      small      differences      between      the      models  \n \n  on      most      tasks. suggesting      that      BART’s      improvements  \n \n  on      generation      tasks      do      not      come      at      the      expense      of      clas-  \n \n  sification      performance."
            },
            {
              "id": 78,
              "title": "C.DE.AB - Chunk 63",
              "type": "chunk",
              "children": [],
              "content": "Summarization      To      provide   \n \na      comparison      with      the  \n \n  state-of-the-art      in      summarization,      we      present      results  \n \n  on      two      summarization      datasets,      CNN/DailyMail      and  \n \n  XSum,      which      have      distinct      properties. Summaries      in      the      CNN/DailyMail      tend      to      resemble  \n \n  source      sentences."
            },
            {
              "id": 79,
              "title": "C.DE.AB - Chunk 64",
              "type": "chunk",
              "children": [],
              "content": "Extractive      models      do      well      here,      and  \n \n  even      the      baseline      of      the      first-three      source      sentences      is  \n \n  highly      competitive. Nevertheless,      BART      outperforms  \n \n  all      existing      work. 5.3. Generation      Tasks\nGeneration      Tasks\n |       We      also      experiment      with      several      text      generation      tasks."
            },
            {
              "id": 80,
              "title": "C.DE.AB - Chunk 65",
              "type": "chunk",
              "children": [],
              "content": "BART      is      fine-tuned      as      a      standard      sequence-to-sequence       model      from      the      input      to      the      output      text. During      fine-       tuning      we      use      a      label      smoothed      cross      entropy      loss       (Pereyra      et      al.,      2017),      with      the      smoothing      parameter       set      to      0.1."
            },
            {
              "id": 81,
              "title": "C.DE.AB - Chunk 66",
              "type": "chunk",
              "children": [],
              "content": "During      generation,      we      set      beam      size      as      5,       remove      duplicated      trigrams      in      beam      search,      and      tuned       the      model      with      min-len,      max-len,      length      penalty      on      the       validation      set      (Fan      et      al.,      2017)."
            },
            {
              "id": 82,
              "title": "C.DE.AB - Chunk 67",
              "type": "chunk",
              "children": [],
              "content": "| 6.0      points      on      all      ROUGE      metrics—trepresenting   \n \na      sig-  \n \n  nificant      advance      in      performance      on      this      problem. Qual-  \n \n  itatively,      sample      quality      is      high      (see      86). In      contrast,      XSum      is      highly      abstractive,      and      extrac-  \n \n  tive      models      perform      poorly."
            },
            {
              "id": 83,
              "title": "C.DE.AB - Chunk 68",
              "type": "chunk",
              "children": [],
              "content": "BART      outperforms      the  \n \n  best      previous      work,      which      leverages      BERT,      by      roughly  \n \n  Dialogue      We      evaluate      dialogue      response      generation  \n \n  on      CONVAI2      (Dinan      et      al.,      2019),      in      which      agents  \n \n  must      generate      responses      conditioned      on      both      the      pre-  \n \n  vious      context      and   \n \na      textually-specified      persona."
            },
            {
              "id": 84,
              "title": "C.DE.AB - Chunk 69",
              "type": "chunk",
              "children": [],
              "content": "BART  \n \n  outperforms      previous      work      on      two      automated      metrics."
            },
            {
              "id": 85,
              "title": "C.DE.AB - Chunk 70",
              "type": "chunk",
              "children": [],
              "content": "Rl      R2      RL\n \n \n  Best      Extractive      23.55      3.1      17.5  \n \n  Language      Model      27.8      47      23.1  \n \n  Seq2Seq      28.3      5.1      22.8  \n \n  Seq2Seq      Multitask      28.9      54      23.1  \n \n  BART      30.6      6.2      24.3  \n \n  Table      5:      BART      achieves      state-of-the-art      results      on  \n \n  the      challenging      ELI5      abstractive      question      answering  \n \n  dataset."
            },
            {
              "id": 86,
              "title": "C.DE.AB - Chunk 71",
              "type": "chunk",
              "children": [],
              "content": "Comparison      models      are      from      Fan      et      al. (2019)."
            }
          ],
          "content": ""
        },
        {
          "id": 87,
          "title": "Token      Deletion Text      Infilling",
          "type": "section",
          "children": [
            {
              "id": 88,
              "title": "Token      Deletion Text      Infilling - Chunk 1",
              "type": "chunk",
              "children": [],
              "content": "Token      Deletion Text      Infilling\n \n \n  Text      Infilling   \n \nA      number      of      text      spans      are      sampled,  \n \n  with      span      lengths      drawn      from   \n \na      Poisson      distribution  \n \n  (A   \n \n=      3). Each      span      is      replaced      with   \n \na      single      [MASK]  \n \n  token. O-length      spans      correspond      to      the      insertion      of  \n \n  [MASK]      tokens."
            },
            {
              "id": 89,
              "title": "Token      Deletion Text      Infilling - Chunk 2",
              "type": "chunk",
              "children": [],
              "content": "Text      infilling      is      inspired      by      Span-  \n \n  BERT      (Joshi      et      al.,      2019),      but      SpanBERT      samples  \n \n  span      lengths      from   \n \na      different      (clamped      geometric)      dis-  \n \n  tribution,      and      replaces      each      span      with   \n \na      sequence      of  \n \n  [MASK]      tokens      of      exactly      the      same      length."
            },
            {
              "id": 90,
              "title": "Token      Deletion Text      Infilling - Chunk 3",
              "type": "chunk",
              "children": [],
              "content": "Text      infill-  \n \n  ing      teaches      the      model      to      predict      how      many      tokens      are  \n \n  missing      from   \n \na      span. Sentence      Permutation   \n \nA      document      is      divided      into  \n \n  sentences      based      on      full      stops,      and      these      sentences      are  \n \n  shuffled      in   \n \na      random      order."
            },
            {
              "id": 91,
              "title": "Token      Deletion Text      Infilling - Chunk 4",
              "type": "chunk",
              "children": [],
              "content": "Document      Rotation   \n \nA      token      is      chosen      uniformly      at  \n \n  random,      and      the      document      is      rotated      so      that      it      begins  \n \n  with      that      token. This      task      trains      the      model      to      identify  \n \n  the      start      of      the      document."
            },
            {
              "id": 92,
              "title": "Token      Deletion Text      Infilling - Chunk 5",
              "type": "chunk",
              "children": [],
              "content": "3\n \n   Fine-tuning      BART The      representations      produced      by      BART      can      be      used      in  \n \n  several      ways      for      downstream      applications."
            },
            {
              "id": 93,
              "title": "Token      Deletion Text      Infilling - Chunk 6",
              "type": "chunk",
              "children": [],
              "content": "3.1      Sequence      Classification      Tasks\nasks  \n \n  For      sequence      classification      tasks,      the      same      input      is      fed  \n \n  into      the      encoder      and      decoder,      and      the      final      hidden      state  \n \n  of      the      final      decoder      token      is      fed      into      new      multi-class  \n \n  linear      classifier."
            },
            {
              "id": 94,
              "title": "Token      Deletion Text      Infilling - Chunk 7",
              "type": "chunk",
              "children": [],
              "content": "This      approach      is      related      to      the      CLS  \n \n  token      in      BERT;      however      we      add      the      additional      token  \n \n  to      the      end      so      that      representation      for      the      token      in      the  \n \n  decoder      can      attend      to      decoder      states      from      the      complete  \n \n  input      (Figure      3a). 3.2."
            },
            {
              "id": 95,
              "title": "Token      Deletion Text      Infilling - Chunk 8",
              "type": "chunk",
              "children": [],
              "content": "Token      Classification      Tasks\nasks  \n \n  For      token      classification      tasks,      such      as      answer      endpoint  \n \n  classification      for      SQUAD,      we      feed      the      complete      doc-  \n \n  ument      into      the      encoder      and      decoder,      and      use      the      top  \n \n  hidden      state      of      the      decoder      as   \n \na      representation      for      each  \n \n  word."
            },
            {
              "id": 96,
              "title": "Token      Deletion Text      Infilling - Chunk 9",
              "type": "chunk",
              "children": [],
              "content": "This      representation      is      used      to      classify      the      token. 3.3      Sequence      Generation      Tasks\nasks  \n \n  Because      BART      has      an      autoregressive      decoder,      it      can      be  \n \n  directly      fine      tuned      for      sequence      generation      tasks      such  \n \n  as      abstractive      question      answering      and      summarization."
            },
            {
              "id": 97,
              "title": "Token      Deletion Text      Infilling - Chunk 10",
              "type": "chunk",
              "children": [],
              "content": "In      both      of      these      tasks,      information      is      copied      from      the  \n \n  input      but      manipulated,      which      is      closely      related      to      the  \n \n  denoising      pre-training      objective. Here,      the      encoder      in-  \n \n  put      is      the      input      sequence,      and      the      decoder      generates  \n \n  outputs      autoregressively."
            },
            {
              "id": 98,
              "title": "Token      Deletion Text      Infilling - Chunk 11",
              "type": "chunk",
              "children": [],
              "content": "3.4      Machine      Translation\ntion  \n \n  We      also      explore      using      BART      to      improve      machine      trans-  \n \n  lation      decoders      for      translating      into      English. Previous  \n \n  work      Edunov      et      al."
            },
            {
              "id": 99,
              "title": "Token      Deletion Text      Infilling - Chunk 12",
              "type": "chunk",
              "children": [],
              "content": "(2019)      has      shown      that      models      can  \n \n  be      improved      by      incorporating      pre-trained      encoders,      but  \n \n  gains      from      using      pre-trained      language      models      in      de-  \n \n  coders      have      been      limited."
            },
            {
              "id": 100,
              "title": "Token      Deletion Text      Infilling - Chunk 13",
              "type": "chunk",
              "children": [],
              "content": "We      show      that      it      is      possible  \n \n  to      use      the      entire      BART      model      (both      encoder      and      de-  \n \n  coder)      as   \n \na      single      pretrained      decoder      for      machine      trans-  \n \n  lation,      by      adding   \n \na      new      set      of      encoder      parameters      that  \n \n  are      learned      from      bitext      (see      Figure      3b)."
            },
            {
              "id": 101,
              "title": "Token      Deletion Text      Infilling - Chunk 14",
              "type": "chunk",
              "children": [],
              "content": "More      precisely,      we      replace      BART’s      encoder      embed-  \n \n  ding      layer      with   \n \na      new      randomly      initialized      encoder. The      model      is      trained      end-to-end,      which      trains      the      new       encoder      to      map      foreign      words      into      an      input      that      BART  \n \n  can      de-noise      to      English."
            },
            {
              "id": 102,
              "title": "Token      Deletion Text      Infilling - Chunk 15",
              "type": "chunk",
              "children": [],
              "content": "The      new      encoder      can      use   \n \na       separate      vocabulary      from      the      original      BART      model. We      train      the      source      encoder      in      two      steps,      in      both  \n \n  cases      backpropagating      the      cross-entropy      loss      from      the  \n \n  output      of      the      BART      model."
            },
            {
              "id": 103,
              "title": "Token      Deletion Text      Infilling - Chunk 16",
              "type": "chunk",
              "children": [],
              "content": "In      the      first      step,      we      freeze  \n \n  most      of      BART      parameters      and      only      update      the      ran-  \n \n  domly      initialized      source      encoder,      the      BART      positional  \n \n  embeddings,      and      the      self-attention      input      projection      ma-  \n \n  trix      of      BART’s      encoder      first      layer."
            },
            {
              "id": 104,
              "title": "Token      Deletion Text      Infilling - Chunk 17",
              "type": "chunk",
              "children": [],
              "content": "In      the      second      step,  \n \n  we      train      all      model      parameters      for   \n \na      small      number      of  \n \n  iterations. 4\n \n   Comparing      Pre-training      Objectives  \n \n  BART      supports   \n \na      much      wider      range      of      noising      schemes  \n \n  during      pre-training      than      previous      work."
            },
            {
              "id": 105,
              "title": "Token      Deletion Text      Infilling - Chunk 18",
              "type": "chunk",
              "children": [],
              "content": "We      compare   \n \na       range      of      options      using      base-size      models      (6      encoder      and  \n \n  6      decoder      layers,      with   \n \na      hidden      size      of      768),      evaluated  \n \n  on   \n \na      representative      subset      of      the      tasks      we      will      consider  \n \n  for      the      full      large      scale      experiments      in      85."
            },
            {
              "id": 106,
              "title": "Token      Deletion Text      Infilling - Chunk 19",
              "type": "chunk",
              "children": [],
              "content": "4.1      Comparison      Objectives\nives  \n \n  While      many      pre-training      objectives      have      been      pro-  \n \n  posed,      fair      comparisons      between      these      have      been      dif-  \n \n  ficult      to      perform,      at      least      in      part      due      to      differences      in  \n \n  training      data,      training      resources,      architectural      differ-  \n \n  ences      between      models,      and      fine-tuning      procedures."
            },
            {
              "id": 107,
              "title": "Token      Deletion Text      Infilling - Chunk 20",
              "type": "chunk",
              "children": [],
              "content": "We  \n \n  ABCDE  \n \n  weee  \n \n  Pre-trained      Pre-trained  \n \n  Craters      Joo      rezapes cise)      Te      TrET Pre-trained  \n \n  Encoder Pre-trained  \n \n  Decoder aa      AB  \n \n  trrrt  \n \n  tr      ttt  \n \n  <s>A      BCD  \n \n  Randomly  \n \n  Initialized      Encoder  \n \n  TFT  \n \n  aBpByoe sentation      from      the      final      output      is      used."
            },
            {
              "id": 108,
              "title": "Token      Deletion Text      Infilling - Chunk 21",
              "type": "chunk",
              "children": [],
              "content": "re-implement      strong      pre-training      approaches      recently  \n \n  proposed      for      discriminative      and      generation      tasks. We  \n \n  aim,      as      much      as      possible,      to      control      for      differences      un-  \n \n  related      to      the      pre-training      objective."
            },
            {
              "id": 109,
              "title": "Token      Deletion Text      Infilling - Chunk 22",
              "type": "chunk",
              "children": [],
              "content": "However,      we      do  \n \n  make      minor      changes      to      the      learning      rate      and      usage      of  \n \n  layer      normalisation      in      order      to      improve      performance  \n \n  (tuning      these      separately      for      each      objective)."
            },
            {
              "id": 110,
              "title": "Token      Deletion Text      Infilling - Chunk 23",
              "type": "chunk",
              "children": [],
              "content": "For      refer-  \n \n  ence,      we      compare      our      implementations      with      published  \n \n  numbers      from      BERT,      which      was      also      trained      for      1M  \n \n  steps      on   \n \na      combination      of      books      and      Wikipedia      data."
            },
            {
              "id": 111,
              "title": "Token      Deletion Text      Infilling - Chunk 24",
              "type": "chunk",
              "children": [],
              "content": "We      compare      the      following      approaches:\n \n \n  Language      Model      Similarly      to      GPT      (Radford      et      al.,  \n \n  2018),      we      train   \n \na      left-to-right      Transformer      language  \n \n  model. This      model      is      equivalent      to      the      BART      decoder,  \n \n  without      cross-attention."
            },
            {
              "id": 112,
              "title": "Token      Deletion Text      Infilling - Chunk 25",
              "type": "chunk",
              "children": [],
              "content": "Permuted      Language      Model      Based      on      XLNet      (Yang  \n \n  et      al.,      2019),      we      sample      1/6      of      the      tokens,      and      gener-  \n \n  ate      them      in   \n \na      random      order      autoregressively."
            },
            {
              "id": 113,
              "title": "Token      Deletion Text      Infilling - Chunk 26",
              "type": "chunk",
              "children": [],
              "content": "For      con-  \n \n  sistency      with      other      models,      we      do      not      implement      the  \n \n  relative      positional      embeddings      or      attention      across      seg-  \n \n  ments      from      XLNet."
            },
            {
              "id": 114,
              "title": "Token      Deletion Text      Infilling - Chunk 27",
              "type": "chunk",
              "children": [],
              "content": "Masked      Language      Model      Following      BERT      (Devlin  \n \n  et      al.,      2019),      we      replace      15%      of      tokens      with      [MASK]  \n \n  symbols,      and      train      the      model      to      independently      predict  \n \n  the      original      tokens."
            },
            {
              "id": 115,
              "title": "Token      Deletion Text      Infilling - Chunk 28",
              "type": "chunk",
              "children": [],
              "content": "Multitask      Masked      Language      Model      As      in      UniLM  \n \n  (Dong      et      al.,      2019),      we      train   \n \na      Masked      Language  \n \n  Model      with      additional      self-attention      masks."
            },
            {
              "id": 116,
              "title": "Token      Deletion Text      Infilling - Chunk 29",
              "type": "chunk",
              "children": [],
              "content": "Self      at-  \n \n  tention      masks      are      chosen      randomly      in      with      the      follow  \n \n  proportions:      1/6      left-to-right,      1/6      right-to-left,      1/3      un-  \n \n  masked,      and      1/3      with      the      first      50%      of      tokens      unmasked  \n \n  and   \n \na      left-to-right      mask      for      the      remainder."
            },
            {
              "id": 117,
              "title": "Token      Deletion Text      Infilling - Chunk 30",
              "type": "chunk",
              "children": [],
              "content": "Masked      Seq-to-Seq      Inspired      by      MASS      (Song      et      al.,  \n \n  2019),      we      mask   \n \na      span      containing      50%      of      tokens,  \n \n  and      train   \n \na      sequence      to      sequence      model      to      predict      the  \n \n  masked      tokens."
            },
            {
              "id": 118,
              "title": "Token      Deletion Text      Infilling - Chunk 31",
              "type": "chunk",
              "children": [],
              "content": "For      the      Permuted      LM,      Masked      LM      and      Multitask  \n \n  Masked      LM,      we      use      two-stream      attention      (Yang      et      al.,  \n \n  2019)      to      efficiently      compute      likelihoods      of      the      output  \n \n  part      of      the      sequence      (using   \n \na      diagonal      self-attention  \n \n  mask      on      the      output      to      predict      words      left-to-right)."
            },
            {
              "id": 119,
              "title": "Token      Deletion Text      Infilling - Chunk 32",
              "type": "chunk",
              "children": [],
              "content": "We      experiment      with      (1)      treating      the      task      as   \n \na      stan-  \n \n  dard      sequence-to-sequence      problem,      where      the      source  \n \n  input      to      the      encoder      and      the      target      is      the      decoder      out-  \n \n  put,      or      (2)      adding      the      source      as      prefix      to      the      target      in  \n \n  the      decoder,      with   \n \na      loss      only      on      the      target      part      of      the  \n \n  sequence."
            },
            {
              "id": 120,
              "title": "Token      Deletion Text      Infilling - Chunk 33",
              "type": "chunk",
              "children": [],
              "content": "We      find      the      former      works      better      for      BART  \n \n  models,      and      the      latter      for      other      models. To      most      directly      compare      our      models      on      their      ability  \n \n  to      model      their      fine-tuning      objective      (the      log      likelihood  \n \n  of      the      human      text),      we      report      perplexity      in      Table      1."
            },
            {
              "id": 121,
              "title": "Token      Deletion Text      Infilling - Chunk 34",
              "type": "chunk",
              "children": [],
              "content": "4.2      Tasks\nasks  \n \n  SQuAD  \n \n  (Rajpurkar      et      al.,      2016)a      an      extractive      ques-  \n \n  tion      answering      task      on      Wikipedia      paragraphs. Answers  \n \n  are      text      spans      extracted      from   \n \na      given      document      context."
            },
            {
              "id": 122,
              "title": "Token      Deletion Text      Infilling - Chunk 35",
              "type": "chunk",
              "children": [],
              "content": "Similar      to      BERT      (Devlin      et      al.,      2019),      we      use      concate-  \n \n  nated      question      and      context      as      input      to      the      encoder      of  \n \n  BART,      and      additionally      pass      them      to      the      decoder. The  \n \n  model      includes      classifiers      to      predict      the      start      and      end  \n \n  indices      of      each      token."
            },
            {
              "id": 123,
              "title": "Token      Deletion Text      Infilling - Chunk 36",
              "type": "chunk",
              "children": [],
              "content": "MNLI      (Williams      et      al.,      2017),   \n \na      bitext      classification  \n \n  task      to      predict      whether      one      sentence      entails      another. The      fine-tuned      model      concatenates      the      two      sentences  \n \n  with      appended      an      EOS      token,      and      passes      them      to      both  \n \n  the      BART      encoder      and      decoder."
            },
            {
              "id": 124,
              "title": "Token      Deletion Text      Infilling - Chunk 37",
              "type": "chunk",
              "children": [],
              "content": "In      contrast      to      BERT,  \n \n  the      representation      of      the      EOS      token      is      used      to      classify  \n \n  the      sentences      relations. ELIS      (Fanetal.,      2019),   \n \na      long-form      abstractive      ques-  \n \n  tion      answering      dataset. Models      generate      answers      con-  \n \n  ditioned      on      the      concatenation      of   \n \na      question      and      sup-  \n \n  porting      documents."
            },
            {
              "id": 125,
              "title": "Token      Deletion Text      Infilling - Chunk 38",
              "type": "chunk",
              "children": [],
              "content": "XSum_      (Narayan      et      al.,      2018),   \n \na      news      summarization  \n \n  dataset      with      highly      abstractive      summaries. ConvAI2_      (Dinan      et      al.,      2019),   \n \na      dialogue      response  \n \n  generation      task,      conditioned      on      context      and   \n \na      persona. CNN/DM_      (Hermann      et      al.,      2015),   \n \na      news      summa-  \n \n  rization      dataset."
            },
            {
              "id": 126,
              "title": "Token      Deletion Text      Infilling - Chunk 39",
              "type": "chunk",
              "children": [],
              "content": "Summaries      here      are      typically      closely  \n \n  related      to      source      sentences. 4.3      Results\nults Results      are      shown      in      Table      1."
            },
            {
              "id": 127,
              "title": "Token      Deletion Text      Infilling - Chunk 40",
              "type": "chunk",
              "children": [],
              "content": "Several      trends      are      clear:\nModel      SQuAD      1.1      MNLI      ELIS      XSum   \n \n_      ConvAI2      CNN/DM  \n \n  Fl      Acc      PPL      PPL      PPL      PPL BERT      Base      (Devlin      et      al.,      2019)      88.5      84.3   \n \n-   \n \n-   \n \n-   \n \n-       Masked      Language      Model      90.0      83.5      24.77      7.87      12.59      7.06  \n \n  Masked      Seq2seq      87.0      82.1      23.40      6.80      11.43      6.19  \n \n  Language      Model      76.7      80.1      21.40      7.00      11.51      6.56  \n \n  Permuted      Language      Model      89.1      83.7      24.03      7.69      12.23      6.96  \n \n  Multitask      Masked      Language      Model      89.2      82.4      23.73      7.50      12.39      6.74  \n \n  BART      Base  \n \n  w/      Token      Masking      90.4      84.1      25.05      7.08      11.73      6.10  \n \n  w/      Token      Deletion      90.4      84.1      24.61      6.90      11.46      5.87  \n \n  w/      Text      Infilling      90.8      84.0      24.26      6.61      11.05      5.83  \n \n  w/      Document      Rotation      77.2      75.3      53.69      17.14      19.87      10.59  \n \n  w/      Sentence      Shuffling      85.4      81.5      41.87      10.93      16.67      7.89  \n \n  w/      Text      Infilling   \n \n+      Sentence      Shuffling      90.8      83.8      24.17      6.62      11.12      5.41\nconsistently      strong      performance."
            },
            {
              "id": 128,
              "title": "Token      Deletion Text      Infilling - Chunk 41",
              "type": "chunk",
              "children": [],
              "content": "Performance      of      pre-training      methods      varies      signifi-  \n \n  cantly      across      tasks      The      effectiveness      of      pre-training  \n \n  methods      is      highly      dependent      on      the      task. For      exam-  \n \n  ple,   \n \na      simple      language      model      achieves      the      best      ELIS  \n \n  performance,      but      the      worst      SQUAD      results."
            },
            {
              "id": 129,
              "title": "Token      Deletion Text      Infilling - Chunk 42",
              "type": "chunk",
              "children": [],
              "content": "Token      masking      is      crucial      Pre-training      objectives  \n \n  based      on      rotating      documents      or      permuting      sentences  \n \n  perform      poorly      in      isolation. The      successful      methods  \n \n  either      use      token      deletion      or      masking,      or      self-attention  \n \n  masks. Deletion      appears      to      outperform      masking      on  \n \n  generation      tasks."
            },
            {
              "id": 130,
              "title": "Token      Deletion Text      Infilling - Chunk 43",
              "type": "chunk",
              "children": [],
              "content": "Left-to-right      pre-training      improves      generation  \n \n  The      Masked      Language      Model      and      the      Permuted  \n \n  Language      Model      perform      less      well      than      others      on  \n \n  generation,      and      are      the      only      models      we      consider      that  \n \n  do      not      include      left-to-right      auto-regressive      language  \n \n  modelling      during      pre-training."
            },
            {
              "id": 131,
              "title": "Token      Deletion Text      Infilling - Chunk 44",
              "type": "chunk",
              "children": [],
              "content": "Bidirectional      encoders      are      crucial      for      SQUAD      As  \n \n  noted      in      previous      work      (Devlin      et      al.,      2019),      just  \n \n  left-to-right      decoder      performs      poorly      on      SQuAD,      be-  \n \n  cause      future      context      is      crucial      in      classification      deci-  \n \n  sions."
            },
            {
              "id": 132,
              "title": "Token      Deletion Text      Infilling - Chunk 45",
              "type": "chunk",
              "children": [],
              "content": "However,      BART      achieves      similar      performance  \n \n  with      only      half      the      number      of      bidirectional      layers. The      pre-training      objective      is      not      the      only      important  \n \n  factor      Our      Permuted      Language      Model      performs      less  \n \n  well      than      XLNet      (Yang      et      al.,      2019)."
            },
            {
              "id": 133,
              "title": "Token      Deletion Text      Infilling - Chunk 46",
              "type": "chunk",
              "children": [],
              "content": "Some      of      this      dif-  \n \n  ference      is      likely      due      to      not      including      other      architectural  \n \n  improvements,      such      as      relative-position      embeddings      or  \n \n  segment-level      recurrence."
            },
            {
              "id": 134,
              "title": "Token      Deletion Text      Infilling - Chunk 47",
              "type": "chunk",
              "children": [],
              "content": "Pure      language      models      perform      best      on      ELIS      The  \n \n  ELIS      dataset      is      an      outlier,      with      much      higher      perplex-  \n \n  ities      than      other      tasks,      and      is      the      only      generation      task  \n \n  where      other      models      outperform      BART."
            },
            {
              "id": 135,
              "title": "Token      Deletion Text      Infilling - Chunk 48",
              "type": "chunk",
              "children": [],
              "content": "A      pure      lan-  \n \n  guage      model      performs      best,      suggesting      that      BART      is  \n \n  less      effective      when      the      output      is      only      loosely      con-  \n \n  strained      by      the      input. BART      achieves      the      most      consistently      strong      perfor-  \n \n  mance."
            },
            {
              "id": 136,
              "title": "Token      Deletion Text      Infilling - Chunk 49",
              "type": "chunk",
              "children": [],
              "content": "With      the      exception      of      ELI5,      BART      models  \n \n  using      text-infilling      perform      well      on      all      tasks."
            },
            {
              "id": 137,
              "title": "Token      Deletion Text      Infilling - Chunk 50",
              "type": "chunk",
              "children": [],
              "content": "5\n \n   Large-scale      Pre-training      Experiments  \n \n  Recent      work      has      shown      that      downstream      performance  \n \n  can      dramatically      improve      when      pre-training      is      scaled  \n \n  to      large      batch      sizes      (Yang      et      al.,      2019;      Liu      et      al.,      2019)  \n \n  and      corpora."
            },
            {
              "id": 138,
              "title": "Token      Deletion Text      Infilling - Chunk 51",
              "type": "chunk",
              "children": [],
              "content": "To      test      how      well      BART      performs      in      this  \n \n  regime,      and      to      create   \n \na      useful      model      for      downstream  \n \n  tasks,      we      trained      BART      using      the      same      scale      as      the  \n \n  RoBERTa      model."
            },
            {
              "id": 139,
              "title": "Token      Deletion Text      Infilling - Chunk 52",
              "type": "chunk",
              "children": [],
              "content": "5.1      Experimental      Setup\netup  \n \n  We      pre-train   \n \na      large      model      with      12      layers      in      each      of      the  \n \n  encoder      and      decoder,      and   \n \na      hidden      size      of      1024. Fol-  \n \n  lowing      RoBERTa      (Liu      et      al.,      2019),      we      use   \n \na      batch      size  \n \n  of      8000,      and      train      the      model      for      500000      steps."
            },
            {
              "id": 140,
              "title": "Token      Deletion Text      Infilling - Chunk 53",
              "type": "chunk",
              "children": [],
              "content": "Docu-  \n \n  ments      are      tokenized      with      the      same      byte-pair      encoding  \n \n  as      GPT-2      (Radford      et      al.,      2019). Based      on      the      results      in  \n \n  Section      §4,      we      use   \n \na      combination      of      text      infilling      and  \n \n  sentence      permutation. We      mask      30%      of      tokens      in      each  \n \n  document,      and      permute      all      sentences."
            },
            {
              "id": 141,
              "title": "Token      Deletion Text      Infilling - Chunk 54",
              "type": "chunk",
              "children": [],
              "content": "Although      sen-  \n \n  tence      permutation      only      shows      significant      additive      gains SQuAD      1.1      SQuAD      2.0      MNLI      SST      QQP      QNLI      STS-B      RTE      MRPC      CoLA  \n \n  EM/F1      EM/F1      m/mm      Acc      Acc      Acc      Acc      Acc      Acc      Mcc\n \n \n  BERT      84.1/90.9      79.0/81.8      86.6/-      93.2      91.3      92.3      90.0      70.4      88.0      60.6  \n \n  UniLM      -/-      80.5/83.4      87.0/85.9      94.5   \n \n-      92.7   \n \n-      70.9   \n \n-      61.1  \n \n  XLNet      89.0/94.5      86.1/88.8      89.8/-      95.6      91.8      93.9      91.8      83.8      89.2      63.6  \n \n  RoBERTa      88.9/94.6      86.5/89.4      90.2/90.2      96.4      92.2      94.7      92.4      86.6      90.9      68.0  \n \n  BART      88.8/94.6      86.1/89.2      89.9/90.1      96.6      92.5      94.9      91.2      87.0      90.4      62.8 Table      2:      Results      for      large      models      on      SQUAD      and      GLUE      tasks."
            },
            {
              "id": 142,
              "title": "Token      Deletion Text      Infilling - Chunk 55",
              "type": "chunk",
              "children": [],
              "content": "BART      performs      comparably      to      ROBERTa      and  \n \n  XLNet,      suggesting      that      BART’s      uni-directional      decoder      layers      do      not      reduce      performance      on      discriminative      tasks."
            },
            {
              "id": 143,
              "title": "Token      Deletion Text      Infilling - Chunk 56",
              "type": "chunk",
              "children": [],
              "content": "CNN/DailyMail      XSum       RI      R2      RL      RI      R2      RL\n \n \n  Lead-3      40.42      17.62      36.67      16.30      1.60      11.95  \n \n  PTGEN      (See      et      al.,      2017)      36.44      15.66      33.42      29.70      9.21      23.24  \n \n  PTGEN+COV      (See      et      al.,      2017)      39.53      17.28      36.38      28.10      8.02      21.72  \n \n  UniLM      43.33      20.21      40.51   \n \n-   \n \n-   \n \n-       BERTSUMABS      (Liu   \n \n&      Lapata,      2019)      41.72      19.39      38.76      38.76      16.33      31.15  \n \n  BERTSUMEXTABS      (Liu   \n \n&      Lapata,      2019)      42.13      19.60      39.18      38.81      16.50      31.27 BART      44.16      21.28      40.90      45.14      22.27      37.25 Table      3:      Results      on      two      standard      summarization      datasets."
            },
            {
              "id": 144,
              "title": "Token      Deletion Text      Infilling - Chunk 57",
              "type": "chunk",
              "children": [],
              "content": "BART      outperforms      previous      work      on      summarization      on\n \n \n  on      the      CNN/DM      summarization      dataset,      we      hypothe-      ConvAI2  \n \n  sised      that      larger      pre-trained      models      may      be      better      able      Valid      F1      Valid      PPL  \n \n  to   \n \n|      fi      this      task."
            },
            {
              "id": 145,
              "title": "Token      Deletion Text      Infilling - Chunk 58",
              "type": "chunk",
              "children": [],
              "content": "To      help      th      del      better      fit      th  \n \n  RE      aa      ae      Pai      Cae      Seq2Seq+      Attention      16.02      35.07  \n \n  data,      we      disabled      dropout      for      the      final      10%      of      training   \n \n. . Best      System      19.09      17.51  \n \n  steps. We      use      the      same      pre-training      data      as      Liu      et      al."
            },
            {
              "id": 146,
              "title": "Token      Deletion Text      Infilling - Chunk 59",
              "type": "chunk",
              "children": [],
              "content": "(2019),      consisting      of      160Gb      of      news,      books,      stories,  \n \n  and      web      text."
            },
            {
              "id": 147,
              "title": "Token      Deletion Text      Infilling - Chunk 60",
              "type": "chunk",
              "children": [],
              "content": "BART      20.72      11.85 5.2      Discriminative      Tasks  \n \n  Table   \n \n2      compares      the      performance      of      BART      with      sev-  \n \n  eral      recent      approaches      on      the      well-studied      SQUAD      and  \n \n  GLUE      tasks      (Warstadt      et      al.,      2018;      Socher      et      al.,      2013;  \n \n  Dolan   \n \n&      Brockett,      2005;      Agirre      et      al.,      2007;      Williams  \n \n  et      al.,      2018;      Dagan      et      al.,      2006;      Levesque      et      al.,      2011)."
            },
            {
              "id": 148,
              "title": "Token      Deletion Text      Infilling - Chunk 61",
              "type": "chunk",
              "children": [],
              "content": "Table      4:      BART      outperforms      previous      work      on      conver-  \n \n  sational      response      generation. Perplexities      are      renor-  \n \n  malized      based      on      official      tokenizer      for      ConvAI2. The      most      directly      comparable      baseline      is      ROBERTa,  \n \n  which      was      pre-trained      with      the      same      resources,      but  \n \n  a      different      objective."
            },
            {
              "id": 149,
              "title": "Token      Deletion Text      Infilling - Chunk 62",
              "type": "chunk",
              "children": [],
              "content": "Overall,      BART      performs      simi-  \n \n  larly,      with      only      small      differences      between      the      models  \n \n  on      most      tasks. suggesting      that      BART’s      improvements  \n \n  on      generation      tasks      do      not      come      at      the      expense      of      clas-  \n \n  sification      performance."
            },
            {
              "id": 150,
              "title": "Token      Deletion Text      Infilling - Chunk 63",
              "type": "chunk",
              "children": [],
              "content": "Summarization      To      provide   \n \na      comparison      with      the  \n \n  state-of-the-art      in      summarization,      we      present      results  \n \n  on      two      summarization      datasets,      CNN/DailyMail      and  \n \n  XSum,      which      have      distinct      properties. Summaries      in      the      CNN/DailyMail      tend      to      resemble  \n \n  source      sentences."
            },
            {
              "id": 151,
              "title": "Token      Deletion Text      Infilling - Chunk 64",
              "type": "chunk",
              "children": [],
              "content": "Extractive      models      do      well      here,      and  \n \n  even      the      baseline      of      the      first-three      source      sentences      is  \n \n  highly      competitive. Nevertheless,      BART      outperforms  \n \n  all      existing      work. 5.3. Generation      Tasks\nGeneration      Tasks\n |       We      also      experiment      with      several      text      generation      tasks."
            },
            {
              "id": 152,
              "title": "Token      Deletion Text      Infilling - Chunk 65",
              "type": "chunk",
              "children": [],
              "content": "BART      is      fine-tuned      as      a      standard      sequence-to-sequence       model      from      the      input      to      the      output      text. During      fine-       tuning      we      use      a      label      smoothed      cross      entropy      loss       (Pereyra      et      al.,      2017),      with      the      smoothing      parameter       set      to      0.1."
            },
            {
              "id": 153,
              "title": "Token      Deletion Text      Infilling - Chunk 66",
              "type": "chunk",
              "children": [],
              "content": "During      generation,      we      set      beam      size      as      5,       remove      duplicated      trigrams      in      beam      search,      and      tuned       the      model      with      min-len,      max-len,      length      penalty      on      the       validation      set      (Fan      et      al.,      2017)."
            },
            {
              "id": 154,
              "title": "Token      Deletion Text      Infilling - Chunk 67",
              "type": "chunk",
              "children": [],
              "content": "| 6.0      points      on      all      ROUGE      metrics—trepresenting   \n \na      sig-  \n \n  nificant      advance      in      performance      on      this      problem. Qual-  \n \n  itatively,      sample      quality      is      high      (see      86). In      contrast,      XSum      is      highly      abstractive,      and      extrac-  \n \n  tive      models      perform      poorly."
            },
            {
              "id": 155,
              "title": "Token      Deletion Text      Infilling - Chunk 68",
              "type": "chunk",
              "children": [],
              "content": "BART      outperforms      the  \n \n  best      previous      work,      which      leverages      BERT,      by      roughly  \n \n  Dialogue      We      evaluate      dialogue      response      generation  \n \n  on      CONVAI2      (Dinan      et      al.,      2019),      in      which      agents  \n \n  must      generate      responses      conditioned      on      both      the      pre-  \n \n  vious      context      and   \n \na      textually-specified      persona."
            },
            {
              "id": 156,
              "title": "Token      Deletion Text      Infilling - Chunk 69",
              "type": "chunk",
              "children": [],
              "content": "BART  \n \n  outperforms      previous      work      on      two      automated      metrics."
            }
          ],
          "content": ""
        },
        {
          "id": 157,
          "title": "3.1      Sequence      Classification      Tasks",
          "type": "section",
          "children": [
            {
              "id": 158,
              "title": "3.1      Sequence      Classification      Tasks - Chunk 1",
              "type": "chunk",
              "children": [],
              "content": "3.1      Sequence      Classification      Tasks\nasks  \n \n  For      sequence      classification      tasks,      the      same      input      is      fed  \n \n  into      the      encoder      and      decoder,      and      the      final      hidden      state  \n \n  of      the      final      decoder      token      is      fed      into      new      multi-class  \n \n  linear      classifier."
            },
            {
              "id": 159,
              "title": "3.1      Sequence      Classification      Tasks - Chunk 2",
              "type": "chunk",
              "children": [],
              "content": "This      approach      is      related      to      the      CLS  \n \n  token      in      BERT;      however      we      add      the      additional      token  \n \n  to      the      end      so      that      representation      for      the      token      in      the  \n \n  decoder      can      attend      to      decoder      states      from      the      complete  \n \n  input      (Figure      3a)."
            }
          ],
          "content": ""
        },
        {
          "id": 160,
          "title": "3.2.      Token      Classification      Tasks",
          "type": "section",
          "children": [
            {
              "id": 161,
              "title": "3.2.      Token      Classification      Tasks - Chunk 1",
              "type": "chunk",
              "children": [],
              "content": "3.2. Token      Classification      Tasks\nasks  \n \n  For      token      classification      tasks,      such      as      answer      endpoint  \n \n  classification      for      SQUAD,      we      feed      the      complete      doc-  \n \n  ument      into      the      encoder      and      decoder,      and      use      the      top  \n \n  hidden      state      of      the      decoder      as   \n \na      representation      for      each  \n \n  word."
            },
            {
              "id": 162,
              "title": "3.2.      Token      Classification      Tasks - Chunk 2",
              "type": "chunk",
              "children": [],
              "content": "This      representation      is      used      to      classify      the      token."
            }
          ],
          "content": ""
        },
        {
          "id": 163,
          "title": "3.3      Sequence      Generation      Tasks",
          "type": "section",
          "children": [
            {
              "id": 164,
              "title": "3.3      Sequence      Generation      Tasks - Chunk 1",
              "type": "chunk",
              "children": [],
              "content": "3.3      Sequence      Generation      Tasks\nasks  \n \n  Because      BART      has      an      autoregressive      decoder,      it      can      be  \n \n  directly      fine      tuned      for      sequence      generation      tasks      such  \n \n  as      abstractive      question      answering      and      summarization."
            },
            {
              "id": 165,
              "title": "3.3      Sequence      Generation      Tasks - Chunk 2",
              "type": "chunk",
              "children": [],
              "content": "In      both      of      these      tasks,      information      is      copied      from      the  \n \n  input      but      manipulated,      which      is      closely      related      to      the  \n \n  denoising      pre-training      objective. Here,      the      encoder      in-  \n \n  put      is      the      input      sequence,      and      the      decoder      generates  \n \n  outputs      autoregressively."
            }
          ],
          "content": ""
        },
        {
          "id": 166,
          "title": "3.4      Machine      Translation",
          "type": "section",
          "children": [
            {
              "id": 167,
              "title": "3.4      Machine      Translation - Chunk 1",
              "type": "chunk",
              "children": [],
              "content": "3.4      Machine      Translation\ntion  \n \n  We      also      explore      using      BART      to      improve      machine      trans-  \n \n  lation      decoders      for      translating      into      English. Previous  \n \n  work      Edunov      et      al."
            },
            {
              "id": 168,
              "title": "3.4      Machine      Translation - Chunk 2",
              "type": "chunk",
              "children": [],
              "content": "(2019)      has      shown      that      models      can  \n \n  be      improved      by      incorporating      pre-trained      encoders,      but  \n \n  gains      from      using      pre-trained      language      models      in      de-  \n \n  coders      have      been      limited."
            },
            {
              "id": 169,
              "title": "3.4      Machine      Translation - Chunk 3",
              "type": "chunk",
              "children": [],
              "content": "We      show      that      it      is      possible  \n \n  to      use      the      entire      BART      model      (both      encoder      and      de-  \n \n  coder)      as   \n \na      single      pretrained      decoder      for      machine      trans-  \n \n  lation,      by      adding   \n \na      new      set      of      encoder      parameters      that  \n \n  are      learned      from      bitext      (see      Figure      3b)."
            },
            {
              "id": 170,
              "title": "3.4      Machine      Translation - Chunk 4",
              "type": "chunk",
              "children": [],
              "content": "More      precisely,      we      replace      BART’s      encoder      embed-  \n \n  ding      layer      with   \n \na      new      randomly      initialized      encoder. The      model      is      trained      end-to-end,      which      trains      the      new       encoder      to      map      foreign      words      into      an      input      that      BART  \n \n  can      de-noise      to      English."
            },
            {
              "id": 171,
              "title": "3.4      Machine      Translation - Chunk 5",
              "type": "chunk",
              "children": [],
              "content": "The      new      encoder      can      use   \n \na       separate      vocabulary      from      the      original      BART      model. We      train      the      source      encoder      in      two      steps,      in      both  \n \n  cases      backpropagating      the      cross-entropy      loss      from      the  \n \n  output      of      the      BART      model."
            },
            {
              "id": 172,
              "title": "3.4      Machine      Translation - Chunk 6",
              "type": "chunk",
              "children": [],
              "content": "In      the      first      step,      we      freeze  \n \n  most      of      BART      parameters      and      only      update      the      ran-  \n \n  domly      initialized      source      encoder,      the      BART      positional  \n \n  embeddings,      and      the      self-attention      input      projection      ma-  \n \n  trix      of      BART’s      encoder      first      layer."
            },
            {
              "id": 173,
              "title": "3.4      Machine      Translation - Chunk 7",
              "type": "chunk",
              "children": [],
              "content": "In      the      second      step,  \n \n  we      train      all      model      parameters      for   \n \na      small      number      of  \n \n  iterations. 4\n \n   Comparing      Pre-training      Objectives  \n \n  BART      supports   \n \na      much      wider      range      of      noising      schemes  \n \n  during      pre-training      than      previous      work."
            },
            {
              "id": 174,
              "title": "3.4      Machine      Translation - Chunk 8",
              "type": "chunk",
              "children": [],
              "content": "We      compare   \n \na       range      of      options      using      base-size      models      (6      encoder      and  \n \n  6      decoder      layers,      with   \n \na      hidden      size      of      768),      evaluated  \n \n  on   \n \na      representative      subset      of      the      tasks      we      will      consider  \n \n  for      the      full      large      scale      experiments      in      85."
            },
            {
              "id": 175,
              "title": "3.4      Machine      Translation - Chunk 9",
              "type": "chunk",
              "children": [],
              "content": "4.1      Comparison      Objectives\nives  \n \n  While      many      pre-training      objectives      have      been      pro-  \n \n  posed,      fair      comparisons      between      these      have      been      dif-  \n \n  ficult      to      perform,      at      least      in      part      due      to      differences      in  \n \n  training      data,      training      resources,      architectural      differ-  \n \n  ences      between      models,      and      fine-tuning      procedures."
            },
            {
              "id": 176,
              "title": "3.4      Machine      Translation - Chunk 10",
              "type": "chunk",
              "children": [],
              "content": "We  \n \n  ABCDE  \n \n  weee  \n \n  Pre-trained      Pre-trained  \n \n  Craters      Joo      rezapes cise)      Te      TrET Pre-trained  \n \n  Encoder Pre-trained  \n \n  Decoder aa      AB  \n \n  trrrt  \n \n  tr      ttt  \n \n  <s>A      BCD  \n \n  Randomly  \n \n  Initialized      Encoder  \n \n  TFT  \n \n  aBpByoe sentation      from      the      final      output      is      used."
            },
            {
              "id": 177,
              "title": "3.4      Machine      Translation - Chunk 11",
              "type": "chunk",
              "children": [],
              "content": "re-implement      strong      pre-training      approaches      recently  \n \n  proposed      for      discriminative      and      generation      tasks. We  \n \n  aim,      as      much      as      possible,      to      control      for      differences      un-  \n \n  related      to      the      pre-training      objective."
            },
            {
              "id": 178,
              "title": "3.4      Machine      Translation - Chunk 12",
              "type": "chunk",
              "children": [],
              "content": "However,      we      do  \n \n  make      minor      changes      to      the      learning      rate      and      usage      of  \n \n  layer      normalisation      in      order      to      improve      performance  \n \n  (tuning      these      separately      for      each      objective)."
            },
            {
              "id": 179,
              "title": "3.4      Machine      Translation - Chunk 13",
              "type": "chunk",
              "children": [],
              "content": "For      refer-  \n \n  ence,      we      compare      our      implementations      with      published  \n \n  numbers      from      BERT,      which      was      also      trained      for      1M  \n \n  steps      on   \n \na      combination      of      books      and      Wikipedia      data."
            },
            {
              "id": 180,
              "title": "3.4      Machine      Translation - Chunk 14",
              "type": "chunk",
              "children": [],
              "content": "We      compare      the      following      approaches:\n \n \n  Language      Model      Similarly      to      GPT      (Radford      et      al.,  \n \n  2018),      we      train   \n \na      left-to-right      Transformer      language  \n \n  model. This      model      is      equivalent      to      the      BART      decoder,  \n \n  without      cross-attention."
            },
            {
              "id": 181,
              "title": "3.4      Machine      Translation - Chunk 15",
              "type": "chunk",
              "children": [],
              "content": "Permuted      Language      Model      Based      on      XLNet      (Yang  \n \n  et      al.,      2019),      we      sample      1/6      of      the      tokens,      and      gener-  \n \n  ate      them      in   \n \na      random      order      autoregressively."
            },
            {
              "id": 182,
              "title": "3.4      Machine      Translation - Chunk 16",
              "type": "chunk",
              "children": [],
              "content": "For      con-  \n \n  sistency      with      other      models,      we      do      not      implement      the  \n \n  relative      positional      embeddings      or      attention      across      seg-  \n \n  ments      from      XLNet."
            },
            {
              "id": 183,
              "title": "3.4      Machine      Translation - Chunk 17",
              "type": "chunk",
              "children": [],
              "content": "Masked      Language      Model      Following      BERT      (Devlin  \n \n  et      al.,      2019),      we      replace      15%      of      tokens      with      [MASK]  \n \n  symbols,      and      train      the      model      to      independently      predict  \n \n  the      original      tokens."
            },
            {
              "id": 184,
              "title": "3.4      Machine      Translation - Chunk 18",
              "type": "chunk",
              "children": [],
              "content": "Multitask      Masked      Language      Model      As      in      UniLM  \n \n  (Dong      et      al.,      2019),      we      train   \n \na      Masked      Language  \n \n  Model      with      additional      self-attention      masks."
            },
            {
              "id": 185,
              "title": "3.4      Machine      Translation - Chunk 19",
              "type": "chunk",
              "children": [],
              "content": "Self      at-  \n \n  tention      masks      are      chosen      randomly      in      with      the      follow  \n \n  proportions:      1/6      left-to-right,      1/6      right-to-left,      1/3      un-  \n \n  masked,      and      1/3      with      the      first      50%      of      tokens      unmasked  \n \n  and   \n \na      left-to-right      mask      for      the      remainder."
            },
            {
              "id": 186,
              "title": "3.4      Machine      Translation - Chunk 20",
              "type": "chunk",
              "children": [],
              "content": "Masked      Seq-to-Seq      Inspired      by      MASS      (Song      et      al.,  \n \n  2019),      we      mask   \n \na      span      containing      50%      of      tokens,  \n \n  and      train   \n \na      sequence      to      sequence      model      to      predict      the  \n \n  masked      tokens."
            },
            {
              "id": 187,
              "title": "3.4      Machine      Translation - Chunk 21",
              "type": "chunk",
              "children": [],
              "content": "For      the      Permuted      LM,      Masked      LM      and      Multitask  \n \n  Masked      LM,      we      use      two-stream      attention      (Yang      et      al.,  \n \n  2019)      to      efficiently      compute      likelihoods      of      the      output  \n \n  part      of      the      sequence      (using   \n \na      diagonal      self-attention  \n \n  mask      on      the      output      to      predict      words      left-to-right)."
            },
            {
              "id": 188,
              "title": "3.4      Machine      Translation - Chunk 22",
              "type": "chunk",
              "children": [],
              "content": "We      experiment      with      (1)      treating      the      task      as   \n \na      stan-  \n \n  dard      sequence-to-sequence      problem,      where      the      source  \n \n  input      to      the      encoder      and      the      target      is      the      decoder      out-  \n \n  put,      or      (2)      adding      the      source      as      prefix      to      the      target      in  \n \n  the      decoder,      with   \n \na      loss      only      on      the      target      part      of      the  \n \n  sequence."
            },
            {
              "id": 189,
              "title": "3.4      Machine      Translation - Chunk 23",
              "type": "chunk",
              "children": [],
              "content": "We      find      the      former      works      better      for      BART  \n \n  models,      and      the      latter      for      other      models. To      most      directly      compare      our      models      on      their      ability  \n \n  to      model      their      fine-tuning      objective      (the      log      likelihood  \n \n  of      the      human      text),      we      report      perplexity      in      Table      1."
            },
            {
              "id": 190,
              "title": "3.4      Machine      Translation - Chunk 24",
              "type": "chunk",
              "children": [],
              "content": "4.2      Tasks\nasks  \n \n  SQuAD  \n \n  (Rajpurkar      et      al.,      2016)a      an      extractive      ques-  \n \n  tion      answering      task      on      Wikipedia      paragraphs. Answers  \n \n  are      text      spans      extracted      from   \n \na      given      document      context."
            },
            {
              "id": 191,
              "title": "3.4      Machine      Translation - Chunk 25",
              "type": "chunk",
              "children": [],
              "content": "Similar      to      BERT      (Devlin      et      al.,      2019),      we      use      concate-  \n \n  nated      question      and      context      as      input      to      the      encoder      of  \n \n  BART,      and      additionally      pass      them      to      the      decoder. The  \n \n  model      includes      classifiers      to      predict      the      start      and      end  \n \n  indices      of      each      token."
            },
            {
              "id": 192,
              "title": "3.4      Machine      Translation - Chunk 26",
              "type": "chunk",
              "children": [],
              "content": "MNLI      (Williams      et      al.,      2017),   \n \na      bitext      classification  \n \n  task      to      predict      whether      one      sentence      entails      another. The      fine-tuned      model      concatenates      the      two      sentences  \n \n  with      appended      an      EOS      token,      and      passes      them      to      both  \n \n  the      BART      encoder      and      decoder."
            },
            {
              "id": 193,
              "title": "3.4      Machine      Translation - Chunk 27",
              "type": "chunk",
              "children": [],
              "content": "In      contrast      to      BERT,  \n \n  the      representation      of      the      EOS      token      is      used      to      classify  \n \n  the      sentences      relations. ELIS      (Fanetal.,      2019),   \n \na      long-form      abstractive      ques-  \n \n  tion      answering      dataset. Models      generate      answers      con-  \n \n  ditioned      on      the      concatenation      of   \n \na      question      and      sup-  \n \n  porting      documents."
            },
            {
              "id": 194,
              "title": "3.4      Machine      Translation - Chunk 28",
              "type": "chunk",
              "children": [],
              "content": "XSum_      (Narayan      et      al.,      2018),   \n \na      news      summarization  \n \n  dataset      with      highly      abstractive      summaries. ConvAI2_      (Dinan      et      al.,      2019),   \n \na      dialogue      response  \n \n  generation      task,      conditioned      on      context      and   \n \na      persona. CNN/DM_      (Hermann      et      al.,      2015),   \n \na      news      summa-  \n \n  rization      dataset."
            },
            {
              "id": 195,
              "title": "3.4      Machine      Translation - Chunk 29",
              "type": "chunk",
              "children": [],
              "content": "Summaries      here      are      typically      closely  \n \n  related      to      source      sentences. 4.3      Results\nults Results      are      shown      in      Table      1."
            },
            {
              "id": 196,
              "title": "3.4      Machine      Translation - Chunk 30",
              "type": "chunk",
              "children": [],
              "content": "Several      trends      are      clear:\nModel      SQuAD      1.1      MNLI      ELIS      XSum   \n \n_      ConvAI2      CNN/DM  \n \n  Fl      Acc      PPL      PPL      PPL      PPL BERT      Base      (Devlin      et      al.,      2019)      88.5      84.3   \n \n-   \n \n-   \n \n-   \n \n-       Masked      Language      Model      90.0      83.5      24.77      7.87      12.59      7.06  \n \n  Masked      Seq2seq      87.0      82.1      23.40      6.80      11.43      6.19  \n \n  Language      Model      76.7      80.1      21.40      7.00      11.51      6.56  \n \n  Permuted      Language      Model      89.1      83.7      24.03      7.69      12.23      6.96  \n \n  Multitask      Masked      Language      Model      89.2      82.4      23.73      7.50      12.39      6.74  \n \n  BART      Base  \n \n  w/      Token      Masking      90.4      84.1      25.05      7.08      11.73      6.10  \n \n  w/      Token      Deletion      90.4      84.1      24.61      6.90      11.46      5.87  \n \n  w/      Text      Infilling      90.8      84.0      24.26      6.61      11.05      5.83  \n \n  w/      Document      Rotation      77.2      75.3      53.69      17.14      19.87      10.59  \n \n  w/      Sentence      Shuffling      85.4      81.5      41.87      10.93      16.67      7.89  \n \n  w/      Text      Infilling   \n \n+      Sentence      Shuffling      90.8      83.8      24.17      6.62      11.12      5.41\nconsistently      strong      performance."
            },
            {
              "id": 197,
              "title": "3.4      Machine      Translation - Chunk 31",
              "type": "chunk",
              "children": [],
              "content": "Performance      of      pre-training      methods      varies      signifi-  \n \n  cantly      across      tasks      The      effectiveness      of      pre-training  \n \n  methods      is      highly      dependent      on      the      task. For      exam-  \n \n  ple,   \n \na      simple      language      model      achieves      the      best      ELIS  \n \n  performance,      but      the      worst      SQUAD      results."
            },
            {
              "id": 198,
              "title": "3.4      Machine      Translation - Chunk 32",
              "type": "chunk",
              "children": [],
              "content": "Token      masking      is      crucial      Pre-training      objectives  \n \n  based      on      rotating      documents      or      permuting      sentences  \n \n  perform      poorly      in      isolation. The      successful      methods  \n \n  either      use      token      deletion      or      masking,      or      self-attention  \n \n  masks. Deletion      appears      to      outperform      masking      on  \n \n  generation      tasks."
            },
            {
              "id": 199,
              "title": "3.4      Machine      Translation - Chunk 33",
              "type": "chunk",
              "children": [],
              "content": "Left-to-right      pre-training      improves      generation  \n \n  The      Masked      Language      Model      and      the      Permuted  \n \n  Language      Model      perform      less      well      than      others      on  \n \n  generation,      and      are      the      only      models      we      consider      that  \n \n  do      not      include      left-to-right      auto-regressive      language  \n \n  modelling      during      pre-training."
            },
            {
              "id": 200,
              "title": "3.4      Machine      Translation - Chunk 34",
              "type": "chunk",
              "children": [],
              "content": "Bidirectional      encoders      are      crucial      for      SQUAD      As  \n \n  noted      in      previous      work      (Devlin      et      al.,      2019),      just  \n \n  left-to-right      decoder      performs      poorly      on      SQuAD,      be-  \n \n  cause      future      context      is      crucial      in      classification      deci-  \n \n  sions."
            },
            {
              "id": 201,
              "title": "3.4      Machine      Translation - Chunk 35",
              "type": "chunk",
              "children": [],
              "content": "However,      BART      achieves      similar      performance  \n \n  with      only      half      the      number      of      bidirectional      layers. The      pre-training      objective      is      not      the      only      important  \n \n  factor      Our      Permuted      Language      Model      performs      less  \n \n  well      than      XLNet      (Yang      et      al.,      2019)."
            },
            {
              "id": 202,
              "title": "3.4      Machine      Translation - Chunk 36",
              "type": "chunk",
              "children": [],
              "content": "Some      of      this      dif-  \n \n  ference      is      likely      due      to      not      including      other      architectural  \n \n  improvements,      such      as      relative-position      embeddings      or  \n \n  segment-level      recurrence."
            },
            {
              "id": 203,
              "title": "3.4      Machine      Translation - Chunk 37",
              "type": "chunk",
              "children": [],
              "content": "Pure      language      models      perform      best      on      ELIS      The  \n \n  ELIS      dataset      is      an      outlier,      with      much      higher      perplex-  \n \n  ities      than      other      tasks,      and      is      the      only      generation      task  \n \n  where      other      models      outperform      BART."
            },
            {
              "id": 204,
              "title": "3.4      Machine      Translation - Chunk 38",
              "type": "chunk",
              "children": [],
              "content": "A      pure      lan-  \n \n  guage      model      performs      best,      suggesting      that      BART      is  \n \n  less      effective      when      the      output      is      only      loosely      con-  \n \n  strained      by      the      input. BART      achieves      the      most      consistently      strong      perfor-  \n \n  mance."
            },
            {
              "id": 205,
              "title": "3.4      Machine      Translation - Chunk 39",
              "type": "chunk",
              "children": [],
              "content": "With      the      exception      of      ELI5,      BART      models  \n \n  using      text-infilling      perform      well      on      all      tasks."
            },
            {
              "id": 206,
              "title": "3.4      Machine      Translation - Chunk 40",
              "type": "chunk",
              "children": [],
              "content": "5\n \n   Large-scale      Pre-training      Experiments  \n \n  Recent      work      has      shown      that      downstream      performance  \n \n  can      dramatically      improve      when      pre-training      is      scaled  \n \n  to      large      batch      sizes      (Yang      et      al.,      2019;      Liu      et      al.,      2019)  \n \n  and      corpora."
            },
            {
              "id": 207,
              "title": "3.4      Machine      Translation - Chunk 41",
              "type": "chunk",
              "children": [],
              "content": "To      test      how      well      BART      performs      in      this  \n \n  regime,      and      to      create   \n \na      useful      model      for      downstream  \n \n  tasks,      we      trained      BART      using      the      same      scale      as      the  \n \n  RoBERTa      model."
            },
            {
              "id": 208,
              "title": "3.4      Machine      Translation - Chunk 42",
              "type": "chunk",
              "children": [],
              "content": "5.1      Experimental      Setup\netup  \n \n  We      pre-train   \n \na      large      model      with      12      layers      in      each      of      the  \n \n  encoder      and      decoder,      and   \n \na      hidden      size      of      1024. Fol-  \n \n  lowing      RoBERTa      (Liu      et      al.,      2019),      we      use   \n \na      batch      size  \n \n  of      8000,      and      train      the      model      for      500000      steps."
            },
            {
              "id": 209,
              "title": "3.4      Machine      Translation - Chunk 43",
              "type": "chunk",
              "children": [],
              "content": "Docu-  \n \n  ments      are      tokenized      with      the      same      byte-pair      encoding  \n \n  as      GPT-2      (Radford      et      al.,      2019). Based      on      the      results      in  \n \n  Section      §4,      we      use   \n \na      combination      of      text      infilling      and  \n \n  sentence      permutation. We      mask      30%      of      tokens      in      each  \n \n  document,      and      permute      all      sentences."
            },
            {
              "id": 210,
              "title": "3.4      Machine      Translation - Chunk 44",
              "type": "chunk",
              "children": [],
              "content": "Although      sen-  \n \n  tence      permutation      only      shows      significant      additive      gains SQuAD      1.1      SQuAD      2.0      MNLI      SST      QQP      QNLI      STS-B      RTE      MRPC      CoLA  \n \n  EM/F1      EM/F1      m/mm      Acc      Acc      Acc      Acc      Acc      Acc      Mcc\n \n \n  BERT      84.1/90.9      79.0/81.8      86.6/-      93.2      91.3      92.3      90.0      70.4      88.0      60.6  \n \n  UniLM      -/-      80.5/83.4      87.0/85.9      94.5   \n \n-      92.7   \n \n-      70.9   \n \n-      61.1  \n \n  XLNet      89.0/94.5      86.1/88.8      89.8/-      95.6      91.8      93.9      91.8      83.8      89.2      63.6  \n \n  RoBERTa      88.9/94.6      86.5/89.4      90.2/90.2      96.4      92.2      94.7      92.4      86.6      90.9      68.0  \n \n  BART      88.8/94.6      86.1/89.2      89.9/90.1      96.6      92.5      94.9      91.2      87.0      90.4      62.8 Table      2:      Results      for      large      models      on      SQUAD      and      GLUE      tasks."
            },
            {
              "id": 211,
              "title": "3.4      Machine      Translation - Chunk 45",
              "type": "chunk",
              "children": [],
              "content": "BART      performs      comparably      to      ROBERTa      and  \n \n  XLNet,      suggesting      that      BART’s      uni-directional      decoder      layers      do      not      reduce      performance      on      discriminative      tasks."
            }
          ],
          "content": ""
        },
        {
          "id": 212,
          "title": "4.1      Comparison      Objectives",
          "type": "section",
          "children": [
            {
              "id": 213,
              "title": "4.1      Comparison      Objectives - Chunk 1",
              "type": "chunk",
              "children": [],
              "content": "4.1      Comparison      Objectives\nives  \n \n  While      many      pre-training      objectives      have      been      pro-  \n \n  posed,      fair      comparisons      between      these      have      been      dif-  \n \n  ficult      to      perform,      at      least      in      part      due      to      differences      in  \n \n  training      data,      training      resources,      architectural      differ-  \n \n  ences      between      models,      and      fine-tuning      procedures."
            },
            {
              "id": 214,
              "title": "4.1      Comparison      Objectives - Chunk 2",
              "type": "chunk",
              "children": [],
              "content": "We  \n \n  ABCDE  \n \n  weee  \n \n  Pre-trained      Pre-trained  \n \n  Craters      Joo      rezapes cise)      Te      TrET Pre-trained  \n \n  Encoder Pre-trained  \n \n  Decoder aa      AB  \n \n  trrrt  \n \n  tr      ttt  \n \n  <s>A      BCD  \n \n  Randomly  \n \n  Initialized      Encoder  \n \n  TFT  \n \n  aBpByoe sentation      from      the      final      output      is      used."
            },
            {
              "id": 215,
              "title": "4.1      Comparison      Objectives - Chunk 3",
              "type": "chunk",
              "children": [],
              "content": "re-implement      strong      pre-training      approaches      recently  \n \n  proposed      for      discriminative      and      generation      tasks. We  \n \n  aim,      as      much      as      possible,      to      control      for      differences      un-  \n \n  related      to      the      pre-training      objective."
            },
            {
              "id": 216,
              "title": "4.1      Comparison      Objectives - Chunk 4",
              "type": "chunk",
              "children": [],
              "content": "However,      we      do  \n \n  make      minor      changes      to      the      learning      rate      and      usage      of  \n \n  layer      normalisation      in      order      to      improve      performance  \n \n  (tuning      these      separately      for      each      objective)."
            },
            {
              "id": 217,
              "title": "4.1      Comparison      Objectives - Chunk 5",
              "type": "chunk",
              "children": [],
              "content": "For      refer-  \n \n  ence,      we      compare      our      implementations      with      published  \n \n  numbers      from      BERT,      which      was      also      trained      for      1M  \n \n  steps      on   \n \na      combination      of      books      and      Wikipedia      data."
            },
            {
              "id": 218,
              "title": "4.1      Comparison      Objectives - Chunk 6",
              "type": "chunk",
              "children": [],
              "content": "We      compare      the      following      approaches:\n \n \n  Language      Model      Similarly      to      GPT      (Radford      et      al.,  \n \n  2018),      we      train   \n \na      left-to-right      Transformer      language  \n \n  model. This      model      is      equivalent      to      the      BART      decoder,  \n \n  without      cross-attention."
            },
            {
              "id": 219,
              "title": "4.1      Comparison      Objectives - Chunk 7",
              "type": "chunk",
              "children": [],
              "content": "Permuted      Language      Model      Based      on      XLNet      (Yang  \n \n  et      al.,      2019),      we      sample      1/6      of      the      tokens,      and      gener-  \n \n  ate      them      in   \n \na      random      order      autoregressively."
            },
            {
              "id": 220,
              "title": "4.1      Comparison      Objectives - Chunk 8",
              "type": "chunk",
              "children": [],
              "content": "For      con-  \n \n  sistency      with      other      models,      we      do      not      implement      the  \n \n  relative      positional      embeddings      or      attention      across      seg-  \n \n  ments      from      XLNet."
            },
            {
              "id": 221,
              "title": "4.1      Comparison      Objectives - Chunk 9",
              "type": "chunk",
              "children": [],
              "content": "Masked      Language      Model      Following      BERT      (Devlin  \n \n  et      al.,      2019),      we      replace      15%      of      tokens      with      [MASK]  \n \n  symbols,      and      train      the      model      to      independently      predict  \n \n  the      original      tokens."
            },
            {
              "id": 222,
              "title": "4.1      Comparison      Objectives - Chunk 10",
              "type": "chunk",
              "children": [],
              "content": "Multitask      Masked      Language      Model      As      in      UniLM  \n \n  (Dong      et      al.,      2019),      we      train   \n \na      Masked      Language  \n \n  Model      with      additional      self-attention      masks."
            },
            {
              "id": 223,
              "title": "4.1      Comparison      Objectives - Chunk 11",
              "type": "chunk",
              "children": [],
              "content": "Self      at-  \n \n  tention      masks      are      chosen      randomly      in      with      the      follow  \n \n  proportions:      1/6      left-to-right,      1/6      right-to-left,      1/3      un-  \n \n  masked,      and      1/3      with      the      first      50%      of      tokens      unmasked  \n \n  and   \n \na      left-to-right      mask      for      the      remainder."
            },
            {
              "id": 224,
              "title": "4.1      Comparison      Objectives - Chunk 12",
              "type": "chunk",
              "children": [],
              "content": "Masked      Seq-to-Seq      Inspired      by      MASS      (Song      et      al.,  \n \n  2019),      we      mask   \n \na      span      containing      50%      of      tokens,  \n \n  and      train   \n \na      sequence      to      sequence      model      to      predict      the  \n \n  masked      tokens."
            },
            {
              "id": 225,
              "title": "4.1      Comparison      Objectives - Chunk 13",
              "type": "chunk",
              "children": [],
              "content": "For      the      Permuted      LM,      Masked      LM      and      Multitask  \n \n  Masked      LM,      we      use      two-stream      attention      (Yang      et      al.,  \n \n  2019)      to      efficiently      compute      likelihoods      of      the      output  \n \n  part      of      the      sequence      (using   \n \na      diagonal      self-attention  \n \n  mask      on      the      output      to      predict      words      left-to-right)."
            },
            {
              "id": 226,
              "title": "4.1      Comparison      Objectives - Chunk 14",
              "type": "chunk",
              "children": [],
              "content": "We      experiment      with      (1)      treating      the      task      as   \n \na      stan-  \n \n  dard      sequence-to-sequence      problem,      where      the      source  \n \n  input      to      the      encoder      and      the      target      is      the      decoder      out-  \n \n  put,      or      (2)      adding      the      source      as      prefix      to      the      target      in  \n \n  the      decoder,      with   \n \na      loss      only      on      the      target      part      of      the  \n \n  sequence."
            },
            {
              "id": 227,
              "title": "4.1      Comparison      Objectives - Chunk 15",
              "type": "chunk",
              "children": [],
              "content": "We      find      the      former      works      better      for      BART  \n \n  models,      and      the      latter      for      other      models. To      most      directly      compare      our      models      on      their      ability  \n \n  to      model      their      fine-tuning      objective      (the      log      likelihood  \n \n  of      the      human      text),      we      report      perplexity      in      Table      1."
            }
          ],
          "content": ""
        },
        {
          "id": 228,
          "title": "4.2      Tasks",
          "type": "section",
          "children": [
            {
              "id": 229,
              "title": "4.2      Tasks - Chunk 1",
              "type": "chunk",
              "children": [],
              "content": "4.2      Tasks\nasks  \n \n  SQuAD  \n \n  (Rajpurkar      et      al.,      2016)a      an      extractive      ques-  \n \n  tion      answering      task      on      Wikipedia      paragraphs. Answers  \n \n  are      text      spans      extracted      from   \n \na      given      document      context."
            },
            {
              "id": 230,
              "title": "4.2      Tasks - Chunk 2",
              "type": "chunk",
              "children": [],
              "content": "Similar      to      BERT      (Devlin      et      al.,      2019),      we      use      concate-  \n \n  nated      question      and      context      as      input      to      the      encoder      of  \n \n  BART,      and      additionally      pass      them      to      the      decoder. The  \n \n  model      includes      classifiers      to      predict      the      start      and      end  \n \n  indices      of      each      token."
            },
            {
              "id": 231,
              "title": "4.2      Tasks - Chunk 3",
              "type": "chunk",
              "children": [],
              "content": "MNLI      (Williams      et      al.,      2017),   \n \na      bitext      classification  \n \n  task      to      predict      whether      one      sentence      entails      another. The      fine-tuned      model      concatenates      the      two      sentences  \n \n  with      appended      an      EOS      token,      and      passes      them      to      both  \n \n  the      BART      encoder      and      decoder."
            },
            {
              "id": 232,
              "title": "4.2      Tasks - Chunk 4",
              "type": "chunk",
              "children": [],
              "content": "In      contrast      to      BERT,  \n \n  the      representation      of      the      EOS      token      is      used      to      classify  \n \n  the      sentences      relations. ELIS      (Fanetal.,      2019),   \n \na      long-form      abstractive      ques-  \n \n  tion      answering      dataset. Models      generate      answers      con-  \n \n  ditioned      on      the      concatenation      of   \n \na      question      and      sup-  \n \n  porting      documents."
            },
            {
              "id": 233,
              "title": "4.2      Tasks - Chunk 5",
              "type": "chunk",
              "children": [],
              "content": "XSum_      (Narayan      et      al.,      2018),   \n \na      news      summarization  \n \n  dataset      with      highly      abstractive      summaries. ConvAI2_      (Dinan      et      al.,      2019),   \n \na      dialogue      response  \n \n  generation      task,      conditioned      on      context      and   \n \na      persona. CNN/DM_      (Hermann      et      al.,      2015),   \n \na      news      summa-  \n \n  rization      dataset."
            },
            {
              "id": 234,
              "title": "4.2      Tasks - Chunk 6",
              "type": "chunk",
              "children": [],
              "content": "Summaries      here      are      typically      closely  \n \n  related      to      source      sentences."
            }
          ],
          "content": ""
        },
        {
          "id": 235,
          "title": "4.3      Results",
          "type": "section",
          "children": [
            {
              "id": 236,
              "title": "4.3      Results - Chunk 1",
              "type": "chunk",
              "children": [],
              "content": "4.3      Results\nults Results      are      shown      in      Table      1."
            },
            {
              "id": 237,
              "title": "4.3      Results - Chunk 2",
              "type": "chunk",
              "children": [],
              "content": "Several      trends      are      clear:\nModel      SQuAD      1.1      MNLI      ELIS      XSum   \n \n_      ConvAI2      CNN/DM  \n \n  Fl      Acc      PPL      PPL      PPL      PPL BERT      Base      (Devlin      et      al.,      2019)      88.5      84.3   \n \n-   \n \n-   \n \n-   \n \n-       Masked      Language      Model      90.0      83.5      24.77      7.87      12.59      7.06  \n \n  Masked      Seq2seq      87.0      82.1      23.40      6.80      11.43      6.19  \n \n  Language      Model      76.7      80.1      21.40      7.00      11.51      6.56  \n \n  Permuted      Language      Model      89.1      83.7      24.03      7.69      12.23      6.96  \n \n  Multitask      Masked      Language      Model      89.2      82.4      23.73      7.50      12.39      6.74  \n \n  BART      Base  \n \n  w/      Token      Masking      90.4      84.1      25.05      7.08      11.73      6.10  \n \n  w/      Token      Deletion      90.4      84.1      24.61      6.90      11.46      5.87  \n \n  w/      Text      Infilling      90.8      84.0      24.26      6.61      11.05      5.83  \n \n  w/      Document      Rotation      77.2      75.3      53.69      17.14      19.87      10.59  \n \n  w/      Sentence      Shuffling      85.4      81.5      41.87      10.93      16.67      7.89  \n \n  w/      Text      Infilling   \n \n+      Sentence      Shuffling      90.8      83.8      24.17      6.62      11.12      5.41\nconsistently      strong      performance."
            },
            {
              "id": 238,
              "title": "4.3      Results - Chunk 3",
              "type": "chunk",
              "children": [],
              "content": "Performance      of      pre-training      methods      varies      signifi-  \n \n  cantly      across      tasks      The      effectiveness      of      pre-training  \n \n  methods      is      highly      dependent      on      the      task. For      exam-  \n \n  ple,   \n \na      simple      language      model      achieves      the      best      ELIS  \n \n  performance,      but      the      worst      SQUAD      results."
            },
            {
              "id": 239,
              "title": "4.3      Results - Chunk 4",
              "type": "chunk",
              "children": [],
              "content": "Token      masking      is      crucial      Pre-training      objectives  \n \n  based      on      rotating      documents      or      permuting      sentences  \n \n  perform      poorly      in      isolation. The      successful      methods  \n \n  either      use      token      deletion      or      masking,      or      self-attention  \n \n  masks. Deletion      appears      to      outperform      masking      on  \n \n  generation      tasks."
            },
            {
              "id": 240,
              "title": "4.3      Results - Chunk 5",
              "type": "chunk",
              "children": [],
              "content": "Left-to-right      pre-training      improves      generation  \n \n  The      Masked      Language      Model      and      the      Permuted  \n \n  Language      Model      perform      less      well      than      others      on  \n \n  generation,      and      are      the      only      models      we      consider      that  \n \n  do      not      include      left-to-right      auto-regressive      language  \n \n  modelling      during      pre-training."
            },
            {
              "id": 241,
              "title": "4.3      Results - Chunk 6",
              "type": "chunk",
              "children": [],
              "content": "Bidirectional      encoders      are      crucial      for      SQUAD      As  \n \n  noted      in      previous      work      (Devlin      et      al.,      2019),      just  \n \n  left-to-right      decoder      performs      poorly      on      SQuAD,      be-  \n \n  cause      future      context      is      crucial      in      classification      deci-  \n \n  sions."
            },
            {
              "id": 242,
              "title": "4.3      Results - Chunk 7",
              "type": "chunk",
              "children": [],
              "content": "However,      BART      achieves      similar      performance  \n \n  with      only      half      the      number      of      bidirectional      layers. The      pre-training      objective      is      not      the      only      important  \n \n  factor      Our      Permuted      Language      Model      performs      less  \n \n  well      than      XLNet      (Yang      et      al.,      2019)."
            },
            {
              "id": 243,
              "title": "4.3      Results - Chunk 8",
              "type": "chunk",
              "children": [],
              "content": "Some      of      this      dif-  \n \n  ference      is      likely      due      to      not      including      other      architectural  \n \n  improvements,      such      as      relative-position      embeddings      or  \n \n  segment-level      recurrence."
            },
            {
              "id": 244,
              "title": "4.3      Results - Chunk 9",
              "type": "chunk",
              "children": [],
              "content": "Pure      language      models      perform      best      on      ELIS      The  \n \n  ELIS      dataset      is      an      outlier,      with      much      higher      perplex-  \n \n  ities      than      other      tasks,      and      is      the      only      generation      task  \n \n  where      other      models      outperform      BART."
            },
            {
              "id": 245,
              "title": "4.3      Results - Chunk 10",
              "type": "chunk",
              "children": [],
              "content": "A      pure      lan-  \n \n  guage      model      performs      best,      suggesting      that      BART      is  \n \n  less      effective      when      the      output      is      only      loosely      con-  \n \n  strained      by      the      input. BART      achieves      the      most      consistently      strong      perfor-  \n \n  mance."
            },
            {
              "id": 246,
              "title": "4.3      Results - Chunk 11",
              "type": "chunk",
              "children": [],
              "content": "With      the      exception      of      ELI5,      BART      models  \n \n  using      text-infilling      perform      well      on      all      tasks."
            },
            {
              "id": 247,
              "title": "4.3      Results - Chunk 12",
              "type": "chunk",
              "children": [],
              "content": "5\n \n   Large-scale      Pre-training      Experiments  \n \n  Recent      work      has      shown      that      downstream      performance  \n \n  can      dramatically      improve      when      pre-training      is      scaled  \n \n  to      large      batch      sizes      (Yang      et      al.,      2019;      Liu      et      al.,      2019)  \n \n  and      corpora."
            },
            {
              "id": 248,
              "title": "4.3      Results - Chunk 13",
              "type": "chunk",
              "children": [],
              "content": "To      test      how      well      BART      performs      in      this  \n \n  regime,      and      to      create   \n \na      useful      model      for      downstream  \n \n  tasks,      we      trained      BART      using      the      same      scale      as      the  \n \n  RoBERTa      model."
            }
          ],
          "content": ""
        },
        {
          "id": 249,
          "title": "5.1      Experimental      Setup",
          "type": "section",
          "children": [
            {
              "id": 250,
              "title": "5.1      Experimental      Setup - Chunk 1",
              "type": "chunk",
              "children": [],
              "content": "5.1      Experimental      Setup\netup  \n \n  We      pre-train   \n \na      large      model      with      12      layers      in      each      of      the  \n \n  encoder      and      decoder,      and   \n \na      hidden      size      of      1024. Fol-  \n \n  lowing      RoBERTa      (Liu      et      al.,      2019),      we      use   \n \na      batch      size  \n \n  of      8000,      and      train      the      model      for      500000      steps."
            },
            {
              "id": 251,
              "title": "5.1      Experimental      Setup - Chunk 2",
              "type": "chunk",
              "children": [],
              "content": "Docu-  \n \n  ments      are      tokenized      with      the      same      byte-pair      encoding  \n \n  as      GPT-2      (Radford      et      al.,      2019). Based      on      the      results      in  \n \n  Section      §4,      we      use   \n \na      combination      of      text      infilling      and  \n \n  sentence      permutation. We      mask      30%      of      tokens      in      each  \n \n  document,      and      permute      all      sentences."
            },
            {
              "id": 252,
              "title": "5.1      Experimental      Setup - Chunk 3",
              "type": "chunk",
              "children": [],
              "content": "Although      sen-  \n \n  tence      permutation      only      shows      significant      additive      gains SQuAD      1.1      SQuAD      2.0      MNLI      SST      QQP      QNLI      STS-B      RTE      MRPC      CoLA  \n \n  EM/F1      EM/F1      m/mm      Acc      Acc      Acc      Acc      Acc      Acc      Mcc\n \n \n  BERT      84.1/90.9      79.0/81.8      86.6/-      93.2      91.3      92.3      90.0      70.4      88.0      60.6  \n \n  UniLM      -/-      80.5/83.4      87.0/85.9      94.5   \n \n-      92.7   \n \n-      70.9   \n \n-      61.1  \n \n  XLNet      89.0/94.5      86.1/88.8      89.8/-      95.6      91.8      93.9      91.8      83.8      89.2      63.6  \n \n  RoBERTa      88.9/94.6      86.5/89.4      90.2/90.2      96.4      92.2      94.7      92.4      86.6      90.9      68.0  \n \n  BART      88.8/94.6      86.1/89.2      89.9/90.1      96.6      92.5      94.9      91.2      87.0      90.4      62.8 Table      2:      Results      for      large      models      on      SQUAD      and      GLUE      tasks."
            },
            {
              "id": 253,
              "title": "5.1      Experimental      Setup - Chunk 4",
              "type": "chunk",
              "children": [],
              "content": "BART      performs      comparably      to      ROBERTa      and  \n \n  XLNet,      suggesting      that      BART’s      uni-directional      decoder      layers      do      not      reduce      performance      on      discriminative      tasks."
            }
          ],
          "content": ""
        },
        {
          "id": 254,
          "title": "CNN/DailyMail      XSum       RI      R2      RL      RI      R2      RL",
          "type": "section",
          "children": [
            {
              "id": 255,
              "title": "CNN/DailyMail      XSum       RI      R2      RL      RI      R2      RL - Chunk 1",
              "type": "chunk",
              "children": [],
              "content": "CNN/DailyMail      XSum       RI      R2      RL      RI      R2      RL\n \n \n  Lead-3      40.42      17.62      36.67      16.30      1.60      11.95  \n \n  PTGEN      (See      et      al.,      2017)      36.44      15.66      33.42      29.70      9.21      23.24  \n \n  PTGEN+COV      (See      et      al.,      2017)      39.53      17.28      36.38      28.10      8.02      21.72  \n \n  UniLM      43.33      20.21      40.51   \n \n-   \n \n-   \n \n-       BERTSUMABS      (Liu   \n \n&      Lapata,      2019)      41.72      19.39      38.76      38.76      16.33      31.15  \n \n  BERTSUMEXTABS      (Liu   \n \n&      Lapata,      2019)      42.13      19.60      39.18      38.81      16.50      31.27 BART      44.16      21.28      40.90      45.14      22.27      37.25 Table      3:      Results      on      two      standard      summarization      datasets."
            },
            {
              "id": 256,
              "title": "CNN/DailyMail      XSum       RI      R2      RL      RI      R2      RL - Chunk 2",
              "type": "chunk",
              "children": [],
              "content": "BART      outperforms      previous      work      on      summarization      on\n \n \n  on      the      CNN/DM      summarization      dataset,      we      hypothe-      ConvAI2  \n \n  sised      that      larger      pre-trained      models      may      be      better      able      Valid      F1      Valid      PPL  \n \n  to   \n \n|      fi      this      task."
            },
            {
              "id": 257,
              "title": "CNN/DailyMail      XSum       RI      R2      RL      RI      R2      RL - Chunk 3",
              "type": "chunk",
              "children": [],
              "content": "To      help      th      del      better      fit      th  \n \n  RE      aa      ae      Pai      Cae      Seq2Seq+      Attention      16.02      35.07  \n \n  data,      we      disabled      dropout      for      the      final      10%      of      training   \n \n. . Best      System      19.09      17.51  \n \n  steps. We      use      the      same      pre-training      data      as      Liu      et      al."
            },
            {
              "id": 258,
              "title": "CNN/DailyMail      XSum       RI      R2      RL      RI      R2      RL - Chunk 4",
              "type": "chunk",
              "children": [],
              "content": "(2019),      consisting      of      160Gb      of      news,      books,      stories,  \n \n  and      web      text."
            },
            {
              "id": 259,
              "title": "CNN/DailyMail      XSum       RI      R2      RL      RI      R2      RL - Chunk 5",
              "type": "chunk",
              "children": [],
              "content": "BART      20.72      11.85 5.2      Discriminative      Tasks  \n \n  Table   \n \n2      compares      the      performance      of      BART      with      sev-  \n \n  eral      recent      approaches      on      the      well-studied      SQUAD      and  \n \n  GLUE      tasks      (Warstadt      et      al.,      2018;      Socher      et      al.,      2013;  \n \n  Dolan   \n \n&      Brockett,      2005;      Agirre      et      al.,      2007;      Williams  \n \n  et      al.,      2018;      Dagan      et      al.,      2006;      Levesque      et      al.,      2011)."
            },
            {
              "id": 260,
              "title": "CNN/DailyMail      XSum       RI      R2      RL      RI      R2      RL - Chunk 6",
              "type": "chunk",
              "children": [],
              "content": "Table      4:      BART      outperforms      previous      work      on      conver-  \n \n  sational      response      generation. Perplexities      are      renor-  \n \n  malized      based      on      official      tokenizer      for      ConvAI2. The      most      directly      comparable      baseline      is      ROBERTa,  \n \n  which      was      pre-trained      with      the      same      resources,      but  \n \n  a      different      objective."
            },
            {
              "id": 261,
              "title": "CNN/DailyMail      XSum       RI      R2      RL      RI      R2      RL - Chunk 7",
              "type": "chunk",
              "children": [],
              "content": "Overall,      BART      performs      simi-  \n \n  larly,      with      only      small      differences      between      the      models  \n \n  on      most      tasks. suggesting      that      BART’s      improvements  \n \n  on      generation      tasks      do      not      come      at      the      expense      of      clas-  \n \n  sification      performance."
            },
            {
              "id": 262,
              "title": "CNN/DailyMail      XSum       RI      R2      RL      RI      R2      RL - Chunk 8",
              "type": "chunk",
              "children": [],
              "content": "Summarization      To      provide   \n \na      comparison      with      the  \n \n  state-of-the-art      in      summarization,      we      present      results  \n \n  on      two      summarization      datasets,      CNN/DailyMail      and  \n \n  XSum,      which      have      distinct      properties. Summaries      in      the      CNN/DailyMail      tend      to      resemble  \n \n  source      sentences."
            },
            {
              "id": 263,
              "title": "CNN/DailyMail      XSum       RI      R2      RL      RI      R2      RL - Chunk 9",
              "type": "chunk",
              "children": [],
              "content": "Extractive      models      do      well      here,      and  \n \n  even      the      baseline      of      the      first-three      source      sentences      is  \n \n  highly      competitive. Nevertheless,      BART      outperforms  \n \n  all      existing      work. 5.3. Generation      Tasks\nGeneration      Tasks\n |       We      also      experiment      with      several      text      generation      tasks."
            },
            {
              "id": 264,
              "title": "CNN/DailyMail      XSum       RI      R2      RL      RI      R2      RL - Chunk 10",
              "type": "chunk",
              "children": [],
              "content": "BART      is      fine-tuned      as      a      standard      sequence-to-sequence       model      from      the      input      to      the      output      text. During      fine-       tuning      we      use      a      label      smoothed      cross      entropy      loss       (Pereyra      et      al.,      2017),      with      the      smoothing      parameter       set      to      0.1."
            },
            {
              "id": 265,
              "title": "CNN/DailyMail      XSum       RI      R2      RL      RI      R2      RL - Chunk 11",
              "type": "chunk",
              "children": [],
              "content": "During      generation,      we      set      beam      size      as      5,       remove      duplicated      trigrams      in      beam      search,      and      tuned       the      model      with      min-len,      max-len,      length      penalty      on      the       validation      set      (Fan      et      al.,      2017)."
            },
            {
              "id": 266,
              "title": "CNN/DailyMail      XSum       RI      R2      RL      RI      R2      RL - Chunk 12",
              "type": "chunk",
              "children": [],
              "content": "| 6.0      points      on      all      ROUGE      metrics—trepresenting   \n \na      sig-  \n \n  nificant      advance      in      performance      on      this      problem. Qual-  \n \n  itatively,      sample      quality      is      high      (see      86). In      contrast,      XSum      is      highly      abstractive,      and      extrac-  \n \n  tive      models      perform      poorly."
            },
            {
              "id": 267,
              "title": "CNN/DailyMail      XSum       RI      R2      RL      RI      R2      RL - Chunk 13",
              "type": "chunk",
              "children": [],
              "content": "BART      outperforms      the  \n \n  best      previous      work,      which      leverages      BERT,      by      roughly  \n \n  Dialogue      We      evaluate      dialogue      response      generation  \n \n  on      CONVAI2      (Dinan      et      al.,      2019),      in      which      agents  \n \n  must      generate      responses      conditioned      on      both      the      pre-  \n \n  vious      context      and   \n \na      textually-specified      persona."
            },
            {
              "id": 268,
              "title": "CNN/DailyMail      XSum       RI      R2      RL      RI      R2      RL - Chunk 14",
              "type": "chunk",
              "children": [],
              "content": "BART  \n \n  outperforms      previous      work      on      two      automated      metrics."
            }
          ],
          "content": ""
        },
        {
          "id": 269,
          "title": "5.3.      Generation      Tasks",
          "type": "section",
          "children": [
            {
              "id": 270,
              "title": "5.3.      Generation      Tasks - Chunk 1",
              "type": "chunk",
              "children": [],
              "content": "5.3. Generation      Tasks\nGeneration      Tasks\n |       We      also      experiment      with      several      text      generation      tasks. BART      is      fine-tuned      as      a      standard      sequence-to-sequence       model      from      the      input      to      the      output      text."
            },
            {
              "id": 271,
              "title": "5.3.      Generation      Tasks - Chunk 2",
              "type": "chunk",
              "children": [],
              "content": "During      fine-       tuning      we      use      a      label      smoothed      cross      entropy      loss       (Pereyra      et      al.,      2017),      with      the      smoothing      parameter       set      to      0.1."
            },
            {
              "id": 272,
              "title": "5.3.      Generation      Tasks - Chunk 3",
              "type": "chunk",
              "children": [],
              "content": "During      generation,      we      set      beam      size      as      5,       remove      duplicated      trigrams      in      beam      search,      and      tuned       the      model      with      min-len,      max-len,      length      penalty      on      the       validation      set      (Fan      et      al.,      2017)."
            },
            {
              "id": 273,
              "title": "5.3.      Generation      Tasks - Chunk 4",
              "type": "chunk",
              "children": [],
              "content": "| 6.0      points      on      all      ROUGE      metrics—trepresenting   \n \na      sig-  \n \n  nificant      advance      in      performance      on      this      problem. Qual-  \n \n  itatively,      sample      quality      is      high      (see      86). In      contrast,      XSum      is      highly      abstractive,      and      extrac-  \n \n  tive      models      perform      poorly."
            },
            {
              "id": 274,
              "title": "5.3.      Generation      Tasks - Chunk 5",
              "type": "chunk",
              "children": [],
              "content": "BART      outperforms      the  \n \n  best      previous      work,      which      leverages      BERT,      by      roughly  \n \n  Dialogue      We      evaluate      dialogue      response      generation  \n \n  on      CONVAI2      (Dinan      et      al.,      2019),      in      which      agents  \n \n  must      generate      responses      conditioned      on      both      the      pre-  \n \n  vious      context      and   \n \na      textually-specified      persona."
            },
            {
              "id": 275,
              "title": "5.3.      Generation      Tasks - Chunk 6",
              "type": "chunk",
              "children": [],
              "content": "BART  \n \n  outperforms      previous      work      on      two      automated      metrics."
            }
          ],
          "content": ""
        },
        {
          "id": 276,
          "title": "Rl      R2      RL",
          "type": "section",
          "children": [
            {
              "id": 277,
              "title": "Rl      R2      RL - Chunk 1",
              "type": "chunk",
              "children": [],
              "content": "Rl      R2      RL\n \n \n  Best      Extractive      23.55      3.1      17.5  \n \n  Language      Model      27.8      47      23.1  \n \n  Seq2Seq      28.3      5.1      22.8  \n \n  Seq2Seq      Multitask      28.9      54      23.1  \n \n  BART      30.6      6.2      24.3  \n \n  Table      5:      BART      achieves      state-of-the-art      results      on  \n \n  the      challenging      ELI5      abstractive      question      answering  \n \n  dataset."
            },
            {
              "id": 278,
              "title": "Rl      R2      RL - Chunk 2",
              "type": "chunk",
              "children": [],
              "content": "Comparison      models      are      from      Fan      et      al. (2019)."
            }
          ],
          "content": ""
        },
        {
          "id": 279,
          "title": "RO-EN",
          "type": "section",
          "children": [
            {
              "id": 280,
              "title": "RO-EN - Chunk 1",
              "type": "chunk",
              "children": [],
              "content": "RO-EN\nBaseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96\n \n \n  Table      6:      The      performance      (BLEU)      of      baseline      and  \n \n  BART      on      WMT’16      RO-EN      augmented      with      back-  \n \n  translation      data. BART      improves      over   \n \na      strong      back-  \n \n  translation      (BT)      baseline      by      using      monolingual      English  \n \n  pre-training."
            },
            {
              "id": 281,
              "title": "RO-EN - Chunk 2",
              "type": "chunk",
              "children": [],
              "content": "Abstractive      QA      We      use      the      recently      proposed      ELIS  \n \n  dataset      to      test      the      model’s      ability      to      generate      long      free-  \n \n  form      answers."
            },
            {
              "id": 282,
              "title": "RO-EN - Chunk 3",
              "type": "chunk",
              "children": [],
              "content": "We      find      BART      outperforms      the      best      pre-  \n \n  vious      work      by      1.2      ROUGE-L,      but      the      dataset      remains  \n \n  a      challenging,      because      answers      are      only      weakly      speci-  \n \n  fied      by      the      question."
            },
            {
              "id": 283,
              "title": "RO-EN - Chunk 4",
              "type": "chunk",
              "children": [],
              "content": "5.4      Translation\ntion  \n \n  We      also      evaluated      performance      on      WMT16      Romanian-  \n \n  English,      augmented      with      back-translation      data  \n \n  from      Sennrich      et      al. (2016)."
            },
            {
              "id": 284,
              "title": "RO-EN - Chunk 5",
              "type": "chunk",
              "children": [],
              "content": "We      use   \n \na      6-layer  \n \n  transformer      source      encoder      to      map      Romanian      into  \n \n  a      representation      that      BART      is      able      to      de-noise      into  \n \n  English,      following      the      approach      introduced      in      83.4. Experiment      results      are      presented      in      Table      6."
            },
            {
              "id": 285,
              "title": "RO-EN - Chunk 6",
              "type": "chunk",
              "children": [],
              "content": "We  \n \n  compare      our      results      against   \n \na      baseline      Transformer  \n \n  architecture      (Vaswani      et      al.,      2017)      with      Transformer-  \n \n  large      settings      (the      baseline      row). We      show      the  \n \n  performance      of      both      steps      of      our      model      in      the      fixed  \n \n  BART      and      tuned      BART      rows."
            },
            {
              "id": 286,
              "title": "RO-EN - Chunk 7",
              "type": "chunk",
              "children": [],
              "content": "For      each      row      we  \n \n  experiment      on      the      original      WMT16      Romanian-English  \n \n  augmented      with      back-translation      data. We      use   \n \na       beam      width      of   \n \n5      and   \n \na      length      penalty      of   \n \na   \n \n=      1."
            },
            {
              "id": 287,
              "title": "RO-EN - Chunk 8",
              "type": "chunk",
              "children": [],
              "content": "Preliminary      results      suggested      that      our      approach      was  \n \n  less      effective      without      back-translation      data,      and      prone  \n \n  to      overfitting—future      work      should      explore      additional  \n \n  regularization      techniques."
            },
            {
              "id": 288,
              "title": "RO-EN - Chunk 9",
              "type": "chunk",
              "children": [],
              "content": "6\n \n   Qualitative      Analysis  \n \n  BART      shows      large      improvements      on      summarization  \n \n  metrics,      of      up      to   \n \n6      points      over      the      prior      state-of-the-art. To      understand      BART’s      performance      beyond      automated  \n \n  metrics,      we      analyse      its      generations      qualitatively. Table   \n \n7      shows      example      summaries      generated      by  \n \n  BART."
            },
            {
              "id": 289,
              "title": "RO-EN - Chunk 10",
              "type": "chunk",
              "children": [],
              "content": "Examples      are      taken      from      WikiNews      articles  \n \n  published      after      the      creation      of      the      pre-training      corpus,  \n \n  to      eliminate      the      possibility      of      the      events      described      be-  \n \n  ing      present      in      the      model’s      training      data. Following  \n \n  Narayan      et      al."
            },
            {
              "id": 290,
              "title": "RO-EN - Chunk 11",
              "type": "chunk",
              "children": [],
              "content": "(2018),      we      remove      the      first      sentence      of  \n \n  the      article      prior      to      summarizing      it,      so      there      is      no      easy  \n \n  extractive      summary      of      the      document. Unsurprisingly,      model      output      is      fluent      and      grammat-  \n \n  ical      English."
            },
            {
              "id": 291,
              "title": "RO-EN - Chunk 12",
              "type": "chunk",
              "children": [],
              "content": "However,      model      output      is      also      highly      ab-  \n \n  stractive,      with      few      phrases      copied      from      the      input."
            },
            {
              "id": 292,
              "title": "RO-EN - Chunk 13",
              "type": "chunk",
              "children": [],
              "content": "The  \n \n  output      is      also      generally      factually      accurate,      and      inte-  \n \n  grates      supporting      evidence      from      across      the      input      doc-  \n \n  ument      with      background      knowledge      (for      example,      cor-  \n \n  rectly      completing      names,      or      inferring      that      PG&E      oper-  \n \n  ates      in      California)."
            },
            {
              "id": 293,
              "title": "RO-EN - Chunk 14",
              "type": "chunk",
              "children": [],
              "content": "In      the      first      example,      inferring      that  \n \n  fish      are      protecting      reefs      from      global      warming      requires  \n \n  non-trivial      inference      from      the      text. However,      the      claim  \n \n  that      the      work      was      published      in      Science      is      not      supported  \n \n  by      the      source."
            },
            {
              "id": 294,
              "title": "RO-EN - Chunk 15",
              "type": "chunk",
              "children": [],
              "content": "These      samples      demonstrate      that      the      BART      pretrain-  \n \n  ing      has      learned   \n \na      strong      combination      of      natural      lan-  \n \n  guage      understanding      and      generation. 7\n \n   Related      Work  \n \n  Early      methods      for      pretraining      were      based      on      language  \n \n  models."
            },
            {
              "id": 295,
              "title": "RO-EN - Chunk 16",
              "type": "chunk",
              "children": [],
              "content": "GPT      (Radford      et      al.,      2018)      only      models      left-  \n \n  ward      context,      which      is      problematic      for      some      tasks. ELMo      (Peters      et      al.,      2018)      concatenates      left-only      and  \n \n  right-only      representations,      but      does      not      pre-train      inter-  \n \n  actions      between      these      features. Radford      et      al."
            },
            {
              "id": 296,
              "title": "RO-EN - Chunk 17",
              "type": "chunk",
              "children": [],
              "content": "(2019)  \n \n  demonstrated      that      very      large      language      models      can      act       as      unsupervised      multitask      models. MASS      (Song      et      al.,      2019)      is      perhaps      the      most      similar  \n \n  model      to      BART."
            },
            {
              "id": 297,
              "title": "RO-EN - Chunk 18",
              "type": "chunk",
              "children": [],
              "content": "An      input      sequence      where   \n \na      contiguous  \n \n  span      of      tokens      is      masked      is      mapped      to   \n \na      sequence      con-  \n \n  sisting      of      the      missing      tokens. MASS      is      less      effective  \n \n  for      discriminative      tasks,      because      disjoint      sets      of      tokens  \n \n  are      fed      into      the      encoder      and      decoder."
            },
            {
              "id": 298,
              "title": "RO-EN - Chunk 19",
              "type": "chunk",
              "children": [],
              "content": "BERT      (Devlin      et      al.,      2019)      introduced      masked      lan-  \n \n  guage      modelling,      which      allows      pre-training      to      learn      in-  \n \n  teractions      between      left      and      right      context      words."
            },
            {
              "id": 299,
              "title": "RO-EN - Chunk 20",
              "type": "chunk",
              "children": [],
              "content": "Re-  \n \n  cent      work      has      shown      that      very      strong      performance      can  \n \n  be      achieved      by      training      for      longer      (Liu      et      al.,      2019),  \n \n  by      tying      parameters      across      layers      (Lan      et      al.,      2019),  \n \n  and      by      masking      spans      instead      of      words      (Joshi      et      al.,  \n \n  2019)."
            },
            {
              "id": 300,
              "title": "RO-EN - Chunk 21",
              "type": "chunk",
              "children": [],
              "content": "Predictions      are      not      made      auto-regressively,      re-  \n \n  ducing      the      effectiveness      of      BERT      for      generation      tasks. UniLM      (Dong      et      al.,      2019)      fine-tunes      BERT      with      an  \n \n  ensemble      of      masks,      some      of      which      allow      only      leftward  \n \n  context."
            },
            {
              "id": 301,
              "title": "RO-EN - Chunk 22",
              "type": "chunk",
              "children": [],
              "content": "Like      BART,      this      allows      UniLM      to      be      used      for  \n \n  both      generative      and      discriminative      tasks. A      difference  \n \n  is      that      UniLM      predictions      are      conditionally      indepen-  \n \n  dent,      whereas      BART’s      are      autoregressive."
            },
            {
              "id": 302,
              "title": "RO-EN - Chunk 23",
              "type": "chunk",
              "children": [],
              "content": "BART      re-  \n \n  duces      the      mismatch      between      pre-training      and      genera-  \n \n  tion      tasks,      because      the      decoder      is      always      trained      on      un-  \n \n  corrupted      context."
            },
            {
              "id": 303,
              "title": "RO-EN - Chunk 24",
              "type": "chunk",
              "children": [],
              "content": "XL-Net      (Yang      et      al.,      2019)      extends      BERT      by      pre-\n |       Source      Document      (abbreviated) |       BART      Summary\n |       The      researchers      examined      three      types      of      coral      in      reefs      off      the coast of Fiji The researchers found when fish were plentiful,       they      would      eat      algae      and      seaweed      off      the      corals,      which      appeared       to      leave      them      more      resistant      to      the      bacterium      Vibrio      coralliilyti-       cus,      a      bacterium      associated      with      bleaching."
            },
            {
              "id": 304,
              "title": "RO-EN - Chunk 25",
              "type": "chunk",
              "children": [],
              "content": "The      researchers      sug-       gested      the      algae,      like      warming      temperatures,      might      render      the       corals’      chemical      defenses      less      effective,      and      the      fish      were      pro-       tecting      the      coral      by      removing      the      algae."
            },
            {
              "id": 305,
              "title": "RO-EN - Chunk 26",
              "type": "chunk",
              "children": [],
              "content": "|       Fisheries      off      the      coast      of      Fiji      are      protect-       ing      coral      reefs      from      the      effects      of      global       warming,      according      to      a      study      in      the      jour-       nal      Science."
            },
            {
              "id": 306,
              "title": "RO-EN - Chunk 27",
              "type": "chunk",
              "children": [],
              "content": "|       Sacoolas,      who      has      immunity      as      a      diplomat’s      wife,      was      involved in a traffic collision Prime Minister Johnson was questioned       about      the      case      while      speaking      to      the      press      at      a      hospital      in      Wat- ford."
            },
            {
              "id": 307,
              "title": "RO-EN - Chunk 28",
              "type": "chunk",
              "children": [],
              "content": "He said, “I hope that Anne Sacoolas will come back       if      we      can’t      resolve      it      then      of      course      I      will      be      raising      it      myself       personally      with      the      White      House.” |       Boris      Johnson      has      said      he      will      raise      the      is-       sue      of      US      diplomat      Anne      Sacoolas’      diplo-       matic      immunity      with      the      White      House."
            },
            {
              "id": 308,
              "title": "RO-EN - Chunk 29",
              "type": "chunk",
              "children": [],
              "content": "|       According      to      Syrian      state      media,      government      forces      began      de- ploying into previously SDF controlled territory yesterday. On      October      6,      US      President      Donald      Trump      and      Turkish      Presi-       dent      Recep      Tayyip      Erdoan      spoke      on      the      phone."
            },
            {
              "id": 309,
              "title": "RO-EN - Chunk 30",
              "type": "chunk",
              "children": [],
              "content": "Then      both      na-       tions      issued      statements      speaking      of      an      imminent      incursion      into northeast Syria On Wednesday, Turkey began a military       offensive      with      airstrikes      followed      by      a      ground      invasion."
            },
            {
              "id": 310,
              "title": "RO-EN - Chunk 31",
              "type": "chunk",
              "children": [],
              "content": "|       Syrian      government      forces      have      entered       territory      held      by      the      US-backed      Syrian       Democratic      Forces      (SDF)      in      response      to       Turkey’s      incursion      into      the      region."
            },
            {
              "id": 311,
              "title": "RO-EN - Chunk 32",
              "type": "chunk",
              "children": [],
              "content": "|       This      is      the      first      time      anyone      has      been      recorded      to      run      a      full       marathon      of      42.195      kilometers      (approximately      26      miles)      under       this      pursued      landmark      time. It      was      not,      however,      an      officially       sanctioned      world      record,      as      it      was      not      an      ’open      race”      of      the       IAAF."
            },
            {
              "id": 312,
              "title": "RO-EN - Chunk 33",
              "type": "chunk",
              "children": [],
              "content": "His      time      was      1      hour      59      minutes      40.2      seconds. Kipchoge       ran      in      Vienna,      Austria. It      was      an      event      specifically      designed      to       help      Kipchoge      break      the      two      hour      barrier. |       Kenyan      runner      Eliud      Kipchoge      has      run      a       marathon      in      less      than      two      hours."
            },
            {
              "id": 313,
              "title": "RO-EN - Chunk 34",
              "type": "chunk",
              "children": [],
              "content": "|       PG&E      stated      it      scheduled      the      blackouts      in      response      to      forecasts       for      high      winds      amid      dry      conditions. The      aim      is      to      reduce      the      risk       of      wildfires."
            },
            {
              "id": 314,
              "title": "RO-EN - Chunk 35",
              "type": "chunk",
              "children": [],
              "content": "Nearly      800      thousand      customers      were      scheduled      to       be      affected      by      the      shutoffs      which      were      expected      to      last      through       at      least      midday      tomorrow. |       Power      has      been      turned      off      to      millions      of       customers      in      California      as      part      of      a      power       shutoff      plan."
            },
            {
              "id": 315,
              "title": "RO-EN - Chunk 36",
              "type": "chunk",
              "children": [],
              "content": "dicting      masked      tokens      auto-regressively      in   \n \na      permuted  \n \n  order. This      objective      allows      predictions      to      condition      on  \n \n  both      left      and      right      context. In      contrast,      the      BART      de-  \n \n  coder      works      left-to-right      during      pre-training,      matching  \n \n  the      setting      during      generation."
            },
            {
              "id": 316,
              "title": "RO-EN - Chunk 37",
              "type": "chunk",
              "children": [],
              "content": "Several      papers      have      explored      using      pre-trained      rep-  \n \n  resentations      to      improve      machine      translation."
            },
            {
              "id": 317,
              "title": "RO-EN - Chunk 38",
              "type": "chunk",
              "children": [],
              "content": "The  \n \n  largest      improvements      have      come      from      pre-training      on  \n \n  both      source      and      target      languages      (Song      et      al.,      2019;  \n \n  Lample   \n \n&      Conneau,      2019),      but      this      requires      pre-  \n \n  training      on      all      languages      of      interest."
            },
            {
              "id": 318,
              "title": "RO-EN - Chunk 39",
              "type": "chunk",
              "children": [],
              "content": "Other      work      has  \n \n  shown      that      encoders      can      be      improved      using      pre-trained  \n \n  representations      (Edunov      et      al.,      2019),      but      gains      in      de-  \n \n  coders      are      more      limited. We      show      how      BART      can      be  \n \n  used      to      improve      machine      translation      decoders."
            },
            {
              "id": 319,
              "title": "RO-EN - Chunk 40",
              "type": "chunk",
              "children": [],
              "content": "8\n \n   Conclusions  \n \n  We      introduced      BART,   \n \na      pre-training      approach      that  \n \n  learns      to      map      corrupted      documents      to      the      original. BART      achieves      similar      performance      to      ROBERTa      on  \n \n  discriminative      tasks,      while      achieving      new      state-of-the-  \n \n  art      results      on   \n \na      number      of      text      generation      tasks."
            },
            {
              "id": 320,
              "title": "RO-EN - Chunk 41",
              "type": "chunk",
              "children": [],
              "content": "Fu-  \n \n  ture      work      should      explore      new      methods      for      corrupting  \n \n  documents      for      pre-training,      perhaps      tailoring      them      to  \n \n  specific      end      tasks. References\n \n \n  Eneko      Agirre,      Llu’is      M‘arquez,      and      Richard      Wicen-  \n \n  towski      (eds.). Proceedings      of      the      Fourth      Interna-  \n \n  tional      Workshop      on      Semantic      Evaluations      (SemEval-  \n \n  2007)."
            },
            {
              "id": 321,
              "title": "RO-EN - Chunk 42",
              "type": "chunk",
              "children": [],
              "content": "Association      for      Computational      Linguistics,  \n \n  Prague,      Czech      Republic,      June      2007. Ido      Dagan,      Oren      Glickman,      and      Bernardo      Magnini. The      PASCAL      recognising      textual      entailment      chal-  \n \n  lenge. In      Machine      learning      challenges. evaluat-  \n \n  ing      predictive      uncertainty,      visual      object      classifica-  \n \n  tion,      and      recognising      tectual      entailment,      pp."
            },
            {
              "id": 322,
              "title": "RO-EN - Chunk 43",
              "type": "chunk",
              "children": [],
              "content": "177—  \n \n  190. Springer,      2006. Jacob      Devlin,      Ming-Wei      Chang,      Kenton      Lee,      and  \n \n  Kristina      Toutanova. BERT:      Pre-training      of      deep  \n \n  bidirectional      transformers      for      language      understand-  \n \n  ing."
            },
            {
              "id": 323,
              "title": "RO-EN - Chunk 44",
              "type": "chunk",
              "children": [],
              "content": "In      Proceedings      of      the      2019      Conference      of      the  \n \n  North      American      Chapter      of      the      Association      for      Com-  \n \n  putational      Linguistics:      Human      Language      Technolo-  \n \n  gies,      Volume   \n \nI      (Long      and      Short      Papers),      pp. 4171-  \n \n  4186,      Minneapolis,      Minnesota,      June      2019. Associa-  \n \n  tion      for      Computational      Linguistics."
            },
            {
              "id": 324,
              "title": "RO-EN - Chunk 45",
              "type": "chunk",
              "children": [],
              "content": "doi:      10.18653/  \n \n  vI/N19-1423. URL      https://www.aclweb. org/anthology/N19-1423. Emily      Dinan,      Varvara      Logacheva,      Valentin      Malykh,  \n \n  Alexander      Miller,      Kurt      Shuster,      Jack      Urbanek,  \n \n  Douwe      Kiela,      Arthur      Szlam,      Iulian      Serban,      Ryan  \n \n  Lowe,      et      al. The      second      conversational      in-  \n \n  telligence      challenge      (convai2)."
            },
            {
              "id": 325,
              "title": "RO-EN - Chunk 46",
              "type": "chunk",
              "children": [],
              "content": "arXiv      preprint  \n \n  arXiv:      1902.00098,      2019. William   \n \nB      Dolan      and      Chris      Brockett. Automatically  \n \n  constructing   \n \na      corpus      of      sentential      paraphrases. In  \n \n  Proceedings      of      the      International      Workshop      on      Para-  \n \n  phrasing,      2005."
            },
            {
              "id": 326,
              "title": "RO-EN - Chunk 47",
              "type": "chunk",
              "children": [],
              "content": "Li      Dong,      Nan      Yang,      Wenhui      Wang,      Furu      Wei,      Xi-  \n \n  aodong      Liu,      Yu      Wang,      Jianfeng      Gao,      Ming      Zhou,  \n \n  and      Hsiao-Wuen      Hon. Unified      language      model      pre-  \n \n  training      for      natural      language      understanding      and      gen-  \n \n  eration. arXiv      preprint      arXiv:      1905.03197,      2019. Sergey      Edunov,      Alexei      Baevski,      and      Michael      Auli."
            },
            {
              "id": 327,
              "title": "RO-EN - Chunk 48",
              "type": "chunk",
              "children": [],
              "content": "Pre-trained      language      model      representations      for      lan-  \n \n  guage      generation. In      Proceedings      of      the      2019      Con-  \n \n  ference      of      the      North      American      Chapter      of      the      Asso-  \n \n  ciation      for      Computational      Linguistics:      Human      Lan-  \n \n  guage      Technologies,      Volume   \n \n1      (Long      and      Short      Pa-  \n \n  pers),      2019."
            },
            {
              "id": 328,
              "title": "RO-EN - Chunk 49",
              "type": "chunk",
              "children": [],
              "content": "Angela      Fan,      David      Grangier,      and      Michael      Auli. Con-  \n \n  trollable      abstractive      summarization. arXiv      preprint  \n \n  arXiv:      1711.05217,      2017. Angela      Fan,      Yacine      Jernite,      Ethan      Perez,      David  \n \n  Grangier,      Jason      Weston,      and      Michael      Auli. Eli5:  \n \n  Long      form      question      answering. arXiv      preprint  \n \n  arXiv:      1907.09190,      2019."
            },
            {
              "id": 329,
              "title": "RO-EN - Chunk 50",
              "type": "chunk",
              "children": [],
              "content": "Dan      Hendrycks      and      Kevin      Gimpel. Gaussian      error      lin-  \n \n  ear      units      (gelus). arXiv      preprint      arXiv:      1606.08415,  \n \n  2016. Karl      Moritz      Hermann,      Tomas      Kocisky,      Edward  \n \n  Grefenstette,      Lasse      Espeholt,      Will      Kay,      Mustafa      Su-  \n \n  leyman,      and      Phil      Blunsom. Teaching      machines      to  \n \n  read      and      comprehend."
            },
            {
              "id": 330,
              "title": "RO-EN - Chunk 51",
              "type": "chunk",
              "children": [],
              "content": "In      Advances      in      neural      infor-  \n \n  mation      processing      systems,      pp. 1693-1701,      2015. Mandar      Joshi,      Danqi      Chen,      Yinhan      Liu,      Daniel   \n \nS      Weld,  \n \n  Luke      Zettlemoyer,      and      Omer      Levy. Spanbert:      Im-  \n \n  proving      pre-training      by      representing      and      predicting  \n \n  spans. arXiv      preprint      arXiv:      1907.10529,      2019."
            },
            {
              "id": 331,
              "title": "RO-EN - Chunk 52",
              "type": "chunk",
              "children": [],
              "content": "Guillaume      Lample      and      Alexis      Conneau. —      Cross-  \n \n  lingual      language      model      pretraining. arXiv      preprint  \n \n  arXiv:      1901.07291,      2019. Zhenzhong      Lan,      Mingda      Chen,      Sebastian      Goodman,  \n \n  Kevin      Gimpel,      Piyush      Sharma,      and      Radu      Sori-  \n \n  cut. Albert:   \n \nA      lite      bert      for      self-supervised      learn-  \n \n  ing      of      language      representations."
            },
            {
              "id": 332,
              "title": "RO-EN - Chunk 53",
              "type": "chunk",
              "children": [],
              "content": "arXiv      preprint  \n \n  arXiv:      1909.11942,      2019. Hector   \n \nJ      Levesque,      Ernest      Davis,      and      Leora      Morgen-  \n \n  stern. The      Winograd      schema      challenge. In      AAAI  \n \n  Spring      Symposium:      Logical      Formalizations      of      Com-  \n \n  monsense      Reasoning,      volume      46,      pp. 47,      2011. Yang      Liu      and      Mirella      Lapata. tion      with      pretrained      encoders."
            },
            {
              "id": 333,
              "title": "RO-EN - Chunk 54",
              "type": "chunk",
              "children": [],
              "content": "arXiv:      1908.08345,      2019. Text      summariza-  \n \n  arXiv      preprint  \n \n  Yinhan      Liu,      Myle      Ott,      Naman      Goyal,      Jingfei      Du,      Man-  \n \n  dar      Joshi,      Dangi      Chen,      Omer      Levy,      Mike      Lewis,  \n \n  Luke      Zettlemoyer,      and      Veselin      Stoyanov. Roberta:\n \n \n \nA      robustly      optimized      bert      pretraining      approach. arXiv      preprint      arXiv:      1907.11692,      2019."
            },
            {
              "id": 334,
              "title": "RO-EN - Chunk 55",
              "type": "chunk",
              "children": [],
              "content": "Tomas      Mikolov,      Kai      Chen,      Greg      Corrado,      and      Jeffrey  \n \n  Dean. Efficient      estimation      of      word      representations  \n \n  in      vector      space. arXiv      preprint      arXiv:1301.3781,  \n \n  2013. Shashi      Narayan,      Shay   \n \nB      Cohen,      and      Mirella      Lapata. Don’t      give      me      the      details,      just      the      summary!"
            },
            {
              "id": 335,
              "title": "RO-EN - Chunk 56",
              "type": "chunk",
              "children": [],
              "content": "topic-  \n \n  aware      convolutional      neural      networks      for      extreme  \n \n  summarization. arXiv      preprint      arXiv:      1808.08745,  \n \n  2018. Gabriel      Pereyra,      George      Tucker,      Jan      Chorowski,  \n \n  Lukasz      Kaiser,      and      Geoffrey      Hinton. Regularizing  \n \n  neural      networks      by      penalizing      confident      output      dis-  \n \n  tributions. arXiv      preprint      arXiv:      1701.06548,      2017."
            },
            {
              "id": 336,
              "title": "RO-EN - Chunk 57",
              "type": "chunk",
              "children": [],
              "content": "Matthew      E      Peters,      Mark      Neumann,      Mohit      Iyyer,      Matt  \n \n  Gardner,      Christopher      Clark,      Kenton      Lee,      and      Luke  \n \n  Zettlemoyer. Deep      contextualized      word      representa-  \n \n  tions. arXiv      preprint      arXiv:      1802.05365,      2018. Alec      Radford,      Karthik      Narasimhan,      Tim      Salimans,  \n \n  and      Ilya      Sutskever."
            },
            {
              "id": 337,
              "title": "RO-EN - Chunk 58",
              "type": "chunk",
              "children": [],
              "content": "Improving      language      un-  \n \n  derstanding      by      generative      pre-training. URL  \n \n  https://s3-us-west-2. |      amazonaws. —      com/openai-  \n \n  assets/researchcovers/languageunsupervised/language  \n \n  understanding      paper. pdf,      2018. Alec      Radford,      Jeffrey      Wu,      Rewon      Child,      David      Luan,  \n \n  Dario      Amodei,      and      Ilya      Sutskever. Language      mod-  \n \n  els      are      unsupervised      multitask      learners."
            },
            {
              "id": 338,
              "title": "RO-EN - Chunk 59",
              "type": "chunk",
              "children": [],
              "content": "OpenAI  \n \n  Blog,      1(8),      2019. Pranav      Rajpurkar,      Jian      Zhang,      Konstantin      Lopyrev,  \n \n  and      Percy      Liang. Squad:      100,000+      questions      for  \n \n  machine      comprehension      of      text. arXiv      preprint  \n \n  arXiv:      1606.05250,      2016. Abigail      See,      Peter   \n \nJ      Liu,      and      Christopher   \n \nD       Manning. Get      to      the      point:      Summarization  \n \n  with      pointer-generator      networks."
            },
            {
              "id": 339,
              "title": "RO-EN - Chunk 60",
              "type": "chunk",
              "children": [],
              "content": "arXiv      preprint  \n \n  arXiv:      1704.04368,      2017. Rico      Sennrich,      Barry      Haddow,      and      Alexandra      Birch. Edinburgh      neural      machine      translation      systems      for  \n \n  WMT      16. In      Proceedings      of      the      First      Conference  \n \n  on      Machine      Translation:      Volume      2,      Shared      Task      Pa-  \n \n  pers,      2016."
            },
            {
              "id": 340,
              "title": "RO-EN - Chunk 61",
              "type": "chunk",
              "children": [],
              "content": "Richard      Socher,      Alex      Perelygin,      Jean      Wu,      Jason  \n \n  Chuang,      Christopher   \n \nD      Manning,      Andrew      Ng,      and  \n \n  Christopher      Potts. Recursive      deep      models      for      se-  \n \n  mantic      compositionality      over   \n \na      sentiment      treebank. In      Proceedings      of      EMNLP,      pp. 1631-1642,      2013."
            },
            {
              "id": 341,
              "title": "RO-EN - Chunk 62",
              "type": "chunk",
              "children": [],
              "content": "Kaitao      Song,      Xu      Tan,      Tao      Qin,      Jianfeng      Lu,      and      Tie-  \n \n  Yan      Liu. Mass:      Masked      sequence      to      sequence      pre-  \n \n  training      for      language      generation. In      International  \n \n  Conference      on      Machine      Learning,      2019."
            },
            {
              "id": 342,
              "title": "RO-EN - Chunk 63",
              "type": "chunk",
              "children": [],
              "content": "Ashish      Vaswani,      Noam      Shazeer,      Niki      Parmar,      Jakob  \n \n  Uszkoreit,      Llion      Jones,      Aidan      N      Gomez,      Lukasz  \n \n  Kaiser,      and      Ilia      Polosukhin. Attention      is      all      you  \n \n  need. In      Advances      in      neural      information      processing  \n \n  systems,      pp. 5998-6008,      2017."
            },
            {
              "id": 343,
              "title": "RO-EN - Chunk 64",
              "type": "chunk",
              "children": [],
              "content": "Alex      Wang,      Amanpreet      Singh,      Julian      Michael,      Felix  \n \n  Hill,      Omer      Levy,      and      Samuel      R      Bowman. Glue:\n \n \n \nA      multi-task      benchmark      and      analysis      platform      for  \n \n  natural      language      understanding. arXiv      preprint  \n \n  arXiv:      1804.07461,      2018. Alex      Warstadt,      Amanpreet      Singh,      and      Samuel      R. Bowman. Neural      network      acceptability      judgments."
            },
            {
              "id": 344,
              "title": "RO-EN - Chunk 65",
              "type": "chunk",
              "children": [],
              "content": "arXiv      preprint      1805.12471,      2018. Adina      Williams,      Nikita      Nangia,      and      Samuel      R      Bow-  \n \n  man. A   \n \n_      broad-coverage      challenge      corpus      for  \n \n  sentence      understanding      through      inference. arXiv  \n \n  preprint      arXiv:      1704.05426,      2017. Adina      Williams,      Nikita      Nangia,      and      Samuel      R. Bow-  \n \n  man."
            },
            {
              "id": 345,
              "title": "RO-EN - Chunk 66",
              "type": "chunk",
              "children": [],
              "content": "A      broad-coverage      challenge      corpus      for      sen-  \n \n  tence      understanding      through      inference. In      Proceed-  \n \n  ings      of      NAACL-HLT,      2018. Zhilin      Yang,      Zihang      Dai,      Yiming      Yang,      Jaime  \n \n  Carbonell,      Ruslan      Salakhutdinov,      and      Quoc   \n \nV       Le. XlInet:      Generalized      autoregressive      pretrain-  \n \n  ing      for      language      understanding."
            },
            {
              "id": 346,
              "title": "RO-EN - Chunk 67",
              "type": "chunk",
              "children": [],
              "content": "arXiv      preprint  \n \n  arXiv:      1906.08237,      2019."
            }
          ],
          "content": ""
        },
        {
          "id": 347,
          "title": "Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96",
          "type": "section",
          "children": [
            {
              "id": 348,
              "title": "Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 1",
              "type": "chunk",
              "children": [],
              "content": "Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96\n \n \n  Table      6:      The      performance      (BLEU)      of      baseline      and  \n \n  BART      on      WMT’16      RO-EN      augmented      with      back-  \n \n  translation      data. BART      improves      over   \n \na      strong      back-  \n \n  translation      (BT)      baseline      by      using      monolingual      English  \n \n  pre-training."
            },
            {
              "id": 349,
              "title": "Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 2",
              "type": "chunk",
              "children": [],
              "content": "Abstractive      QA      We      use      the      recently      proposed      ELIS  \n \n  dataset      to      test      the      model’s      ability      to      generate      long      free-  \n \n  form      answers."
            },
            {
              "id": 350,
              "title": "Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 3",
              "type": "chunk",
              "children": [],
              "content": "We      find      BART      outperforms      the      best      pre-  \n \n  vious      work      by      1.2      ROUGE-L,      but      the      dataset      remains  \n \n  a      challenging,      because      answers      are      only      weakly      speci-  \n \n  fied      by      the      question."
            },
            {
              "id": 351,
              "title": "Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 4",
              "type": "chunk",
              "children": [],
              "content": "5.4      Translation\ntion  \n \n  We      also      evaluated      performance      on      WMT16      Romanian-  \n \n  English,      augmented      with      back-translation      data  \n \n  from      Sennrich      et      al. (2016)."
            },
            {
              "id": 352,
              "title": "Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 5",
              "type": "chunk",
              "children": [],
              "content": "We      use   \n \na      6-layer  \n \n  transformer      source      encoder      to      map      Romanian      into  \n \n  a      representation      that      BART      is      able      to      de-noise      into  \n \n  English,      following      the      approach      introduced      in      83.4. Experiment      results      are      presented      in      Table      6."
            },
            {
              "id": 353,
              "title": "Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 6",
              "type": "chunk",
              "children": [],
              "content": "We  \n \n  compare      our      results      against   \n \na      baseline      Transformer  \n \n  architecture      (Vaswani      et      al.,      2017)      with      Transformer-  \n \n  large      settings      (the      baseline      row). We      show      the  \n \n  performance      of      both      steps      of      our      model      in      the      fixed  \n \n  BART      and      tuned      BART      rows."
            },
            {
              "id": 354,
              "title": "Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 7",
              "type": "chunk",
              "children": [],
              "content": "For      each      row      we  \n \n  experiment      on      the      original      WMT16      Romanian-English  \n \n  augmented      with      back-translation      data. We      use   \n \na       beam      width      of   \n \n5      and   \n \na      length      penalty      of   \n \na   \n \n=      1."
            },
            {
              "id": 355,
              "title": "Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 8",
              "type": "chunk",
              "children": [],
              "content": "Preliminary      results      suggested      that      our      approach      was  \n \n  less      effective      without      back-translation      data,      and      prone  \n \n  to      overfitting—future      work      should      explore      additional  \n \n  regularization      techniques."
            },
            {
              "id": 356,
              "title": "Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 9",
              "type": "chunk",
              "children": [],
              "content": "6\n \n   Qualitative      Analysis  \n \n  BART      shows      large      improvements      on      summarization  \n \n  metrics,      of      up      to   \n \n6      points      over      the      prior      state-of-the-art. To      understand      BART’s      performance      beyond      automated  \n \n  metrics,      we      analyse      its      generations      qualitatively. Table   \n \n7      shows      example      summaries      generated      by  \n \n  BART."
            },
            {
              "id": 357,
              "title": "Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 10",
              "type": "chunk",
              "children": [],
              "content": "Examples      are      taken      from      WikiNews      articles  \n \n  published      after      the      creation      of      the      pre-training      corpus,  \n \n  to      eliminate      the      possibility      of      the      events      described      be-  \n \n  ing      present      in      the      model’s      training      data. Following  \n \n  Narayan      et      al."
            },
            {
              "id": 358,
              "title": "Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 11",
              "type": "chunk",
              "children": [],
              "content": "(2018),      we      remove      the      first      sentence      of  \n \n  the      article      prior      to      summarizing      it,      so      there      is      no      easy  \n \n  extractive      summary      of      the      document. Unsurprisingly,      model      output      is      fluent      and      grammat-  \n \n  ical      English."
            },
            {
              "id": 359,
              "title": "Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 12",
              "type": "chunk",
              "children": [],
              "content": "However,      model      output      is      also      highly      ab-  \n \n  stractive,      with      few      phrases      copied      from      the      input."
            },
            {
              "id": 360,
              "title": "Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 13",
              "type": "chunk",
              "children": [],
              "content": "The  \n \n  output      is      also      generally      factually      accurate,      and      inte-  \n \n  grates      supporting      evidence      from      across      the      input      doc-  \n \n  ument      with      background      knowledge      (for      example,      cor-  \n \n  rectly      completing      names,      or      inferring      that      PG&E      oper-  \n \n  ates      in      California)."
            },
            {
              "id": 361,
              "title": "Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 14",
              "type": "chunk",
              "children": [],
              "content": "In      the      first      example,      inferring      that  \n \n  fish      are      protecting      reefs      from      global      warming      requires  \n \n  non-trivial      inference      from      the      text. However,      the      claim  \n \n  that      the      work      was      published      in      Science      is      not      supported  \n \n  by      the      source."
            },
            {
              "id": 362,
              "title": "Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 15",
              "type": "chunk",
              "children": [],
              "content": "These      samples      demonstrate      that      the      BART      pretrain-  \n \n  ing      has      learned   \n \na      strong      combination      of      natural      lan-  \n \n  guage      understanding      and      generation. 7\n \n   Related      Work  \n \n  Early      methods      for      pretraining      were      based      on      language  \n \n  models."
            },
            {
              "id": 363,
              "title": "Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 16",
              "type": "chunk",
              "children": [],
              "content": "GPT      (Radford      et      al.,      2018)      only      models      left-  \n \n  ward      context,      which      is      problematic      for      some      tasks. ELMo      (Peters      et      al.,      2018)      concatenates      left-only      and  \n \n  right-only      representations,      but      does      not      pre-train      inter-  \n \n  actions      between      these      features. Radford      et      al."
            },
            {
              "id": 364,
              "title": "Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 17",
              "type": "chunk",
              "children": [],
              "content": "(2019)  \n \n  demonstrated      that      very      large      language      models      can      act       as      unsupervised      multitask      models. MASS      (Song      et      al.,      2019)      is      perhaps      the      most      similar  \n \n  model      to      BART."
            },
            {
              "id": 365,
              "title": "Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 18",
              "type": "chunk",
              "children": [],
              "content": "An      input      sequence      where   \n \na      contiguous  \n \n  span      of      tokens      is      masked      is      mapped      to   \n \na      sequence      con-  \n \n  sisting      of      the      missing      tokens. MASS      is      less      effective  \n \n  for      discriminative      tasks,      because      disjoint      sets      of      tokens  \n \n  are      fed      into      the      encoder      and      decoder."
            },
            {
              "id": 366,
              "title": "Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 19",
              "type": "chunk",
              "children": [],
              "content": "BERT      (Devlin      et      al.,      2019)      introduced      masked      lan-  \n \n  guage      modelling,      which      allows      pre-training      to      learn      in-  \n \n  teractions      between      left      and      right      context      words."
            },
            {
              "id": 367,
              "title": "Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 20",
              "type": "chunk",
              "children": [],
              "content": "Re-  \n \n  cent      work      has      shown      that      very      strong      performance      can  \n \n  be      achieved      by      training      for      longer      (Liu      et      al.,      2019),  \n \n  by      tying      parameters      across      layers      (Lan      et      al.,      2019),  \n \n  and      by      masking      spans      instead      of      words      (Joshi      et      al.,  \n \n  2019)."
            },
            {
              "id": 368,
              "title": "Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 21",
              "type": "chunk",
              "children": [],
              "content": "Predictions      are      not      made      auto-regressively,      re-  \n \n  ducing      the      effectiveness      of      BERT      for      generation      tasks. UniLM      (Dong      et      al.,      2019)      fine-tunes      BERT      with      an  \n \n  ensemble      of      masks,      some      of      which      allow      only      leftward  \n \n  context."
            },
            {
              "id": 369,
              "title": "Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 22",
              "type": "chunk",
              "children": [],
              "content": "Like      BART,      this      allows      UniLM      to      be      used      for  \n \n  both      generative      and      discriminative      tasks. A      difference  \n \n  is      that      UniLM      predictions      are      conditionally      indepen-  \n \n  dent,      whereas      BART’s      are      autoregressive."
            },
            {
              "id": 370,
              "title": "Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 23",
              "type": "chunk",
              "children": [],
              "content": "BART      re-  \n \n  duces      the      mismatch      between      pre-training      and      genera-  \n \n  tion      tasks,      because      the      decoder      is      always      trained      on      un-  \n \n  corrupted      context."
            },
            {
              "id": 371,
              "title": "Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 24",
              "type": "chunk",
              "children": [],
              "content": "XL-Net      (Yang      et      al.,      2019)      extends      BERT      by      pre-\n |       Source      Document      (abbreviated) |       BART      Summary\n |       The      researchers      examined      three      types      of      coral      in      reefs      off      the coast of Fiji The researchers found when fish were plentiful,       they      would      eat      algae      and      seaweed      off      the      corals,      which      appeared       to      leave      them      more      resistant      to      the      bacterium      Vibrio      coralliilyti-       cus,      a      bacterium      associated      with      bleaching."
            },
            {
              "id": 372,
              "title": "Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 25",
              "type": "chunk",
              "children": [],
              "content": "The      researchers      sug-       gested      the      algae,      like      warming      temperatures,      might      render      the       corals’      chemical      defenses      less      effective,      and      the      fish      were      pro-       tecting      the      coral      by      removing      the      algae."
            },
            {
              "id": 373,
              "title": "Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 26",
              "type": "chunk",
              "children": [],
              "content": "|       Fisheries      off      the      coast      of      Fiji      are      protect-       ing      coral      reefs      from      the      effects      of      global       warming,      according      to      a      study      in      the      jour-       nal      Science."
            },
            {
              "id": 374,
              "title": "Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 27",
              "type": "chunk",
              "children": [],
              "content": "|       Sacoolas,      who      has      immunity      as      a      diplomat’s      wife,      was      involved in a traffic collision Prime Minister Johnson was questioned       about      the      case      while      speaking      to      the      press      at      a      hospital      in      Wat- ford."
            },
            {
              "id": 375,
              "title": "Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 28",
              "type": "chunk",
              "children": [],
              "content": "He said, “I hope that Anne Sacoolas will come back       if      we      can’t      resolve      it      then      of      course      I      will      be      raising      it      myself       personally      with      the      White      House.” |       Boris      Johnson      has      said      he      will      raise      the      is-       sue      of      US      diplomat      Anne      Sacoolas’      diplo-       matic      immunity      with      the      White      House."
            },
            {
              "id": 376,
              "title": "Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 29",
              "type": "chunk",
              "children": [],
              "content": "|       According      to      Syrian      state      media,      government      forces      began      de- ploying into previously SDF controlled territory yesterday. On      October      6,      US      President      Donald      Trump      and      Turkish      Presi-       dent      Recep      Tayyip      Erdoan      spoke      on      the      phone."
            },
            {
              "id": 377,
              "title": "Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 30",
              "type": "chunk",
              "children": [],
              "content": "Then      both      na-       tions      issued      statements      speaking      of      an      imminent      incursion      into northeast Syria On Wednesday, Turkey began a military       offensive      with      airstrikes      followed      by      a      ground      invasion."
            },
            {
              "id": 378,
              "title": "Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 31",
              "type": "chunk",
              "children": [],
              "content": "|       Syrian      government      forces      have      entered       territory      held      by      the      US-backed      Syrian       Democratic      Forces      (SDF)      in      response      to       Turkey’s      incursion      into      the      region."
            },
            {
              "id": 379,
              "title": "Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 32",
              "type": "chunk",
              "children": [],
              "content": "|       This      is      the      first      time      anyone      has      been      recorded      to      run      a      full       marathon      of      42.195      kilometers      (approximately      26      miles)      under       this      pursued      landmark      time. It      was      not,      however,      an      officially       sanctioned      world      record,      as      it      was      not      an      ’open      race”      of      the       IAAF."
            },
            {
              "id": 380,
              "title": "Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 33",
              "type": "chunk",
              "children": [],
              "content": "His      time      was      1      hour      59      minutes      40.2      seconds. Kipchoge       ran      in      Vienna,      Austria. It      was      an      event      specifically      designed      to       help      Kipchoge      break      the      two      hour      barrier. |       Kenyan      runner      Eliud      Kipchoge      has      run      a       marathon      in      less      than      two      hours."
            },
            {
              "id": 381,
              "title": "Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 34",
              "type": "chunk",
              "children": [],
              "content": "|       PG&E      stated      it      scheduled      the      blackouts      in      response      to      forecasts       for      high      winds      amid      dry      conditions. The      aim      is      to      reduce      the      risk       of      wildfires."
            },
            {
              "id": 382,
              "title": "Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 35",
              "type": "chunk",
              "children": [],
              "content": "Nearly      800      thousand      customers      were      scheduled      to       be      affected      by      the      shutoffs      which      were      expected      to      last      through       at      least      midday      tomorrow. |       Power      has      been      turned      off      to      millions      of       customers      in      California      as      part      of      a      power       shutoff      plan."
            },
            {
              "id": 383,
              "title": "Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 36",
              "type": "chunk",
              "children": [],
              "content": "dicting      masked      tokens      auto-regressively      in   \n \na      permuted  \n \n  order. This      objective      allows      predictions      to      condition      on  \n \n  both      left      and      right      context. In      contrast,      the      BART      de-  \n \n  coder      works      left-to-right      during      pre-training,      matching  \n \n  the      setting      during      generation."
            },
            {
              "id": 384,
              "title": "Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 37",
              "type": "chunk",
              "children": [],
              "content": "Several      papers      have      explored      using      pre-trained      rep-  \n \n  resentations      to      improve      machine      translation."
            },
            {
              "id": 385,
              "title": "Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 38",
              "type": "chunk",
              "children": [],
              "content": "The  \n \n  largest      improvements      have      come      from      pre-training      on  \n \n  both      source      and      target      languages      (Song      et      al.,      2019;  \n \n  Lample   \n \n&      Conneau,      2019),      but      this      requires      pre-  \n \n  training      on      all      languages      of      interest."
            },
            {
              "id": 386,
              "title": "Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 39",
              "type": "chunk",
              "children": [],
              "content": "Other      work      has  \n \n  shown      that      encoders      can      be      improved      using      pre-trained  \n \n  representations      (Edunov      et      al.,      2019),      but      gains      in      de-  \n \n  coders      are      more      limited. We      show      how      BART      can      be  \n \n  used      to      improve      machine      translation      decoders."
            },
            {
              "id": 387,
              "title": "Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 40",
              "type": "chunk",
              "children": [],
              "content": "8\n \n   Conclusions  \n \n  We      introduced      BART,   \n \na      pre-training      approach      that  \n \n  learns      to      map      corrupted      documents      to      the      original. BART      achieves      similar      performance      to      ROBERTa      on  \n \n  discriminative      tasks,      while      achieving      new      state-of-the-  \n \n  art      results      on   \n \na      number      of      text      generation      tasks."
            },
            {
              "id": 388,
              "title": "Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 41",
              "type": "chunk",
              "children": [],
              "content": "Fu-  \n \n  ture      work      should      explore      new      methods      for      corrupting  \n \n  documents      for      pre-training,      perhaps      tailoring      them      to  \n \n  specific      end      tasks."
            }
          ],
          "content": ""
        },
        {
          "id": 389,
          "title": "5.4      Translation",
          "type": "section",
          "children": [
            {
              "id": 390,
              "title": "5.4      Translation - Chunk 1",
              "type": "chunk",
              "children": [],
              "content": "5.4      Translation\ntion  \n \n  We      also      evaluated      performance      on      WMT16      Romanian-  \n \n  English,      augmented      with      back-translation      data  \n \n  from      Sennrich      et      al. (2016)."
            },
            {
              "id": 391,
              "title": "5.4      Translation - Chunk 2",
              "type": "chunk",
              "children": [],
              "content": "We      use   \n \na      6-layer  \n \n  transformer      source      encoder      to      map      Romanian      into  \n \n  a      representation      that      BART      is      able      to      de-noise      into  \n \n  English,      following      the      approach      introduced      in      83.4. Experiment      results      are      presented      in      Table      6."
            },
            {
              "id": 392,
              "title": "5.4      Translation - Chunk 3",
              "type": "chunk",
              "children": [],
              "content": "We  \n \n  compare      our      results      against   \n \na      baseline      Transformer  \n \n  architecture      (Vaswani      et      al.,      2017)      with      Transformer-  \n \n  large      settings      (the      baseline      row). We      show      the  \n \n  performance      of      both      steps      of      our      model      in      the      fixed  \n \n  BART      and      tuned      BART      rows."
            },
            {
              "id": 393,
              "title": "5.4      Translation - Chunk 4",
              "type": "chunk",
              "children": [],
              "content": "For      each      row      we  \n \n  experiment      on      the      original      WMT16      Romanian-English  \n \n  augmented      with      back-translation      data. We      use   \n \na       beam      width      of   \n \n5      and   \n \na      length      penalty      of   \n \na   \n \n=      1."
            },
            {
              "id": 394,
              "title": "5.4      Translation - Chunk 5",
              "type": "chunk",
              "children": [],
              "content": "Preliminary      results      suggested      that      our      approach      was  \n \n  less      effective      without      back-translation      data,      and      prone  \n \n  to      overfitting—future      work      should      explore      additional  \n \n  regularization      techniques."
            },
            {
              "id": 395,
              "title": "5.4      Translation - Chunk 6",
              "type": "chunk",
              "children": [],
              "content": "6\n \n   Qualitative      Analysis  \n \n  BART      shows      large      improvements      on      summarization  \n \n  metrics,      of      up      to   \n \n6      points      over      the      prior      state-of-the-art. To      understand      BART’s      performance      beyond      automated  \n \n  metrics,      we      analyse      its      generations      qualitatively. Table   \n \n7      shows      example      summaries      generated      by  \n \n  BART."
            },
            {
              "id": 396,
              "title": "5.4      Translation - Chunk 7",
              "type": "chunk",
              "children": [],
              "content": "Examples      are      taken      from      WikiNews      articles  \n \n  published      after      the      creation      of      the      pre-training      corpus,  \n \n  to      eliminate      the      possibility      of      the      events      described      be-  \n \n  ing      present      in      the      model’s      training      data. Following  \n \n  Narayan      et      al."
            },
            {
              "id": 397,
              "title": "5.4      Translation - Chunk 8",
              "type": "chunk",
              "children": [],
              "content": "(2018),      we      remove      the      first      sentence      of  \n \n  the      article      prior      to      summarizing      it,      so      there      is      no      easy  \n \n  extractive      summary      of      the      document. Unsurprisingly,      model      output      is      fluent      and      grammat-  \n \n  ical      English."
            },
            {
              "id": 398,
              "title": "5.4      Translation - Chunk 9",
              "type": "chunk",
              "children": [],
              "content": "However,      model      output      is      also      highly      ab-  \n \n  stractive,      with      few      phrases      copied      from      the      input."
            },
            {
              "id": 399,
              "title": "5.4      Translation - Chunk 10",
              "type": "chunk",
              "children": [],
              "content": "The  \n \n  output      is      also      generally      factually      accurate,      and      inte-  \n \n  grates      supporting      evidence      from      across      the      input      doc-  \n \n  ument      with      background      knowledge      (for      example,      cor-  \n \n  rectly      completing      names,      or      inferring      that      PG&E      oper-  \n \n  ates      in      California)."
            },
            {
              "id": 400,
              "title": "5.4      Translation - Chunk 11",
              "type": "chunk",
              "children": [],
              "content": "In      the      first      example,      inferring      that  \n \n  fish      are      protecting      reefs      from      global      warming      requires  \n \n  non-trivial      inference      from      the      text. However,      the      claim  \n \n  that      the      work      was      published      in      Science      is      not      supported  \n \n  by      the      source."
            },
            {
              "id": 401,
              "title": "5.4      Translation - Chunk 12",
              "type": "chunk",
              "children": [],
              "content": "These      samples      demonstrate      that      the      BART      pretrain-  \n \n  ing      has      learned   \n \na      strong      combination      of      natural      lan-  \n \n  guage      understanding      and      generation. 7\n \n   Related      Work  \n \n  Early      methods      for      pretraining      were      based      on      language  \n \n  models."
            },
            {
              "id": 402,
              "title": "5.4      Translation - Chunk 13",
              "type": "chunk",
              "children": [],
              "content": "GPT      (Radford      et      al.,      2018)      only      models      left-  \n \n  ward      context,      which      is      problematic      for      some      tasks. ELMo      (Peters      et      al.,      2018)      concatenates      left-only      and  \n \n  right-only      representations,      but      does      not      pre-train      inter-  \n \n  actions      between      these      features. Radford      et      al."
            },
            {
              "id": 403,
              "title": "5.4      Translation - Chunk 14",
              "type": "chunk",
              "children": [],
              "content": "(2019)  \n \n  demonstrated      that      very      large      language      models      can      act       as      unsupervised      multitask      models. MASS      (Song      et      al.,      2019)      is      perhaps      the      most      similar  \n \n  model      to      BART."
            },
            {
              "id": 404,
              "title": "5.4      Translation - Chunk 15",
              "type": "chunk",
              "children": [],
              "content": "An      input      sequence      where   \n \na      contiguous  \n \n  span      of      tokens      is      masked      is      mapped      to   \n \na      sequence      con-  \n \n  sisting      of      the      missing      tokens. MASS      is      less      effective  \n \n  for      discriminative      tasks,      because      disjoint      sets      of      tokens  \n \n  are      fed      into      the      encoder      and      decoder."
            },
            {
              "id": 405,
              "title": "5.4      Translation - Chunk 16",
              "type": "chunk",
              "children": [],
              "content": "BERT      (Devlin      et      al.,      2019)      introduced      masked      lan-  \n \n  guage      modelling,      which      allows      pre-training      to      learn      in-  \n \n  teractions      between      left      and      right      context      words."
            },
            {
              "id": 406,
              "title": "5.4      Translation - Chunk 17",
              "type": "chunk",
              "children": [],
              "content": "Re-  \n \n  cent      work      has      shown      that      very      strong      performance      can  \n \n  be      achieved      by      training      for      longer      (Liu      et      al.,      2019),  \n \n  by      tying      parameters      across      layers      (Lan      et      al.,      2019),  \n \n  and      by      masking      spans      instead      of      words      (Joshi      et      al.,  \n \n  2019)."
            },
            {
              "id": 407,
              "title": "5.4      Translation - Chunk 18",
              "type": "chunk",
              "children": [],
              "content": "Predictions      are      not      made      auto-regressively,      re-  \n \n  ducing      the      effectiveness      of      BERT      for      generation      tasks. UniLM      (Dong      et      al.,      2019)      fine-tunes      BERT      with      an  \n \n  ensemble      of      masks,      some      of      which      allow      only      leftward  \n \n  context."
            },
            {
              "id": 408,
              "title": "5.4      Translation - Chunk 19",
              "type": "chunk",
              "children": [],
              "content": "Like      BART,      this      allows      UniLM      to      be      used      for  \n \n  both      generative      and      discriminative      tasks. A      difference  \n \n  is      that      UniLM      predictions      are      conditionally      indepen-  \n \n  dent,      whereas      BART’s      are      autoregressive."
            },
            {
              "id": 409,
              "title": "5.4      Translation - Chunk 20",
              "type": "chunk",
              "children": [],
              "content": "BART      re-  \n \n  duces      the      mismatch      between      pre-training      and      genera-  \n \n  tion      tasks,      because      the      decoder      is      always      trained      on      un-  \n \n  corrupted      context."
            },
            {
              "id": 410,
              "title": "5.4      Translation - Chunk 21",
              "type": "chunk",
              "children": [],
              "content": "XL-Net      (Yang      et      al.,      2019)      extends      BERT      by      pre-\n |       Source      Document      (abbreviated) |       BART      Summary\n |       The      researchers      examined      three      types      of      coral      in      reefs      off      the coast of Fiji The researchers found when fish were plentiful,       they      would      eat      algae      and      seaweed      off      the      corals,      which      appeared       to      leave      them      more      resistant      to      the      bacterium      Vibrio      coralliilyti-       cus,      a      bacterium      associated      with      bleaching."
            },
            {
              "id": 411,
              "title": "5.4      Translation - Chunk 22",
              "type": "chunk",
              "children": [],
              "content": "The      researchers      sug-       gested      the      algae,      like      warming      temperatures,      might      render      the       corals’      chemical      defenses      less      effective,      and      the      fish      were      pro-       tecting      the      coral      by      removing      the      algae."
            },
            {
              "id": 412,
              "title": "5.4      Translation - Chunk 23",
              "type": "chunk",
              "children": [],
              "content": "|       Fisheries      off      the      coast      of      Fiji      are      protect-       ing      coral      reefs      from      the      effects      of      global       warming,      according      to      a      study      in      the      jour-       nal      Science."
            },
            {
              "id": 413,
              "title": "5.4      Translation - Chunk 24",
              "type": "chunk",
              "children": [],
              "content": "|       Sacoolas,      who      has      immunity      as      a      diplomat’s      wife,      was      involved in a traffic collision Prime Minister Johnson was questioned       about      the      case      while      speaking      to      the      press      at      a      hospital      in      Wat- ford."
            },
            {
              "id": 414,
              "title": "5.4      Translation - Chunk 25",
              "type": "chunk",
              "children": [],
              "content": "He said, “I hope that Anne Sacoolas will come back       if      we      can’t      resolve      it      then      of      course      I      will      be      raising      it      myself       personally      with      the      White      House.” |       Boris      Johnson      has      said      he      will      raise      the      is-       sue      of      US      diplomat      Anne      Sacoolas’      diplo-       matic      immunity      with      the      White      House."
            },
            {
              "id": 415,
              "title": "5.4      Translation - Chunk 26",
              "type": "chunk",
              "children": [],
              "content": "|       According      to      Syrian      state      media,      government      forces      began      de- ploying into previously SDF controlled territory yesterday. On      October      6,      US      President      Donald      Trump      and      Turkish      Presi-       dent      Recep      Tayyip      Erdoan      spoke      on      the      phone."
            },
            {
              "id": 416,
              "title": "5.4      Translation - Chunk 27",
              "type": "chunk",
              "children": [],
              "content": "Then      both      na-       tions      issued      statements      speaking      of      an      imminent      incursion      into northeast Syria On Wednesday, Turkey began a military       offensive      with      airstrikes      followed      by      a      ground      invasion."
            },
            {
              "id": 417,
              "title": "5.4      Translation - Chunk 28",
              "type": "chunk",
              "children": [],
              "content": "|       Syrian      government      forces      have      entered       territory      held      by      the      US-backed      Syrian       Democratic      Forces      (SDF)      in      response      to       Turkey’s      incursion      into      the      region."
            },
            {
              "id": 418,
              "title": "5.4      Translation - Chunk 29",
              "type": "chunk",
              "children": [],
              "content": "|       This      is      the      first      time      anyone      has      been      recorded      to      run      a      full       marathon      of      42.195      kilometers      (approximately      26      miles)      under       this      pursued      landmark      time. It      was      not,      however,      an      officially       sanctioned      world      record,      as      it      was      not      an      ’open      race”      of      the       IAAF."
            },
            {
              "id": 419,
              "title": "5.4      Translation - Chunk 30",
              "type": "chunk",
              "children": [],
              "content": "His      time      was      1      hour      59      minutes      40.2      seconds. Kipchoge       ran      in      Vienna,      Austria. It      was      an      event      specifically      designed      to       help      Kipchoge      break      the      two      hour      barrier. |       Kenyan      runner      Eliud      Kipchoge      has      run      a       marathon      in      less      than      two      hours."
            },
            {
              "id": 420,
              "title": "5.4      Translation - Chunk 31",
              "type": "chunk",
              "children": [],
              "content": "|       PG&E      stated      it      scheduled      the      blackouts      in      response      to      forecasts       for      high      winds      amid      dry      conditions. The      aim      is      to      reduce      the      risk       of      wildfires."
            },
            {
              "id": 421,
              "title": "5.4      Translation - Chunk 32",
              "type": "chunk",
              "children": [],
              "content": "Nearly      800      thousand      customers      were      scheduled      to       be      affected      by      the      shutoffs      which      were      expected      to      last      through       at      least      midday      tomorrow. |       Power      has      been      turned      off      to      millions      of       customers      in      California      as      part      of      a      power       shutoff      plan."
            },
            {
              "id": 422,
              "title": "5.4      Translation - Chunk 33",
              "type": "chunk",
              "children": [],
              "content": "dicting      masked      tokens      auto-regressively      in   \n \na      permuted  \n \n  order. This      objective      allows      predictions      to      condition      on  \n \n  both      left      and      right      context. In      contrast,      the      BART      de-  \n \n  coder      works      left-to-right      during      pre-training,      matching  \n \n  the      setting      during      generation."
            },
            {
              "id": 423,
              "title": "5.4      Translation - Chunk 34",
              "type": "chunk",
              "children": [],
              "content": "Several      papers      have      explored      using      pre-trained      rep-  \n \n  resentations      to      improve      machine      translation."
            },
            {
              "id": 424,
              "title": "5.4      Translation - Chunk 35",
              "type": "chunk",
              "children": [],
              "content": "The  \n \n  largest      improvements      have      come      from      pre-training      on  \n \n  both      source      and      target      languages      (Song      et      al.,      2019;  \n \n  Lample   \n \n&      Conneau,      2019),      but      this      requires      pre-  \n \n  training      on      all      languages      of      interest."
            },
            {
              "id": 425,
              "title": "5.4      Translation - Chunk 36",
              "type": "chunk",
              "children": [],
              "content": "Other      work      has  \n \n  shown      that      encoders      can      be      improved      using      pre-trained  \n \n  representations      (Edunov      et      al.,      2019),      but      gains      in      de-  \n \n  coders      are      more      limited. We      show      how      BART      can      be  \n \n  used      to      improve      machine      translation      decoders."
            },
            {
              "id": 426,
              "title": "5.4      Translation - Chunk 37",
              "type": "chunk",
              "children": [],
              "content": "8\n \n   Conclusions  \n \n  We      introduced      BART,   \n \na      pre-training      approach      that  \n \n  learns      to      map      corrupted      documents      to      the      original. BART      achieves      similar      performance      to      ROBERTa      on  \n \n  discriminative      tasks,      while      achieving      new      state-of-the-  \n \n  art      results      on   \n \na      number      of      text      generation      tasks."
            },
            {
              "id": 427,
              "title": "5.4      Translation - Chunk 38",
              "type": "chunk",
              "children": [],
              "content": "Fu-  \n \n  ture      work      should      explore      new      methods      for      corrupting  \n \n  documents      for      pre-training,      perhaps      tailoring      them      to  \n \n  specific      end      tasks."
            }
          ],
          "content": ""
        },
        {
          "id": 428,
          "title": "References",
          "type": "section",
          "children": [
            {
              "id": 429,
              "title": "References - Chunk 1",
              "type": "chunk",
              "children": [],
              "content": "References\n \n \n  Eneko      Agirre,      Llu’is      M‘arquez,      and      Richard      Wicen-  \n \n  towski      (eds.). Proceedings      of      the      Fourth      Interna-  \n \n  tional      Workshop      on      Semantic      Evaluations      (SemEval-  \n \n  2007). Association      for      Computational      Linguistics,  \n \n  Prague,      Czech      Republic,      June      2007. Ido      Dagan,      Oren      Glickman,      and      Bernardo      Magnini."
            },
            {
              "id": 430,
              "title": "References - Chunk 2",
              "type": "chunk",
              "children": [],
              "content": "The      PASCAL      recognising      textual      entailment      chal-  \n \n  lenge. In      Machine      learning      challenges. evaluat-  \n \n  ing      predictive      uncertainty,      visual      object      classifica-  \n \n  tion,      and      recognising      tectual      entailment,      pp. 177—  \n \n  190. Springer,      2006. Jacob      Devlin,      Ming-Wei      Chang,      Kenton      Lee,      and  \n \n  Kristina      Toutanova."
            },
            {
              "id": 431,
              "title": "References - Chunk 3",
              "type": "chunk",
              "children": [],
              "content": "BERT:      Pre-training      of      deep  \n \n  bidirectional      transformers      for      language      understand-  \n \n  ing. In      Proceedings      of      the      2019      Conference      of      the  \n \n  North      American      Chapter      of      the      Association      for      Com-  \n \n  putational      Linguistics:      Human      Language      Technolo-  \n \n  gies,      Volume   \n \nI      (Long      and      Short      Papers),      pp."
            },
            {
              "id": 432,
              "title": "References - Chunk 4",
              "type": "chunk",
              "children": [],
              "content": "4171-  \n \n  4186,      Minneapolis,      Minnesota,      June      2019. Associa-  \n \n  tion      for      Computational      Linguistics. doi:      10.18653/  \n \n  vI/N19-1423. URL      https://www.aclweb. org/anthology/N19-1423. Emily      Dinan,      Varvara      Logacheva,      Valentin      Malykh,  \n \n  Alexander      Miller,      Kurt      Shuster,      Jack      Urbanek,  \n \n  Douwe      Kiela,      Arthur      Szlam,      Iulian      Serban,      Ryan  \n \n  Lowe,      et      al."
            },
            {
              "id": 433,
              "title": "References - Chunk 5",
              "type": "chunk",
              "children": [],
              "content": "The      second      conversational      in-  \n \n  telligence      challenge      (convai2). arXiv      preprint  \n \n  arXiv:      1902.00098,      2019. William   \n \nB      Dolan      and      Chris      Brockett. Automatically  \n \n  constructing   \n \na      corpus      of      sentential      paraphrases. In  \n \n  Proceedings      of      the      International      Workshop      on      Para-  \n \n  phrasing,      2005."
            },
            {
              "id": 434,
              "title": "References - Chunk 6",
              "type": "chunk",
              "children": [],
              "content": "Li      Dong,      Nan      Yang,      Wenhui      Wang,      Furu      Wei,      Xi-  \n \n  aodong      Liu,      Yu      Wang,      Jianfeng      Gao,      Ming      Zhou,  \n \n  and      Hsiao-Wuen      Hon. Unified      language      model      pre-  \n \n  training      for      natural      language      understanding      and      gen-  \n \n  eration. arXiv      preprint      arXiv:      1905.03197,      2019. Sergey      Edunov,      Alexei      Baevski,      and      Michael      Auli."
            },
            {
              "id": 435,
              "title": "References - Chunk 7",
              "type": "chunk",
              "children": [],
              "content": "Pre-trained      language      model      representations      for      lan-  \n \n  guage      generation. In      Proceedings      of      the      2019      Con-  \n \n  ference      of      the      North      American      Chapter      of      the      Asso-  \n \n  ciation      for      Computational      Linguistics:      Human      Lan-  \n \n  guage      Technologies,      Volume   \n \n1      (Long      and      Short      Pa-  \n \n  pers),      2019."
            },
            {
              "id": 436,
              "title": "References - Chunk 8",
              "type": "chunk",
              "children": [],
              "content": "Angela      Fan,      David      Grangier,      and      Michael      Auli. Con-  \n \n  trollable      abstractive      summarization. arXiv      preprint  \n \n  arXiv:      1711.05217,      2017. Angela      Fan,      Yacine      Jernite,      Ethan      Perez,      David  \n \n  Grangier,      Jason      Weston,      and      Michael      Auli. Eli5:  \n \n  Long      form      question      answering. arXiv      preprint  \n \n  arXiv:      1907.09190,      2019."
            },
            {
              "id": 437,
              "title": "References - Chunk 9",
              "type": "chunk",
              "children": [],
              "content": "Dan      Hendrycks      and      Kevin      Gimpel. Gaussian      error      lin-  \n \n  ear      units      (gelus). arXiv      preprint      arXiv:      1606.08415,  \n \n  2016. Karl      Moritz      Hermann,      Tomas      Kocisky,      Edward  \n \n  Grefenstette,      Lasse      Espeholt,      Will      Kay,      Mustafa      Su-  \n \n  leyman,      and      Phil      Blunsom. Teaching      machines      to  \n \n  read      and      comprehend."
            },
            {
              "id": 438,
              "title": "References - Chunk 10",
              "type": "chunk",
              "children": [],
              "content": "In      Advances      in      neural      infor-  \n \n  mation      processing      systems,      pp. 1693-1701,      2015. Mandar      Joshi,      Danqi      Chen,      Yinhan      Liu,      Daniel   \n \nS      Weld,  \n \n  Luke      Zettlemoyer,      and      Omer      Levy. Spanbert:      Im-  \n \n  proving      pre-training      by      representing      and      predicting  \n \n  spans. arXiv      preprint      arXiv:      1907.10529,      2019."
            },
            {
              "id": 439,
              "title": "References - Chunk 11",
              "type": "chunk",
              "children": [],
              "content": "Guillaume      Lample      and      Alexis      Conneau. —      Cross-  \n \n  lingual      language      model      pretraining. arXiv      preprint  \n \n  arXiv:      1901.07291,      2019. Zhenzhong      Lan,      Mingda      Chen,      Sebastian      Goodman,  \n \n  Kevin      Gimpel,      Piyush      Sharma,      and      Radu      Sori-  \n \n  cut. Albert:   \n \nA      lite      bert      for      self-supervised      learn-  \n \n  ing      of      language      representations."
            },
            {
              "id": 440,
              "title": "References - Chunk 12",
              "type": "chunk",
              "children": [],
              "content": "arXiv      preprint  \n \n  arXiv:      1909.11942,      2019. Hector   \n \nJ      Levesque,      Ernest      Davis,      and      Leora      Morgen-  \n \n  stern. The      Winograd      schema      challenge. In      AAAI  \n \n  Spring      Symposium:      Logical      Formalizations      of      Com-  \n \n  monsense      Reasoning,      volume      46,      pp. 47,      2011. Yang      Liu      and      Mirella      Lapata. tion      with      pretrained      encoders."
            },
            {
              "id": 441,
              "title": "References - Chunk 13",
              "type": "chunk",
              "children": [],
              "content": "arXiv:      1908.08345,      2019. Text      summariza-  \n \n  arXiv      preprint  \n \n  Yinhan      Liu,      Myle      Ott,      Naman      Goyal,      Jingfei      Du,      Man-  \n \n  dar      Joshi,      Dangi      Chen,      Omer      Levy,      Mike      Lewis,  \n \n  Luke      Zettlemoyer,      and      Veselin      Stoyanov. Roberta:\n \n \n \nA      robustly      optimized      bert      pretraining      approach. arXiv      preprint      arXiv:      1907.11692,      2019."
            },
            {
              "id": 442,
              "title": "References - Chunk 14",
              "type": "chunk",
              "children": [],
              "content": "Tomas      Mikolov,      Kai      Chen,      Greg      Corrado,      and      Jeffrey  \n \n  Dean. Efficient      estimation      of      word      representations  \n \n  in      vector      space. arXiv      preprint      arXiv:1301.3781,  \n \n  2013. Shashi      Narayan,      Shay   \n \nB      Cohen,      and      Mirella      Lapata. Don’t      give      me      the      details,      just      the      summary!"
            },
            {
              "id": 443,
              "title": "References - Chunk 15",
              "type": "chunk",
              "children": [],
              "content": "topic-  \n \n  aware      convolutional      neural      networks      for      extreme  \n \n  summarization. arXiv      preprint      arXiv:      1808.08745,  \n \n  2018. Gabriel      Pereyra,      George      Tucker,      Jan      Chorowski,  \n \n  Lukasz      Kaiser,      and      Geoffrey      Hinton. Regularizing  \n \n  neural      networks      by      penalizing      confident      output      dis-  \n \n  tributions. arXiv      preprint      arXiv:      1701.06548,      2017."
            },
            {
              "id": 444,
              "title": "References - Chunk 16",
              "type": "chunk",
              "children": [],
              "content": "Matthew      E      Peters,      Mark      Neumann,      Mohit      Iyyer,      Matt  \n \n  Gardner,      Christopher      Clark,      Kenton      Lee,      and      Luke  \n \n  Zettlemoyer. Deep      contextualized      word      representa-  \n \n  tions. arXiv      preprint      arXiv:      1802.05365,      2018. Alec      Radford,      Karthik      Narasimhan,      Tim      Salimans,  \n \n  and      Ilya      Sutskever."
            },
            {
              "id": 445,
              "title": "References - Chunk 17",
              "type": "chunk",
              "children": [],
              "content": "Improving      language      un-  \n \n  derstanding      by      generative      pre-training. URL  \n \n  https://s3-us-west-2. |      amazonaws. —      com/openai-  \n \n  assets/researchcovers/languageunsupervised/language  \n \n  understanding      paper. pdf,      2018. Alec      Radford,      Jeffrey      Wu,      Rewon      Child,      David      Luan,  \n \n  Dario      Amodei,      and      Ilya      Sutskever. Language      mod-  \n \n  els      are      unsupervised      multitask      learners."
            },
            {
              "id": 446,
              "title": "References - Chunk 18",
              "type": "chunk",
              "children": [],
              "content": "OpenAI  \n \n  Blog,      1(8),      2019. Pranav      Rajpurkar,      Jian      Zhang,      Konstantin      Lopyrev,  \n \n  and      Percy      Liang. Squad:      100,000+      questions      for  \n \n  machine      comprehension      of      text. arXiv      preprint  \n \n  arXiv:      1606.05250,      2016. Abigail      See,      Peter   \n \nJ      Liu,      and      Christopher   \n \nD       Manning. Get      to      the      point:      Summarization  \n \n  with      pointer-generator      networks."
            },
            {
              "id": 447,
              "title": "References - Chunk 19",
              "type": "chunk",
              "children": [],
              "content": "arXiv      preprint  \n \n  arXiv:      1704.04368,      2017. Rico      Sennrich,      Barry      Haddow,      and      Alexandra      Birch. Edinburgh      neural      machine      translation      systems      for  \n \n  WMT      16. In      Proceedings      of      the      First      Conference  \n \n  on      Machine      Translation:      Volume      2,      Shared      Task      Pa-  \n \n  pers,      2016."
            },
            {
              "id": 448,
              "title": "References - Chunk 20",
              "type": "chunk",
              "children": [],
              "content": "Richard      Socher,      Alex      Perelygin,      Jean      Wu,      Jason  \n \n  Chuang,      Christopher   \n \nD      Manning,      Andrew      Ng,      and  \n \n  Christopher      Potts. Recursive      deep      models      for      se-  \n \n  mantic      compositionality      over   \n \na      sentiment      treebank. In      Proceedings      of      EMNLP,      pp. 1631-1642,      2013."
            },
            {
              "id": 449,
              "title": "References - Chunk 21",
              "type": "chunk",
              "children": [],
              "content": "Kaitao      Song,      Xu      Tan,      Tao      Qin,      Jianfeng      Lu,      and      Tie-  \n \n  Yan      Liu. Mass:      Masked      sequence      to      sequence      pre-  \n \n  training      for      language      generation. In      International  \n \n  Conference      on      Machine      Learning,      2019."
            },
            {
              "id": 450,
              "title": "References - Chunk 22",
              "type": "chunk",
              "children": [],
              "content": "Ashish      Vaswani,      Noam      Shazeer,      Niki      Parmar,      Jakob  \n \n  Uszkoreit,      Llion      Jones,      Aidan      N      Gomez,      Lukasz  \n \n  Kaiser,      and      Ilia      Polosukhin. Attention      is      all      you  \n \n  need. In      Advances      in      neural      information      processing  \n \n  systems,      pp. 5998-6008,      2017."
            },
            {
              "id": 451,
              "title": "References - Chunk 23",
              "type": "chunk",
              "children": [],
              "content": "Alex      Wang,      Amanpreet      Singh,      Julian      Michael,      Felix  \n \n  Hill,      Omer      Levy,      and      Samuel      R      Bowman. Glue:\n \n \n \nA      multi-task      benchmark      and      analysis      platform      for  \n \n  natural      language      understanding. arXiv      preprint  \n \n  arXiv:      1804.07461,      2018. Alex      Warstadt,      Amanpreet      Singh,      and      Samuel      R. Bowman. Neural      network      acceptability      judgments."
            },
            {
              "id": 452,
              "title": "References - Chunk 24",
              "type": "chunk",
              "children": [],
              "content": "arXiv      preprint      1805.12471,      2018. Adina      Williams,      Nikita      Nangia,      and      Samuel      R      Bow-  \n \n  man. A   \n \n_      broad-coverage      challenge      corpus      for  \n \n  sentence      understanding      through      inference. arXiv  \n \n  preprint      arXiv:      1704.05426,      2017. Adina      Williams,      Nikita      Nangia,      and      Samuel      R. Bow-  \n \n  man."
            },
            {
              "id": 453,
              "title": "References - Chunk 25",
              "type": "chunk",
              "children": [],
              "content": "A      broad-coverage      challenge      corpus      for      sen-  \n \n  tence      understanding      through      inference. In      Proceed-  \n \n  ings      of      NAACL-HLT,      2018. Zhilin      Yang,      Zihang      Dai,      Yiming      Yang,      Jaime  \n \n  Carbonell,      Ruslan      Salakhutdinov,      and      Quoc   \n \nV       Le. XlInet:      Generalized      autoregressive      pretrain-  \n \n  ing      for      language      understanding."
            },
            {
              "id": 454,
              "title": "References - Chunk 26",
              "type": "chunk",
              "children": [],
              "content": "arXiv      preprint  \n \n  arXiv:      1906.08237,      2019."
            }
          ],
          "content": ""
        }
      ],
      "content": ""
    },
    {
      "id": 455,
      "title": "Book 2",
      "type": "book",
      "children": [
        {
          "id": 456,
          "title": "Abstract",
          "type": "section",
          "children": [
            {
              "id": 457,
              "title": "Abstract - Chunk 1",
              "type": "chunk",
              "children": [],
              "content": "Abstract\n \n \n  We      study      how      to      apply      large      language      models  \n \n  to      write      grounded      and      organized      long-form      ar-  \n \n  ticles      from      scratch,      with      comparable      breadth  \n \n  and      depth      to      Wikipedia      pages."
            },
            {
              "id": 458,
              "title": "Abstract - Chunk 2",
              "type": "chunk",
              "children": [],
              "content": "This      underex-  \n \n  plored      problem      poses      new      challenges      at      the  \n \n  pre-writing      stage,      including      how      to      research  \n \n  the      topic      and      prepare      an      outline      prior      to      writ-  \n \n  ing. We      propose      STORM,   \n \na      writing      system  \n \n  for      the      Synthesis      of      Topic      Outlines      through  \n \n  Retrieval      and      Multi-perspective      Question      Ask-  \n \n  ing."
            },
            {
              "id": 459,
              "title": "Abstract - Chunk 3",
              "type": "chunk",
              "children": [],
              "content": "STORM      models      the      pre-writing      stage      by\n(1)      discovering      diverse      perspectives      in      research-  \n \n  ing      the      given      topic,      (2)      simulating      conversa-  \n \n  tions      where      writers      carrying      different      perspec-  \n \n  tives      pose      questions      to   \n \na      topic      expert      grounded  \n \n  on      trusted      Internet      sources,      (3)      curating      the      col-  \n \n  lected      information      to      create      an      outline."
            },
            {
              "id": 460,
              "title": "Abstract - Chunk 4",
              "type": "chunk",
              "children": [],
              "content": "For      evaluation,      we      curate      FreshWiki,   \n \na      dataset  \n \n  of      recent      high-quality      Wikipedia      articles,      and  \n \n  formulate      outline      assessments      to      evaluate      the  \n \n  pre-writing      stage. We      further      gather      feedback  \n \n  from      experienced      Wikipedia      editors."
            },
            {
              "id": 461,
              "title": "Abstract - Chunk 5",
              "type": "chunk",
              "children": [],
              "content": "Com-  \n \n  pared      to      articles      generated      by      an      outline-  \n \n  driven      retrieval-augmented      baseline,      more      of  \n \n  STORM’;      articles      are      deemed      to      be      organized  \n \n  (by   \n \na      25%      absolute      increase)      and      broad      in      cov-  \n \n  erage      (by      10%)."
            },
            {
              "id": 462,
              "title": "Abstract - Chunk 6",
              "type": "chunk",
              "children": [],
              "content": "The      expert      feedback      also  \n \n  helps      identify      new      challenges      for      generating  \n \n  grounded      long      articles,      such      as      source      bias  \n \n  transfer      and      over-association      of      unrelated      facts."
            },
            {
              "id": 463,
              "title": "Abstract - Chunk 7",
              "type": "chunk",
              "children": [],
              "content": "1\n \n   Introduction  \n \n  Large      language      models      (LLMs)      have      demonstrated  \n \n  impressive      writing      capabilities      (Yang      et      al.,      2023;  \n \n  Pavlik,      2023;      Wenzlaff      and      Spaeth,      2022;      Fitria,  \n \n  2023),      but      it      is      unclear      how      we      can      use      them      to  \n \n  write      grounded,      long-form      articles,      like      full-length  \n \n  Wikipedia      pages."
            },
            {
              "id": 464,
              "title": "Abstract - Chunk 8",
              "type": "chunk",
              "children": [],
              "content": "Such      expository      writing,      which  \n \n  seeks      to      inform      the      reader      on   \n \na      topic      in      an      or-  \n \n  ganized      manner      (Weaver      II      and      Kintsch,      1991;  \n \n  Balepur      et      al.,      2023),      requires      thorough      research  \n \n  and      planning      in      the      pre-writing      stage      (Rohman,  \n \n  Writing  \n \n  tify,      evaluate,      and      organize      external      sources   \n \n-   \n \na      task  \n \n  that      is      challenging      even      for      experienced      writers."
            },
            {
              "id": 465,
              "title": "Abstract - Chunk 9",
              "type": "chunk",
              "children": [],
              "content": "Automating      this      process      can      facilitate      individuals  \n \n  in      initiating      in-depth      learning      about   \n \na      topic      and  \n \n  greatly      reduce      the      expensive      expert      hours      neces-  \n \n  sary      for      their      expository      writing."
            },
            {
              "id": 466,
              "title": "Abstract - Chunk 10",
              "type": "chunk",
              "children": [],
              "content": "|  |       ee      Prewriting | \n | --- | --- | ---\n |  | =\n \n   Full-length  \n \n  5      Article |       =      Full-length       5      Article\n |       arXiv:2402.14207v2      [cs.CL]      8      Apr      2024 | 2022      Winter      Olympics      [=      Outline   \n \n|       Opening      Ceremony  \n \n  Research      via      Question      Asking  \n \n  (A)      Direct      Prompting  \n \n  -y      Prompt:      Ask      30      questions      about      the      given      topic. 1."
            },
            {
              "id": 467,
              "title": "Abstract - Chunk 11",
              "type": "chunk",
              "children": [],
              "content": "When      was      the      opening      ceremony      held? {22}      2. Where      was      the      opening      ceremony      held? LLM      3. How      many      countries      participated      in      the      opening      ceremony? (B)      Perspective-Guided      Question      Asking  \n \n  Prompt:      You      are      an      event      planner      who      focuses      on      the B® preparation of the opening ceremony. 1."
            },
            {
              "id": 468,
              "title": "Abstract - Chunk 12",
              "type": "chunk",
              "children": [],
              "content": "Can      you      provide      any      information      about      the      transportation  \n \n  arrangements      for      the      opening      ceremony? Lim      2. Can      you      provide      any      information      about      the      budget      for      the  \n \n  2022      Winter      Olympics      opening      ceremony?"
            },
            {
              "id": 469,
              "title": "Abstract - Chunk 13",
              "type": "chunk",
              "children": [],
              "content": "(C)      Conversational      Question      Asking  \n \n  Can      you      provide      me      with   \n \na      list      of      the      participating      countries  \n \n  tim-      in      the      2022      Winter      Olympics      opening      ceremony? Role1  \n \n  The      2022      Winter      Olympics      featured   \n \na      diverse      group      of  \n \n  countries      participating      in      the      opening      ceremony."
            },
            {
              "id": 470,
              "title": "Abstract - Chunk 14",
              "type": "chunk",
              "children": [],
              "content": "These LLM- included Athletes from over 90 countries will enter the  \n \n  Role2      stadium      ina      specific      order. How      is      the      order      of      participating      countries      in      the      2022  \n \n  Winter      Olympics      opening      ceremony      determined?"
            },
            {
              "id": 471,
              "title": "Abstract - Chunk 15",
              "type": "chunk",
              "children": [],
              "content": "LLM-  \n \n  Role1  \n \n  Figure      1:      We      explore      writing      Wikipedia-like      articles  \n \n  from      scratch,      which      demands   \n \na      pre-writing      stage      before  \n \n  producing      the      article. In      this      stage,      simpler      approaches  \n \n  like      Direct      Prompting      have      limited      planning      capacity."
            },
            {
              "id": 472,
              "title": "Abstract - Chunk 16",
              "type": "chunk",
              "children": [],
              "content": "In  \n \n  contrast,      STORM      researches      the      topic      via      perspective-  \n \n  guided      question      asking      in      simulated      conversations. 1965),      even      before      the      actual      writing      process      can  \n \n  start."
            },
            {
              "id": 473,
              "title": "Abstract - Chunk 17",
              "type": "chunk",
              "children": [],
              "content": "However,      prior      work      on      generating      Wikipedia  \n \n  articles      (Banerjee      and      Mitra,      2015;      Minguillén  \n \n  et      al.,      2017;      Liu      et      al.,      2018;      Fan      and      Gardent,  \n \n  2022)      has      generally      bypassed      the      pre-writing      stage:  \n \n  for      instance,      Liu      et      al."
            },
            {
              "id": 474,
              "title": "Abstract - Chunk 18",
              "type": "chunk",
              "children": [],
              "content": "(2018)      presume      reference  \n \n  documents      are      provided      in      advance,      while      Fan      and  \n \n  Gardent      (2022)      assume      an      article      outline      is      avail-  \n \n  able      and      focus      on      expanding      each      section."
            },
            {
              "id": 475,
              "title": "Abstract - Chunk 19",
              "type": "chunk",
              "children": [],
              "content": "These  \n \n  assumptions      do      not      hold      in      general,      as      collecting  \n \n  references      and      crafting      outlines      demand      advanced  \n \n  information      literacy      skills      (Doyle,      1994)      to      iden- | \n\n \n \n  We      explore      these      challenges      by      focusing      on      how  \n \n  to      generate      Wikipedia-like      articles      from      scratch."
            },
            {
              "id": 476,
              "title": "Abstract - Chunk 20",
              "type": "chunk",
              "children": [],
              "content": "We      decompose      this      problem      into      two      tasks. The  \n \n  first      is      to      conduct      research      to      generate      an      outline,  \n \n  i.e.,   \n \na      list      of      multi-level      sections,      and      collect   \n \na      set      of  \n \n  reference      documents. The      second      uses      the      outline  \n \n  and      the      references      to      produce      the      full-length      arti-  \n \n  cle."
            },
            {
              "id": 477,
              "title": "Abstract - Chunk 21",
              "type": "chunk",
              "children": [],
              "content": "Such   \n \na      task      decomposition      mirrors      the      human  \n \n  writing      process      which      usually      includes      phases      of  \n \n  pre-writing,      drafting,      and      revising      (Rohman,      1965;  \n \n  Munoz-Luna,      2015)."
            },
            {
              "id": 478,
              "title": "Abstract - Chunk 22",
              "type": "chunk",
              "children": [],
              "content": "As      pre-trained      language      models      inherently      pos-  \n \n  sess   \n \na      wealth      of      knowledge,   \n \na      direct      approach      is      to  \n \n  rely      on      their      parametric      knowledge      for      generating  \n \n  outlines      or      even      entire      articles      (Direct      Gen)."
            },
            {
              "id": 479,
              "title": "Abstract - Chunk 23",
              "type": "chunk",
              "children": [],
              "content": "How-  \n \n  ever,      this      approach      is      limited      by   \n \na      lack      of      details  \n \n  and      hallucinations      (Xu      et      al.,      2023),      particularly      in  \n \n  addressing      long-tail      topics      (Kandpal      et      al.,      2023)."
            },
            {
              "id": 480,
              "title": "Abstract - Chunk 24",
              "type": "chunk",
              "children": [],
              "content": "This      underscores      the      importance      of      leveraging      ex-  \n \n  ternal      sources,      and      current      strategies      often      involve  \n \n  retrieval-augmented      generation      (RAG),      which      cir-  \n \n  cles      back      to      the      problem      of      researching      the      topic      in  \n \n  the      pre-writing      stage,      as      much      information      cannot  \n \n  be      surfaced      through      simple      topic      searches."
            },
            {
              "id": 481,
              "title": "Abstract - Chunk 25",
              "type": "chunk",
              "children": [],
              "content": "Human      learning      theories      (Tawfik      et      al.,      2020;  \n \n  Booth      et      al.,      2003)      highlight      asking      effective  \n \n  questions      in      information      acquisition."
            },
            {
              "id": 482,
              "title": "Abstract - Chunk 26",
              "type": "chunk",
              "children": [],
              "content": "Although  \n \n  instruction-tuned      models      (Ouyang      et      al.,      2022)      can  \n \n  be      prompted      directly      to      generate      questions,      we      find  \n \n  that      they      typically      produce      basic      “What”,      “When”,  \n \n  and      “Where”      questions      (Figure   \n \n1      (A))      which      often  \n \n  only      address      surface-level      facts      about      the      topic."
            },
            {
              "id": 483,
              "title": "Abstract - Chunk 27",
              "type": "chunk",
              "children": [],
              "content": "To  \n \n  endow      LLMs      with      the      capacity      to      conduct      better  \n \n  research,      we      propose      the      STORM      paradigm      for  \n \n  the      Synthesis      of      Topic      Outlines      through      Retrieval  \n \n  and      Multi-perspective      Question      Asking."
            },
            {
              "id": 484,
              "title": "Abstract - Chunk 28",
              "type": "chunk",
              "children": [],
              "content": "The      design      of      STORM      is      based      on      two      hypothe-  \n \n  ses:      (1)      diverse      perspectives      lead      to      varied      ques-  \n \n  tions;      (2)      formulating      in-depth      questions      requires  \n \n  iterative      research. Building      upon      these      hypotheses,  \n \n  STORM      employs   \n \na      novel      multi-stage      approach."
            },
            {
              "id": 485,
              "title": "Abstract - Chunk 29",
              "type": "chunk",
              "children": [],
              "content": "It  \n \n  first      discovers      diverse      perspectives      by      retrieving  \n \n  and      analyzing      Wikipedia      articles      from      similar      top-  \n \n  ics      and      then      personifies      the      LLM      with      specific      per-  \n \n  spectives      for      question      asking      (Figure   \n \n1      (B))."
            },
            {
              "id": 486,
              "title": "Abstract - Chunk 30",
              "type": "chunk",
              "children": [],
              "content": "Next,  \n \n  to      elicit      follow-up      questions      for      iterative      research  \n \n  (Figure   \n \n1      (C)),      STORM      simulates      multi-turn      con-  \n \n  versations      where      the      answers      to      the      generated      ques-  \n \n  tions      are      grounded      on      the      Internet."
            },
            {
              "id": 487,
              "title": "Abstract - Chunk 31",
              "type": "chunk",
              "children": [],
              "content": "Finally,      based  \n \n  on      the      LLM’s      internal      knowledge      and      the      collected  \n \n  information,      STORM      creates      an      outline      that      can  \n \n  be      expanded      section      by      section      to      develop   \n \na      full-  \n \n  length      Wikipedia-like      article."
            },
            {
              "id": 488,
              "title": "Abstract - Chunk 32",
              "type": "chunk",
              "children": [],
              "content": "We      evaluate      STORM      using      our      FreshWiki  \n \n  dataset      (§2.1)      which      curates      recent,      high-quality  \n \n  Wikipedia      articles      to      avoid      data      leakage      during      pre-  \n \n  training.! To      facilitate      the      study      of      the      pre-writing  \n \n  stage,      we      define      metrics      for      evaluating      the      outline  \n \n  quality      against      human-written      articles."
            },
            {
              "id": 489,
              "title": "Abstract - Chunk 33",
              "type": "chunk",
              "children": [],
              "content": "We      further      invited   \n \na      group      of      experienced  \n \n  Wikipedia      editors      for      expert      evaluation. The      ed-  \n \n  itors      found      STORM      outperforms      an      outline-driven  \n \n  RAG      baseline,      especially      regarding      the      breadth      and  \n \n  organization      of      the      articles."
            },
            {
              "id": 490,
              "title": "Abstract - Chunk 34",
              "type": "chunk",
              "children": [],
              "content": "They      also      identified  \n \n  challenges      for      future      research,      including      address-  \n \n  ing      cases      where:      (1)      the      bias      on      the      Internet      affects  \n \n  the      generated      articles;      (2)      LLMs      fabricate      connec-  \n \n  tions      between      unrelated      facts. These      challenges  \n \n  present      new      frontiers      to      grounded      writing      systems."
            },
            {
              "id": 491,
              "title": "Abstract - Chunk 35",
              "type": "chunk",
              "children": [],
              "content": "Our      main      contributions      include:\n*\n \n   To      evaluate      the      capacity      of      LLM      systems      at  \n \n  generating      long-form      grounded      articles      from  \n \n  scratch,      and      the      pre-writing      challenge      in      par-  \n \n  ticular,      we      curate      the      FreshWiki      dataset      and  \n \n  establish      evaluation      criteria      for      both      outline  \n \n  and      final      article      quality."
            },
            {
              "id": 492,
              "title": "Abstract - Chunk 36",
              "type": "chunk",
              "children": [],
              "content": "¢      We      propose      STORM,   \n \na      novel      system      that      au-  \n \n  tomates      the      pre-writing      stage. STORM      re-  \n \n  searches      the      topic      and      creates      an      outline      by  \n \n  using      LLMs      to      ask      incisive      questions      and      re-  \n \n  trieving      trusted      information      from      the      Internet."
            },
            {
              "id": 493,
              "title": "Abstract - Chunk 37",
              "type": "chunk",
              "children": [],
              "content": "¢      Both      automatic      and      human      evaluation      demon-  \n \n  strate      the      effectiveness      of      our      approach. Ex-  \n \n  pert      feedback      further      reveals      new      challenges  \n \n  in      generating      grounded      long-form      articles."
            },
            {
              "id": 494,
              "title": "Abstract - Chunk 38",
              "type": "chunk",
              "children": [],
              "content": "2\n \n   FreshWiki  \n \n  We      study      generating      Wikipedia-like      articles      from  \n \n  scratch,      placing      emphasis      on      the      pre-writing  \n \n  stage      (Rohman,      1965),      which      involves      the      demand-  \n \n  ing      sub-tasks      of      gathering      and      curating      relevant  \n \n  information      (“‘research’’)."
            },
            {
              "id": 495,
              "title": "Abstract - Chunk 39",
              "type": "chunk",
              "children": [],
              "content": "This      models      the      human ‘Our      resources      and      code      are      released      at      https:      //github. com/stanford-oval/storm."
            }
          ],
          "content": ""
        },
        {
          "id": 496,
          "title": "Domain      Scope      Given      Given       P      Outline?      Refs?",
          "type": "section",
          "children": [
            {
              "id": 497,
              "title": "Domain      Scope      Given      Given       P      Outline?      Refs? - Chunk 1",
              "type": "chunk",
              "children": [],
              "content": "Domain      Scope      Given      Given       P      Outline? Refs? Balepur      et      al. (2023)      One      One      para. /      Yes  \n \n  Qian      et      al. (2023)      All      One      para. /      No  \n \n  Fan      and      Gardent      (2022)      One      Full      article      Yes      No  \n \n  Liu      et      al. (2018)      All      One      para."
            },
            {
              "id": 498,
              "title": "Domain      Scope      Given      Given       P      Outline?      Refs? - Chunk 2",
              "type": "chunk",
              "children": [],
              "content": "/      Yes  \n \n  Sauper      and      Barzilay      (2009)      Two      Full      article      No      No  \n \n  Ours      All      Full      article      No      No  \n \n  Table      1:      Comparison      of      different      Wikipedia      generation  \n \n  setups      in      existing      literature. Generating      one      paragraph  \n \n  does      not      need      an      article      outline."
            },
            {
              "id": 499,
              "title": "Domain      Scope      Given      Given       P      Outline?      Refs? - Chunk 3",
              "type": "chunk",
              "children": [],
              "content": "writing      approach      which      has      prompted      some      educa-  \n \n  tors      to      view      Wikipedia      article      writing      as      an      educa-  \n \n  tional      exercise      for      academic      training      (Tardy,      2010). Table   \n \n1      compares      our      work      against      prior      bench-  \n \n  marks      for      Wikipedia      generation."
            },
            {
              "id": 500,
              "title": "Domain      Scope      Given      Given       P      Outline?      Refs? - Chunk 4",
              "type": "chunk",
              "children": [],
              "content": "Existing      work  \n \n  has      generally      focused      on      evaluating      the      generation  \n \n  of      shorter      snippets      (e.g.,      one      paragraph),      within   \n \na       narrower      scope      (e.g.,   \n \na      specific      domain      or      two),      or  \n \n  when      an      explicit      outline      or      reference      documents  \n \n  are      supplied."
            },
            {
              "id": 501,
              "title": "Domain      Scope      Given      Given       P      Outline?      Refs? - Chunk 5",
              "type": "chunk",
              "children": [],
              "content": "A      notable      example      is      WikiSum      (Liu  \n \n  et      al.,      2018),      which      treats      generating      Wikipedia      ar-  \n \n  ticles      as   \n \na      multi-document      summarization      problem,  \n \n  with      respect      to      the      reference      documents. Our      setup      emphasizes      the      capability      of      long-  \n \n  form      grounded      writing      systems      to      research      and  \n \n  curate      content."
            },
            {
              "id": 502,
              "title": "Domain      Scope      Given      Given       P      Outline?      Refs? - Chunk 6",
              "type": "chunk",
              "children": [],
              "content": "Specifically,      given   \n \na      topic      ¢,      the  \n \n  task      is      to      find   \n \na      set      of      references   \n \n®      and      generate a full-length article S = s1598,, where each  \n \n  sentence      s;      cites   \n \na      list      of      documents      in      R.7\n2.1      The      FreshWiki      Dataset\naset  \n \n  Creating   \n \na      new      Wikipedia-like      article      demands      not  \n \n  only      fluent      writing      but      also      good      research      skills."
            },
            {
              "id": 503,
              "title": "Domain      Scope      Given      Given       P      Outline?      Refs? - Chunk 7",
              "type": "chunk",
              "children": [],
              "content": "As  \n \n  modern      LLMs      are      generally      trained      on      Wikipedia  \n \n  text,      we      mitigate      data      leakage      by      explicitly      seeking  \n \n  out      recent      Wikipedia      articles      that      were      created      (or  \n \n  very      heavily      edited)      after      the      training      cutoff      of      the  \n \n  LLMs      we      test."
            },
            {
              "id": 504,
              "title": "Domain      Scope      Given      Given       P      Outline?      Refs? - Chunk 8",
              "type": "chunk",
              "children": [],
              "content": "Our      process      can      be      repeated      at  \n \n  future      dates      when      new      LLMs      emerge. To      apply      our      date      criteria,      we      focus      on      the      top  \n \n  100      most-edited      pages,      based      on      edit      counts,      for  \n \n  each      month      from      February      2022      to      September  \n \n  2023."
            },
            {
              "id": 505,
              "title": "Domain      Scope      Given      Given       P      Outline?      Refs? - Chunk 9",
              "type": "chunk",
              "children": [],
              "content": "To      ensure      high-quality      references,      we      filter these      articles      to      keep      only      those      having      B-class  \n \n  quality      or      above      assessed      by      ORES*. We      also      ex-  \n \n  \"In      practice,   \n \nS      also      includes      organizational      elements      such  \n \n  as      section      and      subsection      titles,      which      do      not      require      citations."
            },
            {
              "id": 506,
              "title": "Domain      Scope      Given      Given       P      Outline?      Refs? - Chunk 10",
              "type": "chunk",
              "children": [],
              "content": "3      Obtained      from      https:      //wikimedia. org/api/rest_v1/metrics/edited-pages/  \n \n  top-by-edits/en.wikipedia/all-editor-types/  \n \n  content/      {year      }/{month}/all-days  \n \n  ‘https:      //www.mediawiki.org/wiki/ORES  \n \n  clude      list      articles      and      articles      that      have      no      sub-  \n \n  sections."
            },
            {
              "id": 507,
              "title": "Domain      Scope      Given      Given       P      Outline?      Refs? - Chunk 11",
              "type": "chunk",
              "children": [],
              "content": "While      high-quality      Wikipedia      articles  \n \n  usually      contain      structured      data      (e.g.,      tables)      and      are  \n \n  multi-modal,      we      only      consider      the      plain      text      com-  \n \n  ponent      in      constructing      the      dataset      to      simplify      our  \n \n  task. More      details      of      the      dataset      are      in      Appendix      A."
            },
            {
              "id": 508,
              "title": "Domain      Scope      Given      Given       P      Outline?      Refs? - Chunk 12",
              "type": "chunk",
              "children": [],
              "content": "2.2      Outline      Creation      and      Evaluation\ntion  \n \n  A      full-length      article      is      hard      to      generate      or      evalu-  \n \n  ate      (Xu      et      al.,      2023;      Krishna      et      al.,      2023)."
            },
            {
              "id": 509,
              "title": "Domain      Scope      Given      Given       P      Outline?      Refs? - Chunk 13",
              "type": "chunk",
              "children": [],
              "content": "When  \n \n  human      educators      teach      students      academic      writing,  \n \n  they      sometimes      supervise      students      at      the      outline  \n \n  stage      (Eriksson      and      Makitalo,      2015)      because      an  \n \n  extensive      outline      indicates   \n \na      comprehensive      under-  \n \n  standing      of      the      topic      and      provides   \n \na      solid      founda-  \n \n  tion      for      writing      the      full-length      article      (Dietz      and  \n \n  Foley,      2019)."
            },
            {
              "id": 510,
              "title": "Domain      Scope      Given      Given       P      Outline?      Refs? - Chunk 14",
              "type": "chunk",
              "children": [],
              "content": "Inspired      by      this,      we      decompose      the  \n \n  generation      of   \n \nS      into      two      stages. In      the      pre-writing  \n \n  stage,      we      require      the      system      to      create      an      outline  \n \n  O,      which      is      defined      as   \n \na      list      of      multi-level      section  \n \n  headings®."
            },
            {
              "id": 511,
              "title": "Domain      Scope      Given      Given       P      Outline?      Refs? - Chunk 15",
              "type": "chunk",
              "children": [],
              "content": "In      the      writing      stage,      the      system      uses  \n \n  the      topic      t,      the      references      R,      and      an      outline   \n \nO      to  \n \n  produce      the      full-length      article      S. To      evaluate      the      outline      coverage,      we      introduce  \n \n  two      metrics:      heading      soft      recall      and      heading      en-  \n \n  tity      recall."
            },
            {
              "id": 512,
              "title": "Domain      Scope      Given      Given       P      Outline?      Refs? - Chunk 16",
              "type": "chunk",
              "children": [],
              "content": "These      metrics      compare      the      multi-level  \n \n  section      headings      of      the      human-written      article,      con-  \n \n  sidered      as      ground      truth,      and      those      in      O."
            },
            {
              "id": 513,
              "title": "Domain      Scope      Given      Given       P      Outline?      Refs? - Chunk 17",
              "type": "chunk",
              "children": [],
              "content": "Recog-  \n \n  nizing      that      an      exact      match      between      elements      in  \n \n  these      two      sets      of      headings      is      unnecessary,      we      cal-  \n \n  culate      the      heading      soft      recall      (Franti      and      Mariescu-  \n \n  Istodor,      2023)      using      cosine      similarity      derived      from  \n \n  Sentence-BERT      (Reimers      and      Gurevych,      2019)      em-  \n \n  beddings      of      the      headings      (details      in      Appendix      C.1)."
            },
            {
              "id": 514,
              "title": "Domain      Scope      Given      Given       P      Outline?      Refs? - Chunk 18",
              "type": "chunk",
              "children": [],
              "content": "We      also      compute      the      heading      entity      recall      which  \n \n  is      quantified      as      the      percentage      of      named      entities      in  \n \n  human-written      article      headings      covered      by      O. We  \n \n  extract      entities      with      FLAIR      named      entity      recogni-  \n \n  tion      (NER)      (Akbik      et      al.,      2019)."
            },
            {
              "id": 515,
              "title": "Domain      Scope      Given      Given       P      Outline?      Refs? - Chunk 19",
              "type": "chunk",
              "children": [],
              "content": "3\n \n   Method  \n \n  We      present      STORM      to      automate      the      pre-writing  \n \n  stage      by      researching   \n \na      given      topic      via      effective  \n \n  question      asking      (§3.1,      §3.2)      and      creating      an      out-  \n \n  line      (§3.3)."
            },
            {
              "id": 516,
              "title": "Domain      Scope      Given      Given       P      Outline?      Refs? - Chunk 20",
              "type": "chunk",
              "children": [],
              "content": "The      outline      will      be      extended      to   \n \na      full-  \n \n  length      article      grounded      on      the      collected      references  \n \n  Shttps://en.wikipedia.org/wiki/Wikipedia:  \n \n  Stand-alone_lists  \n \n  ®Since      language      models      process      and      produce      sequences,  \n \n  we      can      linearize   \n \nO      by      adding      “#”      to      indicate      section      titles,  \n \n  “#4?”\nto      indicate      subsection      titles,      etc."
            }
          ],
          "content": ""
        },
        {
          "id": 517,
          "title": "2.1      The      FreshWiki      Dataset",
          "type": "section",
          "children": [
            {
              "id": 518,
              "title": "2.1      The      FreshWiki      Dataset - Chunk 1",
              "type": "chunk",
              "children": [],
              "content": "2.1      The      FreshWiki      Dataset\naset  \n \n  Creating   \n \na      new      Wikipedia-like      article      demands      not  \n \n  only      fluent      writing      but      also      good      research      skills."
            },
            {
              "id": 519,
              "title": "2.1      The      FreshWiki      Dataset - Chunk 2",
              "type": "chunk",
              "children": [],
              "content": "As  \n \n  modern      LLMs      are      generally      trained      on      Wikipedia  \n \n  text,      we      mitigate      data      leakage      by      explicitly      seeking  \n \n  out      recent      Wikipedia      articles      that      were      created      (or  \n \n  very      heavily      edited)      after      the      training      cutoff      of      the  \n \n  LLMs      we      test."
            },
            {
              "id": 520,
              "title": "2.1      The      FreshWiki      Dataset - Chunk 3",
              "type": "chunk",
              "children": [],
              "content": "Our      process      can      be      repeated      at  \n \n  future      dates      when      new      LLMs      emerge. To      apply      our      date      criteria,      we      focus      on      the      top  \n \n  100      most-edited      pages,      based      on      edit      counts,      for  \n \n  each      month      from      February      2022      to      September  \n \n  2023."
            },
            {
              "id": 521,
              "title": "2.1      The      FreshWiki      Dataset - Chunk 4",
              "type": "chunk",
              "children": [],
              "content": "To      ensure      high-quality      references,      we      filter these      articles      to      keep      only      those      having      B-class  \n \n  quality      or      above      assessed      by      ORES*. We      also      ex-  \n \n  \"In      practice,   \n \nS      also      includes      organizational      elements      such  \n \n  as      section      and      subsection      titles,      which      do      not      require      citations."
            },
            {
              "id": 522,
              "title": "2.1      The      FreshWiki      Dataset - Chunk 5",
              "type": "chunk",
              "children": [],
              "content": "3      Obtained      from      https:      //wikimedia. org/api/rest_v1/metrics/edited-pages/  \n \n  top-by-edits/en.wikipedia/all-editor-types/  \n \n  content/      {year      }/{month}/all-days  \n \n  ‘https:      //www.mediawiki.org/wiki/ORES  \n \n  clude      list      articles      and      articles      that      have      no      sub-  \n \n  sections."
            },
            {
              "id": 523,
              "title": "2.1      The      FreshWiki      Dataset - Chunk 6",
              "type": "chunk",
              "children": [],
              "content": "While      high-quality      Wikipedia      articles  \n \n  usually      contain      structured      data      (e.g.,      tables)      and      are  \n \n  multi-modal,      we      only      consider      the      plain      text      com-  \n \n  ponent      in      constructing      the      dataset      to      simplify      our  \n \n  task. More      details      of      the      dataset      are      in      Appendix      A."
            }
          ],
          "content": ""
        },
        {
          "id": 524,
          "title": "2.2      Outline      Creation      and      Evaluation",
          "type": "section",
          "children": [
            {
              "id": 525,
              "title": "2.2      Outline      Creation      and      Evaluation - Chunk 1",
              "type": "chunk",
              "children": [],
              "content": "2.2      Outline      Creation      and      Evaluation\ntion  \n \n  A      full-length      article      is      hard      to      generate      or      evalu-  \n \n  ate      (Xu      et      al.,      2023;      Krishna      et      al.,      2023)."
            },
            {
              "id": 526,
              "title": "2.2      Outline      Creation      and      Evaluation - Chunk 2",
              "type": "chunk",
              "children": [],
              "content": "When  \n \n  human      educators      teach      students      academic      writing,  \n \n  they      sometimes      supervise      students      at      the      outline  \n \n  stage      (Eriksson      and      Makitalo,      2015)      because      an  \n \n  extensive      outline      indicates   \n \na      comprehensive      under-  \n \n  standing      of      the      topic      and      provides   \n \na      solid      founda-  \n \n  tion      for      writing      the      full-length      article      (Dietz      and  \n \n  Foley,      2019)."
            },
            {
              "id": 527,
              "title": "2.2      Outline      Creation      and      Evaluation - Chunk 3",
              "type": "chunk",
              "children": [],
              "content": "Inspired      by      this,      we      decompose      the  \n \n  generation      of   \n \nS      into      two      stages. In      the      pre-writing  \n \n  stage,      we      require      the      system      to      create      an      outline  \n \n  O,      which      is      defined      as   \n \na      list      of      multi-level      section  \n \n  headings®."
            },
            {
              "id": 528,
              "title": "2.2      Outline      Creation      and      Evaluation - Chunk 4",
              "type": "chunk",
              "children": [],
              "content": "In      the      writing      stage,      the      system      uses  \n \n  the      topic      t,      the      references      R,      and      an      outline   \n \nO      to  \n \n  produce      the      full-length      article      S. To      evaluate      the      outline      coverage,      we      introduce  \n \n  two      metrics:      heading      soft      recall      and      heading      en-  \n \n  tity      recall."
            },
            {
              "id": 529,
              "title": "2.2      Outline      Creation      and      Evaluation - Chunk 5",
              "type": "chunk",
              "children": [],
              "content": "These      metrics      compare      the      multi-level  \n \n  section      headings      of      the      human-written      article,      con-  \n \n  sidered      as      ground      truth,      and      those      in      O."
            },
            {
              "id": 530,
              "title": "2.2      Outline      Creation      and      Evaluation - Chunk 6",
              "type": "chunk",
              "children": [],
              "content": "Recog-  \n \n  nizing      that      an      exact      match      between      elements      in  \n \n  these      two      sets      of      headings      is      unnecessary,      we      cal-  \n \n  culate      the      heading      soft      recall      (Franti      and      Mariescu-  \n \n  Istodor,      2023)      using      cosine      similarity      derived      from  \n \n  Sentence-BERT      (Reimers      and      Gurevych,      2019)      em-  \n \n  beddings      of      the      headings      (details      in      Appendix      C.1)."
            },
            {
              "id": 531,
              "title": "2.2      Outline      Creation      and      Evaluation - Chunk 7",
              "type": "chunk",
              "children": [],
              "content": "We      also      compute      the      heading      entity      recall      which  \n \n  is      quantified      as      the      percentage      of      named      entities      in  \n \n  human-written      article      headings      covered      by      O. We  \n \n  extract      entities      with      FLAIR      named      entity      recogni-  \n \n  tion      (NER)      (Akbik      et      al.,      2019)."
            },
            {
              "id": 532,
              "title": "2.2      Outline      Creation      and      Evaluation - Chunk 8",
              "type": "chunk",
              "children": [],
              "content": "3\n \n   Method  \n \n  We      present      STORM      to      automate      the      pre-writing  \n \n  stage      by      researching   \n \na      given      topic      via      effective  \n \n  question      asking      (§3.1,      §3.2)      and      creating      an      out-  \n \n  line      (§3.3)."
            },
            {
              "id": 533,
              "title": "2.2      Outline      Creation      and      Evaluation - Chunk 9",
              "type": "chunk",
              "children": [],
              "content": "The      outline      will      be      extended      to   \n \na      full-  \n \n  length      article      grounded      on      the      collected      references  \n \n  Shttps://en.wikipedia.org/wiki/Wikipedia:  \n \n  Stand-alone_lists  \n \n  ®Since      language      models      process      and      produce      sequences,  \n \n  we      can      linearize   \n \nO      by      adding      “#”      to      indicate      section      titles,  \n \n  “#4?”\nto      indicate      subsection      titles,      etc."
            }
          ],
          "content": ""
        },
        {
          "id": 534,
          "title": "Topic      t       @      Identify       Perspectives       @      Survey       —      Writer       =o       —       Related      Articles      |",
          "type": "section",
          "children": [
            {
              "id": 535,
              "title": "Topic      t       @      Identify       Perspectives       @      Survey       —      Writer       =o       —       Related      Articles      | - Chunk 1",
              "type": "chunk",
              "children": [],
              "content": "Topic      t       @      Identify       Perspectives       @      Survey       —      Writer       =o       —       Related      Articles      |\n@\n \n   Direct      Generate Question   \n \nq    \n \n@      Split      Queries  \n \n  ©      Search   \n \n&      Sift  \n \n  ©      Synthesize |\n \n   Answer   \n \na      \\\\\\\\\n |       \\\\\\\\       y      Gather       ‘\\\\\\\\      Add      Trusted | \n | ¥       ,      Cy}\n\n |       Draft      Outline      Op | Conversations {Cg,       Refine |  | \n | --- | --- | --- | ---\n | \\\\\\\\      Sources       Ns"
            }
          ],
          "content": ""
        },
        {
          "id": 536,
          "title": "References      R",
          "type": "section",
          "children": [
            {
              "id": 537,
              "title": "References      R - Chunk 1",
              "type": "chunk",
              "children": [],
              "content": "References      R\n(§3.4). Figure   \n \n2      gives      an      overview      of      STORM      and  \n \n  we      include      the      pseudo      code      in      Appendix      B. 3.1      Perspective-Guided      Question      Asking\nking  \n \n  Rohman      (1965)      defines      pre-writing      as      the      stage  \n \n  of      discovery      in      the      writing      process."
            },
            {
              "id": 538,
              "title": "References      R - Chunk 2",
              "type": "chunk",
              "children": [],
              "content": "In      parallel  \n \n  with      stakeholder      theory      in      business      (Freeman      et      al.,  \n \n  2010),      where      diverse      stakeholders      prioritize      vary-  \n \n  ing      facets      of   \n \na      company,      individuals      with      distinct  \n \n  perspectives      may      concentrate      on      different      aspects  \n \n  when      researching      the      same      topic      and      discover      mul-  \n \n  tifaceted      information."
            },
            {
              "id": 539,
              "title": "References      R - Chunk 3",
              "type": "chunk",
              "children": [],
              "content": "Further,      the      specific      perspec-  \n \n  tives      can      serve      as      prior      knowledge,      guiding      individ-  \n \n  uals      to      ask      more      in-depth      questions."
            },
            {
              "id": 540,
              "title": "References      R - Chunk 4",
              "type": "chunk",
              "children": [],
              "content": "For      example,  \n \n  an      event      planner      might      ask      about      the      “‘transporta-  \n \n  tion      arrangements”      and      “budget”      for      “the      2022  \n \n  Winter      Olympics      opening      ceremony”,      whereas   \n \na       layperson      might      ask      more      general      questions      about  \n \n  the      event’s      basic      information      (Figure   \n \n1      (A))."
            },
            {
              "id": 541,
              "title": "References      R - Chunk 5",
              "type": "chunk",
              "children": [],
              "content": "Given      the      input      topic      t,      STORM      discovers      differ-  \n \n  ent      perspectives      by      surveying      existing      articles      from  \n \n  similar      topics      and      uses      these      perspectives      to      control  \n \n  the      question      asking      process."
            },
            {
              "id": 542,
              "title": "References      R - Chunk 6",
              "type": "chunk",
              "children": [],
              "content": "Specifically,      STORM  \n \n  prompts      an      LLM      to      generate   \n \na      list      of      related      top-  \n \n  ics      and      subsequently      extracts      the      tables      of      contents  \n \n  from      their      corresponding      Wikipedia      articles,      if      such  \n \n  articles      can      be      obtained      through      Wikipedia      API’  \n \n  (Figure   \n \n2      (1)."
            },
            {
              "id": 543,
              "title": "References      R - Chunk 7",
              "type": "chunk",
              "children": [],
              "content": "These      tables      of      contents      are      con- catenated      to      create   \n \na      context      to      prompt      the      LLM to identify N perspectives P = {p1,, pn} that  \n \n  be      evaluated      using   \n \na      rule-based      filter      according      to  \n \n  the      Wikipedia      guideline®      to      exclude      untrustworthy  \n \n  sources      (Figure   \n \n2      (5))."
            },
            {
              "id": 544,
              "title": "References      R - Chunk 8",
              "type": "chunk",
              "children": [],
              "content": "Finally,      the      LLM      synthe-  \n \n  Thttps://pypi.org/project/Wikipedia-API/  \n \n  can      collectively      contribute      to   \n \na      comprehensive      ar-  \n \n  ticle      on   \n \n¢      (Figure   \n \n2      (2))."
            },
            {
              "id": 545,
              "title": "References      R - Chunk 9",
              "type": "chunk",
              "children": [],
              "content": "To      ensure      that      the      basic  \n \n  information      about   \n \n¢      is      also      covered,      we      add      pg      as  \n \n  “basic      fact      writer      focusing      on      broadly      covering      the  \n \n  basic      facts      about      the      topic”      into      P."
            },
            {
              "id": 546,
              "title": "References      R - Chunk 10",
              "type": "chunk",
              "children": [],
              "content": "Each      perspec-  \n \n  tive   \n \np   \n \n€   \n \nP      will      be      utilized      to      guide      the      LLM      in      the  \n \n  process      of      question      asking      in      parallel."
            },
            {
              "id": 547,
              "title": "References      R - Chunk 11",
              "type": "chunk",
              "children": [],
              "content": "3.2      Simulating      Conversations\nions  \n \n  The      theory      of      questions      and      question      asking      (Ram,  \n \n  1991)      highlights      that      while      answers      to      existing  \n \n  questions      contribute      to   \n \na      more      comprehensive  \n \n  understanding      of   \n \na      topic,      they      often      simultane-  \n \n  ously      give      rise      to      new      questions."
            },
            {
              "id": 548,
              "title": "References      R - Chunk 12",
              "type": "chunk",
              "children": [],
              "content": "To      kick      off      this  \n \n  dynamic      process,      STORM      simulates   \n \na      conversa-  \n \n  tion      between   \n \na      Wikipedia      writer      and   \n \na      topic      ex-  \n \n  pert."
            },
            {
              "id": 549,
              "title": "References      R - Chunk 13",
              "type": "chunk",
              "children": [],
              "content": "In      the      z-th      round      of      the      conversation,      the  \n \n  LLM-powered      Wikipedia      writer      generates   \n \na      sin-  \n \n  gle      question      q;      based      on      the      topic      1,      its      assigned  \n \n  perspective   \n \np   \n \n€      P,      and      the      conversation      history  \n \n  {q1,      41,      ---,      Gi-1,      41-1}      where      a;      denotes      the      sim-  \n \n  ulated      expert’s      answer."
            },
            {
              "id": 550,
              "title": "References      R - Chunk 14",
              "type": "chunk",
              "children": [],
              "content": "The      conversation      history  \n \n  enables      the      LLM      to      update      its      understanding      of      the  \n \n  topic      and      ask      follow-up      questions. In      practice,      we  \n \n  limit      the      conversation      to      at      most   \n \n/      rounds."
            },
            {
              "id": 551,
              "title": "References      R - Chunk 15",
              "type": "chunk",
              "children": [],
              "content": "To      ensure      that      the      conversation      history      provides  \n \n  factual      information,      we      use      trusted      sources      from  \n \n  the      Internet      to      ground      the      answer      a;      to      each      query  \n \n  sizes      the      trustworthy      sources      to      generate      the      answer  \n \n  a;,      and      these      sources      will      also      be      added      to      R      for  \n \n  full      article      generation      (§3.4)."
            },
            {
              "id": 552,
              "title": "References      R - Chunk 16",
              "type": "chunk",
              "children": [],
              "content": "q."
            },
            {
              "id": 553,
              "title": "References      R - Chunk 17",
              "type": "chunk",
              "children": [],
              "content": "Since      q;      can      be      complicated,      we      first      prompt  \n \n  the      LLM      to      break      down      q;      into   \n \na      set      of      search  \n \n  queries      (Figure   \n \n2      (4))      and      the      searched      results      will\n3.3      Creating      the      Article      Outline\nline  \n \n  After      thoroughly      researching      the      topic      through  \n \n  N   \n \n+   \n \n1      simulated      conversations,      denoted      as {Co, Ci, -,; Cw }, STORM creates an outline before  \n \n  the      actual      writing      starts."
            },
            {
              "id": 554,
              "title": "References      R - Chunk 18",
              "type": "chunk",
              "children": [],
              "content": "To      fully      leverage      the      inter-  \n \n  nal      knowledge      of      LLMs,      we      first      prompt      the      model  \n \n  to      generate   \n \na      draft      outline      Op      given      only      the      topic  \n \n  t      (Figure   \n \n2      (7)). Op      typically      provides   \n \na      general  \n \n  but      organized      framework."
            },
            {
              "id": 555,
              "title": "References      R - Chunk 19",
              "type": "chunk",
              "children": [],
              "content": "Subsequently,      the      LLM  \n \n  is      prompted      with      the      topic      ¢,      the      draft      outline      Op, and the simulated conversations {Co, Cj,,Cw}  \n \n  to      refine      the      outline      (Figure   \n \n2      (8)). This      results in      an      improved      outline   \n \nO      which      will      be      used      for  \n \n  producing      the      full-length      article."
            },
            {
              "id": 556,
              "title": "References      R - Chunk 20",
              "type": "chunk",
              "children": [],
              "content": "3.4      Writing      the      Full-Length      Article\nicle  \n \n  Building      upon      the      references      R      collected      and      the  \n \n  outline   \n \nO      developed      during      the      pre-writing      stage,  \n \n  the      full-length      article      can      be      composed      section      by  \n \n  section."
            },
            {
              "id": 557,
              "title": "References      R - Chunk 21",
              "type": "chunk",
              "children": [],
              "content": "Since      it      is      usually      impossible      to      fit      the  \n \n  entire   \n \n7      within      the      context      window      of      the      LLM,  \n \n  we      use      the      section      title      and      headings      of      its      all-level  \n \n  subsections      to      retrieve      relevant      documents      from  \n \n  R      based      on      semantic      similarity      calculated      from  \n \n  Sentence-BERT      embeddings."
            },
            {
              "id": 558,
              "title": "References      R - Chunk 22",
              "type": "chunk",
              "children": [],
              "content": "With      the      relevant      in-  \n \n  formation      at      hand,      the      LLM      is      then      prompted      to  \n \n  generate      the      section      with      citations. Once      all      sec-  \n \n  tions      are      generated,      they      are      concatenated      to      form  \n \n  the      full-length      article."
            },
            {
              "id": 559,
              "title": "References      R - Chunk 23",
              "type": "chunk",
              "children": [],
              "content": "Since      the      sections      are      gen-  \n \n  erated      in      parallel,      we      prompt      the      LLM      with      the  \n \n  concatenated      article      to      delete      repeated      information  \n \n  to      improve      coherence."
            },
            {
              "id": 560,
              "title": "References      R - Chunk 24",
              "type": "chunk",
              "children": [],
              "content": "Furthermore,      in      alignment  \n \n  with      Wikipedia’s      stylistic      norms,      the      LLM      is      also  \n \n  utilized      to      synthesize   \n \na      summary      of      the      entire      arti-  \n \n  cle,      forming      the      lead      section      at      the      beginning."
            },
            {
              "id": 561,
              "title": "References      R - Chunk 25",
              "type": "chunk",
              "children": [],
              "content": "4\n \n   Experiments\n4.1      Article      Selection\ntion  \n \n  STORM      is      capable      of      researching      complicated      top-  \n \n  ics      and      writing      long      articles      from      detailed      outlines. However,      in      this      controlled      experiment,      we      limit  \n \n  the      final      output      to      at      most      4000      tokens      (roughly  \n \n  3000      words)."
            },
            {
              "id": 562,
              "title": "References      R - Chunk 26",
              "type": "chunk",
              "children": [],
              "content": "For   \n \na      meaningful      comparison,      we  \n \n  Shttps://en.wikipedia. org/wiki/Wikipedia:  \n \n  Reliable_sources  \n \n  randomly      select      100      samples      from      the      Fresh      Wiki  \n \n  dataset      (see      §2.1)      that      have      human-written      articles  \n \n  not      exceeding      3000      words."
            },
            {
              "id": 563,
              "title": "References      R - Chunk 27",
              "type": "chunk",
              "children": [],
              "content": "4.2      Automatic      Metrics\nrics  \n \n  As      discussed      in      §2.2,      we      evaluate      the      outline      qual-  \n \n  ity      to      assess      the      pre-writing      stage      by      calculating  \n \n  the      heading      soft      recall      and      heading      entity      recall. A       higher      recall      score      signifies   \n \na      more      comprehensive  \n \n  outline      relative      to      the      human-written      article."
            },
            {
              "id": 564,
              "title": "References      R - Chunk 28",
              "type": "chunk",
              "children": [],
              "content": "To      assess      the      full-length      article      quality,      we      adopt  \n \n  ROUGE      scores      (Lin,      2004)      and      compute      the      entity  \n \n  recall      in      the      article      level      based      on      FLAIR      NER  \n \n  results."
            },
            {
              "id": 565,
              "title": "References      R - Chunk 29",
              "type": "chunk",
              "children": [],
              "content": "Moreover,      based      on      Wikipedia      criteria’,  \n \n  we      evaluate      the      article      from      the      aspects      of      (1)      In-  \n \n  terest      Level,      (2)      Coherence      and      Organization,      (3)  \n \n  Relevance      and      Focus,      (4)      Coverage,      and      (5)      Verifia-  \n \n  bility."
            },
            {
              "id": 566,
              "title": "References      R - Chunk 30",
              "type": "chunk",
              "children": [],
              "content": "For      aspects      (1)-(4),      we      use      Prometheus      (Kim  \n \n  et      al.,      2023),   \n \na      13B      evaluator      LLM      to      score      the      arti-  \n \n  cle      based      on   \n \na      5-point      rubric      collaboratively      devel-  \n \n  oped      with      two      experienced      Wikipedia      editors      (see  \n \n  Appendix      C.2)."
            },
            {
              "id": 567,
              "title": "References      R - Chunk 31",
              "type": "chunk",
              "children": [],
              "content": "For      verifiability,      we      calculate      the  \n \n  citation      recall      and      citation      precision      based      on      the  \n \n  definition      in      Gao      et      al. (2023). We      use      Mistral      7B-  \n \n  Instruct      (Jiang      et      al.,      2023a)      to      examine      whether  \n \n  the      cited      passages      entail      the      generated      sentence."
            },
            {
              "id": 568,
              "title": "References      R - Chunk 32",
              "type": "chunk",
              "children": [],
              "content": "4.3      Baselines\nines  \n \n  As      prior      works      use      different      setups      and      do      not      use  \n \n  LLMs,      they      are      hard      to      compare      directly. Instead,  \n \n  we      use      the      following      three      LLM-based      baselines. 1."
            },
            {
              "id": 569,
              "title": "References      R - Chunk 33",
              "type": "chunk",
              "children": [],
              "content": "Direct      Gen,   \n \na      baseline      that      directly      prompts  \n \n  the      LLM      to      generate      an      outline,      which      is      then  \n \n  used      to      generate      the      full-length      article. 2."
            },
            {
              "id": 570,
              "title": "References      R - Chunk 34",
              "type": "chunk",
              "children": [],
              "content": "RAG,   \n \na      retrieval-augmented      generation      base-  \n \n  line      that      searches      with      the      topic      and      uses      the  \n \n  searched      results      together      with      the      topic   \n \n¢      to  \n \n  generate      an      outline      or      the      entire      article. 3."
            },
            {
              "id": 571,
              "title": "References      R - Chunk 35",
              "type": "chunk",
              "children": [],
              "content": "Outline-driven      RAG      (ORAG),      which      is      iden-  \n \n  tical      to      RAG      in      outline      creation,      but      further  \n \n  searches      additional      information      with      section  \n \n  titles      to      generate      the      article      section      by      section."
            },
            {
              "id": 572,
              "title": "References      R - Chunk 36",
              "type": "chunk",
              "children": [],
              "content": "4.4      STORM      Implementation\ntion  \n \n  We      build      STORM      with      zero-shot      prompting      us-  \n \n  ing      the      DSPy      framework      (Khattab      et      al.,      2023). Appendix   \n \nB      includes      the      pseudo      code      and      corre-  \n \n  sponding      prompts. The      hyperparameters      N      and      MZ °https://en.wikipedia.org/wiki/Wikipedia:  \n \n  Good_article_criteria"
            }
          ],
          "content": ""
        },
        {
          "id": 573,
          "title": "3.1      Perspective-Guided      Question      Asking",
          "type": "section",
          "children": [
            {
              "id": 574,
              "title": "3.1      Perspective-Guided      Question      Asking - Chunk 1",
              "type": "chunk",
              "children": [],
              "content": "3.1      Perspective-Guided      Question      Asking\nking  \n \n  Rohman      (1965)      defines      pre-writing      as      the      stage  \n \n  of      discovery      in      the      writing      process."
            },
            {
              "id": 575,
              "title": "3.1      Perspective-Guided      Question      Asking - Chunk 2",
              "type": "chunk",
              "children": [],
              "content": "In      parallel  \n \n  with      stakeholder      theory      in      business      (Freeman      et      al.,  \n \n  2010),      where      diverse      stakeholders      prioritize      vary-  \n \n  ing      facets      of   \n \na      company,      individuals      with      distinct  \n \n  perspectives      may      concentrate      on      different      aspects  \n \n  when      researching      the      same      topic      and      discover      mul-  \n \n  tifaceted      information."
            },
            {
              "id": 576,
              "title": "3.1      Perspective-Guided      Question      Asking - Chunk 3",
              "type": "chunk",
              "children": [],
              "content": "Further,      the      specific      perspec-  \n \n  tives      can      serve      as      prior      knowledge,      guiding      individ-  \n \n  uals      to      ask      more      in-depth      questions."
            },
            {
              "id": 577,
              "title": "3.1      Perspective-Guided      Question      Asking - Chunk 4",
              "type": "chunk",
              "children": [],
              "content": "For      example,  \n \n  an      event      planner      might      ask      about      the      “‘transporta-  \n \n  tion      arrangements”      and      “budget”      for      “the      2022  \n \n  Winter      Olympics      opening      ceremony”,      whereas   \n \na       layperson      might      ask      more      general      questions      about  \n \n  the      event’s      basic      information      (Figure   \n \n1      (A))."
            },
            {
              "id": 578,
              "title": "3.1      Perspective-Guided      Question      Asking - Chunk 5",
              "type": "chunk",
              "children": [],
              "content": "Given      the      input      topic      t,      STORM      discovers      differ-  \n \n  ent      perspectives      by      surveying      existing      articles      from  \n \n  similar      topics      and      uses      these      perspectives      to      control  \n \n  the      question      asking      process."
            },
            {
              "id": 579,
              "title": "3.1      Perspective-Guided      Question      Asking - Chunk 6",
              "type": "chunk",
              "children": [],
              "content": "Specifically,      STORM  \n \n  prompts      an      LLM      to      generate   \n \na      list      of      related      top-  \n \n  ics      and      subsequently      extracts      the      tables      of      contents  \n \n  from      their      corresponding      Wikipedia      articles,      if      such  \n \n  articles      can      be      obtained      through      Wikipedia      API’  \n \n  (Figure   \n \n2      (1)."
            },
            {
              "id": 580,
              "title": "3.1      Perspective-Guided      Question      Asking - Chunk 7",
              "type": "chunk",
              "children": [],
              "content": "These      tables      of      contents      are      con- catenated      to      create   \n \na      context      to      prompt      the      LLM to identify N perspectives P = {p1,, pn} that  \n \n  be      evaluated      using   \n \na      rule-based      filter      according      to  \n \n  the      Wikipedia      guideline®      to      exclude      untrustworthy  \n \n  sources      (Figure   \n \n2      (5))."
            },
            {
              "id": 581,
              "title": "3.1      Perspective-Guided      Question      Asking - Chunk 8",
              "type": "chunk",
              "children": [],
              "content": "Finally,      the      LLM      synthe-  \n \n  Thttps://pypi.org/project/Wikipedia-API/  \n \n  can      collectively      contribute      to   \n \na      comprehensive      ar-  \n \n  ticle      on   \n \n¢      (Figure   \n \n2      (2))."
            },
            {
              "id": 582,
              "title": "3.1      Perspective-Guided      Question      Asking - Chunk 9",
              "type": "chunk",
              "children": [],
              "content": "To      ensure      that      the      basic  \n \n  information      about   \n \n¢      is      also      covered,      we      add      pg      as  \n \n  “basic      fact      writer      focusing      on      broadly      covering      the  \n \n  basic      facts      about      the      topic”      into      P."
            },
            {
              "id": 583,
              "title": "3.1      Perspective-Guided      Question      Asking - Chunk 10",
              "type": "chunk",
              "children": [],
              "content": "Each      perspec-  \n \n  tive   \n \np   \n \n€   \n \nP      will      be      utilized      to      guide      the      LLM      in      the  \n \n  process      of      question      asking      in      parallel."
            }
          ],
          "content": ""
        },
        {
          "id": 584,
          "title": "3.2      Simulating      Conversations",
          "type": "section",
          "children": [
            {
              "id": 585,
              "title": "3.2      Simulating      Conversations - Chunk 1",
              "type": "chunk",
              "children": [],
              "content": "3.2      Simulating      Conversations\nions  \n \n  The      theory      of      questions      and      question      asking      (Ram,  \n \n  1991)      highlights      that      while      answers      to      existing  \n \n  questions      contribute      to   \n \na      more      comprehensive  \n \n  understanding      of   \n \na      topic,      they      often      simultane-  \n \n  ously      give      rise      to      new      questions."
            },
            {
              "id": 586,
              "title": "3.2      Simulating      Conversations - Chunk 2",
              "type": "chunk",
              "children": [],
              "content": "To      kick      off      this  \n \n  dynamic      process,      STORM      simulates   \n \na      conversa-  \n \n  tion      between   \n \na      Wikipedia      writer      and   \n \na      topic      ex-  \n \n  pert."
            },
            {
              "id": 587,
              "title": "3.2      Simulating      Conversations - Chunk 3",
              "type": "chunk",
              "children": [],
              "content": "In      the      z-th      round      of      the      conversation,      the  \n \n  LLM-powered      Wikipedia      writer      generates   \n \na      sin-  \n \n  gle      question      q;      based      on      the      topic      1,      its      assigned  \n \n  perspective   \n \np   \n \n€      P,      and      the      conversation      history  \n \n  {q1,      41,      ---,      Gi-1,      41-1}      where      a;      denotes      the      sim-  \n \n  ulated      expert’s      answer."
            },
            {
              "id": 588,
              "title": "3.2      Simulating      Conversations - Chunk 4",
              "type": "chunk",
              "children": [],
              "content": "The      conversation      history  \n \n  enables      the      LLM      to      update      its      understanding      of      the  \n \n  topic      and      ask      follow-up      questions. In      practice,      we  \n \n  limit      the      conversation      to      at      most   \n \n/      rounds."
            },
            {
              "id": 589,
              "title": "3.2      Simulating      Conversations - Chunk 5",
              "type": "chunk",
              "children": [],
              "content": "To      ensure      that      the      conversation      history      provides  \n \n  factual      information,      we      use      trusted      sources      from  \n \n  the      Internet      to      ground      the      answer      a;      to      each      query  \n \n  sizes      the      trustworthy      sources      to      generate      the      answer  \n \n  a;,      and      these      sources      will      also      be      added      to      R      for  \n \n  full      article      generation      (§3.4)."
            },
            {
              "id": 590,
              "title": "3.2      Simulating      Conversations - Chunk 6",
              "type": "chunk",
              "children": [],
              "content": "q. Since      q;      can      be      complicated,      we      first      prompt  \n \n  the      LLM      to      break      down      q;      into   \n \na      set      of      search  \n \n  queries      (Figure   \n \n2      (4))      and      the      searched      results      will"
            }
          ],
          "content": ""
        },
        {
          "id": 591,
          "title": "3.3      Creating      the      Article      Outline",
          "type": "section",
          "children": [
            {
              "id": 592,
              "title": "3.3      Creating      the      Article      Outline - Chunk 1",
              "type": "chunk",
              "children": [],
              "content": "3.3      Creating      the      Article      Outline\nline  \n \n  After      thoroughly      researching      the      topic      through  \n \n  N   \n \n+   \n \n1      simulated      conversations,      denoted      as {Co, Ci, -,; Cw }, STORM creates an outline before  \n \n  the      actual      writing      starts."
            },
            {
              "id": 593,
              "title": "3.3      Creating      the      Article      Outline - Chunk 2",
              "type": "chunk",
              "children": [],
              "content": "To      fully      leverage      the      inter-  \n \n  nal      knowledge      of      LLMs,      we      first      prompt      the      model  \n \n  to      generate   \n \na      draft      outline      Op      given      only      the      topic  \n \n  t      (Figure   \n \n2      (7)). Op      typically      provides   \n \na      general  \n \n  but      organized      framework."
            },
            {
              "id": 594,
              "title": "3.3      Creating      the      Article      Outline - Chunk 3",
              "type": "chunk",
              "children": [],
              "content": "Subsequently,      the      LLM  \n \n  is      prompted      with      the      topic      ¢,      the      draft      outline      Op, and the simulated conversations {Co, Cj,,Cw}  \n \n  to      refine      the      outline      (Figure   \n \n2      (8)). This      results in      an      improved      outline   \n \nO      which      will      be      used      for  \n \n  producing      the      full-length      article."
            }
          ],
          "content": ""
        },
        {
          "id": 595,
          "title": "3.4      Writing      the      Full-Length      Article",
          "type": "section",
          "children": [
            {
              "id": 596,
              "title": "3.4      Writing      the      Full-Length      Article - Chunk 1",
              "type": "chunk",
              "children": [],
              "content": "3.4      Writing      the      Full-Length      Article\nicle  \n \n  Building      upon      the      references      R      collected      and      the  \n \n  outline   \n \nO      developed      during      the      pre-writing      stage,  \n \n  the      full-length      article      can      be      composed      section      by  \n \n  section."
            },
            {
              "id": 597,
              "title": "3.4      Writing      the      Full-Length      Article - Chunk 2",
              "type": "chunk",
              "children": [],
              "content": "Since      it      is      usually      impossible      to      fit      the  \n \n  entire   \n \n7      within      the      context      window      of      the      LLM,  \n \n  we      use      the      section      title      and      headings      of      its      all-level  \n \n  subsections      to      retrieve      relevant      documents      from  \n \n  R      based      on      semantic      similarity      calculated      from  \n \n  Sentence-BERT      embeddings."
            },
            {
              "id": 598,
              "title": "3.4      Writing      the      Full-Length      Article - Chunk 3",
              "type": "chunk",
              "children": [],
              "content": "With      the      relevant      in-  \n \n  formation      at      hand,      the      LLM      is      then      prompted      to  \n \n  generate      the      section      with      citations. Once      all      sec-  \n \n  tions      are      generated,      they      are      concatenated      to      form  \n \n  the      full-length      article."
            },
            {
              "id": 599,
              "title": "3.4      Writing      the      Full-Length      Article - Chunk 4",
              "type": "chunk",
              "children": [],
              "content": "Since      the      sections      are      gen-  \n \n  erated      in      parallel,      we      prompt      the      LLM      with      the  \n \n  concatenated      article      to      delete      repeated      information  \n \n  to      improve      coherence."
            },
            {
              "id": 600,
              "title": "3.4      Writing      the      Full-Length      Article - Chunk 5",
              "type": "chunk",
              "children": [],
              "content": "Furthermore,      in      alignment  \n \n  with      Wikipedia’s      stylistic      norms,      the      LLM      is      also  \n \n  utilized      to      synthesize   \n \na      summary      of      the      entire      arti-  \n \n  cle,      forming      the      lead      section      at      the      beginning."
            },
            {
              "id": 601,
              "title": "3.4      Writing      the      Full-Length      Article - Chunk 6",
              "type": "chunk",
              "children": [],
              "content": "4\n \n   Experiments\n4.1      Article      Selection\ntion  \n \n  STORM      is      capable      of      researching      complicated      top-  \n \n  ics      and      writing      long      articles      from      detailed      outlines. However,      in      this      controlled      experiment,      we      limit  \n \n  the      final      output      to      at      most      4000      tokens      (roughly  \n \n  3000      words)."
            },
            {
              "id": 602,
              "title": "3.4      Writing      the      Full-Length      Article - Chunk 7",
              "type": "chunk",
              "children": [],
              "content": "For   \n \na      meaningful      comparison,      we  \n \n  Shttps://en.wikipedia. org/wiki/Wikipedia:  \n \n  Reliable_sources  \n \n  randomly      select      100      samples      from      the      Fresh      Wiki  \n \n  dataset      (see      §2.1)      that      have      human-written      articles  \n \n  not      exceeding      3000      words."
            },
            {
              "id": 603,
              "title": "3.4      Writing      the      Full-Length      Article - Chunk 8",
              "type": "chunk",
              "children": [],
              "content": "4.2      Automatic      Metrics\nrics  \n \n  As      discussed      in      §2.2,      we      evaluate      the      outline      qual-  \n \n  ity      to      assess      the      pre-writing      stage      by      calculating  \n \n  the      heading      soft      recall      and      heading      entity      recall. A       higher      recall      score      signifies   \n \na      more      comprehensive  \n \n  outline      relative      to      the      human-written      article."
            },
            {
              "id": 604,
              "title": "3.4      Writing      the      Full-Length      Article - Chunk 9",
              "type": "chunk",
              "children": [],
              "content": "To      assess      the      full-length      article      quality,      we      adopt  \n \n  ROUGE      scores      (Lin,      2004)      and      compute      the      entity  \n \n  recall      in      the      article      level      based      on      FLAIR      NER  \n \n  results."
            },
            {
              "id": 605,
              "title": "3.4      Writing      the      Full-Length      Article - Chunk 10",
              "type": "chunk",
              "children": [],
              "content": "Moreover,      based      on      Wikipedia      criteria’,  \n \n  we      evaluate      the      article      from      the      aspects      of      (1)      In-  \n \n  terest      Level,      (2)      Coherence      and      Organization,      (3)  \n \n  Relevance      and      Focus,      (4)      Coverage,      and      (5)      Verifia-  \n \n  bility."
            },
            {
              "id": 606,
              "title": "3.4      Writing      the      Full-Length      Article - Chunk 11",
              "type": "chunk",
              "children": [],
              "content": "For      aspects      (1)-(4),      we      use      Prometheus      (Kim  \n \n  et      al.,      2023),   \n \na      13B      evaluator      LLM      to      score      the      arti-  \n \n  cle      based      on   \n \na      5-point      rubric      collaboratively      devel-  \n \n  oped      with      two      experienced      Wikipedia      editors      (see  \n \n  Appendix      C.2)."
            },
            {
              "id": 607,
              "title": "3.4      Writing      the      Full-Length      Article - Chunk 12",
              "type": "chunk",
              "children": [],
              "content": "For      verifiability,      we      calculate      the  \n \n  citation      recall      and      citation      precision      based      on      the  \n \n  definition      in      Gao      et      al. (2023). We      use      Mistral      7B-  \n \n  Instruct      (Jiang      et      al.,      2023a)      to      examine      whether  \n \n  the      cited      passages      entail      the      generated      sentence."
            },
            {
              "id": 608,
              "title": "3.4      Writing      the      Full-Length      Article - Chunk 13",
              "type": "chunk",
              "children": [],
              "content": "4.3      Baselines\nines  \n \n  As      prior      works      use      different      setups      and      do      not      use  \n \n  LLMs,      they      are      hard      to      compare      directly. Instead,  \n \n  we      use      the      following      three      LLM-based      baselines. 1."
            },
            {
              "id": 609,
              "title": "3.4      Writing      the      Full-Length      Article - Chunk 14",
              "type": "chunk",
              "children": [],
              "content": "Direct      Gen,   \n \na      baseline      that      directly      prompts  \n \n  the      LLM      to      generate      an      outline,      which      is      then  \n \n  used      to      generate      the      full-length      article. 2."
            },
            {
              "id": 610,
              "title": "3.4      Writing      the      Full-Length      Article - Chunk 15",
              "type": "chunk",
              "children": [],
              "content": "RAG,   \n \na      retrieval-augmented      generation      base-  \n \n  line      that      searches      with      the      topic      and      uses      the  \n \n  searched      results      together      with      the      topic   \n \n¢      to  \n \n  generate      an      outline      or      the      entire      article. 3."
            },
            {
              "id": 611,
              "title": "3.4      Writing      the      Full-Length      Article - Chunk 16",
              "type": "chunk",
              "children": [],
              "content": "Outline-driven      RAG      (ORAG),      which      is      iden-  \n \n  tical      to      RAG      in      outline      creation,      but      further  \n \n  searches      additional      information      with      section  \n \n  titles      to      generate      the      article      section      by      section."
            },
            {
              "id": 612,
              "title": "3.4      Writing      the      Full-Length      Article - Chunk 17",
              "type": "chunk",
              "children": [],
              "content": "4.4      STORM      Implementation\ntion  \n \n  We      build      STORM      with      zero-shot      prompting      us-  \n \n  ing      the      DSPy      framework      (Khattab      et      al.,      2023). Appendix   \n \nB      includes      the      pseudo      code      and      corre-  \n \n  sponding      prompts. The      hyperparameters      N      and      MZ °https://en.wikipedia.org/wiki/Wikipedia:  \n \n  Good_article_criteria"
            }
          ],
          "content": ""
        },
        {
          "id": 613,
          "title": "4.1      Article      Selection",
          "type": "section",
          "children": [
            {
              "id": 614,
              "title": "4.1      Article      Selection - Chunk 1",
              "type": "chunk",
              "children": [],
              "content": "4.1      Article      Selection\ntion  \n \n  STORM      is      capable      of      researching      complicated      top-  \n \n  ics      and      writing      long      articles      from      detailed      outlines. However,      in      this      controlled      experiment,      we      limit  \n \n  the      final      output      to      at      most      4000      tokens      (roughly  \n \n  3000      words). For   \n \na      meaningful      comparison,      we  \n \n  Shttps://en.wikipedia."
            },
            {
              "id": 615,
              "title": "4.1      Article      Selection - Chunk 2",
              "type": "chunk",
              "children": [],
              "content": "org/wiki/Wikipedia:  \n \n  Reliable_sources  \n \n  randomly      select      100      samples      from      the      Fresh      Wiki  \n \n  dataset      (see      §2.1)      that      have      human-written      articles  \n \n  not      exceeding      3000      words."
            }
          ],
          "content": ""
        },
        {
          "id": 616,
          "title": "4.2      Automatic      Metrics",
          "type": "section",
          "children": [
            {
              "id": 617,
              "title": "4.2      Automatic      Metrics - Chunk 1",
              "type": "chunk",
              "children": [],
              "content": "4.2      Automatic      Metrics\nrics  \n \n  As      discussed      in      §2.2,      we      evaluate      the      outline      qual-  \n \n  ity      to      assess      the      pre-writing      stage      by      calculating  \n \n  the      heading      soft      recall      and      heading      entity      recall. A       higher      recall      score      signifies   \n \na      more      comprehensive  \n \n  outline      relative      to      the      human-written      article."
            },
            {
              "id": 618,
              "title": "4.2      Automatic      Metrics - Chunk 2",
              "type": "chunk",
              "children": [],
              "content": "To      assess      the      full-length      article      quality,      we      adopt  \n \n  ROUGE      scores      (Lin,      2004)      and      compute      the      entity  \n \n  recall      in      the      article      level      based      on      FLAIR      NER  \n \n  results."
            },
            {
              "id": 619,
              "title": "4.2      Automatic      Metrics - Chunk 3",
              "type": "chunk",
              "children": [],
              "content": "Moreover,      based      on      Wikipedia      criteria’,  \n \n  we      evaluate      the      article      from      the      aspects      of      (1)      In-  \n \n  terest      Level,      (2)      Coherence      and      Organization,      (3)  \n \n  Relevance      and      Focus,      (4)      Coverage,      and      (5)      Verifia-  \n \n  bility."
            },
            {
              "id": 620,
              "title": "4.2      Automatic      Metrics - Chunk 4",
              "type": "chunk",
              "children": [],
              "content": "For      aspects      (1)-(4),      we      use      Prometheus      (Kim  \n \n  et      al.,      2023),   \n \na      13B      evaluator      LLM      to      score      the      arti-  \n \n  cle      based      on   \n \na      5-point      rubric      collaboratively      devel-  \n \n  oped      with      two      experienced      Wikipedia      editors      (see  \n \n  Appendix      C.2)."
            },
            {
              "id": 621,
              "title": "4.2      Automatic      Metrics - Chunk 5",
              "type": "chunk",
              "children": [],
              "content": "For      verifiability,      we      calculate      the  \n \n  citation      recall      and      citation      precision      based      on      the  \n \n  definition      in      Gao      et      al. (2023). We      use      Mistral      7B-  \n \n  Instruct      (Jiang      et      al.,      2023a)      to      examine      whether  \n \n  the      cited      passages      entail      the      generated      sentence."
            }
          ],
          "content": ""
        },
        {
          "id": 622,
          "title": "4.3      Baselines",
          "type": "section",
          "children": [
            {
              "id": 623,
              "title": "4.3      Baselines - Chunk 1",
              "type": "chunk",
              "children": [],
              "content": "4.3      Baselines\nines  \n \n  As      prior      works      use      different      setups      and      do      not      use  \n \n  LLMs,      they      are      hard      to      compare      directly. Instead,  \n \n  we      use      the      following      three      LLM-based      baselines. 1."
            },
            {
              "id": 624,
              "title": "4.3      Baselines - Chunk 2",
              "type": "chunk",
              "children": [],
              "content": "Direct      Gen,   \n \na      baseline      that      directly      prompts  \n \n  the      LLM      to      generate      an      outline,      which      is      then  \n \n  used      to      generate      the      full-length      article. 2."
            },
            {
              "id": 625,
              "title": "4.3      Baselines - Chunk 3",
              "type": "chunk",
              "children": [],
              "content": "RAG,   \n \na      retrieval-augmented      generation      base-  \n \n  line      that      searches      with      the      topic      and      uses      the  \n \n  searched      results      together      with      the      topic   \n \n¢      to  \n \n  generate      an      outline      or      the      entire      article. 3."
            },
            {
              "id": 626,
              "title": "4.3      Baselines - Chunk 4",
              "type": "chunk",
              "children": [],
              "content": "Outline-driven      RAG      (ORAG),      which      is      iden-  \n \n  tical      to      RAG      in      outline      creation,      but      further  \n \n  searches      additional      information      with      section  \n \n  titles      to      generate      the      article      section      by      section."
            }
          ],
          "content": ""
        },
        {
          "id": 627,
          "title": "4.4      STORM      Implementation",
          "type": "section",
          "children": [
            {
              "id": 628,
              "title": "4.4      STORM      Implementation - Chunk 1",
              "type": "chunk",
              "children": [],
              "content": "4.4      STORM      Implementation\ntion  \n \n  We      build      STORM      with      zero-shot      prompting      us-  \n \n  ing      the      DSPy      framework      (Khattab      et      al.,      2023). Appendix   \n \nB      includes      the      pseudo      code      and      corre-  \n \n  sponding      prompts. The      hyperparameters      N      and      MZ °https://en.wikipedia.org/wiki/Wikipedia:  \n \n  Good_article_criteria"
            }
          ],
          "content": ""
        },
        {
          "id": 629,
          "title": "Comparsion      with      Human-written      Articles      Rubric      Grading",
          "type": "section",
          "children": [
            {
              "id": 630,
              "title": "Comparsion      with      Human-written      Articles      Rubric      Grading - Chunk 1",
              "type": "chunk",
              "children": [],
              "content": "Comparsion      with      Human-written      Articles      Rubric      Grading"
            }
          ],
          "content": ""
        },
        {
          "id": 631,
          "title": "ROUGE-1      ROUGE-L      Entity      Recall      Interest      Level      Organization      Relevance      Coverage",
          "type": "section",
          "children": [
            {
              "id": 632,
              "title": "ROUGE-1      ROUGE-L      Entity      Recall      Interest      Level      Organization      Relevance      Coverage - Chunk 1",
              "type": "chunk",
              "children": [],
              "content": "ROUGE-1      ROUGE-L      Entity      Recall      Interest      Level      Organization      Relevance      Coverage\n \n \n  Direct      Gen      25.62      12.63      5.08      2.87      4.60      3.10      4.16  \n \n  RAG      28.52      13.18      7.57      3.14      4.22      3.05      4.08  \n \n  oRAG      44.26      16.51      12.57      3.90      4.79      4.09      4.70 STORM      45.82      16.70      14.107      3.997      4.82      4.457      4.887  \n \n  w/o      Outline      Stage      26.77      12.77      7.39      3.33      4.87      3.35      4.37"
            }
          ],
          "content": ""
        },
        {
          "id": 633,
          "title": "Heading      Heading       Soft      Recall      Entity      Recall",
          "type": "section",
          "children": [
            {
              "id": 634,
              "title": "Heading      Heading       Soft      Recall      Entity      Recall - Chunk 1",
              "type": "chunk",
              "children": [],
              "content": "Heading      Heading       Soft      Recall      Entity      Recall\n \n \n  Direct      Gen      80.23      32.39  \n \n  RAG/oRAG      73.59      33.85  \n \n  GPT-3.5   \n \n=      RAG-expand      74.40      33.85  \n \n  STORM      86.267      40.527  \n \n  w/o      Perspective      84.49      40.12  \n \n  w/o      Conversation      77.97      31.98  \n \n  Direct      Gen      87.66      34.78  \n \n  RAG/oRAG      89.55      42.38  \n \n  GPT-4      RAG-expand      91.36      43.53  \n \n  STORM      92.737      45.91  \n \n  w/o      Perspective      92.39      42.70  \n \n  w/o      Conversation      88.75      39.30  \n \n  Table      3:      Results      of      outline      quality      evaluation      (%)."
            },
            {
              "id": 635,
              "title": "Heading      Heading       Soft      Recall      Entity      Recall - Chunk 2",
              "type": "chunk",
              "children": [],
              "content": "+      de-  \n \n  notes      significant      differences      (p   \n \n<      0.05)      from   \n \na      paired  \n \n  t-test      between      STORM      and      baselines. in      STORM      are      both      set      as      5. We      use      the      chat  \n \n  model      gpt-3.5-turbo      for      question      asking      and  \n \n  use      gpt-3.5-turbo-instruct      for      other      parts      of  \n \n  STORM."
            },
            {
              "id": 636,
              "title": "Heading      Heading       Soft      Recall      Entity      Recall - Chunk 3",
              "type": "chunk",
              "children": [],
              "content": "We      also      experiment      with      using      gpt-4      for  \n \n  drafting      and      refining      the      outline      (Figure   \n \n2      ()8)). For      reported      results,      the      simulated      topic      expert      in  \n \n  STORM      is      grounded      on      the      You.com      search      API!°,  \n \n  although      the      proposed      pipeline      is      compatible      with  \n \n  other      search      engines."
            },
            {
              "id": 637,
              "title": "Heading      Heading       Soft      Recall      Entity      Recall - Chunk 4",
              "type": "chunk",
              "children": [],
              "content": "The      ground      truth      Wikipedia  \n \n  article      is      excluded      from      the      search      results. For      final      article      generation,      we      only      report      the  \n \n  results      using      gpt-4      as      gpt-3.5      is      not      faithful      to  \n \n  sources      when      generating      text      with      citations      (Gao  \n \n  et      al.,      2023)."
            },
            {
              "id": 638,
              "title": "Heading      Heading       Soft      Recall      Entity      Recall - Chunk 5",
              "type": "chunk",
              "children": [],
              "content": "We      set      temperature      as      1.0      and      top_p  \n \n  as      0.9      for      all      experiments. 5\n \n   Results      and      Analysis\n5.1      Main      Results\nults  \n \n  We      use      outline      coverage      as   \n \na      proxy      to      assess      the      pre-  \n \n  writing      stage      (see      §2.2). Table   \n \n3      shows      the      heading  \n \n  soft      recall      and      entity      recall."
            },
            {
              "id": 639,
              "title": "Heading      Heading       Soft      Recall      Entity      Recall - Chunk 6",
              "type": "chunk",
              "children": [],
              "content": "Outlines      directly      gen-  \n \n  erated      by      LLMs      (Direct      Gen)      already      demonstrate  \n \n  https:      //documentation. you. com/api-reference/  \n \n  search  \n \n  high      heading      soft      recall,      indicating      LLMs’      ability  \n \n  to      grasp      high-level      aspects      of   \n \na      topic      through      their  \n \n  rich      parametric      knowledge."
            },
            {
              "id": 640,
              "title": "Heading      Heading       Soft      Recall      Entity      Recall - Chunk 7",
              "type": "chunk",
              "children": [],
              "content": "However,      STORM,      by  \n \n  asking      effective      questions      to      research      the      topic,      can  \n \n  create      higher      recall      outlines      that      cover      more      topic-  \n \n  specific      aspects."
            },
            {
              "id": 641,
              "title": "Heading      Heading       Soft      Recall      Entity      Recall - Chunk 8",
              "type": "chunk",
              "children": [],
              "content": "Notably,      although      RAG      leverages  \n \n  additional      information,      presenting      unorganized      in-  \n \n  formation      in      the      context      window      makes      outline  \n \n  generation      more      challenging      for      the      weaker      model,  \n \n  i.e.,      GPT-3.5,      leading      to      worse      performance."
            },
            {
              "id": 642,
              "title": "Heading      Heading       Soft      Recall      Entity      Recall - Chunk 9",
              "type": "chunk",
              "children": [],
              "content": "To      test  \n \n  the      limit      of      the      RAG      baseline,      we      further      expand  \n \n  the      retrieved      sources      by      starting      with      the      outline  \n \n  produced      by      RAG,      using      its      section      titles      as      search  \n \n  queries      to      collect      more      sources,      and      inputting      the  \n \n  newly      collected      sources      together      with      the      initial  \n \n  outline      to      LLM      to      generate   \n \na      polished      outline."
            },
            {
              "id": 643,
              "title": "Heading      Heading       Soft      Recall      Entity      Recall - Chunk 10",
              "type": "chunk",
              "children": [],
              "content": "This  \n \n  modified      approach      is      referred      to      as      “RAG-expand”  \n \n  in      Table      3. The      experiment      results      indicate      that  \n \n  even      though      having      an      additional      round      of      search  \n \n  and      refinement      can      improve      the      outline      produced  \n \n  by      RAG,      our      proposed      STORM      still      surpasses      its  \n \n  performance."
            },
            {
              "id": 644,
              "title": "Heading      Heading       Soft      Recall      Entity      Recall - Chunk 11",
              "type": "chunk",
              "children": [],
              "content": "We      further      evaluate      the      full-length      article      quality. As      shown      in      Table      2,      oRAG      significantly      outper-  \n \n  forms      RAG,      highlighting      the      effectiveness      of      using  \n \n  outlines      for      structuring      full-length      article      genera-  \n \n  tion."
            },
            {
              "id": 645,
              "title": "Heading      Heading       Soft      Recall      Entity      Recall - Chunk 12",
              "type": "chunk",
              "children": [],
              "content": "Despite      this      method’s      advantages      in      leverag-  \n \n  ing      retrieval      and      outlining,      our      approach      still      out-  \n \n  performs      it. The      effective      question      asking      mecha-  \n \n  nism      enhances      the      articles      with      greater      entity      recall."
            },
            {
              "id": 646,
              "title": "Heading      Heading       Soft      Recall      Entity      Recall - Chunk 13",
              "type": "chunk",
              "children": [],
              "content": "The      evaluator      LLM      also      rates      these      articles      with      sig-  \n \n  nificantly      higher      scores      in      the      aspects      of      “Interest  \n \n  Level’,      “Relevance      and      Focus’,      and      “Coverage”. Nonetheless,      we      acknowledge      the      possibility      of  \n \n  the      evaluator      LLM      overrating      machine-generated  \n \n  text."
            },
            {
              "id": 647,
              "title": "Heading      Heading       Soft      Recall      Entity      Recall - Chunk 14",
              "type": "chunk",
              "children": [],
              "content": "Our      careful      human      evaluation      (§6)      reveals  \n \n  that      STORM      still      has      much      room      for      improvement. Although      this      work      primarily      focuses      on      the      pre-  \n \n  writing      stage      and      does      not      optimize      generating      text  \n \n  with      citations,      we      still      examine      the      citation      quality  \n \n  of      articles      produced      by      our      approach."
            },
            {
              "id": 648,
              "title": "Heading      Heading       Soft      Recall      Entity      Recall - Chunk 15",
              "type": "chunk",
              "children": [],
              "content": "As      reported Citation      Recall Citation      Precision oRAG      STORM      value  \n \n  Avg. >4Rates      Av.g. >   \n \n4      Rates      peval Table      4:      Citation      quality      judged      by      Mistral      7B-Instruct."
            },
            {
              "id": 649,
              "title": "Heading      Heading       Soft      Recall      Entity      Recall - Chunk 16",
              "type": "chunk",
              "children": [],
              "content": "84.83 85.18  \n \n  STORM\n |  |       STORM      _      w/o      Perspective       w/o      Conversation | \n | --- | --- | ---\n |       IR|      99.83      54.36 |       39.56 |       Interest      Level      3.63      57.5%      4.03      70.0%      0.077       Organization      3.25      45.0%      4.00      70.0%      0.005       Relevance      3.93      62.5%      4.15      65.0%      0.347       Coverage      3.58      57.5%      4.00      67.5%      0.084       Verifiability      3.85      67.5%      3.80      67.5%      0.843       #Preferred      14      26\n | Table      5:      Average      number      of      unique      references      (|R|)       collected      using      different      methods."
            },
            {
              "id": 650,
              "title": "Heading      Heading       Soft      Recall      Entity      Recall - Chunk 17",
              "type": "chunk",
              "children": [],
              "content": "|       in      Table      4,      Mistral      7B-Instruct      judges      84.83%      of       the      sentences      are      supported      by      their      citations."
            },
            {
              "id": 651,
              "title": "Heading      Heading       Soft      Recall      Entity      Recall - Chunk 18",
              "type": "chunk",
              "children": [],
              "content": "Ap-       pendix      C.3      investigates      the      unsupported      sentences       and      reveals      that      the      primary      issues      stem      from      draw-       ing      improper      inferences      and      inaccurate      paraphras-       ing,      rather      than      hallucinating      non-existent      contents."
            },
            {
              "id": 652,
              "title": "Heading      Heading       Soft      Recall      Entity      Recall - Chunk 19",
              "type": "chunk",
              "children": [],
              "content": "| 5.2      Ablation      Studies\n | 5.2      Ablation      Studies\n |       As      introduced      in      §3,      STORM      prompts      LLMs      to       ask      effective      questions      by      discovering      specific       perspectives      and      simulating      multi-turn      conversa-       tions."
            },
            {
              "id": 653,
              "title": "Heading      Heading       Soft      Recall      Entity      Recall - Chunk 20",
              "type": "chunk",
              "children": [],
              "content": "We      conduct      the      ablation      study      on      outline       creation      by      comparing      STORM      with      two      variants:\n | (1)      “STORM      w/o      Perspective”,      which      omits      per-       spective      in      the      question      generation      prompt;      (2)       “STORM      w/o      Conversation”,      which      prompts      LLMs       to      generate      a      set      number      of      questions      altogether."
            },
            {
              "id": 654,
              "title": "Heading      Heading       Soft      Recall      Entity      Recall - Chunk 21",
              "type": "chunk",
              "children": [],
              "content": "To       ensure      a      fair      comparison,      we      control      an      equal      total       number      of      generated      questions      across      all      variants. Table      3      shows      the      ablation      results      and      full      STORM       pipeline      produces      outlines      with      the      highest      recall."
            },
            {
              "id": 655,
              "title": "Heading      Heading       Soft      Recall      Entity      Recall - Chunk 22",
              "type": "chunk",
              "children": [],
              "content": "Also,      “STORM      w/o      Conversation”      gives      much       worse      results,      indicating      reading      relevant      informa-       tion      is      crucial      to      generating      effective      questions. We       further      examine      how      many      unique      sources      are      col-       lected      in      ? via      different      variants."
            },
            {
              "id": 656,
              "title": "Heading      Heading       Soft      Recall      Entity      Recall - Chunk 23",
              "type": "chunk",
              "children": [],
              "content": "As      shown      in      Ta-       ble      5,      the      full      pipeline      discovers      more      different       sources      and      the      trend      is      in      accord      with      the      auto-       matic      metrics      for      outline      quality. We      also      verify      whether      having      an      outline      stage       is      necessary      with      STORM."
            },
            {
              "id": 657,
              "title": "Heading      Heading       Soft      Recall      Entity      Recall - Chunk 24",
              "type": "chunk",
              "children": [],
              "content": "In      Table      2,      “STORM       w/o      Outline      Stage”      denotes      the      results      of      generat-       ing      the      entire      article      given      the      topic      and      the      sim-       ulated      conversations. Removing      the      outline      stage       significantly      deteriorates      the      performance      across       all      metrics."
            },
            {
              "id": 658,
              "title": "Heading      Heading       Soft      Recall      Entity      Recall - Chunk 25",
              "type": "chunk",
              "children": [],
              "content": "| 6      Human      Evaluation\n |       To      better      understand      the      strengths      and      weaknesses       of      STORM,      we      conduct      human      evaluation      by      col-       laborating      with      10      experienced      Wikipedia      editors       Table      6:      Human      evaluation      results      on      20      pairs      of      articles       generated      by      STORM      and      oRAG."
            },
            {
              "id": 659,
              "title": "Heading      Heading       Soft      Recall      Entity      Recall - Chunk 26",
              "type": "chunk",
              "children": [],
              "content": "Each      pair      of      articles       is      evaluated      by      two      Wikipedia      editors. The      ratings      are       given      on      a      scale      between      |      and      7,      with      values      >      4       indicating      good      quality      (see      Table      10). We      conduct       paired      t-test      and      report      the      p-value."
            },
            {
              "id": 660,
              "title": "Heading      Heading       Soft      Recall      Entity      Recall - Chunk 27",
              "type": "chunk",
              "children": [],
              "content": "|       who      have      made      at      least      500      edits      on      Wikipedia      and       have      more      than      |      year      of      experience. We      randomly       sample      20      topics      from      our      dataset      and      evaluate      the       articles      generated      by      our      method      and      oRAG,      the       best      baseline      according      to      the      automatic      evaluation."
            },
            {
              "id": 661,
              "title": "Heading      Heading       Soft      Recall      Entity      Recall - Chunk 28",
              "type": "chunk",
              "children": [],
              "content": "|       Each      pair      of      articles      is      assigned      to      2      editors. |       We      request      editors      to      judge      each      article      from      the       same      five      aspects      defined      in      $4.2,      but      using      a      |      to       7      scale      for      more      fine-grained      evaluation."
            },
            {
              "id": 662,
              "title": "Heading      Heading       Soft      Recall      Entity      Recall - Chunk 29",
              "type": "chunk",
              "children": [],
              "content": "While       our      automatic      evaluation      uses      citation      quality      as       a      proxy      to      evaluate      Verifiability,      we      stick      to      the       Wikipedia      standard      of      “verifiable      with      no      original       research”      in      human      evaluation. Besides      rating      the       articles,      editors      are      asked      to      provide      open-ended       feedback      and      pairwise      preference."
            },
            {
              "id": 663,
              "title": "Heading      Heading       Soft      Recall      Entity      Recall - Chunk 30",
              "type": "chunk",
              "children": [],
              "content": "After      the      evalua-       tion      finishes,      they      are      further      requested      to      compare       an      article      produced      by      our      method,      which      they      have       just      reviewed,      with      its      human-written      counterpart,       and      report      their      perceived      usefulness      of      STORM       using      a      1-5      Likert      scale."
            },
            {
              "id": 664,
              "title": "Heading      Heading       Soft      Recall      Entity      Recall - Chunk 31",
              "type": "chunk",
              "children": [],
              "content": "More      human      evaluation      de-       tails      are      included      in      Appendix      D. Table      6      presents       the      rating      and      pairwise      comparison      results.!! |       Articles      produced      by      STORM      exhibit      greater       breadth      and      depth      than      oRAG      outputs."
            },
            {
              "id": 665,
              "title": "Heading      Heading       Soft      Recall      Entity      Recall - Chunk 32",
              "type": "chunk",
              "children": [],
              "content": "In      ac-       cord      with      the      finding      in      §5.1,      editors      judge      articles       produced      by      STORM      as      more      interesting,      orga-       nized,      and      having      broader      coverage      compared      to       oRAG      outputs."
            },
            {
              "id": 666,
              "title": "Heading      Heading       Soft      Recall      Entity      Recall - Chunk 33",
              "type": "chunk",
              "children": [],
              "content": "Specifically,      25%      more      articles      pro-       duced      by      STORM      are      considered      organized      (Orga-       nization      rating      >      4),      and      10%      more      are      deemed      to       have      good      coverage      (Coverage      rating      >      4)."
            },
            {
              "id": 667,
              "title": "Heading      Heading       Soft      Recall      Entity      Recall - Chunk 34",
              "type": "chunk",
              "children": [],
              "content": "Even       in      comparison      with      human-written      articles,      one       editor      praises      our      result      as      providing      “a      bit      more\n |       \"For      the      1-7      scale      rating      results      on      each      criterion,      we      cal-       culate      the      Krippendorff’s      Alpha      to      measure      the      inter      annotator       agreement      (IAA),      and      the      results      are      as      follows:      Interest      Level       (0.349),      Organization      (0.221),      Relevance      (0.256),      Coverage       (0.346),      Verifiability      (0.388)."
            }
          ],
          "content": ""
        },
        {
          "id": 668,
          "title": "5.1      Main      Results",
          "type": "section",
          "children": [
            {
              "id": 669,
              "title": "5.1      Main      Results - Chunk 1",
              "type": "chunk",
              "children": [],
              "content": "5.1      Main      Results\nults  \n \n  We      use      outline      coverage      as   \n \na      proxy      to      assess      the      pre-  \n \n  writing      stage      (see      §2.2). Table   \n \n3      shows      the      heading  \n \n  soft      recall      and      entity      recall. Outlines      directly      gen-  \n \n  erated      by      LLMs      (Direct      Gen)      already      demonstrate  \n \n  https:      //documentation. you."
            },
            {
              "id": 670,
              "title": "5.1      Main      Results - Chunk 2",
              "type": "chunk",
              "children": [],
              "content": "com/api-reference/  \n \n  search  \n \n  high      heading      soft      recall,      indicating      LLMs’      ability  \n \n  to      grasp      high-level      aspects      of   \n \na      topic      through      their  \n \n  rich      parametric      knowledge."
            },
            {
              "id": 671,
              "title": "5.1      Main      Results - Chunk 3",
              "type": "chunk",
              "children": [],
              "content": "However,      STORM,      by  \n \n  asking      effective      questions      to      research      the      topic,      can  \n \n  create      higher      recall      outlines      that      cover      more      topic-  \n \n  specific      aspects."
            },
            {
              "id": 672,
              "title": "5.1      Main      Results - Chunk 4",
              "type": "chunk",
              "children": [],
              "content": "Notably,      although      RAG      leverages  \n \n  additional      information,      presenting      unorganized      in-  \n \n  formation      in      the      context      window      makes      outline  \n \n  generation      more      challenging      for      the      weaker      model,  \n \n  i.e.,      GPT-3.5,      leading      to      worse      performance."
            },
            {
              "id": 673,
              "title": "5.1      Main      Results - Chunk 5",
              "type": "chunk",
              "children": [],
              "content": "To      test  \n \n  the      limit      of      the      RAG      baseline,      we      further      expand  \n \n  the      retrieved      sources      by      starting      with      the      outline  \n \n  produced      by      RAG,      using      its      section      titles      as      search  \n \n  queries      to      collect      more      sources,      and      inputting      the  \n \n  newly      collected      sources      together      with      the      initial  \n \n  outline      to      LLM      to      generate   \n \na      polished      outline."
            },
            {
              "id": 674,
              "title": "5.1      Main      Results - Chunk 6",
              "type": "chunk",
              "children": [],
              "content": "This  \n \n  modified      approach      is      referred      to      as      “RAG-expand”  \n \n  in      Table      3. The      experiment      results      indicate      that  \n \n  even      though      having      an      additional      round      of      search  \n \n  and      refinement      can      improve      the      outline      produced  \n \n  by      RAG,      our      proposed      STORM      still      surpasses      its  \n \n  performance."
            },
            {
              "id": 675,
              "title": "5.1      Main      Results - Chunk 7",
              "type": "chunk",
              "children": [],
              "content": "We      further      evaluate      the      full-length      article      quality. As      shown      in      Table      2,      oRAG      significantly      outper-  \n \n  forms      RAG,      highlighting      the      effectiveness      of      using  \n \n  outlines      for      structuring      full-length      article      genera-  \n \n  tion."
            },
            {
              "id": 676,
              "title": "5.1      Main      Results - Chunk 8",
              "type": "chunk",
              "children": [],
              "content": "Despite      this      method’s      advantages      in      leverag-  \n \n  ing      retrieval      and      outlining,      our      approach      still      out-  \n \n  performs      it. The      effective      question      asking      mecha-  \n \n  nism      enhances      the      articles      with      greater      entity      recall."
            },
            {
              "id": 677,
              "title": "5.1      Main      Results - Chunk 9",
              "type": "chunk",
              "children": [],
              "content": "The      evaluator      LLM      also      rates      these      articles      with      sig-  \n \n  nificantly      higher      scores      in      the      aspects      of      “Interest  \n \n  Level’,      “Relevance      and      Focus’,      and      “Coverage”. Nonetheless,      we      acknowledge      the      possibility      of  \n \n  the      evaluator      LLM      overrating      machine-generated  \n \n  text."
            },
            {
              "id": 678,
              "title": "5.1      Main      Results - Chunk 10",
              "type": "chunk",
              "children": [],
              "content": "Our      careful      human      evaluation      (§6)      reveals  \n \n  that      STORM      still      has      much      room      for      improvement. Although      this      work      primarily      focuses      on      the      pre-  \n \n  writing      stage      and      does      not      optimize      generating      text  \n \n  with      citations,      we      still      examine      the      citation      quality  \n \n  of      articles      produced      by      our      approach."
            },
            {
              "id": 679,
              "title": "5.1      Main      Results - Chunk 11",
              "type": "chunk",
              "children": [],
              "content": "As      reported Citation      Recall Citation      Precision oRAG      STORM      value  \n \n  Avg. >4Rates      Av.g. >   \n \n4      Rates      peval Table      4:      Citation      quality      judged      by      Mistral      7B-Instruct."
            },
            {
              "id": 680,
              "title": "5.1      Main      Results - Chunk 12",
              "type": "chunk",
              "children": [],
              "content": "84.83 85.18  \n \n  STORM\n |  |       STORM      _      w/o      Perspective       w/o      Conversation | \n | --- | --- | ---\n |       IR|      99.83      54.36 |       39.56 |       Interest      Level      3.63      57.5%      4.03      70.0%      0.077       Organization      3.25      45.0%      4.00      70.0%      0.005       Relevance      3.93      62.5%      4.15      65.0%      0.347       Coverage      3.58      57.5%      4.00      67.5%      0.084       Verifiability      3.85      67.5%      3.80      67.5%      0.843       #Preferred      14      26\n | Table      5:      Average      number      of      unique      references      (|R|)       collected      using      different      methods."
            },
            {
              "id": 681,
              "title": "5.1      Main      Results - Chunk 13",
              "type": "chunk",
              "children": [],
              "content": "|       in      Table      4,      Mistral      7B-Instruct      judges      84.83%      of       the      sentences      are      supported      by      their      citations."
            },
            {
              "id": 682,
              "title": "5.1      Main      Results - Chunk 14",
              "type": "chunk",
              "children": [],
              "content": "Ap-       pendix      C.3      investigates      the      unsupported      sentences       and      reveals      that      the      primary      issues      stem      from      draw-       ing      improper      inferences      and      inaccurate      paraphras-       ing,      rather      than      hallucinating      non-existent      contents."
            },
            {
              "id": 683,
              "title": "5.1      Main      Results - Chunk 15",
              "type": "chunk",
              "children": [],
              "content": "| 5.2      Ablation      Studies\n | 5.2      Ablation      Studies\n |       As      introduced      in      §3,      STORM      prompts      LLMs      to       ask      effective      questions      by      discovering      specific       perspectives      and      simulating      multi-turn      conversa-       tions."
            },
            {
              "id": 684,
              "title": "5.1      Main      Results - Chunk 16",
              "type": "chunk",
              "children": [],
              "content": "We      conduct      the      ablation      study      on      outline       creation      by      comparing      STORM      with      two      variants:\n | (1)      “STORM      w/o      Perspective”,      which      omits      per-       spective      in      the      question      generation      prompt;      (2)       “STORM      w/o      Conversation”,      which      prompts      LLMs       to      generate      a      set      number      of      questions      altogether."
            },
            {
              "id": 685,
              "title": "5.1      Main      Results - Chunk 17",
              "type": "chunk",
              "children": [],
              "content": "To       ensure      a      fair      comparison,      we      control      an      equal      total       number      of      generated      questions      across      all      variants. Table      3      shows      the      ablation      results      and      full      STORM       pipeline      produces      outlines      with      the      highest      recall."
            },
            {
              "id": 686,
              "title": "5.1      Main      Results - Chunk 18",
              "type": "chunk",
              "children": [],
              "content": "Also,      “STORM      w/o      Conversation”      gives      much       worse      results,      indicating      reading      relevant      informa-       tion      is      crucial      to      generating      effective      questions. We       further      examine      how      many      unique      sources      are      col-       lected      in      ? via      different      variants."
            },
            {
              "id": 687,
              "title": "5.1      Main      Results - Chunk 19",
              "type": "chunk",
              "children": [],
              "content": "As      shown      in      Ta-       ble      5,      the      full      pipeline      discovers      more      different       sources      and      the      trend      is      in      accord      with      the      auto-       matic      metrics      for      outline      quality. We      also      verify      whether      having      an      outline      stage       is      necessary      with      STORM."
            },
            {
              "id": 688,
              "title": "5.1      Main      Results - Chunk 20",
              "type": "chunk",
              "children": [],
              "content": "In      Table      2,      “STORM       w/o      Outline      Stage”      denotes      the      results      of      generat-       ing      the      entire      article      given      the      topic      and      the      sim-       ulated      conversations. Removing      the      outline      stage       significantly      deteriorates      the      performance      across       all      metrics."
            },
            {
              "id": 689,
              "title": "5.1      Main      Results - Chunk 21",
              "type": "chunk",
              "children": [],
              "content": "| 6      Human      Evaluation\n |       To      better      understand      the      strengths      and      weaknesses       of      STORM,      we      conduct      human      evaluation      by      col-       laborating      with      10      experienced      Wikipedia      editors       Table      6:      Human      evaluation      results      on      20      pairs      of      articles       generated      by      STORM      and      oRAG."
            },
            {
              "id": 690,
              "title": "5.1      Main      Results - Chunk 22",
              "type": "chunk",
              "children": [],
              "content": "Each      pair      of      articles       is      evaluated      by      two      Wikipedia      editors. The      ratings      are       given      on      a      scale      between      |      and      7,      with      values      >      4       indicating      good      quality      (see      Table      10). We      conduct       paired      t-test      and      report      the      p-value."
            },
            {
              "id": 691,
              "title": "5.1      Main      Results - Chunk 23",
              "type": "chunk",
              "children": [],
              "content": "|       who      have      made      at      least      500      edits      on      Wikipedia      and       have      more      than      |      year      of      experience. We      randomly       sample      20      topics      from      our      dataset      and      evaluate      the       articles      generated      by      our      method      and      oRAG,      the       best      baseline      according      to      the      automatic      evaluation."
            },
            {
              "id": 692,
              "title": "5.1      Main      Results - Chunk 24",
              "type": "chunk",
              "children": [],
              "content": "|       Each      pair      of      articles      is      assigned      to      2      editors. |       We      request      editors      to      judge      each      article      from      the       same      five      aspects      defined      in      $4.2,      but      using      a      |      to       7      scale      for      more      fine-grained      evaluation."
            },
            {
              "id": 693,
              "title": "5.1      Main      Results - Chunk 25",
              "type": "chunk",
              "children": [],
              "content": "While       our      automatic      evaluation      uses      citation      quality      as       a      proxy      to      evaluate      Verifiability,      we      stick      to      the       Wikipedia      standard      of      “verifiable      with      no      original       research”      in      human      evaluation. Besides      rating      the       articles,      editors      are      asked      to      provide      open-ended       feedback      and      pairwise      preference."
            },
            {
              "id": 694,
              "title": "5.1      Main      Results - Chunk 26",
              "type": "chunk",
              "children": [],
              "content": "After      the      evalua-       tion      finishes,      they      are      further      requested      to      compare       an      article      produced      by      our      method,      which      they      have       just      reviewed,      with      its      human-written      counterpart,       and      report      their      perceived      usefulness      of      STORM       using      a      1-5      Likert      scale."
            },
            {
              "id": 695,
              "title": "5.1      Main      Results - Chunk 27",
              "type": "chunk",
              "children": [],
              "content": "More      human      evaluation      de-       tails      are      included      in      Appendix      D. Table      6      presents       the      rating      and      pairwise      comparison      results.!! |       Articles      produced      by      STORM      exhibit      greater       breadth      and      depth      than      oRAG      outputs."
            },
            {
              "id": 696,
              "title": "5.1      Main      Results - Chunk 28",
              "type": "chunk",
              "children": [],
              "content": "In      ac-       cord      with      the      finding      in      §5.1,      editors      judge      articles       produced      by      STORM      as      more      interesting,      orga-       nized,      and      having      broader      coverage      compared      to       oRAG      outputs."
            },
            {
              "id": 697,
              "title": "5.1      Main      Results - Chunk 29",
              "type": "chunk",
              "children": [],
              "content": "Specifically,      25%      more      articles      pro-       duced      by      STORM      are      considered      organized      (Orga-       nization      rating      >      4),      and      10%      more      are      deemed      to       have      good      coverage      (Coverage      rating      >      4)."
            },
            {
              "id": 698,
              "title": "5.1      Main      Results - Chunk 30",
              "type": "chunk",
              "children": [],
              "content": "Even       in      comparison      with      human-written      articles,      one       editor      praises      our      result      as      providing      “a      bit      more\n |       \"For      the      1-7      scale      rating      results      on      each      criterion,      we      cal-       culate      the      Krippendorff’s      Alpha      to      measure      the      inter      annotator       agreement      (IAA),      and      the      results      are      as      follows:      Interest      Level       (0.349),      Organization      (0.221),      Relevance      (0.256),      Coverage       (0.346),      Verifiability      (0.388)."
            }
          ],
          "content": ""
        },
        {
          "id": 699,
          "title": "Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree",
          "type": "section",
          "children": [
            {
              "id": 700,
              "title": "Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree - Chunk 1",
              "type": "chunk",
              "children": [],
              "content": "Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree\n \n \n \nI      think      it      can      be      specifically      helpful  \n \n  wae      70%      30%  \n \n  for      my      pre-writing      stage. I\n \n   think      it      will      help      me      edit   \n \na      Wikipedia      anes   \n \n3      oars      30%  \n \n  article      for   \n \na      new      topic."
            },
            {
              "id": 701,
              "title": "Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree - Chunk 2",
              "type": "chunk",
              "children": [],
              "content": "= I      think      it      can      be   \n \na      potentially      useful      10%      20%   \n \n:      60%      10%  \n \n  tool      for      the      Wikipedia      community. Figure      3:      Survey      results      of      the      perceived      usefulness      of  \n \n  STORM      (n   \n \n=      10)."
            },
            {
              "id": 702,
              "title": "Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree - Chunk 3",
              "type": "chunk",
              "children": [],
              "content": "background      information”      and      another      notes      that      “I  \n \n  found      that      the      AI      articles      had      more      depth      compared  \n \n  to      the      Wikipedia      articles”. STORM      also      outper-  \n \n  forms      the      best      baseline      in      pairwise      comparison. More      information      in      |R|      poses      challenges      be-  \n \n  yond      factual      hallucination."
            },
            {
              "id": 703,
              "title": "Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree - Chunk 4",
              "type": "chunk",
              "children": [],
              "content": "We      examine      14      pair-  \n \n  wise      comparison      responses      where      editors      prefer  \n \n  oORAG      outputs      over      STORM. Excluding   \n \n3      cases  \n \n  where      pairwise      preferences      do      not      align      with      their  \n \n  ratings,      editors      assign      lower      Verifiability      scores      to  \n \n  articles      from      our      approach      in      over      50%      of      the      cases."
            },
            {
              "id": 704,
              "title": "Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree - Chunk 5",
              "type": "chunk",
              "children": [],
              "content": "Through      analyzing      the      articles      and      editors’      free-  \n \n  form      feedback,      we      discover      that      low      Verifiability  \n \n  scores      stem      from      red      herring      fallacy      or      overspec-  \n \n  ulation      issues."
            },
            {
              "id": 705,
              "title": "Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree - Chunk 6",
              "type": "chunk",
              "children": [],
              "content": "These      arise      when      the      generated  \n \n  articles      introduce      unverifiable      connections      between  \n \n  different      pieces      of      information      in      |7?|      or      between  \n \n  the      information      and      the      topic      (examples      included  \n \n  in      Table      11)."
            },
            {
              "id": 706,
              "title": "Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree - Chunk 7",
              "type": "chunk",
              "children": [],
              "content": "Compared      to      the      widely      discussed  \n \n  factual      hallucination      (Shuster      et      al.,      2021;      Huang  \n \n  et      al.,      2023),      addressing      such      verifiability      issues      is  \n \n  more      nuanced,      surpassing      basic      fact-checking      (Min  \n \n  et      al.,      2023). Generated      articles      trail      behind      well-revised      hu-  \n \n  man      works."
            },
            {
              "id": 707,
              "title": "Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree - Chunk 8",
              "type": "chunk",
              "children": [],
              "content": "While      STORM      outperforms      the  \n \n  oRAG      baseline,      editors      comment      that      the      generated  \n \n  articles      are      less      informative      than      actual      Wikipedia  \n \n  pages."
            },
            {
              "id": 708,
              "title": "Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree - Chunk 9",
              "type": "chunk",
              "children": [],
              "content": "Another      major      issue      identified      is      the      trans-  \n \n  fer      of      bias      and      tone      from      Internet      sources      to      the  \n \n  generated      article,      with   \n \n7      out      of      10      editors      men-  \n \n  tioning      that      the      STORM-generated      articles      sound  \n \n  “emotional”      or      “unneutral”. More      analysis      is      dis-  \n \n  cussed      in      Appendix      E."
            },
            {
              "id": 709,
              "title": "Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree - Chunk 10",
              "type": "chunk",
              "children": [],
              "content": "This      feedback      suggests      that  \n \n  reducing      the      retrieval      bias      in      the      pre-writing      stage  \n \n  is   \n \na      worthwhile      direction      for      future      work. Generated      articles      are   \n \na      good      starting      point. As  \n \n  shown      in      Figure      3,      editors      are      unanimous      in      agree-  \n \n  ing      that      STORM      can      aid      them      in      their      pre-writing  \n \n  stage."
            },
            {
              "id": 710,
              "title": "Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree - Chunk 11",
              "type": "chunk",
              "children": [],
              "content": "It      is      gratifying      to      know      that      the      tool      is      help-  \n \n  ful      to      experienced      editors. 80%      of      the      editors      think  \n \n  that      STORM      can      help      them      edit   \n \na      Wikipedia      article  \n \n  for   \n \na      new      topic."
            },
            {
              "id": 711,
              "title": "Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree - Chunk 12",
              "type": "chunk",
              "children": [],
              "content": "More      reservation      is      expressed      to  \n \n  the      usefulness      of      STORM      for      the      Wikipedia      com-  \n \n  munity      at      large;      nonetheless,      70%      of      the      editors  \n \n  think      it      is      useful,      with      only      10%      disagreeing."
            },
            {
              "id": 712,
              "title": "Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree - Chunk 13",
              "type": "chunk",
              "children": [],
              "content": "7\n \n   Related      Works  \n \n  Retrieval-Augmented      Generation      (RAG)      Aug-  \n \n  menting      language      models      (LMs)      with      retrieval      at  \n \n  inference      time      is   \n \na      typical      way      to      leverage      exter-  \n \n  nal      knowledge      stores      (Ram      et      al.,      2023;      Izacard  \n \n  et      al.,      2023)."
            },
            {
              "id": 713,
              "title": "Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree - Chunk 14",
              "type": "chunk",
              "children": [],
              "content": "While      some      works      use      retrieval  \n \n  to      construct      demonstrations      for      in-context      learn-  \n \n  ing      (Li      et      al.,      2023;      Liu      et      al.,      2022;      Agrawal      et      al.,  \n \n  2023;      Poesia      et      al.,      2022;      Shi      et      al.,      2022;      Khattab  \n \n  et      al.,      2022),      another      line      of      works      uses      retrieval      to  \n \n  provide      additional      information      for      LMs      to      ground  \n \n  on."
            },
            {
              "id": 714,
              "title": "Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree - Chunk 15",
              "type": "chunk",
              "children": [],
              "content": "Lewis      et      al. (2020)      study      RAG      on      knowledge-  \n \n  intensive      NLP      tasks      and      find      it      improves      diver-  \n \n  sity      and      factuality. Semnani      et      al. (2023)      de-  \n \n  signs   \n \na      RAG-based      chatbot      grounded      on      English  \n \n  Wikipedia      to      stop      LLM-based      chatbots      from      hal-  \n \n  lucination."
            },
            {
              "id": 715,
              "title": "Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree - Chunk 16",
              "type": "chunk",
              "children": [],
              "content": "Besides,      RAG      can      be      used      to      generate  \n \n  text      with      citations      (Menick      et      al.,      2022;      Gao      et      al.,  \n \n  2023)      and      build      attributed      question      answering      sys-  \n \n  tems      (Bohnet      et      al.,      2023)."
            },
            {
              "id": 716,
              "title": "Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree - Chunk 17",
              "type": "chunk",
              "children": [],
              "content": "While      RAG      is      widely  \n \n  studied      in      question      answering,      how      to      use      it      for  \n \n  long-form      article      generation      is      less      investigated. As   \n \na      general      framework,      RAG      is      flexible      in      both  \n \n  the      retrieval      source      and      time."
            },
            {
              "id": 717,
              "title": "Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree - Chunk 18",
              "type": "chunk",
              "children": [],
              "content": "The      retrieval      sources  \n \n  can      vary      from      domain      databases      (Zakka      et      al.,  \n \n  2023),      code      documentation      (Zhou      et      al.,      2023),  \n \n  to      the      whole      Internet      (Nakano      et      al.,      2022;      Komeili  \n \n  et      al.,      2022)."
            },
            {
              "id": 718,
              "title": "Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree - Chunk 19",
              "type": "chunk",
              "children": [],
              "content": "Regarding      the      time,      besides   \n \na      one-  \n \n  time      retrieval      before      generation,      the      system      can      be  \n \n  designed      to      self-decide      when      to      retrieve      across      the  \n \n  course      of      the      generation      (Jiang      et      al.,      2023b;      Parisi  \n \n  et      al.,      2022;      Shuster      et      al.,      2022;      Yao      et      al.,      2023)."
            },
            {
              "id": 719,
              "title": "Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree - Chunk 20",
              "type": "chunk",
              "children": [],
              "content": "Automatic      Expository      Writing      Different      from  \n \n  other      types      of      long-form      generation      (Yang      et      al.,  \n \n  2022;      Feng      et      al.,      2018),      automatic      expository      writ-  \n \n  ing      requires      grounding      on      external      documents      and  \n \n  leveraging      the      interplay      between      reading      and      writ-  \n \n  ing. Balepur      et      al."
            },
            {
              "id": 720,
              "title": "Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree - Chunk 21",
              "type": "chunk",
              "children": [],
              "content": "(2023)      propose      the      Imitate-  \n \n  Retrieve-Paraphrase      framework      for      expository      writ-  \n \n  ing      at      the      paragraph      level      to      address      the      challenges  \n \n  in      synthesizing      information      from      multiple      sources. Beyond      summarizing      sources,      Shen      et      al."
            },
            {
              "id": 721,
              "title": "Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree - Chunk 22",
              "type": "chunk",
              "children": [],
              "content": "(2023)  \n \n  highlight      that      expository      writing      requires      the      au-  \n \n  thor’s      sensemaking      process      over      source      documents  \n \n  and      good      outline      planning. We      tackle      these      chal-  \n \n  lenges      by      focusing      on      the      pre-writing      stage."
            },
            {
              "id": 722,
              "title": "Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree - Chunk 23",
              "type": "chunk",
              "children": [],
              "content": "Question      Asking      in      NLP      Question      asking      capa-  \n \n  bilities      in      NLP      systems      have      expanded      across      sev-  \n \n  eral      fronts,      including      generating      clarification      ques-  \n \n  tions      to      understand      user      intents      (Aliannejadi      et      al.,  \n \n  2019;      Rahmani      et      al.,      2023),      and      breaking      large  \n \n  questions      into      smaller      ones      to      improve      composi-  \n \n  tional      reasoning      (Press      et      al.,      2023)."
            },
            {
              "id": 723,
              "title": "Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree - Chunk 24",
              "type": "chunk",
              "children": [],
              "content": "While      humans  \n \n  usually      ask      questions      to      learn      new      knowledge      (Taw-  \n \n  fik      et      al.,      2020;      Booth      et      al.,      2003),      how      to      opti-  \n \n  mize      question      informativeness      and      specificity      in  \n \n  information-seeking      conversations      remains      less      ex-  \n \n  plored. The      closest      work      is      Qi      et      al."
            },
            {
              "id": 724,
              "title": "Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree - Chunk 25",
              "type": "chunk",
              "children": [],
              "content": "(2020)      which  \n \n  defines      the      question      informativeness      using      the      un-  \n \n  igram      precision      function      and      uses      reinforcement  \n \n  learning      to      increase      the      question      informativeness."
            },
            {
              "id": 725,
              "title": "Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree - Chunk 26",
              "type": "chunk",
              "children": [],
              "content": "8\n \n   Conclusion  \n \n  We      propose      STORM,      an      LLM-based      writing      sys-  \n \n  tem      that      automates      the      pre-writing      stage      for      creat-  \n \n  ing      Wikipedia-like      articles      from      scratch. We      cu-  \n \n  rate      the      FreshWiki      dataset      and      establish      evaluation  \n \n  criteria      to      study      the      generation      of      grounded      long-  \n \n  form      articles."
            },
            {
              "id": 726,
              "title": "Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree - Chunk 27",
              "type": "chunk",
              "children": [],
              "content": "Experimental      results      demonstrate  \n \n  that      the      question      asking      mechanism      in      STORM  \n \n  improves      both      the      outline      and      article      quality. With  \n \n  the      improved      breadth      and      depth,      STORM      helps  \n \n  surface      new      challenges      for      grounded      writing      sys-  \n \n  tems      through      expert      evaluation."
            },
            {
              "id": 727,
              "title": "Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree - Chunk 28",
              "type": "chunk",
              "children": [],
              "content": "The      experienced  \n \n  Wikipedia      editors      in      our      study      unanimously      agree  \n \n  that      STORM      is      helpful      for      their      pre-writing      stage."
            }
          ],
          "content": ""
        },
        {
          "id": 728,
          "title": "Limitations",
          "type": "section",
          "children": [
            {
              "id": 729,
              "title": "Limitations - Chunk 1",
              "type": "chunk",
              "children": [],
              "content": "Limitations\n \n \n  In      this      work,      we      explore      generating      Wikipedia-  \n \n  like      articles      from      scratch      as   \n \na      way      to      push      the  \n \n  frontier      of      automatic      expository      writing      and      long-  \n \n  form      article      generation."
            },
            {
              "id": 730,
              "title": "Limitations - Chunk 2",
              "type": "chunk",
              "children": [],
              "content": "While      our      approach      sig-  \n \n  nificantly      outperforms      baseline      methods      in      both  \n \n  automatic      and      human      evaluations,      the      quality      of  \n \n  machine-written      articles      still      lags      behind      well-  \n \n  revised      human-authored      articles,      specifically      in  \n \n  aspects      of      neutrality      and      verifiability."
            },
            {
              "id": 731,
              "title": "Limitations - Chunk 3",
              "type": "chunk",
              "children": [],
              "content": "Although  \n \n  STORM      discovers      different      perspectives      in      re-  \n \n  searching      the      given      topic,      the      collected      information  \n \n  may      still      be      biased      towards      dominant      sources      on  \n \n  the      Internet      and      may      contain      promotional      content."
            },
            {
              "id": 732,
              "title": "Limitations - Chunk 4",
              "type": "chunk",
              "children": [],
              "content": "Moreover,      the      verifiability      issues      identified      in      this  \n \n  work      go      beyond      factual      hallucination,      which      high-  \n \n  lights      new      challenges      to      grounded      writing      systems."
            },
            {
              "id": 733,
              "title": "Limitations - Chunk 5",
              "type": "chunk",
              "children": [],
              "content": "Another      limitation      of      this      work      is      that      although  \n \n  we      focus      on      the      task      of      generating      Wikipedia-like  \n \n  articles      from      scratch,      our      task      setup      is      still      simpli-  \n \n  fied      to      only      consider      the      generation      of      free-form  \n \n  text."
            },
            {
              "id": 734,
              "title": "Limitations - Chunk 6",
              "type": "chunk",
              "children": [],
              "content": "Human-authored      high-quality      Wikipedia      ar-  \n \n  ticles      usually      contain      structured      data      and      multi-  \n \n  modal      information. We      leave      the      exploration      of  \n \n  generating      multi-modal      grounded      articles      for      fu-  \n \n  ture      work."
            }
          ],
          "content": ""
        },
        {
          "id": 735,
          "title": "Acknowledgements",
          "type": "section",
          "children": [
            {
              "id": 736,
              "title": "Acknowledgements - Chunk 1",
              "type": "chunk",
              "children": [],
              "content": "Acknowledgements\n \n \n  We      thank      You.com      for      generously      providing      the  \n \n  search      API      that      supported      our      experiments. We  \n \n  also      thank      Sina      J. Semnani,      Shicheng      Liu,      Eric      Ze-  \n \n  likman      for      providing      helpful      feedback      and      the      ACL  \n \n  ARR      reviewers      for      their      valuable      comments."
            },
            {
              "id": 737,
              "title": "Acknowledgements - Chunk 2",
              "type": "chunk",
              "children": [],
              "content": "This  \n \n  work      is      supported      in      part      by      the      Verdant      Founda-  \n \n  tion      and      Microsoft      Azure      AI      credits. Yijia      Shao  \n \n  is      supported      by   \n \na      Stanford      School      of      Engineering  \n \n  Fellowship."
            }
          ],
          "content": ""
        },
        {
          "id": 738,
          "title": "Ethics      Statement",
          "type": "section",
          "children": [
            {
              "id": 739,
              "title": "Ethics      Statement - Chunk 1",
              "type": "chunk",
              "children": [],
              "content": "Ethics      Statement\n \n \n  Different      from      the      creative      generation,      grounded      ar-  \n \n  ticle      generation      may      impact      how      people      learn      about  \n \n  topics      or      consume      source      information."
            },
            {
              "id": 740,
              "title": "Ethics      Statement - Chunk 2",
              "type": "chunk",
              "children": [],
              "content": "All      the      stud-  \n \n  ies      and      the      evaluation      in      this      work      are      designed  \n \n  to      prevent      the      dissemination      of      misinformation      by  \n \n  not      publishing      generated      content      online      and      im-  \n \n  plementing      strict      accuracy      checks."
            },
            {
              "id": 741,
              "title": "Ethics      Statement - Chunk 3",
              "type": "chunk",
              "children": [],
              "content": "We      avoid      any  \n \n  disruption      to      Wikipedia      or      related      communities,      as  \n \n  our      system      does      not      interact      with      live      pages."
            },
            {
              "id": 742,
              "title": "Ethics      Statement - Chunk 4",
              "type": "chunk",
              "children": [],
              "content": "Also,  \n \n  although      we      try      to      generate      grounded      articles,      we  \n \n  believe      there      is      no      privacy      issue      related      to      this      work  \n \n  as      we      only      use      information      publicly      available      on  \n \n  the      Internet."
            },
            {
              "id": 743,
              "title": "Ethics      Statement - Chunk 5",
              "type": "chunk",
              "children": [],
              "content": "The      primary      risk      of      our      work      is      that      the  \n \n  Wikipedia      articles      written      by      our      system      are  \n \n  grounded      on      information      on      the      Internet      which  \n \n  contains      some      biased      or      discriminative      content      on  \n \n  its      own."
            },
            {
              "id": 744,
              "title": "Ethics      Statement - Chunk 6",
              "type": "chunk",
              "children": [],
              "content": "Currently,      our      system      relies      on      the      search  \n \n  engine      to      retrieve      information      but      does      not      include  \n \n  any      post-processing      module."
            },
            {
              "id": 745,
              "title": "Ethics      Statement - Chunk 7",
              "type": "chunk",
              "children": [],
              "content": "We      believe      improv-  \n \n  ing      the      retrieval      module      to      have      good      coverage      of  \n \n  different      viewpoints      and      adding   \n \na      content      sifting  \n \n  module      to      the      current      system      will      be   \n \na      critical      next  \n \n  step      to      achieve      better      neutrality      and      balance      in      the  \n \n  generated      articles."
            },
            {
              "id": 746,
              "title": "Ethics      Statement - Chunk 8",
              "type": "chunk",
              "children": [],
              "content": "Another      limitation      we      see      from      an      ethical      point  \n \n  of      view      is      that      we      only      consider      writing      English  \n \n  Wikipedia      articles      in      this      work."
            },
            {
              "id": 747,
              "title": "Ethics      Statement - Chunk 9",
              "type": "chunk",
              "children": [],
              "content": "Extending      the      cur-  \n \n  rent      system      to   \n \na      multilingual      setup      is   \n \na      meaningful  \n \n  direction      for      future      work      as      more      topics      do      not      have  \n \n  Wikipedia      pages      in      non-English      languages."
            }
          ],
          "content": ""
        },
        {
          "id": 748,
          "title": "References",
          "type": "section",
          "children": [
            {
              "id": 749,
              "title": "References - Chunk 1",
              "type": "chunk",
              "children": [],
              "content": "References\n \n \n  Sweta      Agrawal,      Chunting      Zhou,      Mike      Lewis,      Luke  \n \n  Zettlemoyer,      and      Marjan      Ghazvininejad. 2023. In-  \n \n  context      examples      selection      for      machine      translation. In      Findings      of      the      Association      for      Computational  \n \n  Linguistics:      ACL      2023,      pages      8857-8873,      Toronto,  \n \n  Canada. Association      for      Computational      Linguistics."
            },
            {
              "id": 750,
              "title": "References - Chunk 2",
              "type": "chunk",
              "children": [],
              "content": "Alan      Akbik,      Tanja      Bergmann,      Duncan      Blythe,      Kashif  \n \n  Rasul,      Stefan      Schweter,      and      Roland      Vollgraf. 2019. FLAIR:      An      easy-to-use      framework      for      state-of-the-  \n \n  art      NLP."
            },
            {
              "id": 751,
              "title": "References - Chunk 3",
              "type": "chunk",
              "children": [],
              "content": "In      Proceedings      of      the      2019      Conference      of  \n \n  the      North      American      Chapter      of      the      Association      for  \n \n  Computational      Linguistics      (Demonstrations),      pages  \n \n  54-59,      Minneapolis,      Minnesota. Association      for  \n \n  Computational      Linguistics. Mohammad      Aliannejadi,      Hamed      Zamani,      Fabio  \n \n  Crestani,      and      W      Bruce      Croft. 2019."
            },
            {
              "id": 752,
              "title": "References - Chunk 4",
              "type": "chunk",
              "children": [],
              "content": "Asking      clari-  \n \n  fying      questions      in      open-domain      information-seeking  \n \n  conversations. In      Proceedings      of      the      42nd      interna-  \n \n  tional      acm      sigir      conference      on      research      and      develop-  \n \n  ment      in      information      retrieval,      pages      475-484. Nishant      Balepur,      Jie      Huang,      and      Kevin      Chang. 2023."
            },
            {
              "id": 753,
              "title": "References - Chunk 5",
              "type": "chunk",
              "children": [],
              "content": "Expository      text      generation:      Imitate,      retrieve,      para-  \n \n  phrase. In      Proceedings      of      the      2023      Conference      on  \n \n  Empirical      Methods      in      Natural      Language      Process-  \n \n  ing,      pages      11896-11919,      Singapore. Association      for  \n \n  Computational      Linguistics. Siddhartha      Banerjee      and      Prasenjit      Mitra. 2015."
            },
            {
              "id": 754,
              "title": "References - Chunk 6",
              "type": "chunk",
              "children": [],
              "content": "WikiKreator:      Improving      Wikipedia      stubs      automat-  \n \n  ically. In      Proceedings      of      the      53rd      Annual      Meet-  \n \n  ing      of      the      Association      for      Computational      Linguis-  \n \n  tics      and      the      7th      International      Joint      Conference      on  \n \n  Natural      Language      Processing      (Volume      1:      Long      Pa-  \n \n  pers),      pages      867-877,      Beijing,      China."
            },
            {
              "id": 755,
              "title": "References - Chunk 7",
              "type": "chunk",
              "children": [],
              "content": "Association  \n \n  for      Computational      Linguistics. Bernd      Bohnet,      Vinh      Q."
            },
            {
              "id": 756,
              "title": "References - Chunk 8",
              "type": "chunk",
              "children": [],
              "content": "Tran,      Pat      Verga,      Roee      Aha-  \n \n  roni,      Daniel      Andor,      Livio      Baldini      Soares,      Massimil-  \n \n  iano      Ciaramita,      Jacob      Eisenstein,      Kuzman      Ganchev,  \n \n  Jonathan      Herzig,      Kai      Hui,      Tom      Kwiatkowski,      Ji      Ma,  \n \n  Jianmo      Ni,      Lierni      Sestorain      Saralegui,      Tal      Schus-  \n \n  ter,      William      W."
            },
            {
              "id": 757,
              "title": "References - Chunk 9",
              "type": "chunk",
              "children": [],
              "content": "Cohen,      Michael      Collins,      Dipanjan  \n \n  Das,      Donald      Metzler,      Slav      Petrov,      and      Kellie      Webster. 2023. Attributed      question      answering:      Evaluation      and  \n \n  modeling      for      attributed      large      language      models. Wayne      C      Booth,      Gregory      G      Colomb,      and      Joseph   \n \nM       Williams. 2003. The      craft      of      research. University      of  \n \n  Chicago      press."
            },
            {
              "id": 758,
              "title": "References - Chunk 10",
              "type": "chunk",
              "children": [],
              "content": "Laura      Dietz      and      John      Foley. 2019. Trec      car      y3:      Com-  \n \n  plex      answer      retrieval      overview. In      Proceedings      of  \n \n  Text      REtrieval      Conference      (TREC). Christina   \n \nS      Doyle. 1994. Information      literacy      in      an  \n \n  information      society:   \n \nA      concept      for      the      information  \n \n  age. Diane      Publishing. Ann-Marie      Eriksson      and      Asa      Mikitalo. 2015."
            },
            {
              "id": 759,
              "title": "References - Chunk 11",
              "type": "chunk",
              "children": [],
              "content": "Supervi-  \n \n  sion      at      the      outline      stage:      Introducing      and      encounter-  \n \n  ing      issues      of      sustainable      development      through      aca-  \n \n  demic      writing      assignments. Text   \n \n&      Talk,      35(2):123-  \n \n  153. Angela      Fan      and      Claire      Gardent. 2022."
            },
            {
              "id": 760,
              "title": "References - Chunk 12",
              "type": "chunk",
              "children": [],
              "content": "Generating      bi-  \n \n  ographies      on      Wikipedia:      The      impact      of      gender      bias  \n \n  on      the      retrieval-based      generation      of      women      biogra-  \n \n  phies. In      Proceedings      of      the      60th      Annual      Meeting      of  \n \n  the      Association      for      Computational      Linguistics      (Vol-  \n \n  ume      I:      Long      Papers),      pages      8561-8576,      Dublin,  \n \n  Ireland."
            },
            {
              "id": 761,
              "title": "References - Chunk 13",
              "type": "chunk",
              "children": [],
              "content": "Association      for      Computational      Linguistics. Xiaocheng      Feng,      Ming      Liu,      Jiahao      Liu,      Bing      Qin,      Yibo  \n \n  Sun,      and      Ting      Liu. 2018. Topic-to-essay      generation  \n \n  with      neural      networks. In      JJCAI,      pages      4078-4084. Tira      Nur      Fitria. 2023."
            },
            {
              "id": 762,
              "title": "References - Chunk 14",
              "type": "chunk",
              "children": [],
              "content": "Artificial      intelligence      (ai)      tech-  \n \n  nology      in      openai      chatgpt      application:   \n \nA      review      of  \n \n  chatgpt      in      writing      english      essay. In      ELT      Forum:      Jour-  \n \n  nal      of      English      Language      Teaching,      volume      12,      pages  \n \n  44-58. Pasi      Franti      and      Radu      Mariescu-Istodor. 2023. Soft      preci-  \n \n  sion      and      recall."
            },
            {
              "id": 763,
              "title": "References - Chunk 15",
              "type": "chunk",
              "children": [],
              "content": "Pattern      Recognition      Letters,      167:115—  \n \n  121. R      Edward      Freeman,      Jeffrey   \n \nS      Harrison,      Andrew      C       Wicks,      Bidhan   \n \nL      Parmar,      and      Simone      De      Colle. 2010. Stakeholder      theory:      The      state      of      the      art. Tianyu      Gao,      Howard      Yen,      Jiatong      Yu,      and      Danqi      Chen. 2023."
            },
            {
              "id": 764,
              "title": "References - Chunk 16",
              "type": "chunk",
              "children": [],
              "content": "Enabling      large      language      models      to      generate  \n \n  text      with      citations. In      Proceedings      of      the      2023      Con-  \n \n  ference      on      Empirical      Methods      in      Natural      Language  \n \n  Processing,      pages      6465-6488,      Singapore. Associa-  \n \n  tion      for      Computational      Linguistics."
            },
            {
              "id": 765,
              "title": "References - Chunk 17",
              "type": "chunk",
              "children": [],
              "content": "Lei      Huang,      Weijiang      Yu,      Weitao      Ma,      Weihong      Zhong,  \n \n  Zhangyin      Feng,      Haotian      Wang,      Qianglong      Chen,  \n \n  Weihua      Peng,      Xiaocheng      Feng,      Bing      Qin,      and      Ting  \n \n  Liu. 2023. A      survey      on      hallucination      in      large      lan-  \n \n  guage      models:      Principles,      taxonomy,      challenges,      and  \n \n  open      questions."
            },
            {
              "id": 766,
              "title": "References - Chunk 18",
              "type": "chunk",
              "children": [],
              "content": "Gautier      Izacard,      Patrick      Lewis,      Maria      Lomeli,      Lucas  \n \n  Hosseini,      Fabio      Petroni,      Timo      Schick,      Jane      Dwivedi-  \n \n  Yu,      Armand      Joulin,      Sebastian      Riedel,      and      Edouard  \n \n  Grave. 2023. Atlas:      Few-shot      learning      with      retrieval  \n \n  augmented      language      models. Journal      of      Machine  \n \n  Learning      Research,      24(251):1-43."
            },
            {
              "id": 767,
              "title": "References - Chunk 19",
              "type": "chunk",
              "children": [],
              "content": "Albert   \n \nQ      Jiang,      Alexandre      Sablayrolles,      Arthur      Men-  \n \n  sch,      Chris      Bamford,      Devendra      Singh      Chaplot,      Diego  \n \n  de      las      Casas,      Florian      Bressand,      Gianna      Lengyel,      Guil-  \n \n  laume      Lample,      Lucile      Saulnier,      et      al. 2023a. Mistral  \n \n  7b. arXiv      preprint      arXiv:2310.06825."
            },
            {
              "id": 768,
              "title": "References - Chunk 20",
              "type": "chunk",
              "children": [],
              "content": "Zhengbao      Jiang,      Frank      Xu,      Luyu      Gao,      Zhiqing      Sun,  \n \n  Qian      Liu,      Jane      Dwivedi-Yu,      Yiming      Yang,      Jamie  \n \n  Callan,      and      Graham      Neubig. 2023b. Active      retrieval  \n \n  augmented      generation. In      Proceedings      of      the      2023  \n \n  Conference      on      Empirical      Methods      in      Natural      Lan-  \n \n  guage      Processing,      pages      7969-7992,      Singapore."
            },
            {
              "id": 769,
              "title": "References - Chunk 21",
              "type": "chunk",
              "children": [],
              "content": "As-  \n \n  sociation      for      Computational      Linguistics. Nikhil      Kandpal,      Haikang      Deng,      Adam      Roberts,      Eric  \n \n  Wallace,      and      Colin      Raffel. 2023. Large      language  \n \n  models      struggle      to      learn      long-tail      knowledge. In      In-  \n \n  ternational      Conference      on      Machine      Learning,      pages  \n \n  15696-15707. PMLR."
            },
            {
              "id": 770,
              "title": "References - Chunk 22",
              "type": "chunk",
              "children": [],
              "content": "Omar      Khattab,      Keshav      Santhanam,      Xiang      Lisa  \n \n  Li,      David      Hall,      Percy      Liang,      Christopher      Potts,  \n \n  and      Matei      Zaharia. 2022. Demonstrate-search-  \n \n  predict:      Composing      retrieval      and      language      mod-  \n \n  els      for      knowledge-intensive      NLP. arXiv      preprint  \n \n  arXiv:2212.14024."
            },
            {
              "id": 771,
              "title": "References - Chunk 23",
              "type": "chunk",
              "children": [],
              "content": "Omar      Khattab,      Arnav      Singhvi,      Paridhi      Maheshwari,  \n \n  Zhiyuan      Zhang,      Keshav      Santhanam,      Sri      Vard-  \n \n  hamanan,      Saiful      Haq,      Ashutosh      Sharma,      Thomas      T. Joshi,      Hanna      Moazam,      Heather      Miller,      Matei      Za-  \n \n  haria,      and      Christopher      Potts. 2023. Dspy:      Compiling  \n \n  declarative      language      model      calls      into      self-improving  \n \n  pipelines."
            },
            {
              "id": 772,
              "title": "References - Chunk 24",
              "type": "chunk",
              "children": [],
              "content": "arXiv      preprint      arXiv:2310.03714. Seungone      Kim,      Jamin      Shin,      Yejin      Cho,      Joel      Jang,  \n \n  Shayne      Longpre,      Hwaran      Lee,      Sangdoo      Yun,  \n \n  Seongjin      Shin,      Sungdong      Kim,      James      Thorne,      et      al. 2023. Prometheus:      Inducing      fine-grained      evalua-  \n \n  tion      capability      in      language      models. arXiv      preprint  \n \n  arXiv:2310.08491."
            },
            {
              "id": 773,
              "title": "References - Chunk 25",
              "type": "chunk",
              "children": [],
              "content": "Mojtaba      Komeili,      Kurt      Shuster,      and      Jason      Weston. 2022. Internet-augmented      dialogue      generation. In      Proceed-  \n \n  ings      of      the      60th      Annual      Meeting      of      the      Association  \n \n  for      Computational      Linguistics      (Volume      1:      Long      Pa-  \n \n  pers),      pages      8460-8478,      Dublin,      Ireland. Association  \n \n  for      Computational      Linguistics."
            },
            {
              "id": 774,
              "title": "References - Chunk 26",
              "type": "chunk",
              "children": [],
              "content": "Kalpesh      Krishna,      Erin      Bransom,      Bailey      Kuehl,      Mohit  \n \n  Iyyer,      Pradeep      Dasigi,      Arman      Cohan,      and      Kyle      Lo. 2023. LongEval:      Guidelines      for      human      evaluation      of  \n \n  faithfulness      in      long-form      summarization."
            },
            {
              "id": 775,
              "title": "References - Chunk 27",
              "type": "chunk",
              "children": [],
              "content": "In      Proceed-  \n \n  ings      of      the      17th      Conference      of      the      European      Chap-  \n \n  ter      of      the      Association      for      Computational      Linguistics,  \n \n  pages      1650-1669,      Dubrovnik,      Croatia. Association  \n \n  for      Computational      Linguistics."
            },
            {
              "id": 776,
              "title": "References - Chunk 28",
              "type": "chunk",
              "children": [],
              "content": "Patrick      Lewis,      Ethan      Perez,      Aleksandra      Piktus,      Fabio  \n \n  Petroni,      Vladimir      Karpukhin,      Naman      Goyal,      Hein-  \n \n  rich      Kiittler,      Mike      Lewis,      Wen-tau      Yih,      Tim      Rock-  \n \n  taschel,      et      al. 2020. Retrieval-augmented      generation  \n \n  for      knowledge-intensive      nlp      tasks. Advances      in      Neu-  \n \n  ral      Information      Processing      Systems,      33:9459-9474."
            },
            {
              "id": 777,
              "title": "References - Chunk 29",
              "type": "chunk",
              "children": [],
              "content": "Xiaonan      Li,      Kai      Lv,      Hang      Yan,      Tianyang      Lin,      Wei      Zhu,  \n \n  Yuan      Ni,      Guotong      Xie,      Xiaoling      Wang,      and      Xipeng  \n \n  Qiu. 2023. Unified      demonstration      retriever      for      in-  \n \n  context      learning."
            },
            {
              "id": 778,
              "title": "References - Chunk 30",
              "type": "chunk",
              "children": [],
              "content": "In      Proceedings      of      the      61st      Annual  \n \n  Meeting      of      the      Association      for      Computational      Lin-  \n \n  guistics      (Volume      1:      Long      Papers),      pages      4644-4668,  \n \n  Toronto,      Canada. Association      for      Computational      Lin-  \n \n  guistics. Chin-Yew      Lin. 2004. ROUGE:   \n \nA      package      for      auto-  \n \n  matic      evaluation      of      summaries."
            },
            {
              "id": 779,
              "title": "References - Chunk 31",
              "type": "chunk",
              "children": [],
              "content": "In      Text      Summariza-  \n \n  tion      Branches      Out,      pages      74-81,      Barcelona,      Spain. Association      for      Computational      Linguistics. Jiachang      Liu,      Dinghan      Shen,      Yizhe      Zhang,      Bill      Dolan,  \n \n  Lawrence      Carin,      and      Weizhu      Chen. 2022. What  \n \n  makes      good      in-context      examples      for      GPT-3?"
            },
            {
              "id": 780,
              "title": "References - Chunk 32",
              "type": "chunk",
              "children": [],
              "content": "In  \n \n  Proceedings      of      Deep      Learning      Inside      Out      (DeeLIO  \n \n  2022):      The      3rd      Workshop      on      Knowledge      Extrac-  \n \n  tion      and      Integration      for      Deep      Learning      Architectures,  \n \n  pages      100-114,      Dublin,      Ireland      and      Online. Associa-  \n \n  tion      for      Computational      Linguistics. Peter      J."
            },
            {
              "id": 781,
              "title": "References - Chunk 33",
              "type": "chunk",
              "children": [],
              "content": "Liu,      Mohammad      Saleh,      Etienne      Pot,      Ben  \n \n  Goodrich,      Ryan      Sepassi,      Lukasz      Kaiser,      and      Noam  \n \n  Shazeer. 2018. Generating      wikipedia      by      summariz-  \n \n  ing      long      sequences. In      International      Conference      on  \n \n  Learning      Representations."
            },
            {
              "id": 782,
              "title": "References - Chunk 34",
              "type": "chunk",
              "children": [],
              "content": "Jacob      Menick,      Maja      Trebacz,      Vladimir      Mikulik,  \n \n  John      Aslanides,      Francis      Song,      Martin      Chadwick,  \n \n  Mia      Glaese,      Susannah      Young,      Lucy      Campbell-  \n \n  Gillingham,      Geoffrey      Irving,      and      Nat      McAleese. 2022. Teaching      language      models      to      support      answers  \n \n  with      verified      quotes."
            },
            {
              "id": 783,
              "title": "References - Chunk 35",
              "type": "chunk",
              "children": [],
              "content": "Sewon      Min,      Kalpesh      Krishna,      Xinxi      Lyu,      Mike      Lewis,  \n \n  Wen-tau      Yih,      Pang      Koh,      Mohit      Iyyer,      Luke      Zettle-  \n \n  moyer,      and      Hannaneh      Hajishirzi. 2023. FActScore:\n \n \n  Fine-grained      atomic      evaluation      of      factual      precision  \n \n  in      long      form      text      generation."
            },
            {
              "id": 784,
              "title": "References - Chunk 36",
              "type": "chunk",
              "children": [],
              "content": "In      Proceedings      of      the  \n \n  2023      Conference      on      Empirical      Methods      in      Natural  \n \n  Language      Processing,      pages      12076-12100,      Singa-  \n \n  pore. Association      for      Computational      Linguistics. Julia      Minguill6n,      Maura      Lerga,      Eduard      Aibar,      Josep  \n \n  Lladés-Masllorens,      and      Antoni      Meseguer-Artola. 2017."
            },
            {
              "id": 785,
              "title": "References - Chunk 37",
              "type": "chunk",
              "children": [],
              "content": "Semi-automatic      generation      of   \n \na      corpus      of  \n \n  wikipedia      articles      on      science      and      technology. Profe-  \n \n  sional      de      la      Informacion,      26(5):995—1005. Rosa      Munoz-Luna. 2015. Main      ingredients      for      suc-  \n \n  cess      in      12      academic      writing:      Outlining,      drafting      and  \n \n  proofreading. PloS      one,      10(6):e0128309."
            },
            {
              "id": 786,
              "title": "References - Chunk 38",
              "type": "chunk",
              "children": [],
              "content": "Reiichiro      Nakano,      Jacob      Hilton,      Suchir      Balaji,      Jeff      Wu,  \n \n  Long      Ouyang,      Christina      Kim,      Christopher      Hesse,  \n \n  Shantanu      Jain,      Vineet      Kosaraju,      William      Saunders,  \n \n  Xu      Jiang,      Karl      Cobbe,      Tyna      Eloundou,      Gretchen  \n \n  Krueger,      Kevin      Button,      Matthew      Knight,      Benjamin  \n \n  Chess,      and      John      Schulman. 2022."
            },
            {
              "id": 787,
              "title": "References - Chunk 39",
              "type": "chunk",
              "children": [],
              "content": "Webgpt:      Browser-  \n \n  assisted      question-answering      with      human      feedback. Long      Ouyang,      Jeffrey      Wu,      Xu      Jiang,      Diogo      Almeida,  \n \n  Carroll      Wainwright,      Pamela      Mishkin,      Chong      Zhang,  \n \n  Sandhini      Agarwal,      Katarina      Slama,      Alex      Ray,      et      al. 2022. Training      language      models      to      follow      instruc-  \n \n  tions      with      human      feedback."
            },
            {
              "id": 788,
              "title": "References - Chunk 40",
              "type": "chunk",
              "children": [],
              "content": "Advances      in      Neural  \n \n  Information      Processing      Systems,      35:27730—27744. Aaron      Parisi,      Yao      Zhao,      and      Noah      Fiedel. 2022. Talm:  \n \n  Tool      augmented      language      models. John   \n \nV      Pavlik. 2023. Collaborating      with      chatgpt:      Con-  \n \n  sidering      the      implications      of      generative      artificial      intel-  \n \n  ligence      for      journalism      and      media      education."
            },
            {
              "id": 789,
              "title": "References - Chunk 41",
              "type": "chunk",
              "children": [],
              "content": "Journal-  \n \n  ism   \n \n&      Mass      Communication      Educator,      78(1):84—93. Gabriel      Poesia,      Alex      Polozov,      Vu      Le,      Ashish      Tiwari,  \n \n  Gustavo      Soares,      Christopher      Meek,      and      Sumit      Gul-  \n \n  wani. 2022. Synchromesh:      Reliable      code      generation  \n \n  from      pre-trained      language      models. In      International  \n \n  Conference      on      Learning      Representations."
            },
            {
              "id": 790,
              "title": "References - Chunk 42",
              "type": "chunk",
              "children": [],
              "content": "Ofir      Press,      Muru      Zhang,      Sewon      Min,      Ludwig      Schmidt,  \n \n  Noah      Smith,      and      Mike      Lewis. 2023. Measuring      and  \n \n  narrowing      the      compositionality      gap      in      language      mod-  \n \n  els. In      Findings      of      the      Association      for      Computational  \n \n  Linguistics:      EMNLP      2023,      pages      5687-5711,      Singa-  \n \n  pore. Association      for      Computational      Linguistics."
            },
            {
              "id": 791,
              "title": "References - Chunk 43",
              "type": "chunk",
              "children": [],
              "content": "Peng      Qi,      Yuhao      Zhang,      and      Christopher      D. Manning. 2020. Stay      hungry,      stay      focused:      Generating      infor-  \n \n  mative      and      specific      questions      in      information-seeking  \n \n  conversations. In      Findings      of      the      Association      for  \n \n  Computational      Linguistics:      EMNLP      2020,      pages      25—  \n \n  40,      Online. Association      for      Computational      Linguis-  \n \n  tics."
            },
            {
              "id": 792,
              "title": "References - Chunk 44",
              "type": "chunk",
              "children": [],
              "content": "Hongjing      Qian,      Yutao      Zhu,      Zhicheng      Dou,      Haoqi      Gu,  \n \n  Xinyu      Zhang,      Zheng      Liu,      Ruofei      Lai,      Zhao      Cao,  \n \n  Jian-Yun      Nie,      and      Ji-Rong      Wen. 2023. Webbrain:\n \n \n  Learning      to      generate      factually      correct      articles      for  \n \n  queries      by      grounding      on      large      web      corpus. Hossein      A."
            },
            {
              "id": 793,
              "title": "References - Chunk 45",
              "type": "chunk",
              "children": [],
              "content": "Rahmani,      Xi      Wang,      Yue      Feng,      Qiang      Zhang,  \n \n  Emine      Yilmaz,      and      Aldo      Lipani. 2023. A      survey      on  \n \n  asking      clarification      questions      datasets      in      conversa-  \n \n  tional      systems."
            },
            {
              "id": 794,
              "title": "References - Chunk 46",
              "type": "chunk",
              "children": [],
              "content": "In      Proceedings      of      the      61st      Annual  \n \n  Meeting      of      the      Association      for      Computational      Lin-  \n \n  guistics      (Volume      1:      Long      Papers),      pages      2698-2716,  \n \n  Toronto,      Canada. Association      for      Computational      Lin-  \n \n  guistics. Ashwin      Ram. 1991. A      theory      of      questions      and      question  \n \n  asking."
            },
            {
              "id": 795,
              "title": "References - Chunk 47",
              "type": "chunk",
              "children": [],
              "content": "Journal      of      the      Learning      Sciences,      1(3-4):273-  \n \n  318. Ori      Ram,      Yoav      Levine,      Itay      Dalmedigos,      Dor      Muhlgay,  \n \n  Amnon      Shashua,      Kevin      Leyton-Brown,      and      Yoav  \n \n  Shoham. 2023. In-context      retrieval-augmented      lan-  \n \n  guage      models. Transactions      of      the      Association      for  \n \n  Computational      Linguistics. Nils      Reimers      and      Iryna      Gurevych. 2019."
            },
            {
              "id": 796,
              "title": "References - Chunk 48",
              "type": "chunk",
              "children": [],
              "content": "Sentence-  \n \n  BERT:      Sentence      embeddings      using      Siamese      BERT-  \n \n  networks. In      Proceedings      of      the      2019      Conference      on  \n \n  Empirical      Methods      in      Natural      Language      Processing  \n \n  and      the      9th      International      Joint      Conference      on      Natu-  \n \n  ral      Language      Processing      (EMNLP-IJCNLP),      pages  \n \n  3982-3992,      Hong      Kong,      China."
            },
            {
              "id": 797,
              "title": "References - Chunk 49",
              "type": "chunk",
              "children": [],
              "content": "Association      for      Com-  \n \n  putational      Linguistics. D      Gordon      Rohman. 1965. Pre-writing      the      stage      of      dis-  \n \n  covery      in      the      writing      process. College      composition  \n \n  and      communication,      16(2):106—112. Christina      Sauper      and      Regina      Barzilay. 2009. Auto-  \n \n  matically      generating      Wikipedia      articles:   \n \nA      structure-  \n \n  aware      approach."
            },
            {
              "id": 798,
              "title": "References - Chunk 50",
              "type": "chunk",
              "children": [],
              "content": "In      Proceedings      of      the      Joint      Con-  \n \n  ference      of      the      47th      Annual      Meeting      of      the      ACL      and  \n \n  the      4th      International      Joint      Conference      on      Natural  \n \n  Language      Processing      of      the      AFNLP,      pages      208-216,  \n \n  Suntec,      Singapore. Association      for      Computational  \n \n  Linguistics. Lam. 2023."
            },
            {
              "id": 799,
              "title": "References - Chunk 51",
              "type": "chunk",
              "children": [],
              "content": "WikiChat:      Stopping      the      hallucination      of  \n \n  large      language      model      chatbots      by      few-shot      ground-  \n \n  ing      on      Wikipedia. In      Findings      of      the      Association  \n \n  for      Computational      Linguistics:      EMNLP      2023,      pages  \n \n  2387-2413,      Singapore. Association      for      Computa-  \n \n  tional      Linguistics."
            },
            {
              "id": 800,
              "title": "References - Chunk 52",
              "type": "chunk",
              "children": [],
              "content": "Jonathan      Bragg,      Jeff      Hammerbacher,      Doug      Downey,  \n \n  Joseph      Chee      Chang,      and      David      Sontag. 2023. Be-  \n \n  yond      summarization:      Designing      ai      support      for      real-  \n \n  world      expository      writing      tasks. Luke      Zettlemoyer. 2022. Nearest      neighbor      zero-shot  \n \n  inference."
            },
            {
              "id": 801,
              "title": "References - Chunk 53",
              "type": "chunk",
              "children": [],
              "content": "In      Proceedings      of      the      2022      Conference      on  \n \n  Empirical      Methods      in      Natural      Language      Processing,  \n \n  pages      3254-3265,      Abu      Dhabi,      United      Arab      Emirates. Association      for      Computational      Linguistics. Stephen      Roller,      Arthur      Szlam,      and      Jason      Weston. 2022."
            },
            {
              "id": 802,
              "title": "References - Chunk 54",
              "type": "chunk",
              "children": [],
              "content": "Language      models      that      seek      for      knowledge:  \n \n  Modular      search   \n \n&      generation      for      dialogue      and  \n \n  prompt      completion. In      Findings      of      the      Association  \n \n  for      Computational      Linguistics:      EMNLP      2022,      pages  \n \n  373-393,      Abu      Dhabi,      United      Arab      Emirates. Associ-  \n \n  ation      for      Computational      Linguistics. and      Jason      Weston. 2021."
            },
            {
              "id": 803,
              "title": "References - Chunk 55",
              "type": "chunk",
              "children": [],
              "content": "Retrieval      augmentation  \n \n  reduces      hallucination      in      conversation. In      Findings  \n \n  of      the      Association      for      Computational      Linguistics:  \n \n  EMNLP      2021,      pages      3784-3803,      Punta      Cana,      Do-  \n \n  minican      Republic. Association      for      Computational  \n \n  Linguistics. Wikipedia      as      an      introduction      to      academic      writing."
            },
            {
              "id": 804,
              "title": "References - Chunk 56",
              "type": "chunk",
              "children": [],
              "content": "In  \n \n  English      teaching      forum,      volume      48,      page      12. ERIC. and      Jaclyn      Gishbaugher. 2020. Role      of      questions      in  \n \n  inquiry-based      instruction:      towards   \n \na      design      taxon-  \n \n  omy      for      question-asking      and      implications      for      design. Educational      Technology      Research      and      Development,  \n \n  68:653-678. itory      text. than      humans?"
            },
            {
              "id": 805,
              "title": "References - Chunk 57",
              "type": "chunk",
              "children": [],
              "content": "validating      how      openai’s      chatgpt      model  \n \n  explains      crowdfunding,      alternative      finance      and      com-  \n \n  munity      finance. Validating      how      OpenAlI’s      ChatGPT  \n \n  model      explains      Crowdfunding,      Alternative      Finance  \n \n  and      Community      Finance.(December      22,      2022). Choi. 2023. A      critical      evaluation      of      evaluations      for  \n \n  long-form      question      answering."
            },
            {
              "id": 806,
              "title": "References - Chunk 58",
              "type": "chunk",
              "children": [],
              "content": "In      Proceedings      of      the  \n \n  61st      Annual      Meeting      of      the      Association      for      Compu-  \n \n  tational      Linguistics      (Volume      1:      Long      Papers),      pages  \n \n  3225-3245,      Toronto,      Canada. Association      for      Com-  \n \n  putational      Linguistics. Kevin      Yang,      Dan      Klein,      Nanyun      Peng,      and      Yuandong  \n \n  Tian. 2023."
            },
            {
              "id": 807,
              "title": "References - Chunk 59",
              "type": "chunk",
              "children": [],
              "content": "DOC:      Improving      long      story      coherence  \n \n  with      detailed      outline      control. In      Proceedings      of      the  \n \n  61st      Annual      Meeting      of      the      Association      for      Compu-  \n \n  tational      Linguistics      (Volume      I:      Long      Papers),      pages  \n \n  3378-3465,      Toronto,      Canada. Association      for      Com-  \n \n  putational      Linguistics."
            },
            {
              "id": 808,
              "title": "References - Chunk 60",
              "type": "chunk",
              "children": [],
              "content": "Kevin      Yang,      Yuandong      Tian,      Nanyun      Peng,      and      Dan  \n \n  Klein. 2022. Re3:      Generating      longer      stories      with  \n \n  recursive      reprompting      and      revision. In      Proceedings  \n \n  of      the      2022      Conference      on      Empirical      Methods      in      Nat-  \n \n  ural      Language      Processing,      pages      4393-4479,      Abu  \n \n  Dhabi,      United      Arab      Emirates."
            },
            {
              "id": 809,
              "title": "References - Chunk 61",
              "type": "chunk",
              "children": [],
              "content": "Association      for      Com-  \n \n  putational      Linguistics. Shunyu      Yao,      Jeffrey      Zhao,      Dian      Yu,      Nan      Du,      Izhak  \n \n  Shafran,      Karthik      R      Narasimhan,      and      Yuan      Cao. 2023. React:      Synergizing      reasoning      and      acting      in      language  \n \n  models. In      The      Eleventh      International      Conference  \n \n  on      Learning      Representations."
            },
            {
              "id": 810,
              "title": "References - Chunk 62",
              "type": "chunk",
              "children": [],
              "content": "Cyril      Zakka,      Akash      Chaurasia,      Rohan      Shad,      Alex      R       Dalal,      Jennifer   \n \nL      Kim,      Michael      Moor,      Kevin      Alexan-  \n \n  der,      Euan      Ashley,      Jack      Boyd,      Kathleen      Boyd,      et      al. 2023. Almanac:      Retrieval-augmented      language      mod-  \n \n  els      for      clinical      medicine. Research      Square. Shuyan      Zhou,      Uri      Alon,      Frank      F."
            },
            {
              "id": 811,
              "title": "References - Chunk 63",
              "type": "chunk",
              "children": [],
              "content": "Xu,      Zhengbao      Jiang,  \n \n  and      Graham      Neubig. 2023. Docprompting:      Gener-  \n \n  ating      code      by      retrieving      the      docs. In      The      Eleventh  \n \n  International      Conference      on      Learning      Representa-  \n \n  tions."
            }
          ],
          "content": ""
        },
        {
          "id": 812,
          "title": "Average      Numer      of      Sections      8.4",
          "type": "section",
          "children": [
            {
              "id": 813,
              "title": "Average      Numer      of      Sections      8.4 - Chunk 1",
              "type": "chunk",
              "children": [],
              "content": "Average      Numer      of      Sections      8.4"
            }
          ],
          "content": ""
        },
        {
          "id": 814,
          "title": "Average      Number      of      All-level      Headings      15.8 Average      Length      of      a      Section      327.8       Average      Length      of      Total      Article      2159.1",
          "type": "section",
          "children": [
            {
              "id": 815,
              "title": "Average      Number      of      All-level      Headings      15.8 Average      Length      of      a      Section      327.8       Average      Length      of      Total      Article      2159.1 - Chunk 1",
              "type": "chunk",
              "children": [],
              "content": "Average      Number      of      All-level      Headings      15.8 Average      Length      of      a      Section      327.8       Average      Length      of      Total      Article      2159.1"
            }
          ],
          "content": ""
        },
        {
          "id": 816,
          "title": "Average      Number      of      References      90.1",
          "type": "section",
          "children": [
            {
              "id": 817,
              "title": "Average      Number      of      References      90.1 - Chunk 1",
              "type": "chunk",
              "children": [],
              "content": "Average      Number      of      References      90.1\nTable      7:      Statistics      of      the      dataset      used      in      our      experiments."
            },
            {
              "id": 818,
              "title": "Average      Number      of      References      90.1 - Chunk 2",
              "type": "chunk",
              "children": [],
              "content": "—\n \n   Average Number      of      references  \n \n  a      BR  \n \n  N      Py      oa      feo}      Oo      N       Oo      Oo      oO      Oo      oO      Oo  \n \n  fo)  \n \n  1 0\n \n   20      40      60      80      100  \n \n  Edit      progress      (%      of      total      edits) Figure      4:      Evolution      of      reference      count      in      the      Wikipedia  \n \n  article      editing      process."
            }
          ],
          "content": ""
        },
        {
          "id": 819,
          "title": "A_      Dataset      Details",
          "type": "section",
          "children": [
            {
              "id": 820,
              "title": "A_      Dataset      Details - Chunk 1",
              "type": "chunk",
              "children": [],
              "content": "A_      Dataset      Details\n \n \n  As      discussed      in      §2.1,      we      curate      the      FreshWiki  \n \n  dataset      by      collecting      recent      and      high-quality      En-  \n \n  glish      Wikipedia      articles."
            },
            {
              "id": 821,
              "title": "A_      Dataset      Details - Chunk 2",
              "type": "chunk",
              "children": [],
              "content": "We      select      the      most-edited  \n \n  pages      over   \n \na      specific      period      rather      than      using      cre-  \n \n  ation      dates      as   \n \na      cutoff      because      most      of      Wikipedia  \n \n  articles      are      “stubs”      or      are      of      low      quality      when      they  \n \n  were      created."
            },
            {
              "id": 822,
              "title": "A_      Dataset      Details - Chunk 3",
              "type": "chunk",
              "children": [],
              "content": "For      quality,      we      consider      articles      pre-  \n \n  dicted      to      be      of      B-class      quality      or      above. According  \n \n  to      Wikipedia      statistics! *,      only      around      3%      of      ex-  \n \n  isting      Wikipedia      pages      meet      this      quality      standard."
            },
            {
              "id": 823,
              "title": "A_      Dataset      Details - Chunk 4",
              "type": "chunk",
              "children": [],
              "content": "As      LLMs      can      generate      reasonably      good      outputs,  \n \n  we      think      it      is      important      to      use      high-quality      human-  \n \n  written      articles      as      references      for      further      research."
            },
            {
              "id": 824,
              "title": "A_      Dataset      Details - Chunk 5",
              "type": "chunk",
              "children": [],
              "content": "For      experiments      in      this      work,      we      randomly      se-  \n \n  lect      100      samples      with      human-written      articles      un-  \n \n  der      3000      words      to      have   \n \na      meaningful      comparison. Table   \n \n7      gives      the      data      statistics."
            },
            {
              "id": 825,
              "title": "A_      Dataset      Details - Chunk 6",
              "type": "chunk",
              "children": [],
              "content": "Notably,      human-  \n \n  authored      articles      have   \n \na      large      number      of      references  \n \n  but      they      require      numerous      edits      to      achieve      this."
            },
            {
              "id": 826,
              "title": "A_      Dataset      Details - Chunk 7",
              "type": "chunk",
              "children": [],
              "content": "Fig-  \n \n  ure   \n \n4      illustrates      the      evolution      of      the      reference      count  \n \n  in      the      article      edit      process      and      Figure   \n \n5      gives      the      dis-  \n \n  tribution      of      edit      counts      for      human-authored      articles  \n \n  used      in      our      experiments."
            },
            {
              "id": 827,
              "title": "A_      Dataset      Details - Chunk 8",
              "type": "chunk",
              "children": [],
              "content": "count      (A;)   \n \n=       where      embed(-)      in      Equation      (1)      is      parameterized  \n \n  by      paraphrase-MiniLM-L6-v2      provided      in      the  \n \n  Sentence-Transformers      library!*. The      cardinality  \n \n  https://en.wikipedia."
            },
            {
              "id": 828,
              "title": "A_      Dataset      Details - Chunk 9",
              "type": "chunk",
              "children": [],
              "content": "org/wiki/Wikipedia:  \n \n  Content_assessment  \n \n  100) Bhttps://huggingface.co/sentence-transformers/  \n \n  paraphrase-MiniLM-L6-v2 Percentage      of      articles      (n\n |       T      T      T      T       0      500      1000      1500 |       y      1      7      1      7      7      7       2000      2500      3000      3500      4000      4500      5000       Number      of      edits\n | Figure      5:      Distribution      of      edit      counts      for      Wikipedia      arti-       cles      in      our      experiments      (n      =      100)."
            }
          ],
          "content": ""
        },
        {
          "id": 829,
          "title": "B_      Pseudo      Code      of      STORM",
          "type": "section",
          "children": [
            {
              "id": 830,
              "title": "B_      Pseudo      Code      of      STORM - Chunk 1",
              "type": "chunk",
              "children": [],
              "content": "B_      Pseudo      Code      of      STORM\n \n \n  In      §3,      we      introduce      STORM,   \n \na      framework      that      au-  \n \n  tomates      the      pre-writing      stage      by      discovering      differ-  \n \n  ent      perspectives,      simulating      information-seeking  \n \n  conversations,      and      creating   \n \na      comprehensive      out-  \n \n  line. Algorithm   \n \n1      displays      the      skeleton      of      STORM."
            },
            {
              "id": 831,
              "title": "B_      Pseudo      Code      of      STORM - Chunk 2",
              "type": "chunk",
              "children": [],
              "content": "We      implement      STORM      with      zero-shot      prompt-  \n \n  ing      using      the      DSPy      framework      (Khattab      et      al.,  \n \n  2023). Listing   \n \n1      and   \n \n2      show      the      prompts      used  \n \n  in      our      implementation."
            },
            {
              "id": 832,
              "title": "B_      Pseudo      Code      of      STORM - Chunk 3",
              "type": "chunk",
              "children": [],
              "content": "We      highlight      that      STORM  \n \n  offers   \n \na      general      framework      designed      to      assist      the  \n \n  creation      of      grounded,      long-form      articles,      without  \n \n  depending      extensively      on      prompt      engineering      for   \n \na       single      domain."
            }
          ],
          "content": ""
        },
        {
          "id": 833,
          "title": "C      Automatic      Evaluation      Details",
          "type": "section",
          "children": [
            {
              "id": 834,
              "title": "C      Automatic      Evaluation      Details - Chunk 1",
              "type": "chunk",
              "children": [],
              "content": "C      Automatic      Evaluation      Details"
            }
          ],
          "content": ""
        },
        {
          "id": 835,
          "title": "C.1      Soft      Heading      Recall",
          "type": "section",
          "children": [
            {
              "id": 836,
              "title": "C.1      Soft      Heading      Recall - Chunk 1",
              "type": "chunk",
              "children": [],
              "content": "C.1      Soft      Heading      Recall\n \n \n  We      calculate      the      soft      heading      recall      between      the  \n \n  multi-level      headings      in      the      generated      outline,      con-  \n \n  sidered      as      the      prediction      P,      and      those      in      the      human-  \n \n  written      article,      considered      as      the      ground      truth      G."
            },
            {
              "id": 837,
              "title": "C.1      Soft      Heading      Recall - Chunk 2",
              "type": "chunk",
              "children": [],
              "content": "The      calculation      is      based      on      the      soft      recall      defini-  \n \n  tion      in      Franti      and      Mariescu-Istodor      (2023)."
            },
            {
              "id": 838,
              "title": "C.1      Soft      Heading      Recall - Chunk 3",
              "type": "chunk",
              "children": [],
              "content": "Given  \n \n  aset   \n \nA   \n \n=      {Ai}*.,,      soft      count      of      an      item      is      defined as      the      inverse      of      the      sum      of      its      similarity      to      other  \n \n  items      in      the      set:\n1\n \n \n  Dj      Sim      (Aj,      Aj)      (1) Sim      (A;,      A;)   \n \n=      cos      (embed(A;),      embed(A;))   \n \n,       28  \n \n  29  \n \n  class      GenRelatedTopicsPrompt      (dspy."
            },
            {
              "id": 839,
              "title": "C.1      Soft      Heading      Recall - Chunk 4",
              "type": "chunk",
              "children": [],
              "content": "Signature):\n \n \n  I\\\\\\'m      writing   \n \na      Wikipedia      page      for   \n \na      topic      mentioned      below. Please      identify      and  \n \n  recommend      some      Wikipedia      pages      on      closely      related      subjects."
            },
            {
              "id": 840,
              "title": "C.1      Soft      Heading      Recall - Chunk 5",
              "type": "chunk",
              "children": [],
              "content": "I\\\\\\'m      looking      for  \n \n  examples      that      provide      insights      into      interesting      aspects      commonly      associated  \n \n  with      this      topic,      or      examples      that      help      me      understand      the      typical      content      and  \n \n  structure      included      in      Wikipedia      pages      for      similar      topics. Please      list      the      urls      in      separate      lines."
            },
            {
              "id": 841,
              "title": "C.1      Soft      Heading      Recall - Chunk 6",
              "type": "chunk",
              "children": [],
              "content": "non topic   \n \n=      dspy.InputField(prefix=\"Topic      of      interest:”,      format=str)  \n \n  related_topics   \n \n=      dspy.OutputField() class      GenPerspectivesPrompt      (dspy.Signature):\n \n \n  You      need      to      select   \n \na      group      of      Wikipedia      editors      who      will      work      together      to      create  \n \n  a      comprehensive      article      on      the      topic."
            },
            {
              "id": 842,
              "title": "C.1      Soft      Heading      Recall - Chunk 7",
              "type": "chunk",
              "children": [],
              "content": "Each      of      them      represents   \n \na      different  \n \n  perspective,      role,      or      affiliation      related      to      this      topic. You      can      use      other  \n \n  Wikipedia      pages      of      related      topics      for      inspiration. For      each      editor,      add  \n \n  description      of      what      they      will      focus      on. Give      your      answer      in      the      following      format:      1."
            },
            {
              "id": 843,
              "title": "C.1      Soft      Heading      Recall - Chunk 8",
              "type": "chunk",
              "children": [],
              "content": "short      summary      of      editor      1:\ndescription\\\\2. short summary of editor 2: description\\\\  \n \n  non  \n \n  topic   \n \n=      dspy.InputField(prefix=\\\\\\'Topic      of      interest:\\\\\\',      format=str)  \n \n  examples   \n \n=      dspy.InputField(prefix=\\\\\\'Wiki      page      outlines      of      related      topics      for  \n \n  inspiration:\\\\\\\\\\',      format=str)  \n \n  perspectives   \n \n=      dspy.OutputField() class      GenQnPrompt(dspy."
            },
            {
              "id": 844,
              "title": "C.1      Soft      Heading      Recall - Chunk 9",
              "type": "chunk",
              "children": [],
              "content": "Signature):\n \n \n  You      are      an      experienced      Wikipedia      writer      and      want      to      edit   \n \na      specific      page. Besides      your      identity      as   \n \na      Wikipedia      writer,      you      have   \n \na      specific      focus      when  \n \n  researching      the      topic. Now,      you      are      chatting      with      an      expert      to      get      information."
            },
            {
              "id": 845,
              "title": "C.1      Soft      Heading      Recall - Chunk 10",
              "type": "chunk",
              "children": [],
              "content": "Ask      good      questions      to  \n \n  get      more      useful      information. When      you      have      no      more      question      to      ask,      say      \"Thank      you      so      much      for      your      help!”\nto  \n \n  end      the      conversation. Please      only      ask      one      question      at   \n \na      time      and      don\\\\\\'t      ask      what      you      have      asked      before."
            },
            {
              "id": 846,
              "title": "C.1      Soft      Heading      Recall - Chunk 11",
              "type": "chunk",
              "children": [],
              "content": "Your      questions      should      be      related      to      the      topic      you      want      to      write."
            },
            {
              "id": 847,
              "title": "C.1      Soft      Heading      Recall - Chunk 12",
              "type": "chunk",
              "children": [],
              "content": "non  \n \n  topic   \n \n=      dspy.InputField(prefix=\\\\\\'Topic      you      want      to      write:      \\\\\\',      format=str)  \n \n  persona   \n \n=      dspy.InputField(prefix=\\\\\\'Your      specific      perspective:      \\\\\\',      format=str)  \n \n  conv   \n \n=      dspy.InputField(prefix=\\\\\\'Conversation      history:\\\\\\\\\\',      format=str)  \n \n  question   \n \n=      dspy.OutputField()  \n \n  class      GenQueriesPrompt      (dspy."
            },
            {
              "id": 848,
              "title": "C.1      Soft      Heading      Recall - Chunk 13",
              "type": "chunk",
              "children": [],
              "content": "Signature):  \n \n  nnn  \n \n  You      want      to      answer      the      question      using      Google      search. What      do      you      type      in      the  \n \n  search      box?"
            },
            {
              "id": 849,
              "title": "C.1      Soft      Heading      Recall - Chunk 14",
              "type": "chunk",
              "children": [],
              "content": "Write the queries you will use in the following format:- query 1\\\\- query 2\\\\\n \n \n  topic   \n \n=      dspy.InputField(prefix=\\\\\\'Topic      you      are      discussing      about:      \\\\\\',      format=str)  \n \n  question   \n \n=      dspy.InputField(prefix=\\\\\\'Question      you      want      to      answer:      \\\\\\',      format=str)  \n \n  queries   \n \n=      dspy.OutputField()\nListing      1:      Prompts      used      in      STORM,      corresponding      to      Line      4,      11,      19,      22      in      Algorithm      1."
            },
            {
              "id": 850,
              "title": "C.1      Soft      Heading      Recall - Chunk 15",
              "type": "chunk",
              "children": [],
              "content": "wow      Ne\n \n \n  20  \n \n  21  \n \n  22  \n \n  23  \n \n  24  \n \n  25  \n \n  26  \n \n  27  \n \n  28\n29  \n \n  30 class      GenAnswerPrompt(dspy. Signature):\n \n \n  You      are      an      expert      who      can      use      information      effectively. You      are      chatting      with   \n \na       Wikipedia      writer      who      wants      to      write   \n \na      Wikipedia      page      on      topic      you      know."
            },
            {
              "id": 851,
              "title": "C.1      Soft      Heading      Recall - Chunk 16",
              "type": "chunk",
              "children": [],
              "content": "You  \n \n  have      gathered      the      related      information      and      will      now      use      the      information      to  \n \n  form   \n \na      response. Make      your      response      as      informative      as      possible      and      make      sure      every      sentence      is  \n \n  supported      by      the      gathered      information."
            },
            {
              "id": 852,
              "title": "C.1      Soft      Heading      Recall - Chunk 17",
              "type": "chunk",
              "children": [],
              "content": "non  \n \n  topic   \n \n=      dspy.InputField(prefix=\\\\\\'Topic      you      are      discussing      about:\\\\\\',      format=str)  \n \n  conv      dspy.InputField(prefix=\\\\\\'Question:\\\\\\\\\\',      format=str)  \n \n  info   \n \n=      dspy.InputField(      prefix=\\\\\\'Gathered      information:\\\\\\\\\\',      format=str)  \n \n  answer   \n \n=      dspy.OutputField(prefix=\\\\\\'Now      give      your      response:\\\\\\\\\\')\nclass      DirectGenOutlinePrompt      (dspy."
            },
            {
              "id": 853,
              "title": "C.1      Soft      Heading      Recall - Chunk 18",
              "type": "chunk",
              "children": [],
              "content": "Signature):\nWrite      an      outline      for   \n \na      Wikipedia      page. Here      is      the      format      of      your      writing:\n2. Do      not      include      other      information. non\n1. Use      \"#\"      Title”      to      indicate      section      title,      \"##\"      Title”      to      indicate  \n \n  subsection      title,      \"###\"”      Title”      to      indicate      subsubsection      title,      and      so  \n \n  on."
            },
            {
              "id": 854,
              "title": "C.1      Soft      Heading      Recall - Chunk 19",
              "type": "chunk",
              "children": [],
              "content": "topic   \n \n=      dspy.InputField(prefix=\"Topic      you      want      to      write:      ”\",      format=str)  \n \n  outline   \n \n=      dspy.OutputField(prefix=\"Write      the      Wikipedia      page      outline:\\\\\"”)”\nclass      RefineOutlinePrompt(dspy. Signature):\n \n \n  Improve      an      outline      for   \n \na      Wikipedia      page. You      already      have   \n \na      draft      outline      that  \n \n  covers      the      general      information."
            },
            {
              "id": 855,
              "title": "C.1      Soft      Heading      Recall - Chunk 20",
              "type": "chunk",
              "children": [],
              "content": "Now      you      want      to      improve      it      based      on      the  \n \n  information      learned      from      an      information-seeking      conversation      to      make      it      more  \n \n  comprehensive. Here      is      the      format      of      your      writing:\n2. Do      not      include      other      information. non\n1."
            },
            {
              "id": 856,
              "title": "C.1      Soft      Heading      Recall - Chunk 21",
              "type": "chunk",
              "children": [],
              "content": "Use      \"#\"      Title”      to      indicate      section      title,      \"##\"      Title”      to      indicate  \n \n  subsection      title,      \"###\"      Title”      to      indicate      subsubsection      title,      and      so  \n \n  on."
            },
            {
              "id": 857,
              "title": "C.1      Soft      Heading      Recall - Chunk 22",
              "type": "chunk",
              "children": [],
              "content": "topic   \n \n=      dspy.InputField(prefix=\"Topic      you      want      to      write:      \",      format=str)  \n \n  conv   \n \n=      dspy.InputField(prefix=\"Conversation      history:\\\\\",      format=str)  \n \n  old_outline   \n \n=      dspy.OutputField(prefix=\"Current      outline:\\\\”,      format=str)  \n \n  outline   \n \n=      dspy.OutputField(      prefix=\\\\\\'Write      the      Wikipedia      page      outline:\\\\\\\\\\')”\nListing      2:      Prompts      used      in      STORM      (continue),      corresponding      to      Line      24,      31,      32      in      Algorithm      1."
            },
            {
              "id": 858,
              "title": "C.1      Soft      Heading      Recall - Chunk 23",
              "type": "chunk",
              "children": [],
              "content": "yay      aA      uu      &}      WwW      YY      —\n \n \n  11  \n \n  12  \n \n  13  \n \n  14  \n \n  15  \n \n  16  \n \n  17  \n \n  18  \n \n  19  \n \n  20  \n \n  21  \n \n  22  \n \n  23\n \n \n  24  \n \n  25  \n \n  26  \n \n  27  \n \n  28  \n \n  29  \n \n  30  \n \n  31  \n \n  32  \n \n  33  \n \n  Input      :Topic      t,      maximum      perspective      N,  \n \n  maximum      conversation      round      MJ  \n \n  Output   \n \n:      Outline      O,      references   \n \nR PO = \"basic fact writer \" // Constant."
            },
            {
              "id": 859,
              "title": "C.1      Soft      Heading      Recall - Chunk 24",
              "type": "chunk",
              "children": [],
              "content": "R-[]  \n \n  //      Discover      perspectives      P. related_topics   \n \n+      gen_related_topics(t)  \n \n  tocs   \n \n+   \n \n|   \n \n|       foreach      related_t      in      related_topics      do  \n \n  article   \n \n<      get_wiki_article(related_t)  \n \n  if      article      then  \n \n  |      tocs.append(extract_toc(article))  \n \n  end  \n \n  end  \n \n  P   \n \n<      gen_perspectives(t,      tocs)  \n \n  P<      [PO]   \n \n+      P[:N]  \n \n  //      Simulate      conversations."
            },
            {
              "id": 860,
              "title": "C.1      Soft      Heading      Recall - Chunk 25",
              "type": "chunk",
              "children": [],
              "content": "convos   \n \n<      [|  \n \n  foreach   \n \np      in   \n \nP      do  \n \n  convo_history   \n \n<   \n \n|   \n \n]       for:      =1to   \n \nM      do  \n \n  //      Question      asking. q+      gen_qn(t,      p,      dlg_history)  \n \n  convo_history.append(q)  \n \n  //      Question      answering."
            },
            {
              "id": 861,
              "title": "C.1      Soft      Heading      Recall - Chunk 26",
              "type": "chunk",
              "children": [],
              "content": "queries   \n \n<      gen_queries(t,      q)  \n \n  sources      <—  \n \n  search_and_sift(queries)  \n \n  a   \n \n+      gen_ans(t,      q,      sources)  \n \n  convo_history.append(a)  \n \n  R.append(sources)  \n \n  end  \n \n  convos.append(convo_history)  \n \n  end  \n \n  //      Create      the      outline."
            },
            {
              "id": 862,
              "title": "C.1      Soft      Heading      Recall - Chunk 27",
              "type": "chunk",
              "children": [],
              "content": "Op   \n \n<      direct_gen_outline(t)  \n \n  O      «<      refine_outline(t,      Op,      convos)  \n \n  return      O,      R       of   \n \nA      is      the      sum      of      the      counts      of      its      individual      items:"
            }
          ],
          "content": ""
        },
        {
          "id": 863,
          "title": "K       card(A)      =      S-      count      (A;)      (2)",
          "type": "section",
          "children": [
            {
              "id": 864,
              "title": "K       card(A)      =      S-      count      (A;)      (2) - Chunk 1",
              "type": "chunk",
              "children": [],
              "content": "K       card(A)      =      S-      count      (A;)      (2)\ni=1 The      soft      heading      recall      is      calculated      as card(Gn      P)  \n \n  card(G)   \n \n”      @) soft      heading      recall      =\nwhere      the      cardinality      of      intersection      is      defined      via  \n \n  the      union      as      follows:\ncard(Gn      P)   \n \n=       card(G)   \n \n+      card(P)   \n \n—      card(G   \n \nU      P). ®"
            }
          ],
          "content": ""
        },
        {
          "id": 865,
          "title": "C.2.      LLM      Evaluator",
          "type": "section",
          "children": [
            {
              "id": 866,
              "title": "C.2.      LLM      Evaluator - Chunk 1",
              "type": "chunk",
              "children": [],
              "content": "C.2. LLM      Evaluator\nWe      use      Prometheus!"
            },
            {
              "id": 867,
              "title": "C.2.      LLM      Evaluator - Chunk 2",
              "type": "chunk",
              "children": [],
              "content": "*      (Kim      et      al.,      2023),   \n \na      13B  \n \n  open-source      evaluator      LLM      that      can      assess      long-  \n \n  form      text      based      on      customized      1-5      scale      rubric,      to  \n \n  grade      the      article      from      the      aspects      of      Interest      level,  \n \n  Coherence      and      Organization,      Relevance      and      Fo-  \n \n  cus,      and      Coverage."
            },
            {
              "id": 868,
              "title": "C.2.      LLM      Evaluator - Chunk 3",
              "type": "chunk",
              "children": [],
              "content": "Table   \n \n8      gives      our      grading      rubric. While      Prometheus      is      best      used      with   \n \na      score   \n \n5      ref-  \n \n  erence      answer,      we      find      adding      the      reference      will  \n \n  exceed      the      context      length      limit      of      the      model. Since  \n \n  Kim      et      al."
            },
            {
              "id": 869,
              "title": "C.2.      LLM      Evaluator - Chunk 4",
              "type": "chunk",
              "children": [],
              "content": "(2023)      show      Prometheus      ratings      without  \n \n  reference      also      correlate      well      with      human      prefer-  \n \n  ences,      we      omit      the      reference      and      trim      the      input  \n \n  article      to      be      within      2000      words      by      iteratively      re-  \n \n  moving      contents      from      the      shortest      section      to      ensure  \n \n  the      input      can      fit      into      the      model’s      context      window."
            }
          ],
          "content": ""
        },
        {
          "id": 870,
          "title": "C.3      More      Discussion      of      the      Citation      Quality",
          "type": "section",
          "children": [
            {
              "id": 871,
              "title": "C.3      More      Discussion      of      the      Citation      Quality - Chunk 1",
              "type": "chunk",
              "children": [],
              "content": "C.3      More      Discussion      of      the      Citation      Quality\n \n \n  Irrelevant  \n \n  Source  \n \n  Inaccurate  \n \n  Othe      Paraphrasing  \n \n  1%      4%  \n \n  7%  \n \n  Improper  \n \n  Inferential      Linking  \n \n  Lack      Citation      14%  \n \n  47%  \n \n  Incorrectly      Split  \n \n  12%"
            }
          ],
          "content": ""
        },
        {
          "id": 872,
          "title": "False      Negative       15%",
          "type": "section",
          "children": [
            {
              "id": 873,
              "title": "False      Negative       15% - Chunk 1",
              "type": "chunk",
              "children": [],
              "content": "False      Negative       15%\nFigure      6:      Error      analysis      of      unsupported      sentences      in      10  \n \n  sampled      articles."
            },
            {
              "id": 874,
              "title": "False      Negative       15% - Chunk 2",
              "type": "chunk",
              "children": [],
              "content": "https:      //huggingface.co/kaist-ai/  \n \n  Criteria      Description  \n \n  Score   \n \n|      Description  \n \n  Score   \n \n2      Description  \n \n  Score   \n \n3      Description  \n \n  Score   \n \n4      Description  \n \n  Score   \n \n5      Description\n \n \n  Interest      Level:      How      engaging      and      thought-provoking      is      the      article? Not      engaging      at      all;      no      attempt      to      capture      the      reader’s      attention."
            },
            {
              "id": 875,
              "title": "False      Negative       15% - Chunk 3",
              "type": "chunk",
              "children": [],
              "content": "Fairly      engaging      with   \n \na      basic      narrative      but      lacking      depth. Moderately      engaging      with      several      interesting      points. Quite      engaging      with   \n \na      well-structured      narrative      and      noteworthy      points      that      frequently      capture      and      retain      attention."
            },
            {
              "id": 876,
              "title": "False      Negative       15% - Chunk 4",
              "type": "chunk",
              "children": [],
              "content": "Exceptionally      engaging      throughout,      with   \n \na      compelling      narrative      that      consistently      stimulates      interest. Criteria      Description  \n \n  Score   \n \n|      Description  \n \n  Score   \n \n2      Description  \n \n  Score   \n \n3      Description  \n \n  Score   \n \n4      Description  \n \n  Score   \n \n5      Description  \n \n  Coherence      and      Organization:      Is      the      article      well-organized      and      logically      structured?"
            },
            {
              "id": 877,
              "title": "False      Negative       15% - Chunk 5",
              "type": "chunk",
              "children": [],
              "content": "Disorganized;      lacks      logical      structure      and      coherence. Fairly      organized;   \n \na      basic      structure      is      present      but      not      consistently      followed. Organized;   \n \na      clear      structure      is      mostly      followed      with      some      lapses      in      coherence. Good      organization;   \n \na      clear      structure      with      minor      lapses      in      coherence."
            },
            {
              "id": 878,
              "title": "False      Negative       15% - Chunk 6",
              "type": "chunk",
              "children": [],
              "content": "Excellently      organized;      the      article      is      logically      structured      with      seamless      transitions      and   \n \na      clear      argument."
            },
            {
              "id": 879,
              "title": "False      Negative       15% - Chunk 7",
              "type": "chunk",
              "children": [],
              "content": "Criteria      Description  \n \n  Score   \n \n|      Description  \n \n  Score   \n \n2      Description  \n \n  Score   \n \n3      Description  \n \n  Score   \n \n4      Description  \n \n  Score   \n \n5      Description  \n \n  Relevance      and      Focus:      Does      the      article      stay      on      topic      and      maintain   \n \na      clear      focus? Off-topic;      the      content      does      not      align      with      the      headline      or      core      subject."
            },
            {
              "id": 880,
              "title": "False      Negative       15% - Chunk 8",
              "type": "chunk",
              "children": [],
              "content": "Somewhat      on      topic      but      with      several      digressions;      the      core      subject      is      evident      but      not      consistently      adhered      to. Generally      on      topic,      despite   \n \na      few      unrelated      details. Mostly      on      topic      and      focused;      the      narrative      has   \n \na      consistent      relevance      to      the      core      subject      with      infrequent      digressions."
            },
            {
              "id": 881,
              "title": "False      Negative       15% - Chunk 9",
              "type": "chunk",
              "children": [],
              "content": "Exceptionally      focused      and      entirely      on      topic;      the      article      is      tightly      centered      on      the      subject,      with      every      piece      of      information      contributing  \n \n  to   \n \na      comprehensive      understanding      of      the      topic."
            },
            {
              "id": 882,
              "title": "False      Negative       15% - Chunk 10",
              "type": "chunk",
              "children": [],
              "content": "Criteria      Description  \n \n  Score   \n \n|      Description  \n \n  Score   \n \n2      Description  \n \n  Score   \n \n3      Description  \n \n  Score   \n \n4      Description  \n \n  Score   \n \n5      Description  \n \n  Broad      Coverage:      Does      the      article      provide      an      in-depth      exploration      of      the      topic      and      have      good      coverage?"
            },
            {
              "id": 883,
              "title": "False      Negative       15% - Chunk 11",
              "type": "chunk",
              "children": [],
              "content": "Severely      lacking;      offers      little      to      no      coverage      of      the      topic’s      primary      aspects,      resulting      in   \n \na      very      narrow      perspective. Partial      coverage;      includes      some      of      the      topic’s      main      aspects      but      misses      others,      resulting      in      an      incomplete      portrayal."
            },
            {
              "id": 884,
              "title": "False      Negative       15% - Chunk 12",
              "type": "chunk",
              "children": [],
              "content": "Acceptable      breadth;      covers      most      main      aspects,      though      it      may      stray      into      minor      unnecessary      details      or      overlook      some      relevant      points. Good      coverage;      achieves      broad      coverage      of      the      topic,      hitting      on      all      major      points      with      minimal      extraneous      information."
            },
            {
              "id": 885,
              "title": "False      Negative       15% - Chunk 13",
              "type": "chunk",
              "children": [],
              "content": "Exemplary      in      breadth;      delivers      outstanding      coverage,      thoroughly      detailing      all      crucial      aspects      of      the      topic      without      including      irrelevant  \n \n  information. Table      8:      Scoring      rubrics      on   \n \na      1-5      scale      for      the      evaluator      LLM."
            }
          ],
          "content": ""
        },
        {
          "id": 886,
          "title": "Error      Type       Topic      Unsupported      Sentence      Source",
          "type": "section",
          "children": [
            {
              "id": 887,
              "title": "Error      Type       Topic      Unsupported      Sentence      Source - Chunk 1",
              "type": "chunk",
              "children": [],
              "content": "Error      Type       Topic      Unsupported      Sentence      Source\n\n |       Throughout      its      history,      religion      has      remained      the       paramount      aspect      of      Hawaiian      life      in      Lahaina      ,       permeating      every      daily      activity      and      significant      event[5]."
            },
            {
              "id": 888,
              "title": "Error      Type       Topic      Unsupported      Sentence      Source - Chunk 2",
              "type": "chunk",
              "children": [],
              "content": "|       [5]      “Religion,      Beliefs      &      Spirituality”       (The      source      discusses      religion      as      part      of      Hawaiian      life       but      does      not      mention      Lahania      .)\n |       Lahaina,      Hawaii\n |       [2]      “Crimean      Bridge      -      Wikipedia”       (The      source      says      “The      first      scheduled      passenger      train       crossed      the      bridge      on      25      December      2019,      while      the       bridge      was      opened      for      freight      trains      on      30      June      2020      ”.)       Completed      in      June      2020      ,      the      bridge      serves      as      a       major      supply      route      for      Russian      forces      in      the      region       and      is      significant      to      Russia’s      claim      over      the      disputed       territory[2][11]."
            },
            {
              "id": 889,
              "title": "Error      Type       Topic      Unsupported      Sentence      Source - Chunk 3",
              "type": "chunk",
              "children": [],
              "content": "| 2022      Crimean       Bridge      explosion\n |       For      example,      comparisons      have      been      drawn      between       the      performance      of      LK-9      and      the      dynamic      resolution       capabilities      of      video      games      such      as      Battlefield      2042[22]."
            },
            {
              "id": 890,
              "title": "Error      Type       Topic      Unsupported      Sentence      Source - Chunk 4",
              "type": "chunk",
              "children": [],
              "content": "|       [22]      “Battlefield      2042      PC      performance      guide:      The      best       settings      for      a      high      frame      rate”       (      The      source      is      irrelevant      to      LK-99. )\n |       LK-99\n\nTable      9:      Examples      of      different      error      types      of      unsupported      sentences."
            },
            {
              "id": 891,
              "title": "Error      Type       Topic      Unsupported      Sentence      Source - Chunk 5",
              "type": "chunk",
              "children": [],
              "content": "We      use      Mistral      7B-Instruct!>      (Jiang      et      al.,      2023a)\n \n \n  to      examine      whether      the      cited      passages      entail      the  \n \n  generated      sentence. Table   \n \n4      reports      the      citation  \n \n  quality      of      articles      produced      by      our      approach,      show-  \n \n  ing      that      around      15%      sentences      in      generated      articles  \n \n  are      unsupported      by      citations."
            },
            {
              "id": 892,
              "title": "Error      Type       Topic      Unsupported      Sentence      Source - Chunk 6",
              "type": "chunk",
              "children": [],
              "content": "We      further      investi-  \n \n  gate      the      failure      cases      by      randomly      sampling      10  \n \n  articles      and      an      author      manually      examines      all      the  \n \n  unsupported      sentences      in      these      articles."
            },
            {
              "id": 893,
              "title": "Error      Type       Topic      Unsupported      Sentence      Source - Chunk 7",
              "type": "chunk",
              "children": [],
              "content": "Besides  \n \n  sentences      that      are      incorrectly      split!®,      lack      citations,  \n \n  or      are      deemed      supported      by      the      author’s      judgment,  \n \n  our      analysis      identifies      three      main      error      categories  \n \n  (examples      are      given      in      Table      9):      improper      inferen-  \n \n  tial      linking,      inaccurate      paraphrasing,      and      citing  \n \n  irrelevant      sources."
            },
            {
              "id": 894,
              "title": "Error      Type       Topic      Unsupported      Sentence      Source - Chunk 8",
              "type": "chunk",
              "children": [],
              "content": "We      show      the      error      distribution      in      Figure      6. No-  \n \n  tably,      the      most      common      errors      stem      from      the      ten-  \n \n  dency      of      LLMs      to      form      improper      inferential      links  \n \n  between      different      pieces      of      information      presented  \n \n  in      the      context      window."
            },
            {
              "id": 895,
              "title": "Error      Type       Topic      Unsupported      Sentence      Source - Chunk 9",
              "type": "chunk",
              "children": [],
              "content": "Our      analysis      of      citation  \n \n  quality      suggests      that,      in      addition      to      avoiding      hallu-  \n \n  cinations,      future      research      in      grounded      text      gener-  \n \n  ation      should      also      focus      on      preventing      LLMs      from  \n \n  making      overly      inferential      leaps      based      on      the      pro-  \n \n  vided      information."
            },
            {
              "id": 896,
              "title": "Error      Type       Topic      Unsupported      Sentence      Source - Chunk 10",
              "type": "chunk",
              "children": [],
              "content": "D      Human      Evaluation      Details\n \n \n  We      recruited      10      experienced      Wikipedia      editors  \n \n  to      participate      in      our      study      by      creating   \n \na      research  \n \n  page      on      Meta-Wiki!”\nand      reaching      out      to      active editors      who      have      recently      approved      articles      for  \n \n  Wikipedia.\\\\\\'®      Our      participation      group      includes   \n \n3       editors      with      1-5      years      of      experience,   \n \n4      with      6-10  \n \n  years,      and   \n \n3      with      over      15      years      of      contribution."
            },
            {
              "id": 897,
              "title": "Error      Type       Topic      Unsupported      Sentence      Source - Chunk 11",
              "type": "chunk",
              "children": [],
              "content": "The      study      was      approved      by      the      Institutional      Re-  \n \n  view      Board      of      our      institution      and      the      participants  \n \n  signed      the      consent      form      through      Qualtrics      ques-  \n \n  tionnaires      before      the      study      started."
            },
            {
              "id": 898,
              "title": "Error      Type       Topic      Unsupported      Sentence      Source - Chunk 12",
              "type": "chunk",
              "children": [],
              "content": "To      streamline      the      evaluation      of      grounded      articles,  \n \n  we      developed   \n \na      web      application,      which      features   \n \na       side-by-side      display      of      the      article      and      its      citation  \n \n  snippets,      to      gather      ratings      and      open-ended      feedback  \n \n  Shttps      ://huggingface.co/mistralai/  \n \n  Mistral-7B-Instruct-vQ.1  \n \n  \\\\\\'6Rollowing      Gao      et      al."
            },
            {
              "id": 899,
              "title": "Error      Type       Topic      Unsupported      Sentence      Source - Chunk 13",
              "type": "chunk",
              "children": [],
              "content": "(2023),      we      check      citation      quality      in  \n \n  the      sentence      level      and      split      articles      into      sentences      using      NLTK  \n \n  sent_tokenize. sent_tokenize      sometimes      fails      to      split      sen-  \n \n  tences      correctly      when      the      article      contains      special      words      like  \n \n  “No.12847”,      “Bhatia      et      al. ”,      etc."
            },
            {
              "id": 900,
              "title": "Error      Type       Topic      Unsupported      Sentence      Source - Chunk 14",
              "type": "chunk",
              "children": [],
              "content": "\"https      ://meta.wikimedia.org  \n \n  \\\\\\'8Since      evaluating      Wikipedia-like      articles      is      time-  \n \n  consuming      and      requires      expertise,      we      paid      each      participant  \n \n  50$      for      our      study. for      each      article."
            },
            {
              "id": 901,
              "title": "Error      Type       Topic      Unsupported      Sentence      Source - Chunk 15",
              "type": "chunk",
              "children": [],
              "content": "Figure   \n \n7      shows      the      screenshot      of  \n \n  our      web      application      and      the      full      article      produced  \n \n  by      STORM      is      included      in      Table      12. For      human  \n \n  evaluation,      we      use   \n \na   \n \n|      to   \n \n7      scale      for      more      fine-  \n \n  grained      evaluation. The      grading      rubric      is      included  \n \n  in      Table      10."
            },
            {
              "id": 902,
              "title": "Error      Type       Topic      Unsupported      Sentence      Source - Chunk 16",
              "type": "chunk",
              "children": [],
              "content": "We      collected      the      pairwise      preferences      and      the  \n \n  perceived      usefulness      of      STORM      via      an      online      ques-  \n \n  tionnaire."
            },
            {
              "id": 903,
              "title": "Error      Type       Topic      Unsupported      Sentence      Source - Chunk 17",
              "type": "chunk",
              "children": [],
              "content": "Specifically,      for      the      perceived      usefulness,  \n \n  we      request      editors      to      rate      their      agreement      with      state-  \n \n  ments      “I      think      it      can      be      specifically      helpful      for      my  \n \n  pre-writing      stage      (e.g.,      collecting      relevant      sources,  \n \n  outlining,      drafting)."
            },
            {
              "id": 904,
              "title": "Error      Type       Topic      Unsupported      Sentence      Source - Chunk 18",
              "type": "chunk",
              "children": [],
              "content": "”,      “I      think      it      will      help      me      edit  \n \n  a      Wikipedia      article      for   \n \na      new      topic”,      “I      think      it  \n \n  can      be   \n \na      potentially      useful      tool      for      the      Wikipedia  \n \n  community”      on   \n \na      Likert      scale      of      1-5,      correspond-  \n \n  ing      to      Strongly      disagree,      Somewhat      disagree,      Nei-  \n \n  ther      agree      nor      disagree,      Somewhat      agree,      Strongly agree."
            },
            {
              "id": 905,
              "title": "Error      Type       Topic      Unsupported      Sentence      Source - Chunk 19",
              "type": "chunk",
              "children": [],
              "content": "E_      Error      Analysis\n \n \n  While      articles      produced      by      STORM      are      preferred  \n \n  by      both      automatic      metrics      and      human      evaluation,  \n \n  experienced      editors      still      identified      multiple      prob-  \n \n  lems      with      the      machine-generated      articles. We      an-  \n \n  alyze      the      free-form      comments      and      summarize      the  \n \n  major      issues      in      Table      11."
            },
            {
              "id": 906,
              "title": "Error      Type       Topic      Unsupported      Sentence      Source - Chunk 20",
              "type": "chunk",
              "children": [],
              "content": "The      primary      issue      raised      is      that      the      generated  \n \n  articles      often      contain      emotional      language      and      lack  \n \n  neutrality,      primarily      due      to      the      source      material. STORM      currently      retrieves      grounding      sources  \n \n  from      the      Internet      which      is      not      neutral      and      con-  \n \n  tains      considerable      promotional      content      on      its      own."
            },
            {
              "id": 907,
              "title": "Error      Type       Topic      Unsupported      Sentence      Source - Chunk 21",
              "type": "chunk",
              "children": [],
              "content": "Addressing      this      bias      in      the      pre-writing      stage      repre-  \n \n  sents   \n \na      valuable      direction      for      future      research. An-  \n \n  other      major      issue      is      the      red      herring      fallacy      or      the  \n \n  over-association      of      unrelated      facts. Addressing      this  \n \n  challenge      calls      for      high-level      sensemaking      rather  \n \n  than      mere      fact-level      verification."
            },
            {
              "id": 908,
              "title": "Error      Type       Topic      Unsupported      Sentence      Source - Chunk 22",
              "type": "chunk",
              "children": [],
              "content": "Interest      Level  \n \n  Not      engaging      at      all;      no      attempt      to      capture      the      reader’s      attention. Slightly      engaging      with      rare      moments      that      capture      attention. Fairly      engaging      with   \n \na      basic      narrative      but      lacking      depth. Moderately      engaging      with      several      interesting      points."
            },
            {
              "id": 909,
              "title": "Error      Type       Topic      Unsupported      Sentence      Source - Chunk 23",
              "type": "chunk",
              "children": [],
              "content": "Quite      engaging      with   \n \na      well-structured      narrative      and      noteworthy      points      that      frequently      capture      and      retain      attention. Very      engaging      with   \n \na      compelling      narrative      that      captures      and      mostly      retains      attention. Exceptionally      engaging      throughout,      with   \n \na      compelling      narrative      that      consistently      stimulates      interest."
            },
            {
              "id": 910,
              "title": "Error      Type       Topic      Unsupported      Sentence      Source - Chunk 24",
              "type": "chunk",
              "children": [],
              "content": "MOawWPYWNr\n \n \n  Coherence      and      Organization  \n \n  Disorganized;      lacks      logical      structure      and      coherence. Poor      organization;      some      structure      is      evident      but      very      weak. Fairly      organized;   \n \na      basic      structure      is      present      but      not      consistently      followed. Organized;   \n \na      clear      structure      is      mostly      followed      with      some      lapses      in      coherence."
            },
            {
              "id": 911,
              "title": "Error      Type       Topic      Unsupported      Sentence      Source - Chunk 25",
              "type": "chunk",
              "children": [],
              "content": "Good      organization;   \n \na      clear      structure      with      minor      lapses      in      coherence. Very      well-organized;   \n \na      logical      structure      with      transitions      that      effectively      guide      the      reader. Excellently      organized;      the      article      is      logically      structured      with      seamless      transitions      and   \n \na      clear      argument."
            },
            {
              "id": 912,
              "title": "Error      Type       Topic      Unsupported      Sentence      Source - Chunk 26",
              "type": "chunk",
              "children": [],
              "content": "aw:\n \n \n  Relevance      and      Focus  \n \n  1:      Off-topic;      the      content      does      not      align      with      the      headline      or      core      subject. 2:      Mostly      off-topic      with      some      relevant      points. 3:      Somewhat      on      topic      but      with      several      digressions;      the      core      subject      is      evident      but      not      consistently      adhered      to."
            },
            {
              "id": 913,
              "title": "Error      Type       Topic      Unsupported      Sentence      Source - Chunk 27",
              "type": "chunk",
              "children": [],
              "content": "4:      Generally      on      topic,      despite   \n \na      few      unrelated      details. 5:      Mostly      on      topic      and      focused;      the      narrative      has   \n \na      consistent      relevance      to      the      core      subject      with      infrequent      digressions. 6:      Highly      relevant      with   \n \na      focused      narrative      and      purpose."
            },
            {
              "id": 914,
              "title": "Error      Type       Topic      Unsupported      Sentence      Source - Chunk 28",
              "type": "chunk",
              "children": [],
              "content": "7:      Exceptionally      focused      and      entirely      on      topic;      the      article      is      tightly      centered      on      the      subject,      with      every      piece      of      information      contributing      to   \n \na       comprehensive      understanding      of      the      topic."
            },
            {
              "id": 915,
              "title": "Error      Type       Topic      Unsupported      Sentence      Source - Chunk 29",
              "type": "chunk",
              "children": [],
              "content": "Broad      Coverage  \n \n  Severely      lacking;      offers      little      to      no      coverage      of      the      topic’s      primary      aspects,      resulting      in   \n \na      very      narrow      perspective. Minimal      coverage;      addresses      only   \n \na      small      selection      of      the      topic’s      main      aspects,      with      significant      omissions."
            },
            {
              "id": 916,
              "title": "Error      Type       Topic      Unsupported      Sentence      Source - Chunk 30",
              "type": "chunk",
              "children": [],
              "content": "Partial      coverage;      includes      some      of      the      topic’s      main      aspects      but      misses      others,      resulting      in      an      incomplete      portrayal. Acceptable      breadth;      covers      most      main      aspects,      though      it      may      stray      into      minor      unnecessary      details      or      overlook      some      relevant      points."
            },
            {
              "id": 917,
              "title": "Error      Type       Topic      Unsupported      Sentence      Source - Chunk 31",
              "type": "chunk",
              "children": [],
              "content": "Good      coverage;      achieves      broad      coverage      of      the      topic,      hitting      on      all      major      points      with      minimal      extraneous      information. Comprehensive;      provides      thorough      coverage      of      all      significant      aspects      of      the      topic,      with   \n \na      well-balanced      focus."
            },
            {
              "id": 918,
              "title": "Error      Type       Topic      Unsupported      Sentence      Source - Chunk 32",
              "type": "chunk",
              "children": [],
              "content": "Exemplary      in      breadth;      delivers      outstanding      coverage,      thoroughly      detailing      all      crucial      aspects      of      the      topic      without      including      irrelevant      information."
            }
          ],
          "content": ""
        },
        {
          "id": 919,
          "title": "We      use      Mistral      7B-Instruct!>      (Jiang      et      al.,      2023a)",
          "type": "section",
          "children": [
            {
              "id": 920,
              "title": "We      use      Mistral      7B-Instruct!>      (Jiang      et      al.,      2023a) - Chunk 1",
              "type": "chunk",
              "children": [],
              "content": "We      use      Mistral      7B-Instruct!>      (Jiang      et      al.,      2023a)\n \n \n  to      examine      whether      the      cited      passages      entail      the  \n \n  generated      sentence. Table   \n \n4      reports      the      citation  \n \n  quality      of      articles      produced      by      our      approach,      show-  \n \n  ing      that      around      15%      sentences      in      generated      articles  \n \n  are      unsupported      by      citations."
            },
            {
              "id": 921,
              "title": "We      use      Mistral      7B-Instruct!>      (Jiang      et      al.,      2023a) - Chunk 2",
              "type": "chunk",
              "children": [],
              "content": "We      further      investi-  \n \n  gate      the      failure      cases      by      randomly      sampling      10  \n \n  articles      and      an      author      manually      examines      all      the  \n \n  unsupported      sentences      in      these      articles."
            },
            {
              "id": 922,
              "title": "We      use      Mistral      7B-Instruct!>      (Jiang      et      al.,      2023a) - Chunk 3",
              "type": "chunk",
              "children": [],
              "content": "Besides  \n \n  sentences      that      are      incorrectly      split!®,      lack      citations,  \n \n  or      are      deemed      supported      by      the      author’s      judgment,  \n \n  our      analysis      identifies      three      main      error      categories  \n \n  (examples      are      given      in      Table      9):      improper      inferen-  \n \n  tial      linking,      inaccurate      paraphrasing,      and      citing  \n \n  irrelevant      sources."
            },
            {
              "id": 923,
              "title": "We      use      Mistral      7B-Instruct!>      (Jiang      et      al.,      2023a) - Chunk 4",
              "type": "chunk",
              "children": [],
              "content": "We      show      the      error      distribution      in      Figure      6. No-  \n \n  tably,      the      most      common      errors      stem      from      the      ten-  \n \n  dency      of      LLMs      to      form      improper      inferential      links  \n \n  between      different      pieces      of      information      presented  \n \n  in      the      context      window."
            },
            {
              "id": 924,
              "title": "We      use      Mistral      7B-Instruct!>      (Jiang      et      al.,      2023a) - Chunk 5",
              "type": "chunk",
              "children": [],
              "content": "Our      analysis      of      citation  \n \n  quality      suggests      that,      in      addition      to      avoiding      hallu-  \n \n  cinations,      future      research      in      grounded      text      gener-  \n \n  ation      should      also      focus      on      preventing      LLMs      from  \n \n  making      overly      inferential      leaps      based      on      the      pro-  \n \n  vided      information."
            }
          ],
          "content": ""
        },
        {
          "id": 925,
          "title": "D      Human      Evaluation      Details",
          "type": "section",
          "children": [
            {
              "id": 926,
              "title": "D      Human      Evaluation      Details - Chunk 1",
              "type": "chunk",
              "children": [],
              "content": "D      Human      Evaluation      Details\n \n \n  We      recruited      10      experienced      Wikipedia      editors  \n \n  to      participate      in      our      study      by      creating   \n \na      research  \n \n  page      on      Meta-Wiki!”\nand      reaching      out      to      active editors      who      have      recently      approved      articles      for  \n \n  Wikipedia.\\\\\\'®      Our      participation      group      includes   \n \n3       editors      with      1-5      years      of      experience,   \n \n4      with      6-10  \n \n  years,      and   \n \n3      with      over      15      years      of      contribution."
            },
            {
              "id": 927,
              "title": "D      Human      Evaluation      Details - Chunk 2",
              "type": "chunk",
              "children": [],
              "content": "The      study      was      approved      by      the      Institutional      Re-  \n \n  view      Board      of      our      institution      and      the      participants  \n \n  signed      the      consent      form      through      Qualtrics      ques-  \n \n  tionnaires      before      the      study      started."
            },
            {
              "id": 928,
              "title": "D      Human      Evaluation      Details - Chunk 3",
              "type": "chunk",
              "children": [],
              "content": "To      streamline      the      evaluation      of      grounded      articles,  \n \n  we      developed   \n \na      web      application,      which      features   \n \na       side-by-side      display      of      the      article      and      its      citation  \n \n  snippets,      to      gather      ratings      and      open-ended      feedback  \n \n  Shttps      ://huggingface.co/mistralai/  \n \n  Mistral-7B-Instruct-vQ.1  \n \n  \\\\\\'6Rollowing      Gao      et      al."
            },
            {
              "id": 929,
              "title": "D      Human      Evaluation      Details - Chunk 4",
              "type": "chunk",
              "children": [],
              "content": "(2023),      we      check      citation      quality      in  \n \n  the      sentence      level      and      split      articles      into      sentences      using      NLTK  \n \n  sent_tokenize. sent_tokenize      sometimes      fails      to      split      sen-  \n \n  tences      correctly      when      the      article      contains      special      words      like  \n \n  “No.12847”,      “Bhatia      et      al. ”,      etc."
            },
            {
              "id": 930,
              "title": "D      Human      Evaluation      Details - Chunk 5",
              "type": "chunk",
              "children": [],
              "content": "\"https      ://meta.wikimedia.org  \n \n  \\\\\\'8Since      evaluating      Wikipedia-like      articles      is      time-  \n \n  consuming      and      requires      expertise,      we      paid      each      participant  \n \n  50$      for      our      study. for      each      article."
            },
            {
              "id": 931,
              "title": "D      Human      Evaluation      Details - Chunk 6",
              "type": "chunk",
              "children": [],
              "content": "Figure   \n \n7      shows      the      screenshot      of  \n \n  our      web      application      and      the      full      article      produced  \n \n  by      STORM      is      included      in      Table      12. For      human  \n \n  evaluation,      we      use   \n \na   \n \n|      to   \n \n7      scale      for      more      fine-  \n \n  grained      evaluation. The      grading      rubric      is      included  \n \n  in      Table      10."
            },
            {
              "id": 932,
              "title": "D      Human      Evaluation      Details - Chunk 7",
              "type": "chunk",
              "children": [],
              "content": "We      collected      the      pairwise      preferences      and      the  \n \n  perceived      usefulness      of      STORM      via      an      online      ques-  \n \n  tionnaire."
            },
            {
              "id": 933,
              "title": "D      Human      Evaluation      Details - Chunk 8",
              "type": "chunk",
              "children": [],
              "content": "Specifically,      for      the      perceived      usefulness,  \n \n  we      request      editors      to      rate      their      agreement      with      state-  \n \n  ments      “I      think      it      can      be      specifically      helpful      for      my  \n \n  pre-writing      stage      (e.g.,      collecting      relevant      sources,  \n \n  outlining,      drafting)."
            },
            {
              "id": 934,
              "title": "D      Human      Evaluation      Details - Chunk 9",
              "type": "chunk",
              "children": [],
              "content": "”,      “I      think      it      will      help      me      edit  \n \n  a      Wikipedia      article      for   \n \na      new      topic”,      “I      think      it  \n \n  can      be   \n \na      potentially      useful      tool      for      the      Wikipedia  \n \n  community”      on   \n \na      Likert      scale      of      1-5,      correspond-  \n \n  ing      to      Strongly      disagree,      Somewhat      disagree,      Nei-  \n \n  ther      agree      nor      disagree,      Somewhat      agree,      Strongly agree."
            }
          ],
          "content": ""
        },
        {
          "id": 935,
          "title": "E_      Error      Analysis",
          "type": "section",
          "children": [
            {
              "id": 936,
              "title": "E_      Error      Analysis - Chunk 1",
              "type": "chunk",
              "children": [],
              "content": "E_      Error      Analysis\n \n \n  While      articles      produced      by      STORM      are      preferred  \n \n  by      both      automatic      metrics      and      human      evaluation,  \n \n  experienced      editors      still      identified      multiple      prob-  \n \n  lems      with      the      machine-generated      articles. We      an-  \n \n  alyze      the      free-form      comments      and      summarize      the  \n \n  major      issues      in      Table      11."
            },
            {
              "id": 937,
              "title": "E_      Error      Analysis - Chunk 2",
              "type": "chunk",
              "children": [],
              "content": "The      primary      issue      raised      is      that      the      generated  \n \n  articles      often      contain      emotional      language      and      lack  \n \n  neutrality,      primarily      due      to      the      source      material. STORM      currently      retrieves      grounding      sources  \n \n  from      the      Internet      which      is      not      neutral      and      con-  \n \n  tains      considerable      promotional      content      on      its      own."
            },
            {
              "id": 938,
              "title": "E_      Error      Analysis - Chunk 3",
              "type": "chunk",
              "children": [],
              "content": "Addressing      this      bias      in      the      pre-writing      stage      repre-  \n \n  sents   \n \na      valuable      direction      for      future      research. An-  \n \n  other      major      issue      is      the      red      herring      fallacy      or      the  \n \n  over-association      of      unrelated      facts. Addressing      this  \n \n  challenge      calls      for      high-level      sensemaking      rather  \n \n  than      mere      fact-level      verification."
            },
            {
              "id": 939,
              "title": "E_      Error      Analysis - Chunk 4",
              "type": "chunk",
              "children": [],
              "content": "Interest      Level  \n \n  Not      engaging      at      all;      no      attempt      to      capture      the      reader’s      attention. Slightly      engaging      with      rare      moments      that      capture      attention. Fairly      engaging      with   \n \na      basic      narrative      but      lacking      depth. Moderately      engaging      with      several      interesting      points."
            },
            {
              "id": 940,
              "title": "E_      Error      Analysis - Chunk 5",
              "type": "chunk",
              "children": [],
              "content": "Quite      engaging      with   \n \na      well-structured      narrative      and      noteworthy      points      that      frequently      capture      and      retain      attention. Very      engaging      with   \n \na      compelling      narrative      that      captures      and      mostly      retains      attention. Exceptionally      engaging      throughout,      with   \n \na      compelling      narrative      that      consistently      stimulates      interest."
            },
            {
              "id": 941,
              "title": "E_      Error      Analysis - Chunk 6",
              "type": "chunk",
              "children": [],
              "content": "MOawWPYWNr\n \n \n  Coherence      and      Organization  \n \n  Disorganized;      lacks      logical      structure      and      coherence. Poor      organization;      some      structure      is      evident      but      very      weak. Fairly      organized;   \n \na      basic      structure      is      present      but      not      consistently      followed. Organized;   \n \na      clear      structure      is      mostly      followed      with      some      lapses      in      coherence."
            },
            {
              "id": 942,
              "title": "E_      Error      Analysis - Chunk 7",
              "type": "chunk",
              "children": [],
              "content": "Good      organization;   \n \na      clear      structure      with      minor      lapses      in      coherence. Very      well-organized;   \n \na      logical      structure      with      transitions      that      effectively      guide      the      reader. Excellently      organized;      the      article      is      logically      structured      with      seamless      transitions      and   \n \na      clear      argument."
            },
            {
              "id": 943,
              "title": "E_      Error      Analysis - Chunk 8",
              "type": "chunk",
              "children": [],
              "content": "aw:\n \n \n  Relevance      and      Focus  \n \n  1:      Off-topic;      the      content      does      not      align      with      the      headline      or      core      subject. 2:      Mostly      off-topic      with      some      relevant      points. 3:      Somewhat      on      topic      but      with      several      digressions;      the      core      subject      is      evident      but      not      consistently      adhered      to."
            },
            {
              "id": 944,
              "title": "E_      Error      Analysis - Chunk 9",
              "type": "chunk",
              "children": [],
              "content": "4:      Generally      on      topic,      despite   \n \na      few      unrelated      details. 5:      Mostly      on      topic      and      focused;      the      narrative      has   \n \na      consistent      relevance      to      the      core      subject      with      infrequent      digressions. 6:      Highly      relevant      with   \n \na      focused      narrative      and      purpose."
            },
            {
              "id": 945,
              "title": "E_      Error      Analysis - Chunk 10",
              "type": "chunk",
              "children": [],
              "content": "7:      Exceptionally      focused      and      entirely      on      topic;      the      article      is      tightly      centered      on      the      subject,      with      every      piece      of      information      contributing      to   \n \na       comprehensive      understanding      of      the      topic."
            },
            {
              "id": 946,
              "title": "E_      Error      Analysis - Chunk 11",
              "type": "chunk",
              "children": [],
              "content": "Broad      Coverage  \n \n  Severely      lacking;      offers      little      to      no      coverage      of      the      topic’s      primary      aspects,      resulting      in   \n \na      very      narrow      perspective. Minimal      coverage;      addresses      only   \n \na      small      selection      of      the      topic’s      main      aspects,      with      significant      omissions."
            },
            {
              "id": 947,
              "title": "E_      Error      Analysis - Chunk 12",
              "type": "chunk",
              "children": [],
              "content": "Partial      coverage;      includes      some      of      the      topic’s      main      aspects      but      misses      others,      resulting      in      an      incomplete      portrayal. Acceptable      breadth;      covers      most      main      aspects,      though      it      may      stray      into      minor      unnecessary      details      or      overlook      some      relevant      points."
            },
            {
              "id": 948,
              "title": "E_      Error      Analysis - Chunk 13",
              "type": "chunk",
              "children": [],
              "content": "Good      coverage;      achieves      broad      coverage      of      the      topic,      hitting      on      all      major      points      with      minimal      extraneous      information. Comprehensive;      provides      thorough      coverage      of      all      significant      aspects      of      the      topic,      with   \n \na      well-balanced      focus."
            },
            {
              "id": 949,
              "title": "E_      Error      Analysis - Chunk 14",
              "type": "chunk",
              "children": [],
              "content": "Exemplary      in      breadth;      delivers      outstanding      coverage,      thoroughly      detailing      all      crucial      aspects      of      the      topic      without      including      irrelevant      information."
            }
          ],
          "content": ""
        },
        {
          "id": 950,
          "title": "TMAWP      YN",
          "type": "section",
          "children": [
            {
              "id": 951,
              "title": "TMAWP      YN - Chunk 1",
              "type": "chunk",
              "children": [],
              "content": "TMAWP      YN\nVerifiability\n \n \n  1:      No      supporting      evidence;      claims      are      unsubstantiated. 2:      Rarely      supported      with      evidence;      many      claims      are      unsubstantiated. 3:      Inconsistently      verified;      some      claims      are      supported;      evidence      is      occasionally      provided."
            },
            {
              "id": 952,
              "title": "TMAWP      YN - Chunk 2",
              "type": "chunk",
              "children": [],
              "content": "4:      Generally      verified;      claims      are      usually      supported      with      evidence;      however,      there      might      be   \n \na      few      instances      where      verification      is      lacking  \n \n  5:      Well-supported;      claims      are      very      well      supported      with      credible      evidence,      and      instances      of      unsupported      claims      are      rare."
            },
            {
              "id": 953,
              "title": "TMAWP      YN - Chunk 3",
              "type": "chunk",
              "children": [],
              "content": "6:      Very      well-supported;      almost      every      claim      is      substantiated      with      credible      evidence,      showing   \n \na      high      level      of      thorough      verification. 7:      Exemplary      verification;      each      claim      is      supported      by      robust,      credible      evidence      from      authoritative      sources,      reflecting      strict      adherence      to      the      no  \n \n  original      research      policy."
            },
            {
              "id": 954,
              "title": "TMAWP      YN - Chunk 4",
              "type": "chunk",
              "children": [],
              "content": "Table      10:      Scoring      rubrics      on   \n \na      1-7      scale      for      human      evaluation. Issue       Mentioned      Time       Example      Comments\n \n \n  The      word      “significant”      is      used      17      times      in      this      article. Vague      and      unsupported      claims      are  \n \n  made      about      broader      political      importance      and      “pivotal      role[s]”,      and      is      unencyclopedic."
            },
            {
              "id": 955,
              "title": "TMAWP      YN - Chunk 5",
              "type": "chunk",
              "children": [],
              "content": "Use      of      emotional      words,  \n \n  (comment      on      article      Lahaina,      Hawaii) [] but they still have not fixed the issue of neutral point of view. It is also evident in this  \n \n  article      that      the      writer’s      standpoint      is      biased      towards      Taylor      Swift. Other      than      that,      it      did  \n \n  a      good      job      at      summarizing      key      points      and      putting      depth      into      this."
            },
            {
              "id": 956,
              "title": "TMAWP      YN - Chunk 6",
              "type": "chunk",
              "children": [],
              "content": "unneutral      12      (comment      on      article      Speak      Now      (Taylor’s      Version))  \n \n  “The      film      was      also      featured      in      an      art      and      film      festival      hosted      by      The      California      Endowment,  \n \n  highlighting      the      power      of      stories      in      reshaping      narratives      about      communities.”\nYes,      technically  \n \n  the      source      says      that,      but      it’s   \n \na      stretch      to      say      in      Wikipedia      voice      and      just      sounds      like  \n \n  non-neutral,      promotional      prose."
            },
            {
              "id": 957,
              "title": "TMAWP      YN - Chunk 7",
              "type": "chunk",
              "children": [],
              "content": "(comment      on      article      Gehraiyaan)  \n \n  Polling      from      America      shouldn’t      be      included      and      links      to      climate      change      shouldn’t      be  \n \n  made      unless      explicitly      connected      by      the      source."
            },
            {
              "id": 958,
              "title": "TMAWP      YN - Chunk 8",
              "type": "chunk",
              "children": [],
              "content": "(comment      on      article      Typhoon      Hinnamnor)  \n \n  Red      herring      fallacy,   \n \nu      Sourcing      seems      mostly      fine,      though      some      aren’t      directly      related      (Ex. 39,40)."
            },
            {
              "id": 959,
              "title": "TMAWP      YN - Chunk 9",
              "type": "chunk",
              "children": [],
              "content": "associating      unrelated      sources      (comment      on      article      Gehraiyaan)  \n \n  Here      is   \n \na      lengthy      digression      about      KISS,      not      necessary      because      the      article      on      the      band  \n \n  should      be      linked      to."
            },
            {
              "id": 960,
              "title": "TMAWP      YN - Chunk 10",
              "type": "chunk",
              "children": [],
              "content": "(comment      on      article      2022      AFL      Grand      Final)  \n \n  “One      study,      conducted      by      Sinéad      Griffin,   \n \na      physicist      at      the      Lawrence      Berkeley      National  \n \n  Laboratory,      provided      some      analysis      of      LK-99’s      abilities      using      supercomputer      simulations[20].”\n \n \n  This      is      not      enough      information      about      the      analysis,      which      would      have      been      very      useful      in      the  \n \n  rr  \n \n."
            },
            {
              "id": 961,
              "title": "TMAWP      YN - Chunk 11",
              "type": "chunk",
              "children": [],
              "content": ". article. (comment      on      article      LK-99)  \n \n  Missing      important      information   \n \n6       Although      the      earthquake’s      immediate      aftermath      and      response      are      adequately      covered,      there  \n \n  could      be      more      about      the      long-term      socioeconomic      impact      and      recovery      processes."
            },
            {
              "id": 962,
              "title": "TMAWP      YN - Chunk 12",
              "type": "chunk",
              "children": [],
              "content": "(comment      on      article      2022      West      Java      earthquake)  \n \n  Words      like      “now”      should      be      avoided      in      Wikipedia      articles      to      prevent      them      from      becoming  \n \n  dated      and      phrases      such      as,      “as      of      December      2023”      should      be      used      instead."
            },
            {
              "id": 963,
              "title": "TMAWP      YN - Chunk 13",
              "type": "chunk",
              "children": [],
              "content": "Improper      handling      of   \n \n5      (comment      on      article      Cyclone      Batsirai)  \n \n  time-sensitive      information      “as      of      December      13”      doesn’t      specify   \n \na      year,      and      is      old      information  \n \n  (comment      on      article      2022      West      Java      earthquake) too      many      subsections      in      the      “Recovery      and      Rehabilitation”      section  \n \n  (comment      on      article      2022      West      Java      earthquake)\n      (comment      on      article      2022      Crimean      Bridge      explosion)\n \n \n  Section      organization      problem   \n \n5   \n \nI      do      not      like      how      the      article      is      organized,      with      too      many      headers      cluttering      the      article,  \n \n  making      it      not      as      readable."
            },
            {
              "id": 964,
              "title": "TMAWP      YN - Chunk 14",
              "type": "chunk",
              "children": [],
              "content": "Other      than      that,      the      AI      did      great      work      on      the      piece."
            },
            {
              "id": 965,
              "title": "TMAWP      YN - Chunk 15",
              "type": "chunk",
              "children": [],
              "content": "|       Select      a      key       8      v       Selected      Key:      8       Title:      Taylor      Hawkins      -      marshall.com       Snippets:       In      1995,      off      the      back      of      her      hit      album       ‘Jagged      Little      Pill’,      Canadian      American       superstar      Alanis      Morissette      recruited      him       to      be      her      touring      drummer      for      her      18       month      album      tour,      along      with      him       featuring      in      the      music      videos      for      “You       Oughta      Know’,      “All|      Really      Want”      and       “You      Learn’."
            },
            {
              "id": 966,
              "title": "TMAWP      YN - Chunk 16",
              "type": "chunk",
              "children": [],
              "content": "It      was      during      this      tour      that      he       met      his      musical      soul      mate,      Dave      Grohl. The      tour      with      Alanis      ended      and      he\\\\\\'d      heard       that      Dave      and      Foo      Fighters      were      looking       for      anew      drummer,      so      Taylor      enquired."
            },
            {
              "id": 967,
              "title": "TMAWP      YN - Chunk 17",
              "type": "chunk",
              "children": [],
              "content": "Dave      initially      thought      that      Taylor      wouldn’t       be      interested      in      joining      as      Alanis       Morissette      was      much      bigger      than      the      Foo       Fighters      at      that      time,      but      Taylor      jumped      at       the      chance      due      to      his      overwhelming       desire      to      be      in      a      rock      band."
            },
            {
              "id": 968,
              "title": "TMAWP      YN - Chunk 18",
              "type": "chunk",
              "children": [],
              "content": "It      was      from       that      point      that      the      bromance      between       them      began      and      on      March      18th      1997       Taylor      was      announced      as      their      new       drummer."
            },
            {
              "id": 969,
              "title": "TMAWP      YN - Chunk 19",
              "type": "chunk",
              "children": [],
              "content": "Url:      https://marshall.com/live-for-       music/drum-icons/taylor-hawkins | Logout  \n \n  Select      an      option:  \n \n  (11,      ‘Taylor      Hawkins      [Quite      Richard}’)   \n \n¥       You      selected:      Taylor      Hawkins      [Quite      Richard]  \n \n  Taylor      Hawkins  \n \n  Oliver      Taylor      Hawkins      (1972-2022)      was      an      American      musician,      best      known      as      the      drummer      for      the      rock      band      Foo      Fighters[1]."
            },
            {
              "id": 970,
              "title": "TMAWP      YN - Chunk 20",
              "type": "chunk",
              "children": [],
              "content": "Born      in      Fort      Walton,      Texas,      Hawkins’      love      for      music      was      ignited      at   \n \na      young  \n \n  age,      particularly      after      watching   \n \na      Queen      concert      in      1982[2][3][5]."
            },
            {
              "id": 971,
              "title": "TMAWP      YN - Chunk 21",
              "type": "chunk",
              "children": [],
              "content": "He      kick-started      his      professional      career      as      the      drummer      for      Alanis      Morissette’s      band      during      her      world      tour      for      the      hit      album      ‘Jagged      Little  \n \n  Pill\\\\\\'[8][9]."
            },
            {
              "id": 972,
              "title": "TMAWP      YN - Chunk 22",
              "type": "chunk",
              "children": [],
              "content": "His      talents      were      recognized      by      Dave      Grohl,      who      invited      him      to      join      the      Foo      Fighters      in      1997,      marking      the      beginning      of      his      impactful      tenure      with      the      band{7][8]."
            },
            {
              "id": 973,
              "title": "TMAWP      YN - Chunk 23",
              "type": "chunk",
              "children": [],
              "content": "Hawkins      was      celebrated      for      his      versatile      drumming      style,      drawing      influence      from      renowned      drummers      like      Roger      Taylor,      Neil      Peart,      Phil      Collins,      Alex      Van      Halen,      and      Stewart      Copeland[14)."
            },
            {
              "id": 974,
              "title": "TMAWP      YN - Chunk 24",
              "type": "chunk",
              "children": [],
              "content": "His  \n \n  performances,      marked      by   \n \na      unique      energy      and      aggressive      style      of      play,      earned      him      recognition      as      one      of      the      top      rock      drummers      of      his      era[15]."
            },
            {
              "id": 975,
              "title": "TMAWP      YN - Chunk 25",
              "type": "chunk",
              "children": [],
              "content": "Apart      from      his      role      in      the      Foo      Fighters,      Hawkins\\\\\\'      passion  \n \n  for      music      saw      him      involved      in      numerous      side      projects      and      collaborations,      cementing      his      place      in      the      world      of      rock      music(10]."
            },
            {
              "id": 976,
              "title": "TMAWP      YN - Chunk 26",
              "type": "chunk",
              "children": [],
              "content": "Outside      of      his      professional      life,      Hawkins      was      known      for      his      resilience      and      dedication      to      his      family. Despite      personal      struggles,      including   \n \na      near-fatal      drug      overdose      in      2001,      Hawkins      remained      committed  \n \n  to      his      musical      career{4][9]."
            },
            {
              "id": 977,
              "title": "TMAWP      YN - Chunk 27",
              "type": "chunk",
              "children": [],
              "content": "His      legacy      continues      to      inspire      musicians      and      fans      alike,      as      his      contributions      to      rock      music,      coupled      with      his   \n \ni       industry(13}. spirit,      made      him      an      icon      in      the      music  \n \n  Hawkins’      sudden      death      in      2022      while      on      tour      in      Bogotd,      Colombia,      sent      shockwaves      through      the      music      world[34]."
            },
            {
              "id": 978,
              "title": "TMAWP      YN - Chunk 28",
              "type": "chunk",
              "children": [],
              "content": "Tributes      poured      in      from      around      the      globe,      reflecting      the      respect      and      admiration  \n \n  Hawkins      had      garnered      during      his      lifetime[21][31]. His      life      and      career      were      honored      at   \n \na      star-studded      tribute      concert      in      Los      Angeles,      attesting      to      the      enduring      impact      of      his      music[22]."
            },
            {
              "id": 979,
              "title": "TMAWP      YN - Chunk 29",
              "type": "chunk",
              "children": [],
              "content": "| Early      Life      and      Background\n |       Oliver      Taylor      Hawkins,      known      as      Taylor      Hawkins,      was      born      and      raised      in      Fort      Walton,      Texas[3]. His      family      moved      to      Laguna      Beach,      California      when      he      was      four      years      old[3]. He      has      two      younger      siblings,       a      brother      named      Jason,      and      a      sister      named      Heather[3]."
            },
            {
              "id": 980,
              "title": "TMAWP      YN - Chunk 30",
              "type": "chunk",
              "children": [],
              "content": "Asa      child,      Hawkins      wa:      influenced      by      his      paternal      Josie      Hawkins,      who      had      grown      up      during      the      Great      Depression      and       lived      in      Jackson,      Mississippi(2]."
            },
            {
              "id": 981,
              "title": "TMAWP      YN - Chunk 31",
              "type": "chunk",
              "children": [],
              "content": "|       During      his      high      school      days      at      Laguna      Beach      High      School,      from      where      he      graduated      in      1990,      he      became      friends      with      Jon      Davison,      who      later      became      the      lead      vocalist      of      the      band      Yes(2][3]."
            },
            {
              "id": 982,
              "title": "TMAWP      YN - Chunk 32",
              "type": "chunk",
              "children": [],
              "content": "His      interest      in       music      was      nurtured      from      an      early      age,      particularly      after      watching      a      Queen      concert      in      1982      which      inspired      him      to      learn      to      play      the      drums(2][5]."
            },
            {
              "id": 983,
              "title": "TMAWP      YN - Chunk 33",
              "type": "chunk",
              "children": [],
              "content": "He      noted      that      music      was      a      constant      presence      in      his      family       Taylor      Hawkins\n\n \n \n  Oliver      Taylor      Hawkins      (1972-2022)      was      an      American      musician,      best      known      as      the      drummer      for      the      rock  \n \n  band      Foo      Fighters[1]."
            },
            {
              "id": 984,
              "title": "TMAWP      YN - Chunk 34",
              "type": "chunk",
              "children": [],
              "content": "Born      in      Fort      Walton,      Texas,      Hawkins’      love      for      music      was      ignited      at   \n \na      young      age,  \n \n  particularly      after      watching   \n \na      Queen      concert      in      1982[2][3][5]."
            },
            {
              "id": 985,
              "title": "TMAWP      YN - Chunk 35",
              "type": "chunk",
              "children": [],
              "content": "He      kick-started      his      professional      career  \n \n  as      the      drummer      for      Alanis      Morissette’s      band      during      her      world      tour      for      the      hit      album      ’Jagged      Little  \n \n  Pill’[8]L[9]."
            },
            {
              "id": 986,
              "title": "TMAWP      YN - Chunk 36",
              "type": "chunk",
              "children": [],
              "content": "His      talents      were      recognized      by      Dave      Grohl,      who      invited      him      to      join      the      Foo      Fighters      in  \n \n  1997,      marking      the      beginning      of      his      impactful      tenure      with      the      band[7][8]."
            },
            {
              "id": 987,
              "title": "TMAWP      YN - Chunk 37",
              "type": "chunk",
              "children": [],
              "content": "Hawkins      was      celebrated      for      his      versatile      drumming      style,      drawing      influence      from      renowned      drummers      like  \n \n  Roger      Taylor,      Neil      Peart,      Phil      Collins,      Alex      Van      Halen,      and      Stewart      Copeland[14]."
            },
            {
              "id": 988,
              "title": "TMAWP      YN - Chunk 38",
              "type": "chunk",
              "children": [],
              "content": "His      performances,  \n \n  marked      by   \n \na      unique      energy      and      aggressive      style      of      play,      earned      him      recognition      as      one      of      the      top      rock  \n \n  drummers      of      his      era[15]."
            },
            {
              "id": 989,
              "title": "TMAWP      YN - Chunk 39",
              "type": "chunk",
              "children": [],
              "content": "Apart      from      his      role      in      the      Foo      Fighters,      Hawkins’      passion      for      music      saw  \n \n  him      involved      in      numerous      side      projects      and      collaborations,      cementing      his      place      in      the      world      of      rock  \n \n  music[10]."
            },
            {
              "id": 990,
              "title": "TMAWP      YN - Chunk 40",
              "type": "chunk",
              "children": [],
              "content": "Outside      of      his      professional      life,      Hawkins      was      known      for      his      resilience      and      dedication      to      his      family. Despite      personal      struggles,      including   \n \na      near-fatal      drug      overdose      in      2001,      Hawkins      remained      committed      to  \n \n  his      musical      career[4][9]."
            },
            {
              "id": 991,
              "title": "TMAWP      YN - Chunk 41",
              "type": "chunk",
              "children": [],
              "content": "His      legacy      continues      to      inspire      musicians      and      fans      alike,      as      his      contributions  \n \n  to      rock      music,      coupled      with      his      indomitable      spirit,      made      him      an      unforgettable      icon      in      the      music  \n \n  industry[13]."
            },
            {
              "id": 992,
              "title": "TMAWP      YN - Chunk 42",
              "type": "chunk",
              "children": [],
              "content": "Hawkins’      sudden      death      in      2022      while      on      tour      in      Bogota,      Colombia,      sent      shockwaves      through      the      music  \n \n  world[34]. Tributes      poured      in      from      around      the      globe,      reflecting      the      respect      and      admiration      Hawkins  \n \n  had      garnered      during      his      lifetime[21][31]."
            },
            {
              "id": 993,
              "title": "TMAWP      YN - Chunk 43",
              "type": "chunk",
              "children": [],
              "content": "His      life      and      career      were      honored      at   \n \na      star-studded      tribute  \n \n  concert      in      Los      Angeles,      attesting      to      the      enduring      impact      of      his      music[22]. #\n \n   Early      Life      and      Background  \n \n  Oliver      Taylor      Hawkins,      known      as      Taylor      Hawkins,      was      born      and      raised      in      Fort      Walton,      Texas[3]."
            },
            {
              "id": 994,
              "title": "TMAWP      YN - Chunk 44",
              "type": "chunk",
              "children": [],
              "content": "His  \n \n  family      moved      to      Laguna      Beach,      California      when      he      was      four      years      old[3]. He      has      two      younger      siblings,  \n \n  a      brother      named      Jason,      and   \n \na      sister      named      Heather[3]."
            },
            {
              "id": 995,
              "title": "TMAWP      YN - Chunk 45",
              "type": "chunk",
              "children": [],
              "content": "As   \n \na      child,      Hawkins      was      particularly      influenced  \n \n  by      his      paternal      grandmother,      Josie      Hawkins,      who      had      grown      up      during      the      Great      Depression      and      lived      in  \n \n  Jackson,      Mississippi[1]."
            },
            {
              "id": 996,
              "title": "TMAWP      YN - Chunk 46",
              "type": "chunk",
              "children": [],
              "content": "During      his      high      school      days      at      Laguna      Beach      High      School,      from      where      he      graduated      in      1990,      he      became  \n \n  friends      with      Jon      Davison,      who      later      became      the      lead      vocalist      of      the      band      Yes[2][3]."
            },
            {
              "id": 997,
              "title": "TMAWP      YN - Chunk 47",
              "type": "chunk",
              "children": [],
              "content": "His      interest      in  \n \n  music      was      nurtured      from      an      early      age,      particularly      after      watching   \n \na      Queen      concert      in      1982      which      inspired  \n \n  him      to      learn      to      play      the      drums[2][5]. He      noted      that      music      was   \n \na      constant      presence      in      his      family      home[5]."
            },
            {
              "id": 998,
              "title": "TMAWP      YN - Chunk 48",
              "type": "chunk",
              "children": [],
              "content": "Despite      facing      certain      hardships      during      his      upbringing,      including      his      mother’s      struggles      with      \"demons\",  \n \n  Hawkins      pursued      his      musical      ambitions[4]. He      credits      his      older      sister      Heather      for      taking      care      of      the  \n \n  family      during      difficult      times[4]."
            },
            {
              "id": 999,
              "title": "TMAWP      YN - Chunk 49",
              "type": "chunk",
              "children": [],
              "content": "His      first      major      musical      experience      came      from      playing      drums      for      Alanis      Morissette’s      album,      Jagged      Little  \n \n  Pill,      and      accompanying      her      on      the      subsequent      tour[3]. This      marked      the      beginning      of      his      professional  \n \n  career      in      the      music      industry."
            },
            {
              "id": 1000,
              "title": "TMAWP      YN - Chunk 50",
              "type": "chunk",
              "children": [],
              "content": "#\n \n   Career  \n \n  Taylor      Hawkins      began      his      professional      music      career      playing      in      Alanis      Morissette’s      band      during      her  \n \n  18-month      world      tour      in      support      of      the      hit      album      ’Jagged      Little      Pill’      from      1995      to      1997[8][9]."
            },
            {
              "id": 1001,
              "title": "TMAWP      YN - Chunk 51",
              "type": "chunk",
              "children": [],
              "content": "His  \n \n  performances      not      only      in      the      tour      but      also      in      the      music      videos      for      “You      Oughta      Know”,      “All   \n \nI      Really      Want”  \n \n  and      “You      Learn”      introduced      him      to      the      world      of      rock      music      and      ultimately      led      to      his      meeting      with      Dave  \n \n  Grohl[8]."
            },
            {
              "id": 1002,
              "title": "TMAWP      YN - Chunk 52",
              "type": "chunk",
              "children": [],
              "content": "Throughout      this      time,      Hawkins      contributed      significantly      to      the      band’s      sound      and      performance,  \n \n  transforming      the      songs      from      their      original      drum      loop      format      to   \n \na      rock-band      vibe      that      resonated      with  \n \n  audiences[1][7]."
            },
            {
              "id": 1003,
              "title": "TMAWP      YN - Chunk 53",
              "type": "chunk",
              "children": [],
              "content": "In      1997,      Hawkins      was      asked      by      Grohl      to      join      the      Foo      Fighters,      an      invitation      that      he      readily      accepted[7][8]."
            },
            {
              "id": 1004,
              "title": "TMAWP      YN - Chunk 54",
              "type": "chunk",
              "children": [],
              "content": "At      the      time,      Grohl      thought      it      was   \n \na      long      shot      to      recruit      Hawkins      given      that      Morissette      was      at      the      height  \n \n  of      her      career,      but      Hawkins’      desire      to      be   \n \na      part      of   \n \na      rock      band      compelled      him      to      make      the      move[7]."
            },
            {
              "id": 1005,
              "title": "TMAWP      YN - Chunk 55",
              "type": "chunk",
              "children": [],
              "content": "This  \n \n  marked      the      beginning      of      Hawkins’      tenure      as      the      drummer      of      the      Foo      Fighters,   \n \na      role      that      he      would      play  \n \n  until      his      passing[6][9]. Apart      from      his      work      with      Morissette      and      the      Foo      Fighters,      Hawkins      had      an      array      of      other      musical  \n \n  experiences[10]."
            },
            {
              "id": 1006,
              "title": "TMAWP      YN - Chunk 56",
              "type": "chunk",
              "children": [],
              "content": "He      drummed      for      Sass      Jordan      before      joining      Morissette’s      touring      band[10]. He      was      part  \n \n  of      an      ad      hoc      drum      supergroup      called      SOS      Allstars      and      filled      the      void      for      Coheed      and      Cambria’s      2007  \n \n  album      after      their      drummer      Josh      Eppard      left      the      group[10]."
            },
            {
              "id": 1007,
              "title": "TMAWP      YN - Chunk 57",
              "type": "chunk",
              "children": [],
              "content": "In      addition,      Hawkins      formed      his      own      side  \n \n  project,      the      Coattail      Riders,      in      2005,      through      which      he      recorded      his      own      music      and      took      the      project      on  \n \n  the      road,      performing      in      small      clubs      despite      the      Foo      Fighters’      arena-status[7]."
            },
            {
              "id": 1008,
              "title": "TMAWP      YN - Chunk 58",
              "type": "chunk",
              "children": [],
              "content": "His      son,      Shane      Hawkins,  \n \n  has      since      taken      on      his      father’s      legacy,      joining      the      Foo      Fighters      for   \n \na      performance      during      the      Boston  \n \n  Calling      Music      Festival      in      2023[6]."
            },
            {
              "id": 1009,
              "title": "TMAWP      YN - Chunk 59",
              "type": "chunk",
              "children": [],
              "content": "#\n \n   Musical      Style      and      Influences  \n \n  Taylor      Hawkins      was   \n \na      profound      drummer,      with      his      musical      style      and      influences      spreading      across   \n \na      wide  \n \n  array      of      rock      genres[11]."
            },
            {
              "id": 1010,
              "title": "TMAWP      YN - Chunk 60",
              "type": "chunk",
              "children": [],
              "content": "Known      for      his      passionate      fandom      of      groups      that      came      before      him,      Hawkins  \n \n  regularly      expressed      his      admiration      for      bands      like      Rush,      Genesis,      and      the      Police,      all      of      which      featured  \n \n  some      of      the      greatest      drummers      in      rock      history      like      Neil      Peart,      Phil      Collins,      and      Stewart      Copeland[11]."
            },
            {
              "id": 1011,
              "title": "TMAWP      YN - Chunk 61",
              "type": "chunk",
              "children": [],
              "content": "He      was      heavily      influenced      by      his      love      for      classic      rock,      as      evidenced      by      his      performances,      where      he  \n \n  covered      songs      from      bands      like      Van      Halen[11]."
            },
            {
              "id": 1012,
              "title": "TMAWP      YN - Chunk 62",
              "type": "chunk",
              "children": [],
              "content": "Hawkins      drew      influences      from   \n \na      variety      of      drumming      styles,      developing   \n \na      signature      style      inspired      by  \n \n  greats      like      Roger      Taylor,      Neil      Peart,      Phil      Collins,      Alex      Van      Halen,      and      Stewart      Copeland[14]."
            },
            {
              "id": 1013,
              "title": "TMAWP      YN - Chunk 63",
              "type": "chunk",
              "children": [],
              "content": "This  \n \n  distinctive      style      and      influence      extended      to      his      drum      kit,      which      incorporated      elements      like      rototoms  \n \n  and      concert      toms[14]. Beyond      his      influences,      Hawkins      had   \n \na      unique      energy      that      made      him      stand      out      as   \n \na      drummer."
            },
            {
              "id": 1014,
              "title": "TMAWP      YN - Chunk 64",
              "type": "chunk",
              "children": [],
              "content": "His      performances  \n \n  were      recognized      for      their      power,      and      he      was      known      for      his      enthusiastic      and      aggressive      style      of      play[15]. This      earned      him      recognition      as      one      of      the      top      rock      drummers      of      his      time,      with      his      passion      for      music  \n \n  living      on      through      his      performances[14]."
            },
            {
              "id": 1015,
              "title": "TMAWP      YN - Chunk 65",
              "type": "chunk",
              "children": [],
              "content": "Through      his      career,      Hawkins      left      an      indelible      mark      on      rock      music,      through      his      distinct      style,      passion,  \n \n  and      contributions      to      the      music      industry[13]. His      love      for      music      and      dedication      to      his      craft      made      him       an      unforgettable      icon      in      the      world      of      rock      music[13]."
            },
            {
              "id": 1016,
              "title": "TMAWP      YN - Chunk 66",
              "type": "chunk",
              "children": [],
              "content": "#\n \n   Personal      Life  \n \n  Taylor      Hawkins      married      Alison      Hawkins,      an      American      celebrity      and      entrepreneur,      in      2005[18]. The      couple  \n \n  had      three      children,      Oliver,      Annabelle,      and      Everleigh[19]. Hawkins’      commitment      to      his      family      was      evident;\n \n \n  in      fact,      he      even      wrote   \n \na      song      for      his      middle      child,      Annabelle[9]."
            },
            {
              "id": 1017,
              "title": "TMAWP      YN - Chunk 67",
              "type": "chunk",
              "children": [],
              "content": "In      his      personal      life,      Hawkins      had      also      struggled      with      drug      use,      which      nearly      claimed      his      life      in   \n \na      2001  \n \n  overdose[9][7][4]."
            },
            {
              "id": 1018,
              "title": "TMAWP      YN - Chunk 68",
              "type": "chunk",
              "children": [],
              "content": "However,      he      managed      to      overcome      this      challenge,      and      later      expressed      gratitude      for  \n \n  the      experience      as   \n \na      lesson      that      allowed      him      to      realize      the      destructive      path      he      was      on[7]."
            },
            {
              "id": 1019,
              "title": "TMAWP      YN - Chunk 69",
              "type": "chunk",
              "children": [],
              "content": "Outside      of      his      main      role      in      the      Foo      Fighters,      Hawkins      also      pursued      various      side      projects      including      the  \n \n  Birds      of      Satan,      NHC,      and      Chevy      Metal. His      motivation      for      such      ventures      was   \n \na      constant      drive      to      create  \n \n  and      his      love      for      music[7]."
            },
            {
              "id": 1020,
              "title": "TMAWP      YN - Chunk 70",
              "type": "chunk",
              "children": [],
              "content": "Hawkins      was      also      known      for      his      unabashed      fanboy      nature,      often      vocalizing  \n \n  his      admiration      for      fellow      musicians      and      his      heroes[7]. #\n \n   Legacy      and      Impact Taylor      Hawkins      was      known      for      his      raw      and      authentic      drumming      style,      described      as      \"courageous,      damaged  \n \n  and      unflinchingly      authentic”[20]."
            },
            {
              "id": 1021,
              "title": "TMAWP      YN - Chunk 71",
              "type": "chunk",
              "children": [],
              "content": "His      work      with      the      Foo      Fighters,      as      well      as      his      various      collaborations n’      roll[10]. ‘ and      side      projects,      made      him   \n \na      celebrated      figure      in      rock  \n \n  Hawkins’      death      in      2022      was      met      with      heartfelt      tributes      from      colleagues      and      fans      around      the      world."
            },
            {
              "id": 1022,
              "title": "TMAWP      YN - Chunk 72",
              "type": "chunk",
              "children": [],
              "content": "Notable      tributes      came      from      rock      legends      like      Roger      Taylor      of      Queen,      who      considered      Hawkins      as   \n \na      kind,  \n \n  brilliant      man      and      an      inspirational      mentor,      likening      his      death      to      \"losing   \n \na      younger      favourite      brother”[21]."
            },
            {
              "id": 1023,
              "title": "TMAWP      YN - Chunk 73",
              "type": "chunk",
              "children": [],
              "content": "Similarly,      Led      Zeppelin’s      Jimmy      Page      admired      his      technique,      energy      and      spirited      enthusiasm[21]. An      LA      tribute      concert      held      in      his      honor      included      guest      drummers      like      Lars      Ulrich      of      Metallica,      Travis  \n \n  Barker      of      blink-182,      and      Brad      Wilk      of      Rage      Against      the      Machine."
            },
            {
              "id": 1024,
              "title": "TMAWP      YN - Chunk 74",
              "type": "chunk",
              "children": [],
              "content": "Singers      like      Miley      Cyrus      and      Alanis  \n \n  Morissette      also      performed      at      the      concert[22]. Apart      from      his      music,      Taylor      Hawkins      also      contributed      to      charities      Music      Support      and      MusiCares,      both      of  \n \n  which      were      chosen      by      the      Hawkins      family[23]."
            },
            {
              "id": 1025,
              "title": "TMAWP      YN - Chunk 75",
              "type": "chunk",
              "children": [],
              "content": "He      had      received      numerous      accolades      throughout      his      career,  \n \n  including      27      Grammy      nominations,      of      which      he      won      14[2]. In      2021,      the      Foo      Fighters      were      inducted      into  \n \n  the      Rock      and      Roll      Hall      of      Fame[9]."
            },
            {
              "id": 1026,
              "title": "TMAWP      YN - Chunk 76",
              "type": "chunk",
              "children": [],
              "content": "#\n \n   Discography  \n \n  Taylor      Hawkins      also      led   \n \na      notable      music      career      through      his      own      side      projects      and      collaborations[10]."
            },
            {
              "id": 1027,
              "title": "TMAWP      YN - Chunk 77",
              "type": "chunk",
              "children": [],
              "content": "Aside      from      his      work      with      the      Foo      Fighters,      Hawkins      formed      and      fronted      the      band      Taylor      Hawkins   \n \n&      The  \n \n  Coattail      Riders,   \n \na      project      which      originated      from      jamming      sessions      with      his      friend      Drew      Hester[10]."
            },
            {
              "id": 1028,
              "title": "TMAWP      YN - Chunk 78",
              "type": "chunk",
              "children": [],
              "content": "###      Taylor      Hawkins   \n \n&      The      Coattail      Riders  \n \n  Taylor      Hawkins   \n \n&      The      Coattail      Riders,   \n \na      band      formed      in      2004,      have      released      three      albums      and      their  \n \n  music      spans      genres      including      Hard      Rock,      Art      Rock,      and      Alternative      Rock[24][25][26]."
            },
            {
              "id": 1029,
              "title": "TMAWP      YN - Chunk 79",
              "type": "chunk",
              "children": [],
              "content": "The      band      grew      from  \n \n  an      initial      casual      jamming      session,      gradually      evolving      into   \n \na      more      formal      arrangement      that      led      to      the  \n \n  production      of      record      albums."
            },
            {
              "id": 1030,
              "title": "TMAWP      YN - Chunk 80",
              "type": "chunk",
              "children": [],
              "content": "Notably,      these      albums      featured      guest      appearances      by      renowned      musicians  \n \n  such      as      Dave      Grohl,      Queen’s      Brian      May      and      Roger      Taylor,      The      Cars’      Elliot      Easton,      Perry      Farrell,      and  \n \n  Jon      Davison,      who      is   \n \na      school      friend      of      Hawkins’[10]."
            },
            {
              "id": 1031,
              "title": "TMAWP      YN - Chunk 81",
              "type": "chunk",
              "children": [],
              "content": "###      Red      Light      Fever  \n \n  Red      Light      Fever,      released      on      April      19,      2010,      was      the      band’s      first      album[29][30]."
            },
            {
              "id": 1032,
              "title": "TMAWP      YN - Chunk 82",
              "type": "chunk",
              "children": [],
              "content": "Prior      to      its      release,  \n \n  Hawkins      revealed      in      an      interview      that      the      album      had      completed      the      recording      and      production      stages,      but  \n \n  its      title      and      release      date      were      yet      to      be      determined[29]."
            },
            {
              "id": 1033,
              "title": "TMAWP      YN - Chunk 83",
              "type": "chunk",
              "children": [],
              "content": "Red      Light      Fever      was      recorded      at      the      Foo  \n \n  Fighters’      Studio      606      in      California      and      featured      guest      musicians      such      as      Brian      May      and      Roger      Taylor      of  \n \n  Queen,      Dave      Grohl      of      Foo      Fighters,      and      Elliot      Easton      of      The      Cars[29][30]."
            },
            {
              "id": 1034,
              "title": "TMAWP      YN - Chunk 84",
              "type": "chunk",
              "children": [],
              "content": "##      Get      the      Money  \n \n  Get      the      Money,      the      third      album      from      Taylor      Hawkins   \n \n&      The      Coattail      Riders,      was      released      on      November      8,  \n \n  2019[29]. The      album’s      first      single,      \"Crossed      the      Line”,      released      on      October      15,      2019,      featured      Dave  \n \n  Grohl      and      Jon      Davison,      the      frontman      of      Yes[29]."
            },
            {
              "id": 1035,
              "title": "TMAWP      YN - Chunk 85",
              "type": "chunk",
              "children": [],
              "content": "The      music      video      for      the      single      \"I      Really      Blew      It”      also  \n \n  featured      appearances      from      Grohl      and      Perry      Farrel1[29]. #\n \n   Collaborations      and      Guest      Appearances  \n \n  Throughout      his      career,      Taylor      Hawkins      collaborated      with      various      prominent      artists      and      bands."
            },
            {
              "id": 1036,
              "title": "TMAWP      YN - Chunk 86",
              "type": "chunk",
              "children": [],
              "content": "The  \n \n  Coattail      Riders’      albums      notably      featured      appearances      from      luminaries      such      as      Brian      May      and      Roger      Taylor  \n \n  of      Queen,      Chrissie      Hynde,      Nancy      Wilson      of      Heart,      Sex      Pistol      Steve      Jones      and      James      Gang’s      Joe      Walsh[28]."
            },
            {
              "id": 1037,
              "title": "TMAWP      YN - Chunk 87",
              "type": "chunk",
              "children": [],
              "content": "Hawkins      also      fronted      another      group,      The      Birds      of      Satan,      which      evolved      from      his      heavy      rock      covers      band,  \n \n  Chevy      Metal[28]. Despite      his      diverse      musical      engagements,      Hawkins      always      maintained   \n \na      close      allegiance      with      the      Foo  \n \n  Fighters,      which      remained      the      center      of      his      music      life[7][28]."
            },
            {
              "id": 1038,
              "title": "TMAWP      YN - Chunk 88",
              "type": "chunk",
              "children": [],
              "content": "#\n \n   Tragic      Passing  \n \n  Taylor      Hawkins,      the      esteemed      drummer      of      the      alt-rock      band      Foo      Fighters,      passed      away      suddenly      on      March  \n \n  25,      2022,      while      on      tour      with      his      band      in      Bogota,      Colombia[34]."
            },
            {
              "id": 1039,
              "title": "TMAWP      YN - Chunk 89",
              "type": "chunk",
              "children": [],
              "content": "The      official      cause      of      death      was      cardiac  \n \n  arrest,      though      inquiries      were      raised      concerning      the      presence      of      drugs      in      his      system      and      their      potential  \n \n  contribution      to      his      death[33][34]."
            },
            {
              "id": 1040,
              "title": "TMAWP      YN - Chunk 90",
              "type": "chunk",
              "children": [],
              "content": "On      the      night      of      his      passing,      paramedics      were      called      to      the      Four  \n \n  Seasons      hotel      in      Bogota      due      to      reports      of      chest      pain      from      an      unnamed      guest,      later      revealed      to      be  \n \n  Hawkins[34]. Unfortunately,      resuscitation      efforts      were      unsuccessful,      and      Hawkins      was      declared      dead      at  \n \n  the      scene[34]."
            },
            {
              "id": 1041,
              "title": "TMAWP      YN - Chunk 91",
              "type": "chunk",
              "children": [],
              "content": "The      news      of      Hawkins’      sudden      demise      was      announced      on      the      morning      of      March      25th,      2022,      which      left      the      music  \n \n  world      in      shock[32]."
            },
            {
              "id": 1042,
              "title": "TMAWP      YN - Chunk 92",
              "type": "chunk",
              "children": [],
              "content": "The      band      confirmed      the      news      with   \n \na      short      statement,      expressing      their      devastation  \n \n  at      the      loss      of      Hawkins,      whose      \"musical      spirit      and      infectious      laughter”      would      live      on      forever[32]. As   \n \na      result      of      Hawkins’      untimely      passing,      the      band      canceled      their      ongoing      South      American      tour[33]."
            },
            {
              "id": 1043,
              "title": "TMAWP      YN - Chunk 93",
              "type": "chunk",
              "children": [],
              "content": "The  \n \n  festival      stage      at      the      Estéreo      Picnic      Festival,      where      the      Foo      Fighters      were      scheduled      to      perform      that  \n \n  night,      was      transformed      into   \n \na      candlelight      vigil      in      memory      of      Hawkins[33]."
            },
            {
              "id": 1044,
              "title": "TMAWP      YN - Chunk 94",
              "type": "chunk",
              "children": [],
              "content": "##      Tributes      and      Remembrances  \n \n  In      the      wake      of      Hawkins’      death,      tributes      from      fans      and      colleagues      alike      poured      in      from      around      the  \n \n  world[21][31]."
            },
            {
              "id": 1045,
              "title": "TMAWP      YN - Chunk 95",
              "type": "chunk",
              "children": [],
              "content": "Among      the      many      paying      their      respects      were      legendary      rock      and      roll      musicians      like      Roger  \n \n  Taylor,      the      drummer      of      Queen,      who      Hawkins      credited      with      inspiring      his      own      career      behind      the      drum      set[21]."
            },
            {
              "id": 1046,
              "title": "TMAWP      YN - Chunk 96",
              "type": "chunk",
              "children": [],
              "content": "In      heartfelt      social      media      posts,      Taylor      described      Hawkins      as      an      \"inspirational      mentor”      and   \n \na      \"kind  \n \n  brilliant      man\"[21],      while      Led      Zeppelin’s      Jimmy      Page      reminisced      about      sharing      the      stage      with      Hawkins  \n \n  and      praised      his      \"technique,      energy      and      spirited      enthusiasm\"[21]."
            },
            {
              "id": 1047,
              "title": "TMAWP      YN - Chunk 97",
              "type": "chunk",
              "children": [],
              "content": "There      were      also      numerous      onstage      tributes      to      Hawkins. Notably,      Miley      Cyrus      expressed      her      grief      and      sent  \n \n  peaceful      wishes      to      the      Foo      Fighters      and      the      Hawkins      family      during   \n \na      performance      at      Lollapalooza[31]."
            },
            {
              "id": 1048,
              "title": "TMAWP      YN - Chunk 98",
              "type": "chunk",
              "children": [],
              "content": "Similarly,      Liam      Gallagher      of      Oasis      dedicated      one      of      the      band’s      biggest      hits      to      Hawkins      during   \n \na      concert  \n \n  at      the      Royal      Albert      Hall      in      London[31]. Fans      gathered      outside      the      hotel      where      Hawkins      died,      lighting      candles,      leaving      flowers,      and      singing      the  \n \n  band’s      songs      in      his      honor[31]."
            },
            {
              "id": 1049,
              "title": "TMAWP      YN - Chunk 99",
              "type": "chunk",
              "children": [],
              "content": "Hawkins’      life      and      career      were      celebrated      in   \n \na      star-studded      tribute      concert      in      Los      Angeles,      which      saw  \n \n  performances      from      over      5@      musicians,      including      his      former      bands      and      colleagues      from      Def      Leppard,      Queen,  \n \n  and      Foo      Fighters[22]. Table      12:      STORM’s      generated      article      for      “Taylor      Hawkins”."
            },
            {
              "id": 1050,
              "title": "TMAWP      YN - Chunk 100",
              "type": "chunk",
              "children": [],
              "content": "“#’,      “##”      indicate      the      section      title      and      subsection      title  \n \n  respectively. Numbers      in      brackets      indicate      the      cited      references."
            }
          ],
          "content": ""
        },
        {
          "id": 1051,
          "title": "Verifiability",
          "type": "section",
          "children": [
            {
              "id": 1052,
              "title": "Verifiability - Chunk 1",
              "type": "chunk",
              "children": [],
              "content": "Verifiability\n \n \n  1:      No      supporting      evidence;      claims      are      unsubstantiated. 2:      Rarely      supported      with      evidence;      many      claims      are      unsubstantiated. 3:      Inconsistently      verified;      some      claims      are      supported;      evidence      is      occasionally      provided."
            },
            {
              "id": 1053,
              "title": "Verifiability - Chunk 2",
              "type": "chunk",
              "children": [],
              "content": "4:      Generally      verified;      claims      are      usually      supported      with      evidence;      however,      there      might      be   \n \na      few      instances      where      verification      is      lacking  \n \n  5:      Well-supported;      claims      are      very      well      supported      with      credible      evidence,      and      instances      of      unsupported      claims      are      rare."
            },
            {
              "id": 1054,
              "title": "Verifiability - Chunk 3",
              "type": "chunk",
              "children": [],
              "content": "6:      Very      well-supported;      almost      every      claim      is      substantiated      with      credible      evidence,      showing   \n \na      high      level      of      thorough      verification. 7:      Exemplary      verification;      each      claim      is      supported      by      robust,      credible      evidence      from      authoritative      sources,      reflecting      strict      adherence      to      the      no  \n \n  original      research      policy."
            },
            {
              "id": 1055,
              "title": "Verifiability - Chunk 4",
              "type": "chunk",
              "children": [],
              "content": "Table      10:      Scoring      rubrics      on   \n \na      1-7      scale      for      human      evaluation."
            }
          ],
          "content": ""
        },
        {
          "id": 1056,
          "title": "Issue       Mentioned      Time       Example      Comments",
          "type": "section",
          "children": [
            {
              "id": 1057,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 1",
              "type": "chunk",
              "children": [],
              "content": "Issue       Mentioned      Time       Example      Comments\n \n \n  The      word      “significant”      is      used      17      times      in      this      article. Vague      and      unsupported      claims      are  \n \n  made      about      broader      political      importance      and      “pivotal      role[s]”,      and      is      unencyclopedic."
            },
            {
              "id": 1058,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 2",
              "type": "chunk",
              "children": [],
              "content": "Use      of      emotional      words,  \n \n  (comment      on      article      Lahaina,      Hawaii) [] but they still have not fixed the issue of neutral point of view. It is also evident in this  \n \n  article      that      the      writer’s      standpoint      is      biased      towards      Taylor      Swift. Other      than      that,      it      did  \n \n  a      good      job      at      summarizing      key      points      and      putting      depth      into      this."
            },
            {
              "id": 1059,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 3",
              "type": "chunk",
              "children": [],
              "content": "unneutral      12      (comment      on      article      Speak      Now      (Taylor’s      Version))  \n \n  “The      film      was      also      featured      in      an      art      and      film      festival      hosted      by      The      California      Endowment,  \n \n  highlighting      the      power      of      stories      in      reshaping      narratives      about      communities.”\nYes,      technically  \n \n  the      source      says      that,      but      it’s   \n \na      stretch      to      say      in      Wikipedia      voice      and      just      sounds      like  \n \n  non-neutral,      promotional      prose."
            },
            {
              "id": 1060,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 4",
              "type": "chunk",
              "children": [],
              "content": "(comment      on      article      Gehraiyaan)  \n \n  Polling      from      America      shouldn’t      be      included      and      links      to      climate      change      shouldn’t      be  \n \n  made      unless      explicitly      connected      by      the      source."
            },
            {
              "id": 1061,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 5",
              "type": "chunk",
              "children": [],
              "content": "(comment      on      article      Typhoon      Hinnamnor)  \n \n  Red      herring      fallacy,   \n \nu      Sourcing      seems      mostly      fine,      though      some      aren’t      directly      related      (Ex. 39,40)."
            },
            {
              "id": 1062,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 6",
              "type": "chunk",
              "children": [],
              "content": "associating      unrelated      sources      (comment      on      article      Gehraiyaan)  \n \n  Here      is   \n \na      lengthy      digression      about      KISS,      not      necessary      because      the      article      on      the      band  \n \n  should      be      linked      to."
            },
            {
              "id": 1063,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 7",
              "type": "chunk",
              "children": [],
              "content": "(comment      on      article      2022      AFL      Grand      Final)  \n \n  “One      study,      conducted      by      Sinéad      Griffin,   \n \na      physicist      at      the      Lawrence      Berkeley      National  \n \n  Laboratory,      provided      some      analysis      of      LK-99’s      abilities      using      supercomputer      simulations[20].”\n \n \n  This      is      not      enough      information      about      the      analysis,      which      would      have      been      very      useful      in      the  \n \n  rr  \n \n."
            },
            {
              "id": 1064,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 8",
              "type": "chunk",
              "children": [],
              "content": ". article. (comment      on      article      LK-99)  \n \n  Missing      important      information   \n \n6       Although      the      earthquake’s      immediate      aftermath      and      response      are      adequately      covered,      there  \n \n  could      be      more      about      the      long-term      socioeconomic      impact      and      recovery      processes."
            },
            {
              "id": 1065,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 9",
              "type": "chunk",
              "children": [],
              "content": "(comment      on      article      2022      West      Java      earthquake)  \n \n  Words      like      “now”      should      be      avoided      in      Wikipedia      articles      to      prevent      them      from      becoming  \n \n  dated      and      phrases      such      as,      “as      of      December      2023”      should      be      used      instead."
            },
            {
              "id": 1066,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 10",
              "type": "chunk",
              "children": [],
              "content": "Improper      handling      of   \n \n5      (comment      on      article      Cyclone      Batsirai)  \n \n  time-sensitive      information      “as      of      December      13”      doesn’t      specify   \n \na      year,      and      is      old      information  \n \n  (comment      on      article      2022      West      Java      earthquake) too      many      subsections      in      the      “Recovery      and      Rehabilitation”      section  \n \n  (comment      on      article      2022      West      Java      earthquake)\n      (comment      on      article      2022      Crimean      Bridge      explosion)\n \n \n  Section      organization      problem   \n \n5   \n \nI      do      not      like      how      the      article      is      organized,      with      too      many      headers      cluttering      the      article,  \n \n  making      it      not      as      readable."
            },
            {
              "id": 1067,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 11",
              "type": "chunk",
              "children": [],
              "content": "Other      than      that,      the      AI      did      great      work      on      the      piece."
            },
            {
              "id": 1068,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 12",
              "type": "chunk",
              "children": [],
              "content": "|       Select      a      key       8      v       Selected      Key:      8       Title:      Taylor      Hawkins      -      marshall.com       Snippets:       In      1995,      off      the      back      of      her      hit      album       ‘Jagged      Little      Pill’,      Canadian      American       superstar      Alanis      Morissette      recruited      him       to      be      her      touring      drummer      for      her      18       month      album      tour,      along      with      him       featuring      in      the      music      videos      for      “You       Oughta      Know’,      “All|      Really      Want”      and       “You      Learn’."
            },
            {
              "id": 1069,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 13",
              "type": "chunk",
              "children": [],
              "content": "It      was      during      this      tour      that      he       met      his      musical      soul      mate,      Dave      Grohl. The      tour      with      Alanis      ended      and      he\\\\\\'d      heard       that      Dave      and      Foo      Fighters      were      looking       for      anew      drummer,      so      Taylor      enquired."
            },
            {
              "id": 1070,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 14",
              "type": "chunk",
              "children": [],
              "content": "Dave      initially      thought      that      Taylor      wouldn’t       be      interested      in      joining      as      Alanis       Morissette      was      much      bigger      than      the      Foo       Fighters      at      that      time,      but      Taylor      jumped      at       the      chance      due      to      his      overwhelming       desire      to      be      in      a      rock      band."
            },
            {
              "id": 1071,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 15",
              "type": "chunk",
              "children": [],
              "content": "It      was      from       that      point      that      the      bromance      between       them      began      and      on      March      18th      1997       Taylor      was      announced      as      their      new       drummer."
            },
            {
              "id": 1072,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 16",
              "type": "chunk",
              "children": [],
              "content": "Url:      https://marshall.com/live-for-       music/drum-icons/taylor-hawkins | Logout  \n \n  Select      an      option:  \n \n  (11,      ‘Taylor      Hawkins      [Quite      Richard}’)   \n \n¥       You      selected:      Taylor      Hawkins      [Quite      Richard]  \n \n  Taylor      Hawkins  \n \n  Oliver      Taylor      Hawkins      (1972-2022)      was      an      American      musician,      best      known      as      the      drummer      for      the      rock      band      Foo      Fighters[1]."
            },
            {
              "id": 1073,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 17",
              "type": "chunk",
              "children": [],
              "content": "Born      in      Fort      Walton,      Texas,      Hawkins’      love      for      music      was      ignited      at   \n \na      young  \n \n  age,      particularly      after      watching   \n \na      Queen      concert      in      1982[2][3][5]."
            },
            {
              "id": 1074,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 18",
              "type": "chunk",
              "children": [],
              "content": "He      kick-started      his      professional      career      as      the      drummer      for      Alanis      Morissette’s      band      during      her      world      tour      for      the      hit      album      ‘Jagged      Little  \n \n  Pill\\\\\\'[8][9]."
            },
            {
              "id": 1075,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 19",
              "type": "chunk",
              "children": [],
              "content": "His      talents      were      recognized      by      Dave      Grohl,      who      invited      him      to      join      the      Foo      Fighters      in      1997,      marking      the      beginning      of      his      impactful      tenure      with      the      band{7][8]."
            },
            {
              "id": 1076,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 20",
              "type": "chunk",
              "children": [],
              "content": "Hawkins      was      celebrated      for      his      versatile      drumming      style,      drawing      influence      from      renowned      drummers      like      Roger      Taylor,      Neil      Peart,      Phil      Collins,      Alex      Van      Halen,      and      Stewart      Copeland[14)."
            },
            {
              "id": 1077,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 21",
              "type": "chunk",
              "children": [],
              "content": "His  \n \n  performances,      marked      by   \n \na      unique      energy      and      aggressive      style      of      play,      earned      him      recognition      as      one      of      the      top      rock      drummers      of      his      era[15]."
            },
            {
              "id": 1078,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 22",
              "type": "chunk",
              "children": [],
              "content": "Apart      from      his      role      in      the      Foo      Fighters,      Hawkins\\\\\\'      passion  \n \n  for      music      saw      him      involved      in      numerous      side      projects      and      collaborations,      cementing      his      place      in      the      world      of      rock      music(10]."
            },
            {
              "id": 1079,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 23",
              "type": "chunk",
              "children": [],
              "content": "Outside      of      his      professional      life,      Hawkins      was      known      for      his      resilience      and      dedication      to      his      family. Despite      personal      struggles,      including   \n \na      near-fatal      drug      overdose      in      2001,      Hawkins      remained      committed  \n \n  to      his      musical      career{4][9]."
            },
            {
              "id": 1080,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 24",
              "type": "chunk",
              "children": [],
              "content": "His      legacy      continues      to      inspire      musicians      and      fans      alike,      as      his      contributions      to      rock      music,      coupled      with      his   \n \ni       industry(13}. spirit,      made      him      an      icon      in      the      music  \n \n  Hawkins’      sudden      death      in      2022      while      on      tour      in      Bogotd,      Colombia,      sent      shockwaves      through      the      music      world[34]."
            },
            {
              "id": 1081,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 25",
              "type": "chunk",
              "children": [],
              "content": "Tributes      poured      in      from      around      the      globe,      reflecting      the      respect      and      admiration  \n \n  Hawkins      had      garnered      during      his      lifetime[21][31]. His      life      and      career      were      honored      at   \n \na      star-studded      tribute      concert      in      Los      Angeles,      attesting      to      the      enduring      impact      of      his      music[22]."
            },
            {
              "id": 1082,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 26",
              "type": "chunk",
              "children": [],
              "content": "| Early      Life      and      Background\n |       Oliver      Taylor      Hawkins,      known      as      Taylor      Hawkins,      was      born      and      raised      in      Fort      Walton,      Texas[3]. His      family      moved      to      Laguna      Beach,      California      when      he      was      four      years      old[3]. He      has      two      younger      siblings,       a      brother      named      Jason,      and      a      sister      named      Heather[3]."
            },
            {
              "id": 1083,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 27",
              "type": "chunk",
              "children": [],
              "content": "Asa      child,      Hawkins      wa:      influenced      by      his      paternal      Josie      Hawkins,      who      had      grown      up      during      the      Great      Depression      and       lived      in      Jackson,      Mississippi(2]."
            },
            {
              "id": 1084,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 28",
              "type": "chunk",
              "children": [],
              "content": "|       During      his      high      school      days      at      Laguna      Beach      High      School,      from      where      he      graduated      in      1990,      he      became      friends      with      Jon      Davison,      who      later      became      the      lead      vocalist      of      the      band      Yes(2][3]."
            },
            {
              "id": 1085,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 29",
              "type": "chunk",
              "children": [],
              "content": "His      interest      in       music      was      nurtured      from      an      early      age,      particularly      after      watching      a      Queen      concert      in      1982      which      inspired      him      to      learn      to      play      the      drums(2][5]."
            },
            {
              "id": 1086,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 30",
              "type": "chunk",
              "children": [],
              "content": "He      noted      that      music      was      a      constant      presence      in      his      family       Taylor      Hawkins\n\n \n \n  Oliver      Taylor      Hawkins      (1972-2022)      was      an      American      musician,      best      known      as      the      drummer      for      the      rock  \n \n  band      Foo      Fighters[1]."
            },
            {
              "id": 1087,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 31",
              "type": "chunk",
              "children": [],
              "content": "Born      in      Fort      Walton,      Texas,      Hawkins’      love      for      music      was      ignited      at   \n \na      young      age,  \n \n  particularly      after      watching   \n \na      Queen      concert      in      1982[2][3][5]."
            },
            {
              "id": 1088,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 32",
              "type": "chunk",
              "children": [],
              "content": "He      kick-started      his      professional      career  \n \n  as      the      drummer      for      Alanis      Morissette’s      band      during      her      world      tour      for      the      hit      album      ’Jagged      Little  \n \n  Pill’[8]L[9]."
            },
            {
              "id": 1089,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 33",
              "type": "chunk",
              "children": [],
              "content": "His      talents      were      recognized      by      Dave      Grohl,      who      invited      him      to      join      the      Foo      Fighters      in  \n \n  1997,      marking      the      beginning      of      his      impactful      tenure      with      the      band[7][8]."
            },
            {
              "id": 1090,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 34",
              "type": "chunk",
              "children": [],
              "content": "Hawkins      was      celebrated      for      his      versatile      drumming      style,      drawing      influence      from      renowned      drummers      like  \n \n  Roger      Taylor,      Neil      Peart,      Phil      Collins,      Alex      Van      Halen,      and      Stewart      Copeland[14]."
            },
            {
              "id": 1091,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 35",
              "type": "chunk",
              "children": [],
              "content": "His      performances,  \n \n  marked      by   \n \na      unique      energy      and      aggressive      style      of      play,      earned      him      recognition      as      one      of      the      top      rock  \n \n  drummers      of      his      era[15]."
            },
            {
              "id": 1092,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 36",
              "type": "chunk",
              "children": [],
              "content": "Apart      from      his      role      in      the      Foo      Fighters,      Hawkins’      passion      for      music      saw  \n \n  him      involved      in      numerous      side      projects      and      collaborations,      cementing      his      place      in      the      world      of      rock  \n \n  music[10]."
            },
            {
              "id": 1093,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 37",
              "type": "chunk",
              "children": [],
              "content": "Outside      of      his      professional      life,      Hawkins      was      known      for      his      resilience      and      dedication      to      his      family. Despite      personal      struggles,      including   \n \na      near-fatal      drug      overdose      in      2001,      Hawkins      remained      committed      to  \n \n  his      musical      career[4][9]."
            },
            {
              "id": 1094,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 38",
              "type": "chunk",
              "children": [],
              "content": "His      legacy      continues      to      inspire      musicians      and      fans      alike,      as      his      contributions  \n \n  to      rock      music,      coupled      with      his      indomitable      spirit,      made      him      an      unforgettable      icon      in      the      music  \n \n  industry[13]."
            },
            {
              "id": 1095,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 39",
              "type": "chunk",
              "children": [],
              "content": "Hawkins’      sudden      death      in      2022      while      on      tour      in      Bogota,      Colombia,      sent      shockwaves      through      the      music  \n \n  world[34]. Tributes      poured      in      from      around      the      globe,      reflecting      the      respect      and      admiration      Hawkins  \n \n  had      garnered      during      his      lifetime[21][31]."
            },
            {
              "id": 1096,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 40",
              "type": "chunk",
              "children": [],
              "content": "His      life      and      career      were      honored      at   \n \na      star-studded      tribute  \n \n  concert      in      Los      Angeles,      attesting      to      the      enduring      impact      of      his      music[22]. #\n \n   Early      Life      and      Background  \n \n  Oliver      Taylor      Hawkins,      known      as      Taylor      Hawkins,      was      born      and      raised      in      Fort      Walton,      Texas[3]."
            },
            {
              "id": 1097,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 41",
              "type": "chunk",
              "children": [],
              "content": "His  \n \n  family      moved      to      Laguna      Beach,      California      when      he      was      four      years      old[3]. He      has      two      younger      siblings,  \n \n  a      brother      named      Jason,      and   \n \na      sister      named      Heather[3]."
            },
            {
              "id": 1098,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 42",
              "type": "chunk",
              "children": [],
              "content": "As   \n \na      child,      Hawkins      was      particularly      influenced  \n \n  by      his      paternal      grandmother,      Josie      Hawkins,      who      had      grown      up      during      the      Great      Depression      and      lived      in  \n \n  Jackson,      Mississippi[1]."
            },
            {
              "id": 1099,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 43",
              "type": "chunk",
              "children": [],
              "content": "During      his      high      school      days      at      Laguna      Beach      High      School,      from      where      he      graduated      in      1990,      he      became  \n \n  friends      with      Jon      Davison,      who      later      became      the      lead      vocalist      of      the      band      Yes[2][3]."
            },
            {
              "id": 1100,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 44",
              "type": "chunk",
              "children": [],
              "content": "His      interest      in  \n \n  music      was      nurtured      from      an      early      age,      particularly      after      watching   \n \na      Queen      concert      in      1982      which      inspired  \n \n  him      to      learn      to      play      the      drums[2][5]. He      noted      that      music      was   \n \na      constant      presence      in      his      family      home[5]."
            },
            {
              "id": 1101,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 45",
              "type": "chunk",
              "children": [],
              "content": "Despite      facing      certain      hardships      during      his      upbringing,      including      his      mother’s      struggles      with      \"demons\",  \n \n  Hawkins      pursued      his      musical      ambitions[4]. He      credits      his      older      sister      Heather      for      taking      care      of      the  \n \n  family      during      difficult      times[4]."
            },
            {
              "id": 1102,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 46",
              "type": "chunk",
              "children": [],
              "content": "His      first      major      musical      experience      came      from      playing      drums      for      Alanis      Morissette’s      album,      Jagged      Little  \n \n  Pill,      and      accompanying      her      on      the      subsequent      tour[3]. This      marked      the      beginning      of      his      professional  \n \n  career      in      the      music      industry."
            },
            {
              "id": 1103,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 47",
              "type": "chunk",
              "children": [],
              "content": "#\n \n   Career  \n \n  Taylor      Hawkins      began      his      professional      music      career      playing      in      Alanis      Morissette’s      band      during      her  \n \n  18-month      world      tour      in      support      of      the      hit      album      ’Jagged      Little      Pill’      from      1995      to      1997[8][9]."
            },
            {
              "id": 1104,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 48",
              "type": "chunk",
              "children": [],
              "content": "His  \n \n  performances      not      only      in      the      tour      but      also      in      the      music      videos      for      “You      Oughta      Know”,      “All   \n \nI      Really      Want”  \n \n  and      “You      Learn”      introduced      him      to      the      world      of      rock      music      and      ultimately      led      to      his      meeting      with      Dave  \n \n  Grohl[8]."
            },
            {
              "id": 1105,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 49",
              "type": "chunk",
              "children": [],
              "content": "Throughout      this      time,      Hawkins      contributed      significantly      to      the      band’s      sound      and      performance,  \n \n  transforming      the      songs      from      their      original      drum      loop      format      to   \n \na      rock-band      vibe      that      resonated      with  \n \n  audiences[1][7]."
            },
            {
              "id": 1106,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 50",
              "type": "chunk",
              "children": [],
              "content": "In      1997,      Hawkins      was      asked      by      Grohl      to      join      the      Foo      Fighters,      an      invitation      that      he      readily      accepted[7][8]."
            },
            {
              "id": 1107,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 51",
              "type": "chunk",
              "children": [],
              "content": "At      the      time,      Grohl      thought      it      was   \n \na      long      shot      to      recruit      Hawkins      given      that      Morissette      was      at      the      height  \n \n  of      her      career,      but      Hawkins’      desire      to      be   \n \na      part      of   \n \na      rock      band      compelled      him      to      make      the      move[7]."
            },
            {
              "id": 1108,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 52",
              "type": "chunk",
              "children": [],
              "content": "This  \n \n  marked      the      beginning      of      Hawkins’      tenure      as      the      drummer      of      the      Foo      Fighters,   \n \na      role      that      he      would      play  \n \n  until      his      passing[6][9]. Apart      from      his      work      with      Morissette      and      the      Foo      Fighters,      Hawkins      had      an      array      of      other      musical  \n \n  experiences[10]."
            },
            {
              "id": 1109,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 53",
              "type": "chunk",
              "children": [],
              "content": "He      drummed      for      Sass      Jordan      before      joining      Morissette’s      touring      band[10]. He      was      part  \n \n  of      an      ad      hoc      drum      supergroup      called      SOS      Allstars      and      filled      the      void      for      Coheed      and      Cambria’s      2007  \n \n  album      after      their      drummer      Josh      Eppard      left      the      group[10]."
            },
            {
              "id": 1110,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 54",
              "type": "chunk",
              "children": [],
              "content": "In      addition,      Hawkins      formed      his      own      side  \n \n  project,      the      Coattail      Riders,      in      2005,      through      which      he      recorded      his      own      music      and      took      the      project      on  \n \n  the      road,      performing      in      small      clubs      despite      the      Foo      Fighters’      arena-status[7]."
            },
            {
              "id": 1111,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 55",
              "type": "chunk",
              "children": [],
              "content": "His      son,      Shane      Hawkins,  \n \n  has      since      taken      on      his      father’s      legacy,      joining      the      Foo      Fighters      for   \n \na      performance      during      the      Boston  \n \n  Calling      Music      Festival      in      2023[6]."
            },
            {
              "id": 1112,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 56",
              "type": "chunk",
              "children": [],
              "content": "#\n \n   Musical      Style      and      Influences  \n \n  Taylor      Hawkins      was   \n \na      profound      drummer,      with      his      musical      style      and      influences      spreading      across   \n \na      wide  \n \n  array      of      rock      genres[11]."
            },
            {
              "id": 1113,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 57",
              "type": "chunk",
              "children": [],
              "content": "Known      for      his      passionate      fandom      of      groups      that      came      before      him,      Hawkins  \n \n  regularly      expressed      his      admiration      for      bands      like      Rush,      Genesis,      and      the      Police,      all      of      which      featured  \n \n  some      of      the      greatest      drummers      in      rock      history      like      Neil      Peart,      Phil      Collins,      and      Stewart      Copeland[11]."
            },
            {
              "id": 1114,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 58",
              "type": "chunk",
              "children": [],
              "content": "He      was      heavily      influenced      by      his      love      for      classic      rock,      as      evidenced      by      his      performances,      where      he  \n \n  covered      songs      from      bands      like      Van      Halen[11]."
            },
            {
              "id": 1115,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 59",
              "type": "chunk",
              "children": [],
              "content": "Hawkins      drew      influences      from   \n \na      variety      of      drumming      styles,      developing   \n \na      signature      style      inspired      by  \n \n  greats      like      Roger      Taylor,      Neil      Peart,      Phil      Collins,      Alex      Van      Halen,      and      Stewart      Copeland[14]."
            },
            {
              "id": 1116,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 60",
              "type": "chunk",
              "children": [],
              "content": "This  \n \n  distinctive      style      and      influence      extended      to      his      drum      kit,      which      incorporated      elements      like      rototoms  \n \n  and      concert      toms[14]. Beyond      his      influences,      Hawkins      had   \n \na      unique      energy      that      made      him      stand      out      as   \n \na      drummer."
            },
            {
              "id": 1117,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 61",
              "type": "chunk",
              "children": [],
              "content": "His      performances  \n \n  were      recognized      for      their      power,      and      he      was      known      for      his      enthusiastic      and      aggressive      style      of      play[15]. This      earned      him      recognition      as      one      of      the      top      rock      drummers      of      his      time,      with      his      passion      for      music  \n \n  living      on      through      his      performances[14]."
            },
            {
              "id": 1118,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 62",
              "type": "chunk",
              "children": [],
              "content": "Through      his      career,      Hawkins      left      an      indelible      mark      on      rock      music,      through      his      distinct      style,      passion,  \n \n  and      contributions      to      the      music      industry[13]. His      love      for      music      and      dedication      to      his      craft      made      him       an      unforgettable      icon      in      the      world      of      rock      music[13]."
            },
            {
              "id": 1119,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 63",
              "type": "chunk",
              "children": [],
              "content": "#\n \n   Personal      Life  \n \n  Taylor      Hawkins      married      Alison      Hawkins,      an      American      celebrity      and      entrepreneur,      in      2005[18]. The      couple  \n \n  had      three      children,      Oliver,      Annabelle,      and      Everleigh[19]. Hawkins’      commitment      to      his      family      was      evident;\n \n \n  in      fact,      he      even      wrote   \n \na      song      for      his      middle      child,      Annabelle[9]."
            },
            {
              "id": 1120,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 64",
              "type": "chunk",
              "children": [],
              "content": "In      his      personal      life,      Hawkins      had      also      struggled      with      drug      use,      which      nearly      claimed      his      life      in   \n \na      2001  \n \n  overdose[9][7][4]."
            },
            {
              "id": 1121,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 65",
              "type": "chunk",
              "children": [],
              "content": "However,      he      managed      to      overcome      this      challenge,      and      later      expressed      gratitude      for  \n \n  the      experience      as   \n \na      lesson      that      allowed      him      to      realize      the      destructive      path      he      was      on[7]."
            },
            {
              "id": 1122,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 66",
              "type": "chunk",
              "children": [],
              "content": "Outside      of      his      main      role      in      the      Foo      Fighters,      Hawkins      also      pursued      various      side      projects      including      the  \n \n  Birds      of      Satan,      NHC,      and      Chevy      Metal. His      motivation      for      such      ventures      was   \n \na      constant      drive      to      create  \n \n  and      his      love      for      music[7]."
            },
            {
              "id": 1123,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 67",
              "type": "chunk",
              "children": [],
              "content": "Hawkins      was      also      known      for      his      unabashed      fanboy      nature,      often      vocalizing  \n \n  his      admiration      for      fellow      musicians      and      his      heroes[7]. #\n \n   Legacy      and      Impact Taylor      Hawkins      was      known      for      his      raw      and      authentic      drumming      style,      described      as      \"courageous,      damaged  \n \n  and      unflinchingly      authentic”[20]."
            },
            {
              "id": 1124,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 68",
              "type": "chunk",
              "children": [],
              "content": "His      work      with      the      Foo      Fighters,      as      well      as      his      various      collaborations n’      roll[10]. ‘ and      side      projects,      made      him   \n \na      celebrated      figure      in      rock  \n \n  Hawkins’      death      in      2022      was      met      with      heartfelt      tributes      from      colleagues      and      fans      around      the      world."
            },
            {
              "id": 1125,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 69",
              "type": "chunk",
              "children": [],
              "content": "Notable      tributes      came      from      rock      legends      like      Roger      Taylor      of      Queen,      who      considered      Hawkins      as   \n \na      kind,  \n \n  brilliant      man      and      an      inspirational      mentor,      likening      his      death      to      \"losing   \n \na      younger      favourite      brother”[21]."
            },
            {
              "id": 1126,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 70",
              "type": "chunk",
              "children": [],
              "content": "Similarly,      Led      Zeppelin’s      Jimmy      Page      admired      his      technique,      energy      and      spirited      enthusiasm[21]. An      LA      tribute      concert      held      in      his      honor      included      guest      drummers      like      Lars      Ulrich      of      Metallica,      Travis  \n \n  Barker      of      blink-182,      and      Brad      Wilk      of      Rage      Against      the      Machine."
            },
            {
              "id": 1127,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 71",
              "type": "chunk",
              "children": [],
              "content": "Singers      like      Miley      Cyrus      and      Alanis  \n \n  Morissette      also      performed      at      the      concert[22]. Apart      from      his      music,      Taylor      Hawkins      also      contributed      to      charities      Music      Support      and      MusiCares,      both      of  \n \n  which      were      chosen      by      the      Hawkins      family[23]."
            },
            {
              "id": 1128,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 72",
              "type": "chunk",
              "children": [],
              "content": "He      had      received      numerous      accolades      throughout      his      career,  \n \n  including      27      Grammy      nominations,      of      which      he      won      14[2]. In      2021,      the      Foo      Fighters      were      inducted      into  \n \n  the      Rock      and      Roll      Hall      of      Fame[9]."
            },
            {
              "id": 1129,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 73",
              "type": "chunk",
              "children": [],
              "content": "#\n \n   Discography  \n \n  Taylor      Hawkins      also      led   \n \na      notable      music      career      through      his      own      side      projects      and      collaborations[10]."
            },
            {
              "id": 1130,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 74",
              "type": "chunk",
              "children": [],
              "content": "Aside      from      his      work      with      the      Foo      Fighters,      Hawkins      formed      and      fronted      the      band      Taylor      Hawkins   \n \n&      The  \n \n  Coattail      Riders,   \n \na      project      which      originated      from      jamming      sessions      with      his      friend      Drew      Hester[10]."
            },
            {
              "id": 1131,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 75",
              "type": "chunk",
              "children": [],
              "content": "###      Taylor      Hawkins   \n \n&      The      Coattail      Riders  \n \n  Taylor      Hawkins   \n \n&      The      Coattail      Riders,   \n \na      band      formed      in      2004,      have      released      three      albums      and      their  \n \n  music      spans      genres      including      Hard      Rock,      Art      Rock,      and      Alternative      Rock[24][25][26]."
            },
            {
              "id": 1132,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 76",
              "type": "chunk",
              "children": [],
              "content": "The      band      grew      from  \n \n  an      initial      casual      jamming      session,      gradually      evolving      into   \n \na      more      formal      arrangement      that      led      to      the  \n \n  production      of      record      albums."
            },
            {
              "id": 1133,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 77",
              "type": "chunk",
              "children": [],
              "content": "Notably,      these      albums      featured      guest      appearances      by      renowned      musicians  \n \n  such      as      Dave      Grohl,      Queen’s      Brian      May      and      Roger      Taylor,      The      Cars’      Elliot      Easton,      Perry      Farrell,      and  \n \n  Jon      Davison,      who      is   \n \na      school      friend      of      Hawkins’[10]."
            },
            {
              "id": 1134,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 78",
              "type": "chunk",
              "children": [],
              "content": "###      Red      Light      Fever  \n \n  Red      Light      Fever,      released      on      April      19,      2010,      was      the      band’s      first      album[29][30]."
            },
            {
              "id": 1135,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 79",
              "type": "chunk",
              "children": [],
              "content": "Prior      to      its      release,  \n \n  Hawkins      revealed      in      an      interview      that      the      album      had      completed      the      recording      and      production      stages,      but  \n \n  its      title      and      release      date      were      yet      to      be      determined[29]."
            },
            {
              "id": 1136,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 80",
              "type": "chunk",
              "children": [],
              "content": "Red      Light      Fever      was      recorded      at      the      Foo  \n \n  Fighters’      Studio      606      in      California      and      featured      guest      musicians      such      as      Brian      May      and      Roger      Taylor      of  \n \n  Queen,      Dave      Grohl      of      Foo      Fighters,      and      Elliot      Easton      of      The      Cars[29][30]."
            },
            {
              "id": 1137,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 81",
              "type": "chunk",
              "children": [],
              "content": "##      Get      the      Money  \n \n  Get      the      Money,      the      third      album      from      Taylor      Hawkins   \n \n&      The      Coattail      Riders,      was      released      on      November      8,  \n \n  2019[29]. The      album’s      first      single,      \"Crossed      the      Line”,      released      on      October      15,      2019,      featured      Dave  \n \n  Grohl      and      Jon      Davison,      the      frontman      of      Yes[29]."
            },
            {
              "id": 1138,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 82",
              "type": "chunk",
              "children": [],
              "content": "The      music      video      for      the      single      \"I      Really      Blew      It”      also  \n \n  featured      appearances      from      Grohl      and      Perry      Farrel1[29]. #\n \n   Collaborations      and      Guest      Appearances  \n \n  Throughout      his      career,      Taylor      Hawkins      collaborated      with      various      prominent      artists      and      bands."
            },
            {
              "id": 1139,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 83",
              "type": "chunk",
              "children": [],
              "content": "The  \n \n  Coattail      Riders’      albums      notably      featured      appearances      from      luminaries      such      as      Brian      May      and      Roger      Taylor  \n \n  of      Queen,      Chrissie      Hynde,      Nancy      Wilson      of      Heart,      Sex      Pistol      Steve      Jones      and      James      Gang’s      Joe      Walsh[28]."
            },
            {
              "id": 1140,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 84",
              "type": "chunk",
              "children": [],
              "content": "Hawkins      also      fronted      another      group,      The      Birds      of      Satan,      which      evolved      from      his      heavy      rock      covers      band,  \n \n  Chevy      Metal[28]. Despite      his      diverse      musical      engagements,      Hawkins      always      maintained   \n \na      close      allegiance      with      the      Foo  \n \n  Fighters,      which      remained      the      center      of      his      music      life[7][28]."
            },
            {
              "id": 1141,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 85",
              "type": "chunk",
              "children": [],
              "content": "#\n \n   Tragic      Passing  \n \n  Taylor      Hawkins,      the      esteemed      drummer      of      the      alt-rock      band      Foo      Fighters,      passed      away      suddenly      on      March  \n \n  25,      2022,      while      on      tour      with      his      band      in      Bogota,      Colombia[34]."
            },
            {
              "id": 1142,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 86",
              "type": "chunk",
              "children": [],
              "content": "The      official      cause      of      death      was      cardiac  \n \n  arrest,      though      inquiries      were      raised      concerning      the      presence      of      drugs      in      his      system      and      their      potential  \n \n  contribution      to      his      death[33][34]."
            },
            {
              "id": 1143,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 87",
              "type": "chunk",
              "children": [],
              "content": "On      the      night      of      his      passing,      paramedics      were      called      to      the      Four  \n \n  Seasons      hotel      in      Bogota      due      to      reports      of      chest      pain      from      an      unnamed      guest,      later      revealed      to      be  \n \n  Hawkins[34]. Unfortunately,      resuscitation      efforts      were      unsuccessful,      and      Hawkins      was      declared      dead      at  \n \n  the      scene[34]."
            },
            {
              "id": 1144,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 88",
              "type": "chunk",
              "children": [],
              "content": "The      news      of      Hawkins’      sudden      demise      was      announced      on      the      morning      of      March      25th,      2022,      which      left      the      music  \n \n  world      in      shock[32]."
            },
            {
              "id": 1145,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 89",
              "type": "chunk",
              "children": [],
              "content": "The      band      confirmed      the      news      with   \n \na      short      statement,      expressing      their      devastation  \n \n  at      the      loss      of      Hawkins,      whose      \"musical      spirit      and      infectious      laughter”      would      live      on      forever[32]. As   \n \na      result      of      Hawkins’      untimely      passing,      the      band      canceled      their      ongoing      South      American      tour[33]."
            },
            {
              "id": 1146,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 90",
              "type": "chunk",
              "children": [],
              "content": "The  \n \n  festival      stage      at      the      Estéreo      Picnic      Festival,      where      the      Foo      Fighters      were      scheduled      to      perform      that  \n \n  night,      was      transformed      into   \n \na      candlelight      vigil      in      memory      of      Hawkins[33]."
            },
            {
              "id": 1147,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 91",
              "type": "chunk",
              "children": [],
              "content": "##      Tributes      and      Remembrances  \n \n  In      the      wake      of      Hawkins’      death,      tributes      from      fans      and      colleagues      alike      poured      in      from      around      the  \n \n  world[21][31]."
            },
            {
              "id": 1148,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 92",
              "type": "chunk",
              "children": [],
              "content": "Among      the      many      paying      their      respects      were      legendary      rock      and      roll      musicians      like      Roger  \n \n  Taylor,      the      drummer      of      Queen,      who      Hawkins      credited      with      inspiring      his      own      career      behind      the      drum      set[21]."
            },
            {
              "id": 1149,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 93",
              "type": "chunk",
              "children": [],
              "content": "In      heartfelt      social      media      posts,      Taylor      described      Hawkins      as      an      \"inspirational      mentor”      and   \n \na      \"kind  \n \n  brilliant      man\"[21],      while      Led      Zeppelin’s      Jimmy      Page      reminisced      about      sharing      the      stage      with      Hawkins  \n \n  and      praised      his      \"technique,      energy      and      spirited      enthusiasm\"[21]."
            },
            {
              "id": 1150,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 94",
              "type": "chunk",
              "children": [],
              "content": "There      were      also      numerous      onstage      tributes      to      Hawkins. Notably,      Miley      Cyrus      expressed      her      grief      and      sent  \n \n  peaceful      wishes      to      the      Foo      Fighters      and      the      Hawkins      family      during   \n \na      performance      at      Lollapalooza[31]."
            },
            {
              "id": 1151,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 95",
              "type": "chunk",
              "children": [],
              "content": "Similarly,      Liam      Gallagher      of      Oasis      dedicated      one      of      the      band’s      biggest      hits      to      Hawkins      during   \n \na      concert  \n \n  at      the      Royal      Albert      Hall      in      London[31]. Fans      gathered      outside      the      hotel      where      Hawkins      died,      lighting      candles,      leaving      flowers,      and      singing      the  \n \n  band’s      songs      in      his      honor[31]."
            },
            {
              "id": 1152,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 96",
              "type": "chunk",
              "children": [],
              "content": "Hawkins’      life      and      career      were      celebrated      in   \n \na      star-studded      tribute      concert      in      Los      Angeles,      which      saw  \n \n  performances      from      over      5@      musicians,      including      his      former      bands      and      colleagues      from      Def      Leppard,      Queen,  \n \n  and      Foo      Fighters[22]. Table      12:      STORM’s      generated      article      for      “Taylor      Hawkins”."
            },
            {
              "id": 1153,
              "title": "Issue       Mentioned      Time       Example      Comments - Chunk 97",
              "type": "chunk",
              "children": [],
              "content": "“#’,      “##”      indicate      the      section      title      and      subsection      title  \n \n  respectively. Numbers      in      brackets      indicate      the      cited      references."
            }
          ],
          "content": ""
        },
        {
          "id": 1154,
          "title": "      (comment      on      article      2022      Crimean      Bridge      explosion)",
          "type": "section",
          "children": [
            {
              "id": 1155,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 1",
              "type": "chunk",
              "children": [],
              "content": "(comment      on      article      2022      Crimean      Bridge      explosion)\n \n \n  Section      organization      problem   \n \n5   \n \nI      do      not      like      how      the      article      is      organized,      with      too      many      headers      cluttering      the      article,  \n \n  making      it      not      as      readable. Other      than      that,      the      AI      did      great      work      on      the      piece."
            },
            {
              "id": 1156,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 2",
              "type": "chunk",
              "children": [],
              "content": "|       Select      a      key       8      v       Selected      Key:      8       Title:      Taylor      Hawkins      -      marshall.com       Snippets:       In      1995,      off      the      back      of      her      hit      album       ‘Jagged      Little      Pill’,      Canadian      American       superstar      Alanis      Morissette      recruited      him       to      be      her      touring      drummer      for      her      18       month      album      tour,      along      with      him       featuring      in      the      music      videos      for      “You       Oughta      Know’,      “All|      Really      Want”      and       “You      Learn’."
            },
            {
              "id": 1157,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 3",
              "type": "chunk",
              "children": [],
              "content": "It      was      during      this      tour      that      he       met      his      musical      soul      mate,      Dave      Grohl. The      tour      with      Alanis      ended      and      he\\\\\\'d      heard       that      Dave      and      Foo      Fighters      were      looking       for      anew      drummer,      so      Taylor      enquired."
            },
            {
              "id": 1158,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 4",
              "type": "chunk",
              "children": [],
              "content": "Dave      initially      thought      that      Taylor      wouldn’t       be      interested      in      joining      as      Alanis       Morissette      was      much      bigger      than      the      Foo       Fighters      at      that      time,      but      Taylor      jumped      at       the      chance      due      to      his      overwhelming       desire      to      be      in      a      rock      band."
            },
            {
              "id": 1159,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 5",
              "type": "chunk",
              "children": [],
              "content": "It      was      from       that      point      that      the      bromance      between       them      began      and      on      March      18th      1997       Taylor      was      announced      as      their      new       drummer."
            },
            {
              "id": 1160,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 6",
              "type": "chunk",
              "children": [],
              "content": "Url:      https://marshall.com/live-for-       music/drum-icons/taylor-hawkins | Logout  \n \n  Select      an      option:  \n \n  (11,      ‘Taylor      Hawkins      [Quite      Richard}’)   \n \n¥       You      selected:      Taylor      Hawkins      [Quite      Richard]  \n \n  Taylor      Hawkins  \n \n  Oliver      Taylor      Hawkins      (1972-2022)      was      an      American      musician,      best      known      as      the      drummer      for      the      rock      band      Foo      Fighters[1]."
            },
            {
              "id": 1161,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 7",
              "type": "chunk",
              "children": [],
              "content": "Born      in      Fort      Walton,      Texas,      Hawkins’      love      for      music      was      ignited      at   \n \na      young  \n \n  age,      particularly      after      watching   \n \na      Queen      concert      in      1982[2][3][5]."
            },
            {
              "id": 1162,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 8",
              "type": "chunk",
              "children": [],
              "content": "He      kick-started      his      professional      career      as      the      drummer      for      Alanis      Morissette’s      band      during      her      world      tour      for      the      hit      album      ‘Jagged      Little  \n \n  Pill\\\\\\'[8][9]."
            },
            {
              "id": 1163,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 9",
              "type": "chunk",
              "children": [],
              "content": "His      talents      were      recognized      by      Dave      Grohl,      who      invited      him      to      join      the      Foo      Fighters      in      1997,      marking      the      beginning      of      his      impactful      tenure      with      the      band{7][8]."
            },
            {
              "id": 1164,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 10",
              "type": "chunk",
              "children": [],
              "content": "Hawkins      was      celebrated      for      his      versatile      drumming      style,      drawing      influence      from      renowned      drummers      like      Roger      Taylor,      Neil      Peart,      Phil      Collins,      Alex      Van      Halen,      and      Stewart      Copeland[14)."
            },
            {
              "id": 1165,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 11",
              "type": "chunk",
              "children": [],
              "content": "His  \n \n  performances,      marked      by   \n \na      unique      energy      and      aggressive      style      of      play,      earned      him      recognition      as      one      of      the      top      rock      drummers      of      his      era[15]."
            },
            {
              "id": 1166,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 12",
              "type": "chunk",
              "children": [],
              "content": "Apart      from      his      role      in      the      Foo      Fighters,      Hawkins\\\\\\'      passion  \n \n  for      music      saw      him      involved      in      numerous      side      projects      and      collaborations,      cementing      his      place      in      the      world      of      rock      music(10]."
            },
            {
              "id": 1167,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 13",
              "type": "chunk",
              "children": [],
              "content": "Outside      of      his      professional      life,      Hawkins      was      known      for      his      resilience      and      dedication      to      his      family. Despite      personal      struggles,      including   \n \na      near-fatal      drug      overdose      in      2001,      Hawkins      remained      committed  \n \n  to      his      musical      career{4][9]."
            },
            {
              "id": 1168,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 14",
              "type": "chunk",
              "children": [],
              "content": "His      legacy      continues      to      inspire      musicians      and      fans      alike,      as      his      contributions      to      rock      music,      coupled      with      his   \n \ni       industry(13}. spirit,      made      him      an      icon      in      the      music  \n \n  Hawkins’      sudden      death      in      2022      while      on      tour      in      Bogotd,      Colombia,      sent      shockwaves      through      the      music      world[34]."
            },
            {
              "id": 1169,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 15",
              "type": "chunk",
              "children": [],
              "content": "Tributes      poured      in      from      around      the      globe,      reflecting      the      respect      and      admiration  \n \n  Hawkins      had      garnered      during      his      lifetime[21][31]. His      life      and      career      were      honored      at   \n \na      star-studded      tribute      concert      in      Los      Angeles,      attesting      to      the      enduring      impact      of      his      music[22]."
            },
            {
              "id": 1170,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 16",
              "type": "chunk",
              "children": [],
              "content": "| Early      Life      and      Background\n |       Oliver      Taylor      Hawkins,      known      as      Taylor      Hawkins,      was      born      and      raised      in      Fort      Walton,      Texas[3]. His      family      moved      to      Laguna      Beach,      California      when      he      was      four      years      old[3]. He      has      two      younger      siblings,       a      brother      named      Jason,      and      a      sister      named      Heather[3]."
            },
            {
              "id": 1171,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 17",
              "type": "chunk",
              "children": [],
              "content": "Asa      child,      Hawkins      wa:      influenced      by      his      paternal      Josie      Hawkins,      who      had      grown      up      during      the      Great      Depression      and       lived      in      Jackson,      Mississippi(2]."
            },
            {
              "id": 1172,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 18",
              "type": "chunk",
              "children": [],
              "content": "|       During      his      high      school      days      at      Laguna      Beach      High      School,      from      where      he      graduated      in      1990,      he      became      friends      with      Jon      Davison,      who      later      became      the      lead      vocalist      of      the      band      Yes(2][3]."
            },
            {
              "id": 1173,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 19",
              "type": "chunk",
              "children": [],
              "content": "His      interest      in       music      was      nurtured      from      an      early      age,      particularly      after      watching      a      Queen      concert      in      1982      which      inspired      him      to      learn      to      play      the      drums(2][5]."
            },
            {
              "id": 1174,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 20",
              "type": "chunk",
              "children": [],
              "content": "He      noted      that      music      was      a      constant      presence      in      his      family       Taylor      Hawkins\n\n \n \n  Oliver      Taylor      Hawkins      (1972-2022)      was      an      American      musician,      best      known      as      the      drummer      for      the      rock  \n \n  band      Foo      Fighters[1]."
            },
            {
              "id": 1175,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 21",
              "type": "chunk",
              "children": [],
              "content": "Born      in      Fort      Walton,      Texas,      Hawkins’      love      for      music      was      ignited      at   \n \na      young      age,  \n \n  particularly      after      watching   \n \na      Queen      concert      in      1982[2][3][5]."
            },
            {
              "id": 1176,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 22",
              "type": "chunk",
              "children": [],
              "content": "He      kick-started      his      professional      career  \n \n  as      the      drummer      for      Alanis      Morissette’s      band      during      her      world      tour      for      the      hit      album      ’Jagged      Little  \n \n  Pill’[8]L[9]."
            },
            {
              "id": 1177,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 23",
              "type": "chunk",
              "children": [],
              "content": "His      talents      were      recognized      by      Dave      Grohl,      who      invited      him      to      join      the      Foo      Fighters      in  \n \n  1997,      marking      the      beginning      of      his      impactful      tenure      with      the      band[7][8]."
            },
            {
              "id": 1178,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 24",
              "type": "chunk",
              "children": [],
              "content": "Hawkins      was      celebrated      for      his      versatile      drumming      style,      drawing      influence      from      renowned      drummers      like  \n \n  Roger      Taylor,      Neil      Peart,      Phil      Collins,      Alex      Van      Halen,      and      Stewart      Copeland[14]."
            },
            {
              "id": 1179,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 25",
              "type": "chunk",
              "children": [],
              "content": "His      performances,  \n \n  marked      by   \n \na      unique      energy      and      aggressive      style      of      play,      earned      him      recognition      as      one      of      the      top      rock  \n \n  drummers      of      his      era[15]."
            },
            {
              "id": 1180,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 26",
              "type": "chunk",
              "children": [],
              "content": "Apart      from      his      role      in      the      Foo      Fighters,      Hawkins’      passion      for      music      saw  \n \n  him      involved      in      numerous      side      projects      and      collaborations,      cementing      his      place      in      the      world      of      rock  \n \n  music[10]."
            },
            {
              "id": 1181,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 27",
              "type": "chunk",
              "children": [],
              "content": "Outside      of      his      professional      life,      Hawkins      was      known      for      his      resilience      and      dedication      to      his      family. Despite      personal      struggles,      including   \n \na      near-fatal      drug      overdose      in      2001,      Hawkins      remained      committed      to  \n \n  his      musical      career[4][9]."
            },
            {
              "id": 1182,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 28",
              "type": "chunk",
              "children": [],
              "content": "His      legacy      continues      to      inspire      musicians      and      fans      alike,      as      his      contributions  \n \n  to      rock      music,      coupled      with      his      indomitable      spirit,      made      him      an      unforgettable      icon      in      the      music  \n \n  industry[13]."
            },
            {
              "id": 1183,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 29",
              "type": "chunk",
              "children": [],
              "content": "Hawkins’      sudden      death      in      2022      while      on      tour      in      Bogota,      Colombia,      sent      shockwaves      through      the      music  \n \n  world[34]. Tributes      poured      in      from      around      the      globe,      reflecting      the      respect      and      admiration      Hawkins  \n \n  had      garnered      during      his      lifetime[21][31]."
            },
            {
              "id": 1184,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 30",
              "type": "chunk",
              "children": [],
              "content": "His      life      and      career      were      honored      at   \n \na      star-studded      tribute  \n \n  concert      in      Los      Angeles,      attesting      to      the      enduring      impact      of      his      music[22]. #\n \n   Early      Life      and      Background  \n \n  Oliver      Taylor      Hawkins,      known      as      Taylor      Hawkins,      was      born      and      raised      in      Fort      Walton,      Texas[3]."
            },
            {
              "id": 1185,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 31",
              "type": "chunk",
              "children": [],
              "content": "His  \n \n  family      moved      to      Laguna      Beach,      California      when      he      was      four      years      old[3]. He      has      two      younger      siblings,  \n \n  a      brother      named      Jason,      and   \n \na      sister      named      Heather[3]."
            },
            {
              "id": 1186,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 32",
              "type": "chunk",
              "children": [],
              "content": "As   \n \na      child,      Hawkins      was      particularly      influenced  \n \n  by      his      paternal      grandmother,      Josie      Hawkins,      who      had      grown      up      during      the      Great      Depression      and      lived      in  \n \n  Jackson,      Mississippi[1]."
            },
            {
              "id": 1187,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 33",
              "type": "chunk",
              "children": [],
              "content": "During      his      high      school      days      at      Laguna      Beach      High      School,      from      where      he      graduated      in      1990,      he      became  \n \n  friends      with      Jon      Davison,      who      later      became      the      lead      vocalist      of      the      band      Yes[2][3]."
            },
            {
              "id": 1188,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 34",
              "type": "chunk",
              "children": [],
              "content": "His      interest      in  \n \n  music      was      nurtured      from      an      early      age,      particularly      after      watching   \n \na      Queen      concert      in      1982      which      inspired  \n \n  him      to      learn      to      play      the      drums[2][5]. He      noted      that      music      was   \n \na      constant      presence      in      his      family      home[5]."
            },
            {
              "id": 1189,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 35",
              "type": "chunk",
              "children": [],
              "content": "Despite      facing      certain      hardships      during      his      upbringing,      including      his      mother’s      struggles      with      \"demons\",  \n \n  Hawkins      pursued      his      musical      ambitions[4]. He      credits      his      older      sister      Heather      for      taking      care      of      the  \n \n  family      during      difficult      times[4]."
            },
            {
              "id": 1190,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 36",
              "type": "chunk",
              "children": [],
              "content": "His      first      major      musical      experience      came      from      playing      drums      for      Alanis      Morissette’s      album,      Jagged      Little  \n \n  Pill,      and      accompanying      her      on      the      subsequent      tour[3]. This      marked      the      beginning      of      his      professional  \n \n  career      in      the      music      industry."
            },
            {
              "id": 1191,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 37",
              "type": "chunk",
              "children": [],
              "content": "#\n \n   Career  \n \n  Taylor      Hawkins      began      his      professional      music      career      playing      in      Alanis      Morissette’s      band      during      her  \n \n  18-month      world      tour      in      support      of      the      hit      album      ’Jagged      Little      Pill’      from      1995      to      1997[8][9]."
            },
            {
              "id": 1192,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 38",
              "type": "chunk",
              "children": [],
              "content": "His  \n \n  performances      not      only      in      the      tour      but      also      in      the      music      videos      for      “You      Oughta      Know”,      “All   \n \nI      Really      Want”  \n \n  and      “You      Learn”      introduced      him      to      the      world      of      rock      music      and      ultimately      led      to      his      meeting      with      Dave  \n \n  Grohl[8]."
            },
            {
              "id": 1193,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 39",
              "type": "chunk",
              "children": [],
              "content": "Throughout      this      time,      Hawkins      contributed      significantly      to      the      band’s      sound      and      performance,  \n \n  transforming      the      songs      from      their      original      drum      loop      format      to   \n \na      rock-band      vibe      that      resonated      with  \n \n  audiences[1][7]."
            },
            {
              "id": 1194,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 40",
              "type": "chunk",
              "children": [],
              "content": "In      1997,      Hawkins      was      asked      by      Grohl      to      join      the      Foo      Fighters,      an      invitation      that      he      readily      accepted[7][8]."
            },
            {
              "id": 1195,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 41",
              "type": "chunk",
              "children": [],
              "content": "At      the      time,      Grohl      thought      it      was   \n \na      long      shot      to      recruit      Hawkins      given      that      Morissette      was      at      the      height  \n \n  of      her      career,      but      Hawkins’      desire      to      be   \n \na      part      of   \n \na      rock      band      compelled      him      to      make      the      move[7]."
            },
            {
              "id": 1196,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 42",
              "type": "chunk",
              "children": [],
              "content": "This  \n \n  marked      the      beginning      of      Hawkins’      tenure      as      the      drummer      of      the      Foo      Fighters,   \n \na      role      that      he      would      play  \n \n  until      his      passing[6][9]. Apart      from      his      work      with      Morissette      and      the      Foo      Fighters,      Hawkins      had      an      array      of      other      musical  \n \n  experiences[10]."
            },
            {
              "id": 1197,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 43",
              "type": "chunk",
              "children": [],
              "content": "He      drummed      for      Sass      Jordan      before      joining      Morissette’s      touring      band[10]. He      was      part  \n \n  of      an      ad      hoc      drum      supergroup      called      SOS      Allstars      and      filled      the      void      for      Coheed      and      Cambria’s      2007  \n \n  album      after      their      drummer      Josh      Eppard      left      the      group[10]."
            },
            {
              "id": 1198,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 44",
              "type": "chunk",
              "children": [],
              "content": "In      addition,      Hawkins      formed      his      own      side  \n \n  project,      the      Coattail      Riders,      in      2005,      through      which      he      recorded      his      own      music      and      took      the      project      on  \n \n  the      road,      performing      in      small      clubs      despite      the      Foo      Fighters’      arena-status[7]."
            },
            {
              "id": 1199,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 45",
              "type": "chunk",
              "children": [],
              "content": "His      son,      Shane      Hawkins,  \n \n  has      since      taken      on      his      father’s      legacy,      joining      the      Foo      Fighters      for   \n \na      performance      during      the      Boston  \n \n  Calling      Music      Festival      in      2023[6]."
            },
            {
              "id": 1200,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 46",
              "type": "chunk",
              "children": [],
              "content": "#\n \n   Musical      Style      and      Influences  \n \n  Taylor      Hawkins      was   \n \na      profound      drummer,      with      his      musical      style      and      influences      spreading      across   \n \na      wide  \n \n  array      of      rock      genres[11]."
            },
            {
              "id": 1201,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 47",
              "type": "chunk",
              "children": [],
              "content": "Known      for      his      passionate      fandom      of      groups      that      came      before      him,      Hawkins  \n \n  regularly      expressed      his      admiration      for      bands      like      Rush,      Genesis,      and      the      Police,      all      of      which      featured  \n \n  some      of      the      greatest      drummers      in      rock      history      like      Neil      Peart,      Phil      Collins,      and      Stewart      Copeland[11]."
            },
            {
              "id": 1202,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 48",
              "type": "chunk",
              "children": [],
              "content": "He      was      heavily      influenced      by      his      love      for      classic      rock,      as      evidenced      by      his      performances,      where      he  \n \n  covered      songs      from      bands      like      Van      Halen[11]."
            },
            {
              "id": 1203,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 49",
              "type": "chunk",
              "children": [],
              "content": "Hawkins      drew      influences      from   \n \na      variety      of      drumming      styles,      developing   \n \na      signature      style      inspired      by  \n \n  greats      like      Roger      Taylor,      Neil      Peart,      Phil      Collins,      Alex      Van      Halen,      and      Stewart      Copeland[14]."
            },
            {
              "id": 1204,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 50",
              "type": "chunk",
              "children": [],
              "content": "This  \n \n  distinctive      style      and      influence      extended      to      his      drum      kit,      which      incorporated      elements      like      rototoms  \n \n  and      concert      toms[14]. Beyond      his      influences,      Hawkins      had   \n \na      unique      energy      that      made      him      stand      out      as   \n \na      drummer."
            },
            {
              "id": 1205,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 51",
              "type": "chunk",
              "children": [],
              "content": "His      performances  \n \n  were      recognized      for      their      power,      and      he      was      known      for      his      enthusiastic      and      aggressive      style      of      play[15]. This      earned      him      recognition      as      one      of      the      top      rock      drummers      of      his      time,      with      his      passion      for      music  \n \n  living      on      through      his      performances[14]."
            },
            {
              "id": 1206,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 52",
              "type": "chunk",
              "children": [],
              "content": "Through      his      career,      Hawkins      left      an      indelible      mark      on      rock      music,      through      his      distinct      style,      passion,  \n \n  and      contributions      to      the      music      industry[13]. His      love      for      music      and      dedication      to      his      craft      made      him       an      unforgettable      icon      in      the      world      of      rock      music[13]."
            },
            {
              "id": 1207,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 53",
              "type": "chunk",
              "children": [],
              "content": "#\n \n   Personal      Life  \n \n  Taylor      Hawkins      married      Alison      Hawkins,      an      American      celebrity      and      entrepreneur,      in      2005[18]. The      couple  \n \n  had      three      children,      Oliver,      Annabelle,      and      Everleigh[19]. Hawkins’      commitment      to      his      family      was      evident;\n \n \n  in      fact,      he      even      wrote   \n \na      song      for      his      middle      child,      Annabelle[9]."
            },
            {
              "id": 1208,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 54",
              "type": "chunk",
              "children": [],
              "content": "In      his      personal      life,      Hawkins      had      also      struggled      with      drug      use,      which      nearly      claimed      his      life      in   \n \na      2001  \n \n  overdose[9][7][4]."
            },
            {
              "id": 1209,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 55",
              "type": "chunk",
              "children": [],
              "content": "However,      he      managed      to      overcome      this      challenge,      and      later      expressed      gratitude      for  \n \n  the      experience      as   \n \na      lesson      that      allowed      him      to      realize      the      destructive      path      he      was      on[7]."
            },
            {
              "id": 1210,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 56",
              "type": "chunk",
              "children": [],
              "content": "Outside      of      his      main      role      in      the      Foo      Fighters,      Hawkins      also      pursued      various      side      projects      including      the  \n \n  Birds      of      Satan,      NHC,      and      Chevy      Metal. His      motivation      for      such      ventures      was   \n \na      constant      drive      to      create  \n \n  and      his      love      for      music[7]."
            },
            {
              "id": 1211,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 57",
              "type": "chunk",
              "children": [],
              "content": "Hawkins      was      also      known      for      his      unabashed      fanboy      nature,      often      vocalizing  \n \n  his      admiration      for      fellow      musicians      and      his      heroes[7]. #\n \n   Legacy      and      Impact Taylor      Hawkins      was      known      for      his      raw      and      authentic      drumming      style,      described      as      \"courageous,      damaged  \n \n  and      unflinchingly      authentic”[20]."
            },
            {
              "id": 1212,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 58",
              "type": "chunk",
              "children": [],
              "content": "His      work      with      the      Foo      Fighters,      as      well      as      his      various      collaborations n’      roll[10]. ‘ and      side      projects,      made      him   \n \na      celebrated      figure      in      rock  \n \n  Hawkins’      death      in      2022      was      met      with      heartfelt      tributes      from      colleagues      and      fans      around      the      world."
            },
            {
              "id": 1213,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 59",
              "type": "chunk",
              "children": [],
              "content": "Notable      tributes      came      from      rock      legends      like      Roger      Taylor      of      Queen,      who      considered      Hawkins      as   \n \na      kind,  \n \n  brilliant      man      and      an      inspirational      mentor,      likening      his      death      to      \"losing   \n \na      younger      favourite      brother”[21]."
            },
            {
              "id": 1214,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 60",
              "type": "chunk",
              "children": [],
              "content": "Similarly,      Led      Zeppelin’s      Jimmy      Page      admired      his      technique,      energy      and      spirited      enthusiasm[21]. An      LA      tribute      concert      held      in      his      honor      included      guest      drummers      like      Lars      Ulrich      of      Metallica,      Travis  \n \n  Barker      of      blink-182,      and      Brad      Wilk      of      Rage      Against      the      Machine."
            },
            {
              "id": 1215,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 61",
              "type": "chunk",
              "children": [],
              "content": "Singers      like      Miley      Cyrus      and      Alanis  \n \n  Morissette      also      performed      at      the      concert[22]. Apart      from      his      music,      Taylor      Hawkins      also      contributed      to      charities      Music      Support      and      MusiCares,      both      of  \n \n  which      were      chosen      by      the      Hawkins      family[23]."
            },
            {
              "id": 1216,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 62",
              "type": "chunk",
              "children": [],
              "content": "He      had      received      numerous      accolades      throughout      his      career,  \n \n  including      27      Grammy      nominations,      of      which      he      won      14[2]. In      2021,      the      Foo      Fighters      were      inducted      into  \n \n  the      Rock      and      Roll      Hall      of      Fame[9]."
            },
            {
              "id": 1217,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 63",
              "type": "chunk",
              "children": [],
              "content": "#\n \n   Discography  \n \n  Taylor      Hawkins      also      led   \n \na      notable      music      career      through      his      own      side      projects      and      collaborations[10]."
            },
            {
              "id": 1218,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 64",
              "type": "chunk",
              "children": [],
              "content": "Aside      from      his      work      with      the      Foo      Fighters,      Hawkins      formed      and      fronted      the      band      Taylor      Hawkins   \n \n&      The  \n \n  Coattail      Riders,   \n \na      project      which      originated      from      jamming      sessions      with      his      friend      Drew      Hester[10]."
            },
            {
              "id": 1219,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 65",
              "type": "chunk",
              "children": [],
              "content": "###      Taylor      Hawkins   \n \n&      The      Coattail      Riders  \n \n  Taylor      Hawkins   \n \n&      The      Coattail      Riders,   \n \na      band      formed      in      2004,      have      released      three      albums      and      their  \n \n  music      spans      genres      including      Hard      Rock,      Art      Rock,      and      Alternative      Rock[24][25][26]."
            },
            {
              "id": 1220,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 66",
              "type": "chunk",
              "children": [],
              "content": "The      band      grew      from  \n \n  an      initial      casual      jamming      session,      gradually      evolving      into   \n \na      more      formal      arrangement      that      led      to      the  \n \n  production      of      record      albums."
            },
            {
              "id": 1221,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 67",
              "type": "chunk",
              "children": [],
              "content": "Notably,      these      albums      featured      guest      appearances      by      renowned      musicians  \n \n  such      as      Dave      Grohl,      Queen’s      Brian      May      and      Roger      Taylor,      The      Cars’      Elliot      Easton,      Perry      Farrell,      and  \n \n  Jon      Davison,      who      is   \n \na      school      friend      of      Hawkins’[10]."
            },
            {
              "id": 1222,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 68",
              "type": "chunk",
              "children": [],
              "content": "###      Red      Light      Fever  \n \n  Red      Light      Fever,      released      on      April      19,      2010,      was      the      band’s      first      album[29][30]."
            },
            {
              "id": 1223,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 69",
              "type": "chunk",
              "children": [],
              "content": "Prior      to      its      release,  \n \n  Hawkins      revealed      in      an      interview      that      the      album      had      completed      the      recording      and      production      stages,      but  \n \n  its      title      and      release      date      were      yet      to      be      determined[29]."
            },
            {
              "id": 1224,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 70",
              "type": "chunk",
              "children": [],
              "content": "Red      Light      Fever      was      recorded      at      the      Foo  \n \n  Fighters’      Studio      606      in      California      and      featured      guest      musicians      such      as      Brian      May      and      Roger      Taylor      of  \n \n  Queen,      Dave      Grohl      of      Foo      Fighters,      and      Elliot      Easton      of      The      Cars[29][30]."
            },
            {
              "id": 1225,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 71",
              "type": "chunk",
              "children": [],
              "content": "##      Get      the      Money  \n \n  Get      the      Money,      the      third      album      from      Taylor      Hawkins   \n \n&      The      Coattail      Riders,      was      released      on      November      8,  \n \n  2019[29]. The      album’s      first      single,      \"Crossed      the      Line”,      released      on      October      15,      2019,      featured      Dave  \n \n  Grohl      and      Jon      Davison,      the      frontman      of      Yes[29]."
            },
            {
              "id": 1226,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 72",
              "type": "chunk",
              "children": [],
              "content": "The      music      video      for      the      single      \"I      Really      Blew      It”      also  \n \n  featured      appearances      from      Grohl      and      Perry      Farrel1[29]. #\n \n   Collaborations      and      Guest      Appearances  \n \n  Throughout      his      career,      Taylor      Hawkins      collaborated      with      various      prominent      artists      and      bands."
            },
            {
              "id": 1227,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 73",
              "type": "chunk",
              "children": [],
              "content": "The  \n \n  Coattail      Riders’      albums      notably      featured      appearances      from      luminaries      such      as      Brian      May      and      Roger      Taylor  \n \n  of      Queen,      Chrissie      Hynde,      Nancy      Wilson      of      Heart,      Sex      Pistol      Steve      Jones      and      James      Gang’s      Joe      Walsh[28]."
            },
            {
              "id": 1228,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 74",
              "type": "chunk",
              "children": [],
              "content": "Hawkins      also      fronted      another      group,      The      Birds      of      Satan,      which      evolved      from      his      heavy      rock      covers      band,  \n \n  Chevy      Metal[28]. Despite      his      diverse      musical      engagements,      Hawkins      always      maintained   \n \na      close      allegiance      with      the      Foo  \n \n  Fighters,      which      remained      the      center      of      his      music      life[7][28]."
            },
            {
              "id": 1229,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 75",
              "type": "chunk",
              "children": [],
              "content": "#\n \n   Tragic      Passing  \n \n  Taylor      Hawkins,      the      esteemed      drummer      of      the      alt-rock      band      Foo      Fighters,      passed      away      suddenly      on      March  \n \n  25,      2022,      while      on      tour      with      his      band      in      Bogota,      Colombia[34]."
            },
            {
              "id": 1230,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 76",
              "type": "chunk",
              "children": [],
              "content": "The      official      cause      of      death      was      cardiac  \n \n  arrest,      though      inquiries      were      raised      concerning      the      presence      of      drugs      in      his      system      and      their      potential  \n \n  contribution      to      his      death[33][34]."
            },
            {
              "id": 1231,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 77",
              "type": "chunk",
              "children": [],
              "content": "On      the      night      of      his      passing,      paramedics      were      called      to      the      Four  \n \n  Seasons      hotel      in      Bogota      due      to      reports      of      chest      pain      from      an      unnamed      guest,      later      revealed      to      be  \n \n  Hawkins[34]. Unfortunately,      resuscitation      efforts      were      unsuccessful,      and      Hawkins      was      declared      dead      at  \n \n  the      scene[34]."
            },
            {
              "id": 1232,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 78",
              "type": "chunk",
              "children": [],
              "content": "The      news      of      Hawkins’      sudden      demise      was      announced      on      the      morning      of      March      25th,      2022,      which      left      the      music  \n \n  world      in      shock[32]."
            },
            {
              "id": 1233,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 79",
              "type": "chunk",
              "children": [],
              "content": "The      band      confirmed      the      news      with   \n \na      short      statement,      expressing      their      devastation  \n \n  at      the      loss      of      Hawkins,      whose      \"musical      spirit      and      infectious      laughter”      would      live      on      forever[32]. As   \n \na      result      of      Hawkins’      untimely      passing,      the      band      canceled      their      ongoing      South      American      tour[33]."
            },
            {
              "id": 1234,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 80",
              "type": "chunk",
              "children": [],
              "content": "The  \n \n  festival      stage      at      the      Estéreo      Picnic      Festival,      where      the      Foo      Fighters      were      scheduled      to      perform      that  \n \n  night,      was      transformed      into   \n \na      candlelight      vigil      in      memory      of      Hawkins[33]."
            },
            {
              "id": 1235,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 81",
              "type": "chunk",
              "children": [],
              "content": "##      Tributes      and      Remembrances  \n \n  In      the      wake      of      Hawkins’      death,      tributes      from      fans      and      colleagues      alike      poured      in      from      around      the  \n \n  world[21][31]."
            },
            {
              "id": 1236,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 82",
              "type": "chunk",
              "children": [],
              "content": "Among      the      many      paying      their      respects      were      legendary      rock      and      roll      musicians      like      Roger  \n \n  Taylor,      the      drummer      of      Queen,      who      Hawkins      credited      with      inspiring      his      own      career      behind      the      drum      set[21]."
            },
            {
              "id": 1237,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 83",
              "type": "chunk",
              "children": [],
              "content": "In      heartfelt      social      media      posts,      Taylor      described      Hawkins      as      an      \"inspirational      mentor”      and   \n \na      \"kind  \n \n  brilliant      man\"[21],      while      Led      Zeppelin’s      Jimmy      Page      reminisced      about      sharing      the      stage      with      Hawkins  \n \n  and      praised      his      \"technique,      energy      and      spirited      enthusiasm\"[21]."
            },
            {
              "id": 1238,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 84",
              "type": "chunk",
              "children": [],
              "content": "There      were      also      numerous      onstage      tributes      to      Hawkins. Notably,      Miley      Cyrus      expressed      her      grief      and      sent  \n \n  peaceful      wishes      to      the      Foo      Fighters      and      the      Hawkins      family      during   \n \na      performance      at      Lollapalooza[31]."
            },
            {
              "id": 1239,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 85",
              "type": "chunk",
              "children": [],
              "content": "Similarly,      Liam      Gallagher      of      Oasis      dedicated      one      of      the      band’s      biggest      hits      to      Hawkins      during   \n \na      concert  \n \n  at      the      Royal      Albert      Hall      in      London[31]. Fans      gathered      outside      the      hotel      where      Hawkins      died,      lighting      candles,      leaving      flowers,      and      singing      the  \n \n  band’s      songs      in      his      honor[31]."
            },
            {
              "id": 1240,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 86",
              "type": "chunk",
              "children": [],
              "content": "Hawkins’      life      and      career      were      celebrated      in   \n \na      star-studded      tribute      concert      in      Los      Angeles,      which      saw  \n \n  performances      from      over      5@      musicians,      including      his      former      bands      and      colleagues      from      Def      Leppard,      Queen,  \n \n  and      Foo      Fighters[22]. Table      12:      STORM’s      generated      article      for      “Taylor      Hawkins”."
            },
            {
              "id": 1241,
              "title": "      (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 87",
              "type": "chunk",
              "children": [],
              "content": "“#’,      “##”      indicate      the      section      title      and      subsection      title  \n \n  respectively. Numbers      in      brackets      indicate      the      cited      references."
            }
          ],
          "content": ""
        }
      ],
      "content": ""
    }
  ],
  "content": ""
}