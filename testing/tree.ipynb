{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.llmsherpa import LLMSherpaFileLoader\n",
    "loader = LLMSherpaFileLoader(\n",
    "    file_path=\"/home/ubuntu/T_RAG/testing/2407.13734v1.pdf\",\n",
    "    new_indent_parser=True,\n",
    "    apply_ocr=True,\n",
    "    strategy=\"html\",\n",
    "    llmsherpa_api_url=\"http://localhost:5010/api/parseDocument?renderFormat=all\",\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '/home/ubuntu/T_RAG/testing/2407.13734v1.pdf'}, page_content='<html><h1>Understanding      Reinforcement      Learning-Based</h1><h1>Fine-Tuning      of      Diffusion      Models:      A      Tutorial      and      Review</h1><p>Masatoshi      Uehara*!,      Yulai      Zhao\\\\\\\\\\\\\\'*,      Tommaso      Biancalani!,      and      Sergey      Levine?</p><p>‘Genentech</p><li>*Princeton      University</li><p>>University      of      California,      Berkeley</p><h2>July      19,      2024</h2><h2>Abstract</h2><p> \\n \\n  This      tutorial      provides   \\n \\na      comprehensive      survey      of      methods      for      fine-tuning      diffusion      models  \\n \\n  to      optimize      downstream      reward      functions.\\nWhile      diffusion      models      are      widely      known      to      provide  \\n \\n  excellent      generative      modeling      capability,      practical      applications      in      domains      such      as      biology  \\n \\n  require      generating      samples      that      maximize      some      desired      metric      (e.g.,      translation      efficiency      in  \\n \\n  RNA,      docking      score      in      molecules,      stability      in      protein).\\nIn      these      cases,      the      diffusion      model      can  \\n \\n  be      optimized      not      only      to      generate      realistic      samples      but      also      to      maximize      the      measure      of      interest  \\n \\n  explicitly.\\nSuch      methods      are      based      on      concepts      from      reinforcement      learning      (RL).\\nWe      explain  \\n \\n  the      application      of      various      RL      algorithms,      including      PPO,      differentiable      optimization,      reward-  \\n \\n  weighted      MLE,      value-weighted      sampling,      and      path      consistency      learning,      tailored      specifically      for  \\n \\n  fine-tuning      diffusion      models.\\nWe      aim      to      explore      fundamental      aspects      such      as      the      strengths      and  \\n \\n  limitations      of      different      RL-based      fine-tuning      algorithms      across      various      scenarios,      the      benefits  \\n \\n  of      RL-based      fine-tuning      compared      to      non-RL-based      approaches,      and      the      formal      objectives      of  \\n \\n  RL-based      fine-tuning      (target      distributions).\\nAdditionally,      we      aim      to      examine      their      connections  \\n \\n  with      related      topics      such      as      classifier      guidance,      Gflownets,      flow-based      diffusion      models,      path  \\n \\n  integral      control      theory,      and      sampling      from      unnormalized      distributions      such      as      MCMC.\\nThe  \\n \\n  code      of      this      tutorial      is      available      at      https://github.com/masa-ue/RLfinetuning_Diffusion_Bioseq.</p><p>arXiv:2407.13734v1      [cs.LG]      18      Jul      2024</p><h2>July      19,      2024</h2><h2>Abstract</h2><p> \\n \\n  This      tutorial      provides   \\n \\na      comprehensive      survey      of      methods      for      fine-tuning      diffusion      models  \\n \\n  to      optimize      downstream      reward      functions.\\nWhile      diffusion      models      are      widely      known      to      provide  \\n \\n  excellent      generative      modeling      capability,      practical      applications      in      domains      such      as      biology  \\n \\n  require      generating      samples      that      maximize      some      desired      metric      (e.g.,      translation      efficiency      in  \\n \\n  RNA,      docking      score      in      molecules,      stability      in      protein).\\nIn      these      cases,      the      diffusion      model      can  \\n \\n  be      optimized      not      only      to      generate      realistic      samples      but      also      to      maximize      the      measure      of      interest  \\n \\n  explicitly.\\nSuch      methods      are      based      on      concepts      from      reinforcement      learning      (RL).\\nWe      explain  \\n \\n  the      application      of      various      RL      algorithms,      including      PPO,      differentiable      optimization,      reward-  \\n \\n  weighted      MLE,      value-weighted      sampling,      and      path      consistency      learning,      tailored      specifically      for  \\n \\n  fine-tuning      diffusion      models.\\nWe      aim      to      explore      fundamental      aspects      such      as      the      strengths      and  \\n \\n  limitations      of      different      RL-based      fine-tuning      algorithms      across      various      scenarios,      the      benefits  \\n \\n  of      RL-based      fine-tuning      compared      to      non-RL-based      approaches,      and      the      formal      objectives      of  \\n \\n  RL-based      fine-tuning      (target      distributions).\\nAdditionally,      we      aim      to      examine      their      connections  \\n \\n  with      related      topics      such      as      classifier      guidance,      Gflownets,      flow-based      diffusion      models,      path  \\n \\n  integral      control      theory,      and      sampling      from      unnormalized      distributions      such      as      MCMC.\\nThe  \\n \\n  code      of      this      tutorial      is      available      at      https://github.com/masa-ue/RLfinetuning_Diffusion_Bioseq.</p><p>arXiv:2407.13734v1      [cs.LG]      18      Jul      2024</p><h1>Introduction</h1><p> \\n \\n  Diffusion      models      (Sohl-Dickstein      et      al.,      2015;      Ho      et      al.,      2020;      Song      et      al.,      2020)      are      widely      rec-  \\n \\n  ognized      as      powerful      tools      for      generative      modeling.\\nThey      are      able      to      accurately      model      complex  \\n \\n  distributions      by      closely      emulating      the      characteristics      of      the      training      data.\\nThere      are      many      applica-  \\n \\n  tions      of      diffusion      models      in      various      fields,      including      computer      vision      (Podell      et      al.,      2023),      natural  \\n \\n  language      processing      (Austin      et      al.,      2021),      biology      (Avdeyev      et      al.,      2023;      Stark      et      al.,      2024;      Li      et      al., *“uehara.masatoshi@gene.com  \\n \\n  tyulaiz@princeton.\\nedu.\\nEqual      contribution.</p><p> \\n \\n  Reward      models  \\n \\n  Images   \\n \\n—      Aesthetic      score   \\n \\n|      Images   \\n \\n—      Aesthetic      score      Images   \\n \\n—      Aesthetic      score  \\n \\n  Molecules   \\n \\n~      QED      Molecules   \\n \\n—      QED      Molecules   \\n \\n—      QED  \\n \\n  DNAs   \\n \\n=      Cell-specificity   \\n \\n[      DNAs   \\n \\n=      Cell-specificity      DNAs   \\n \\n—      Cell-specificity freely      f      rel      f      rel      ainsoms.</p><p>es   \\n \\n(   \\n \\n_   \\n \\n|   \\n \\n) Images      with      high  \\n \\n  aesthetic      score</p><h2>Images</h2><p> \\n \\n \\ng   \\n \\n|   \\n \\na      ge   \\n \\nz       evi      do}      f      g,   \\n \\n2      3°      DNAs      with      high  \\n \\n  aor   \\n \\na   \\n \\n-      e      e       DNAs      io      si   \\n \\ni      og   \\n \\n+   \\n \\na      i:      cell-specificity</p><p>Figure      1:      Illustrative      examples      of      RL-based      fine-tuning,      aimed      at      optimizing      pre-trained      diffusion  \\n \\n  models      to      maximize      downstream      reward      functions.</p><p> \\n \\n  2023),      chemistry      (Jo      et      al.,      2022;      Xu      et      al.,      2022;      Hoogeboom      et      al.,      2022),      and      biology      (Avdeyev  \\n \\n  et      al.,      2023;      Stark      et      al.,      2024;      Campbell      et      al.,      2024).</p><p> \\n \\n  While      diffusion      models      exhibit      significant      power      in      capturing      the      training      data      distribution,  \\n \\n  there’s      often   \\n \\na      need      to      customize      these      models      for      particular      downstream      reward      functions.\\nFor  \\n \\n  instance,      in      computer      vision,      Stable      Diffusion      (Rombach      et      al.,      2022)      serves      as   \\n \\na      strong      backbone  \\n \\n  pre-trained      model.\\nHowever,      we      may      want      to      fine-tune      it      further      by      optimizing      downstream      reward  \\n \\n  functions      such      as      aesthetic      scores      or      human-alignment      scores      (Black      et      al.,      2023;      Fan      et      al.,      2023).\\n \\n \\n  Similarly,      in      fields      such      as      biology      and      chemistry,      various      sophisticated      diffusion      models      have  \\n \\n  been      developed      for      DNA,      RNA,      protein      sequences,      and      molecules,      effectively      modeling      biological  \\n \\n  and      chemical      spaces.\\nNonetheless,      biologists      and      chemists      typically      aim      to      optimize      specific  \\n \\n  downstream      objectives      such      as      cell-specific      expression      in      DNA      sequences      (Gosai      et      al.,      2023;      Lal  \\n \\n  et      al.,      2024;      Sarkar      et      al.,      2024),      translational      efficiency/stability      of      RNA      sequences      (Castillo-Hair  \\n \\n  and      Seelig,      2021;      Agarwal      and      Kelley,      2022),      stability/bioactivity      of      protein      sequence      (Frey      et      al.,  \\n \\n  2023;      Widatalla      et      al.,      2024)      or      QED/SA      scores      of      molecules      (Zhou      et      al.,      2019).</p><p> \\n \\n  To      achieve      this      goal,      numerous      algorithms      have      been      proposed      for      fine-tuning      diffusion      models  \\n \\n  via      reinforcement      learning      (RL)      (e.g.,      Black      et      al.      (2023);      Fan      et      al.\\n(2023);      Clark      et      al.\\n(2023);  \\n \\n  Prabhudesai      et      al.\\n(2023);      Uehara      et      al.\\n(2024)),      aiming      to      optimize      downstream      reward      functions.\\n \\n \\n  RL      is   \\n \\na      machine      learning      paradigm      where      agents      learn      to      make      sequential      decisions      to      maximize  \\n \\n  reward      signals      (Sutton      and      Barto,      2018;      Agarwal      et      al.,      2019).\\nIn      our      context,      RL      naturally      emerges  \\n \\n  as   \\n \\na      Suitable      approach      due      to      the      sequential      structure      inherent      in      diffusion      models,      where      each      time  \\n \\n  step      involves   \\n \\na      “decision”      corresponding      to      how      the      sample      is      denoised      at      that      step.\\nThis      tutorial  \\n \\n  aims      to      review      recent      works      for      readers      interested      in      understanding      the      fundamentals      of      RL-based  \\n \\n  fine-tuning      from   \\n \\na      holistic      perspective,      including      the      advantages      of      RL-based      fine-tuning      over  \\n \\n  non-RL      approaches,      the      pros      and      cons      of      different      RL-based      fine-tuning      algorithms,      the      formalized  \\n \\n  goal      of      RL-based      fine-tuning,      and      its      connections      with      related      topics      such      as      classifier      guidance.</p><p> \\n \\n  The      content      of      this      tutorial      is      primarily      divided      into      three      parts.\\nIn      addition,      as      an      implementation  \\n \\n  example,      we      also      release      the      code      that      employs      RL-based      fine-tuning      for      guided      biological      sequences  \\n \\n  (DNA/RNA)      generation      at      https://github.com/masa-ue/RLfinetuning_Diffusion_Bioseq.</p><li>1.      We      aim      to      provide   \\n \\na      comprehensive      overview      of      current      algorithms.\\nNotably,      given      the  \\n \\n  sequential      nature      of      diffusion      models,      we      can      naturally      frame      fine-tuning      as   \\n \\na      reinforcement  \\n \\n  learning      (RL)      problem      within      Markov      Decision      Processes      (MDPs),      as      detailed      in      Section   \\n \\n3       and      4.\\nTherefore,      we      can      employ      any      off-the-shelf      RL      algorithms      such      as      PPO      (Schulman  \\n \\n  et      al.,      2017),      differentiable      optimization      (direct      reward      backpropagation),      weighted      MLE  \\n \\n  (Peters      et      al.,      2010;      Peng      et      al.,      2019),      value-weighted      sampling      (close      to      classifier      guidance  \\n \\n  in      Dhariwal      and      Nichol      (2021)),      and      path      consistency      learning      (Nachum      et      al.,      2017).\\nWe  \\n \\n  discuss      these      algorithms      in      detail      in      Section      4.2      and      6.\\nInstead      of      merely      outlining      each  \\n \\n  algorithm,      we      aim      to      present      both      their      advantages      and      disadvantages      so      readers      can      select  \\n \\n  the      most      suitable      algorithms      for      their      specific      purposes.</li><li>2.      We      categorize      various      fine-tuning      scenarios      based      on      how      reward      feedback      is      acquired      in  \\n \\n  Section      7.\\nThis      distinction      is      pivotal      for      practical      algorithm      design.\\nFor      example,      if      we      can  \\n \\n  access      accurate      reward      functions,      computational      efficiency      would      become      our      primary      focus.\\n \\n \\n  However,      in      cases      where      reward      functions      are      unknown,      it      is      essential      to      learn      them      from  \\n \\n  data      with      reward      feedback,      leading      us      to      take      feedback      efficiency      and      distributional      shift      into  \\n \\n  consideration      as      well.\\nSpecifically,      when      reward      functions      need      to      be      learned      from      static  \\n \\n  offline      data      without      any      online      interactions,      we      must      address      the      issue      of      overoptimization,  \\n \\n  where      fine-tuned      models      are      misled      by      out-of-distribution      samples,      and      generate      samples  \\n \\n  with      low      genuine      rewards.\\nThis      is      crucial      because,      in      an      offline      scenario,      the      coverage      of  \\n \\n  offline      data      distribution      with      feedback      is      limited;      hence,      the      out-of-distribution      region      could  \\n \\n  be      extensive      (Uehara      et      al.,      2024).</li><li>3.      We      provide   \\n \\na      detailed      discussion      on      the      relationship      between      RL-based      fine-tuning      methods  \\n \\n  and      closely      related      methods      in      the      literature,      such      as      classifier      guidance      (Dhariwal      and      Nichol,  \\n \\n  2021)      in      Section      8,      flow-based      diffusion      models      (Liu      et      al.,      2022;      Lipman      et      al.,      2023;      Tong  \\n \\n  et      al.,      2023)      in      Section      9,      sampling      from      unnormalized      distributions      (Zhang      and      Chen,      2021)  \\n \\n  in      Section      10,      Gflownets      (Bengio      et      al.,      2023)      in      Section      6.3,      and      path      integral      control      theory  \\n \\n  (Theodorou      et      al.,      2010;      Williams      et      al.,      2017;      Kazim      et      al.,      2024)      in      Section      6.2.3.      We  \\n \\n  summarize      the      key      messages      as      follows.</li><p>¢\\n \\n   Section      6.3:      The      losses      used      in      Gflownets      are      fundamentally      equivalent      to      those      derived  \\n \\n  from   \\n \\na      specific      RL      algorithm      called      path      consistency      learning.</p><p> \\n \\n \\n¢      Section      8:      Classifier      guidance      employed      in      conditional      generation      is      regarded      as   \\n \\na       specific      RL-based      fine-tuning      method,      which      we      call      value-weighted      sampling.\\nAs  \\n \\n  formalized      in      Zhao      et      al.\\n(2024),      this      observation      indicates      that      any      off-the-shelf      RL-  \\n \\n  based      fine-tuning      algorithms      (e.g.,      PPO      and      differentiable      optimization)      can      be      applied  \\n \\n  to      conditional      generation.</p><p> \\n \\n \\n¢      Section      10:      Sampling      from      unnormalized      distributions,      often      referred      to      as      Gibbs  \\n \\n  distributions,      is      an      important      and      challenging      problem      in      diverse      domains.\\nWhile  \\n \\n  MCMC      methods      are      traditionally      used      for      this      task,      recognizing      its      similarity      to      the  \\n \\n  objectives      of      RL-based      fine-tuning      suggests      that      off-the-shelf      RL      algorithms      can      also  \\n \\n  effectively      address      the      challenge      of      sampling      from      unnormalized      distributions.</p><h2>Preliminaries</h2><li>1.1 DiffusionModels 2 2.20.\\n00 2 ee ee</li><table><th><td colSpan=1>1.1.1 Score-Based Diffusion Models (Optional)</td><td colSpan=1></td><td colSpan=1></td></th><tr><td colSpan=1>1.2 Fine-Tuning Diffusion Models withRL</td><td colSpan=1>0.00.0.</td><td colSpan=1></td></tr><tr><td colSpan=1>1.2.1. Brief Overview: Fine-tuning withRL 1.2.2 Motivation for Using RL over Non-RL Alternatives.</td><td colSpan=1></td><td colSpan=1>2.</td></tr><tr><td>Brief      Overview      of      Entropy-Regularized      MDPs</td></tr><tr><td>Fine-Tuning      Diffusion      Models      with      RL      in      Entropy      Regularized      MDPs</td></tr><tr><td>Theory      of      RL-Based      Fine-Tuning</td></tr></table><h2>RL-Based      Fine-Tuning      Algorithms      1:      Non-Distribution-Constrained      Approaches</h2><h2>RL-Based      Fine-Tuning      Algorithms      2:      Distribution-Constrained      Approaches</h2><h2>Fine-Tuning      Settings      Taxonomy</h2><h2>Connection      with      Classifier      Guidance</h2><p> \\n \\n  Connection      with      Flow-Based      Diffusion      Models  \\n \\n  10  \\n \\n  10  \\n \\n  11 12  \\n \\n  14  \\n \\n  14  \\n \\n  15  \\n \\n  16  \\n \\n  16  \\n \\n  17  \\n \\n  18  \\n \\n  19  \\n \\n  21  \\n \\n  21  \\n \\n  23  \\n \\n  24  \\n \\n  25  \\n \\n  25  \\n \\n  28  \\n \\n  28  \\n \\n  28</p><p>10      Connection      with      Sampling      from      Unnormalized      Distributions      35 10.1 Markov Chain Monte Carlo(MCMC) 2.00000 eee ene 35 10.2 RL-Based Approaches 0 2.0.0 pee ee 36</p><p>11      Closely      Related      Directions      36</p><p>12      Summary      37</p><p>1\\n \\n   Preliminaries</p><p>In      this      section,      we      outline      the      fundamentals      of      diffusion      models      and      elucidate      the      objective      of  \\n \\n  fine-tuning      them.</p><h3>1.1.      Diffusion      Models</h3><p>dels  \\n \\n  We      present      an      overview      of      denoising      diffusion      probabilistic      models      (DDPM)      (Ho      et      al.,      2020).\\n \\n \\n  For      more      details,      refer      to      Yang      et      al.\\n(2023);      Cao      et      al.\\n(2024);      Chen      et      al.\\n(2024);      Tang      and      Zhao  \\n \\n  (2024).</p><p> \\n \\n  In      diffusion      models,      the      objective      is      to      develop   \\n \\na      deep      generative      model      that      accurately      captures  \\n \\n  the      true      data      distribution.\\nSpecifically,      denoting      the      data      distribution      by      pr.\\n \\n \\n€      A(4’)      where   \\n \\n¥      is      an  \\n \\n  input      space,   \\n \\na      DDPM      aims      to      approximate      p,,.\\nusing   \\n \\na      parametric      model      structured      as 1 p(xo3      8)   \\n \\n=   \\n \\n[      rlco      O)dx1.r,      where      p(%o:7;      9)   \\n \\n=      pr+i(@7;      9)      [[      peter:      0).</p><p>t=T When   \\n \\n4      is      an      Euclidean      space      (in      R®),      the      forward      process      is      modeled      as      the      following      dynamics:</p><p>pra(er)      =N(0,1),      pe(ve-r)2;      9)   \\n \\n=      N(p(a1,      t;      8),      07      (4)   \\n \\nx      D),  \\n \\n  where      N(-,-)      denotes   \\n \\na      normal      distribution,   \\n \\nJ      is      an      identity      matrix      and      p   \\n \\n:      R¢   \\n \\nx      [0,7]   \\n \\n>      R¢.\\nIn  \\n \\n  DDPMs,      we      aim      to      obtain   \\n \\na      set      of      policies      (i.e.,      denoising      process)      {p;}{_741,      pr:   \\n \\n&   \\n \\n—      A(A&)      such  \\n \\n  that      p(x;      0)   \\n \\n©      Ppre(#o).\\nIndeed,      by      optimizing      the      variational      bound      on      the      negative      log-likelihood,  \\n \\n  we      can      derive      such   \\n \\na      set      of      policies.\\nFor      more      details,      refer      to      Section      1.1.1.  \\n \\n  Hereafter,      we      consider   \\n \\na      situation      where      we      have   \\n \\na      pre-trained      diffusion      model      that      is      already  \\n \\n  trained      on   \\n \\na      large      dataset,      such      that      the      model      can      accurately      capture      the      underlying      data      distribution.</p><p> \\n \\n  pre We      refer      to      the      pre-trained      policies      as      {p?\\n\"*(-|-)}{_7,,,      and      to      the      marginal      distribution      at   \\n \\nt   \\n \\n=   \\n \\n0       induced      by      the      pre-trained      diffusion      model      as      pp.\\nIn      other      words, 1\\n \\n    pr      (x0)   \\n \\n=      [iler)      [[°@eledderr.</p><p>t=T  \\n \\n  Remark   \\n \\n1      (Non-Euclidean      space).\\nFor      simplicity,      we      typically      assume      that      the      domain      space      is  \\n \\n  Euclidean.\\nHowever,      we      can      easily      extend      most      of      the      discussion      to   \\n \\na      more      general      space,      such      as  \\n \\n  a      Riemannian      manifold      (De      Bortoli      et      al.,      2022)      or      discrete      space      (Austin      et      al.,      2021;      Campbell  \\n \\n  et      al.,      2022;      Benton      et      al.,      2024;      Lou      et      al.,      2023).</p><p> \\n \\n  Remark   \\n \\n2      (Conditional      generative      models).\\nPre-trained      models      can      be      conditional      diffusion      models,  \\n \\n  such      as      text-to-image      diffusion      models      (Ramesh      et      al.,      2022).\\nThe      extension      is      straightforward:  \\n \\n  augmenting      the      input      spaces      of      policies      with      an      additional      space      on      which      we      want      to      condition.\\n \\n \\n  More      specifically,      by      denoting      that      space      by   \\n \\nc   \\n \\n€      C,      each      policy      becomes      p;(x¢|%1,      650)   \\n \\n:   \\n \\n&   \\n \\nx   \\n \\nC   \\n \\n>       A(X).</p><p> \\n \\n  Remark   \\n \\n3      (Extension      to      Continuous-time      diffusion      models).\\nJn      this      tutorial,      our      discussion      on  \\n \\n  fine-tuning      diffusion      models      will      be      primarily      formulated      on      the      discrete-time      formulation,      as      we      did  \\n \\n  above      Nonetheless,      much      of      our      discussion      is      also      applicable      to      continuous-time      diffusion      models,  \\n \\n  as      formalized      in      Uehara      et      al.\\n(2024)</p><h4>1.1.1      Score-Based      Diffusion      Models      (Optional)</h4><p>nal)  \\n \\n  We      briefly      discuss      how      to      train      diffusion      models      in   \\n \\na      continuous-time      framework      (Song      et      al.,  \\n \\n  2021).\\nIn      our      tutorial,      this      section      is      mainly      used      to      discuss      several      algorithms      (e.g.,      value-weighted  \\n \\n  sampling      in      Section      6.2)      and      their      relationship      with      flow-based      diffusion      models      (Section      9)      later.</p><p> \\n \\n  Therefore,      readers      may      skip      it      based      on      their      individual      needs.</p><p> \\n \\n  The      training      process      of      diffusion      models      can      be      summarized      as      follows.\\nOur      objective      is      to      train  \\n \\n  a      sequential      mapping      from   \\n \\na      known      noise      distribution      to   \\n \\na      data      distribution,      formalized      through  \\n \\n  a      stochastic      differential      equation      (SDE).\\nFirstly,      we      define   \\n \\na      forward      (fixed)      SDE      that      maps      from  \\n \\n  the      data      distribution      to      the      noise      distribution.\\nThen,   \\n \\na      time-reversal      SDE      is      expressed      as      an      SDE,  \\n \\n  which      includes      the      (unknown)      score      function.\\nNow,      by      learning      this      unknown      score      function      from  \\n \\n  the      training      data,      the      time-reversal      SDE      can      be      utilized      as   \\n \\na      generative      model.\\nHere      are      the      details.</p><p>Forward      and      time-reversal      SDE.\\nFirstly,      we      introduce   \\n \\na      forward      (a.k.a.,      reference)      Stochastic  \\n \\n  differential      equations      (SDE)      from   \\n \\n0      to      7’.\\n \\n \\nA      common      choice      is   \\n \\na      variance-preserving      (VP)      process:</p><p>t\\n \\n   €\\n \\n   [0,7];      dx,   \\n \\n=      —0.52,dt   \\n \\n+      du;,,      x9   \\n \\n~      p(x),      (1)</p><p>where      dw;      represents      standard      Brownian      motion.\\nHere      are      two      crucial      observations:</p><p>¢\\n \\n   As   \\n \\nT      approaches      oo,      the      limiting      distribution      is      \\\\\\\\\\\\\\\\V(0,      I).</p><p>¢\\n \\n   The      time-reversal      SDE      (Anderson,      1982),      which      preserves      the      marginal      distribution,      is  \\n \\n  expressed      as      follows:</p><p>t\\n \\n   €\\n \\n   [0,7];      dz,   \\n \\n=      [0.5z,   \\n \\n+      V      log      qr_s(%)|dt   \\n \\n+      du,.\\n(2) Here,      g,   \\n \\n€      A(R“)      denotes      the      marginal      distribution      at      time   \\n \\nt      induced      by      the      reference      SDE (1).\\nNotably,      the      marginal      distribution      of      zr_;      is      the      same      as      the      one      of      x;      induced      by      the  \\n \\n  reference      SDE.</p><p> \\n \\n  These      observations      suggest      that      with      sufficiently      large      T,      starting      from      V      (0,      I)      and      following      the  \\n \\n  time-reversal      SDE      (2)   \\n \\n,      we      are      able      to      sample      from      the      data      distribution      (i.e.,      p?\\n\"°)      at      the      terminal  \\n \\n  time      point      7’.</p><p>Training      score      functions      from      the      training      data.\\nNow,      the      remaining      question      is      how      to      estimate  \\n \\n  the      marginal      score      function      V      log      q(-)      in      (2).\\nThis      can      be      computed      using      the      following      principles:</p><p>°\\n \\n   We      have      V      log      (a+)   \\n \\n=      Exgxppre[Va,      log      qyo(ae   \\n \\n|      2o)],      where      q@jo      represents      the      conditional  \\n \\n  distribution      of      x;      given      Xo      induced      by      the      reference      SDE.</p><p>¢\\n \\n   The      conditional      score      function      V,,,      log      dejo      (Xe   \\n \\n|      xo)      can      be      analytically      derived      as  \\n \\n  °</p><h5>      Myo      —      Ut</h5><p> \\n \\n  {ory      ue   \\n \\n=      exp(—0.5t)xo,      {o?\\n}?\\n \\n \\n=   \\n \\n1   \\n \\n—      exp(—0.5t).\\n(3) t Subsequently,      by      defining   \\n \\na      parameterized      model      sg   \\n \\n:      R¢   \\n \\nx      [0,7]   \\n \\n>      R¢      and      utilizing      the      following weighted      regression,      the      marginal      score      is      estimated:</p><p>A\\n \\n \\n  Dore   \\n \\n=      argmin      Eywuni((0,7))      ce~aejo(w+|ar0),c0~pPr®      [A(t)      ||      Ver,      log      jo      (Le   \\n \\n|      ro)   \\n \\n_      So(£t,      t)      17)      (4)</p><p> \\n \\n  where   \\n \\n2   \\n \\n:      [0,7]   \\n \\n+      R      is   \\n \\na      weighted      function.\\n \\n \\n  Note      we      typically      use      another      parametrization      well.\\nFrom      (3),      we      can      easily      see      that      this      score      is  \\n \\n  estimated      by      introducing      networks      €9   \\n \\n:      R@   \\n \\nx      [0,7]   \\n \\n—      R®,      which      aims      to      estimate      the      noise      Bere      eO</p><p>Ore   \\n \\n=      argmin      Ey.\\nuni({0,7]),cr~p2xo+o?\\ne~N      (0,1)      ,co~pPte      [A(t)/{op}?\\nlle   \\n \\n—      €9      (1,      t)      7)   \\n \\n:      (5)</p><p>Then,      by      denoting      the      training      data      as      {a}      and      setting      A(t)   \\n \\n=      {a7      }”,      the      actual      loss      function      from the      training      data      is argmin   \\n \\n)      Ey      ai(o.r))a   \\n \\nO      peal      +e02,e~N      (0,1)      [Ile   \\n \\n—      €o(ae?,      t)      [17].\\n(6) i=1 Inference      time.\\nOnce      this      Bore      (or      Ore)      is      learned,      we      insert      it      into      the      time-reversal      SDE      (2)      and use      it      as   \\n \\na      generative      model.\\nUltimately,      with      standard      discretization,      we      obtain:</p><p>p(      xe,      t;      a)   \\n \\n=      24+      (0.54,   \\n \\n+      Sire      (,,T      —t)|(ot),      {o?(t)}?\\n \\n \\n=      (6t).</p><p>Equivalently,      this      is (21,      t;      Ore)   \\n \\n=   \\n \\n2   \\n \\n+      [0.52   \\n \\n—      1/o?\\n \\n \\nx      e%,,,(a1,T   \\n \\n—      t)]|      (6t),      (7)</p><p>where      (dt)      denotes      the      discretization      step.</p><p> \\n \\n  Equivalence      to      DDPM.\\nThe      objective      function      derived      here      is      equivalent      to      the      one      formulated  \\n \\n  based      on      variational      inference      in      the      discretized      formulation,      which      is      commonly      referred      to      as  \\n \\n  DDPMs      (Ho      et      al.,      2020).\\nIn      DDPMs      (Ho      et      al.,      2020),      we      often      see      the      following      form:</p><p> \\n \\n \\n1      1-      (ey;</p><p> \\n \\n  it;      0)   \\n \\n=      ~~   \\n \\n’      ),  \\n \\n  p(xy   \\n \\n)      Xt      Jina      ae      [om</p><h6>Vor</h6><p>When      a;   \\n \\n=   \\n \\n1   \\n \\n—      (dt),      the      above      is      equivalent      to      (7)      when      dt      goes      to   \\n \\n0      by      noting      1/,/a;   \\n \\n©      1+0.5(d¢).</p><h3>1.2      Fine-Tuning      Diffusion      Models      with      RL</h3><p>RL  \\n \\n  Importantly,      our      focus      on      RL-based      fine-tuning      distinguishes      itself      from      the      standard      fine-tuning  \\n \\n  methods.\\nStandard      fine-tuning      typically      involves      scenarios      where      we      have      pre-trained      models  \\n \\n  (e.g.,      diffusion      models)      and      new      training      data      {2,      y}.\\nIn      such      cases,      the      common      approach  \\n \\n  for      fine-tuning      is      to      retrain      diffusion      models      with      the      new      training      data      using      the      same      loss  \\n \\n  function      employed      during      pre-training.\\nIn      sharp      contrast,      RL-based      fine-tuning      directly      employs  \\n \\n  the      downstream      reward      functions      as      the      primary      optimization      objectives,      making      the      loss      functions  \\n \\n  different      from      those      used      in      pre-training.</p><p> \\n \\n  Hereafter,      we      start      with   \\n \\na      concise      overview      of      RL-based      fine-tuning.\\nThen,      before      delving      into  \\n \\n  specifics,      we      discuss      simpler      non-RL      alternatives      to      provide      motivation      for      adopting      RL-based  \\n \\n  fine-tuning.</p><li>1.2.1      Brief      Overview:      Fine-tuning      with      RL</li><p>In      this      article,      we      explore      the      fine-tuning      of      pre-trained      diffusion      models      to      optimize      downstream  \\n \\n  reward      functions      r   \\n \\n:      R¢   \\n \\n+      R.      In      domains      such      as      images,      these      backbone      diffusion      models      to      be  \\n \\n  fine-tuned      include      Stable      Diffusion      (Rombach      et      al.,      2022),      while      the      reward      functions      are      aesthetic  \\n \\n  scores      and      alignment      scores      (Clark      et      al.,      2023;      Black      et      al.,      2023;      Fan      et      al.,      2023).\\nMore      examples  \\n \\n  are      detailed      in      the      introduction.\\nThese      rewards      are      often      unknown,      necessitating      learning      from  \\n \\n  data      with      feedback:      {2      r(x™)}.\\nWe      will      explore      this      aspect      further      in      Section      7.\\nUntil      then,      we  \\n \\n  assume      r      is      known.\\n \\n \\n  Now,      readers      may      wonder      about      the      objectives      we      aim      to      achieve      during      the      fine-tuning      process.\\n \\n \\n  A      natural      approach      is      to      define      the      optimization      problem:</p><p>argmax      E,.\\n[r()|      (8)  \\n \\n  qeA(¥)  \\n \\n  where   \\n \\nq      is      initialized      with   \\n \\na      pre-trained      diffusion      model      p?\\n*®   \\n \\n€      A(2).\\nIn      this      tutorial,      we      will      detail  \\n \\n  the      procedure      of      solving      (8)      with      RL      in      the      upcoming      sections.\\nIn      essence,      we      leverage      the      fact  \\n \\n  that      diffusion      models      are      formulated      as   \\n \\na      sequential      decision-making      problem,      where      each      decision  \\n \\n  corresponds      to      how      samples      are      denoised.</p><p> \\n \\n  Although      the      above      objective      function      (8)      is      reasonable,      the      resulting      distribution      might      deviate  \\n \\n  too      much      from      the      pre-trained      diffusion      model.\\nTo      circumvent      this      issue,   \\n \\na      natural      way      is      to      add  \\n \\n  penalization      against      pre-trained      diffusion      models.\\nThen,      the      target      distribution      is      defined      as:</p><p>argmax      E,.4[r(x)]   \\n \\n—      aKL(q||p’*).\\n(9)  \\n \\n  qeA(¥) Notably,      (9)      reduces      to      the      following      distribution:</p><li>(10)</li><p>5\\n \\n   exp(r()/a)p\"\"()  \\n \\n  Pr)   \\n \\n=      Fra)      apa)</p><p> \\n \\n  Here,      the      first      term      in      (9)      corresponds      to      the      mean      reward,      which      we      want      to      optimize      in      the  \\n \\n  fine-tuning      process.\\nThe      second      term      in      (10)      serves      as   \\n \\na      penalty      term,      indicating      the      deviation      of   \\n \\nq       from      the      pre-trained      model.\\nThe      parameter   \\n \\na      controls      the      strength      of      this      regularization      term.\\nThe  \\n \\n  proper      choice      of   \\n \\na      depends      on      the      task      we      are      interested      in.</p><h6>1.2.2      Motivation      for      Using      RL      over      Non-RL      Alternatives</h6><p>ives  \\n \\n  To      achieve      our      goal      of      maximizing      downstream      reward      functions      with      diffusion      models,      readers  \\n \\n  may      question      whether      alternative      approaches      can      be      employed      apart      from      RL.\\nHere,      we      investigate  \\n \\n  these      potential      alternatives      and      explain      why      RL      approaches      may      offer      advantages      over      them.</p><p> \\n \\n  Rejection      sampling.\\nOne      approach      involves      generating      multiple      samples      from      pre-trained      diffu-  \\n \\n  sion      models      and      selecting      only      those      with      high      rewards.\\nThis      method,      called      rejection      sampling,  \\n \\n  operates      without      needing      fine-tuning.\\nHowever,      rejection      sampling      is      effective      primarily      when      the  \\n \\n  pre-trained      model      already      has   \\n \\na      high      probability      of      producing      high-reward      samples.\\nIt      resembles  \\n \\n  sampling      from   \\n \\na      prior      distribution      to      obtain      posterior      samples      (in      this      case,      high-reward      points).\\n \\n \\n  This      approach      works      efficiently      when      the      posterior      closely      matches      the      prior      but      can      become      highly  \\n \\n  inefficient      otherwise.\\nIn      contrast,      by      explicitly      updating      weight      in      diffusion      models,      RL-based      fine-  \\n \\n  tuning      allows      us      to      obtain      these      high-reward      samples,      which      are      seldom      generated      by      pre-trained  \\n \\n  models.</p><p> \\n \\n  Conditional      diffusion      models      (classifier-free      guidance).\\nIn      conditional      generative      models,      the  \\n \\n  general      goal      is      to      sample      from      p(|c),      where   \\n \\nx      is      the      output      and   \\n \\nc      denotes      the      conditioning      variable.\\n \\n \\n  For      example,      in      text-to-language      diffusion      models,   \\n \\nc      is   \\n \\na      text,      and   \\n \\nx      is      the      generated      image.\\nSimilarly,  \\n \\n  in      the      context      of      protein      engineering      for      addressing      inverse      folding      problems,      models      often      define   \\n \\nc       as      the      protein      backbone      structure      and   \\n \\nx      as      the      corresponding      amino      acid      sequence.\\nHere,      using      the  \\n \\n  training      data      {c,      ¢},      the      model      is      trained      by      using      the      loss      function:</p><p>argmin   \\n \\n»      E,uni(l0,7)2)      ~upa      +o?\\nsewN\\\\\\\\\\\\\\'(O)      [lle   \\n \\n—      co(2”,      }),      t)      I\"), t where      the      denoising      function      €g      additionally      receives      the      conditioning      information   \\n \\nc      as      input.\\nIn  \\n \\n  practice,   \\n \\na      variety      of      improvements      such      as      classifier-free      guidance      (Ho      and      Salimans,      2022)      can  \\n \\n  further      improve      the      model’s      ability      to      learn      the      conditional      distribution      p(|c).</p><p> \\n \\n  These      conditional      generative      models      can      be      used      to      optimize      down-stream      rewards      by      condi-  \\n \\n  tioning      on      the      reward      values,      then      sampling   \\n \\nx      conditioned      on      high      reward      values      (Krishnamoorthy  \\n \\n  et      al.,      2023;      Yuan      et      al.,      2023).\\nWhile      this      method      is,      in      principle,      capable      of      generating      plausible x\\n \\n   values      across   \\n \\na      range      of      reward      levels      within      the      training      data      distribution,      it      is      not      the      most  \\n \\n  effective      optimization      strategy.\\nThis      is      primarily      because      high-reward      inputs      frequently      reside  \\n \\n  in      the      tails      of      the      training      distribution      or      even      beyond      it.\\nConsequently,      this      method      may      not  \\n \\n  effectively      generate      high-reward      samples      that      lie      outside      the      training      data      distribution.\\nIn      contrast,  \\n \\n  RL-based      fine-tuning      has      the      capability      to      generate      samples      with      higher      rewards      beyond      the      training  \\n \\n  data.\\nThis      is      achieved      by      explicitly      maximizing      reward      models      learned      from      the      training      data  \\n \\n  and      leveraging      their      extrapolative      capabilities      of      reward      models,      as      theoretically      formalized      and  \\n \\n  empirically      observed      in      Uehara      et      al.\\n(2024).</p><p>Reward-weighted      training.\\nAnother      alternative      approach      is      to      use   \\n \\na      reward-weighted      version  \\n \\n  of      the      standard      training      loss      for      diffusion      models.\\nSuppose      that      we      have      data      {x      r(a)}.\\nThen, after      learning   \\n \\na      reward   \\n \\n7   \\n \\n:   \\n \\n¥   \\n \\n—      R      with      regression      from      the      data,      to      achieve      our      goal,      it      looks  \\n \\n  natural      to      use   \\n \\na      reward-weighted      version      of      the      training      loss      for      diffusion      models      (5),      1.e.,.\\na7      (4)      i)   \\n \\n2 areaun   \\n \\n»      E,uni((0,2))      peel?\\n+eo¢,enncoal      (to      ile   \\n \\n—      co(ay?,t)      |\") i\\n \\n    There      are      two      potential      drawbacks      to      this      approach.\\nFirst,      in      practice,      it      may      struggle      to      generate  \\n \\n  samples      with      higher      rewards      beyond      the      training      data.\\nAs      we      will      explain      later,      many      RL      algorithms  \\n \\n  are      more      directly      focused      on      optimizing      reward      functions,      which      are      expected      to      excel      in      obtaining  \\n \\n  samples      with      high      rewards      not      observed      in      the      original      data,      as      empirically      observed      in      Black      et      al.\\n \\n \\n  (2023).\\nSecond,      when      fine-tuning   \\n \\na      conditional      diffusion      model      p(x|c),      the      alternative      approach  \\n \\n  here      requires   \\n \\na      pair      of      {c,      «}      during      fine-tuning      to      ensure      the      validity      of      the      loss      function.\\nWhen  \\n \\n  we      only      have      data      {2      r(x      )}      but      not      {c,      2,      r(a)},      this      implies      that      we      might      need      to      solve an      inverse      problem      from   \\n \\nz      to      c,      which      can      often      be      challenging.\\nIn      contrast,      in      these      scenarios,      RL  \\n \\n  algorithms,      which      we      will      introduce      later,      can      operate      without      needing      such      pairs      {c,      x},      as  \\n \\n  long      as      we      have      learned      reward      functions      7.  \\n \\n  Finally,      it      should      be      noted      that      reward-weighted      training      technically      falls      under      the      broader  \\n \\n  category      of      RL      methods.\\nIt      shares   \\n \\na      close      connection      with      “reward-weighted      MLE”      introduced      in  \\n \\n  Section      6.1,      as      discussed      later.\\nEmploying      this      reward-weighted      MLE      helps      address      the      second  \\n \\n  concern      of      “reward-weighted      training”      mentioned      earlier.</p><p>2\\n \\n   Brief      Overview      of      Entropy-Regularized      MDPs</p><p> \\n \\n  In      this      tutorial,      we      explain      how      fine-tuning      diffusion      models      can      be      naturally      formulated      as      an      RL  \\n \\n  problem      in      entropy-regularized      MDPs.\\nThis      perspective      is      natural      because      RL      involves      sequential  \\n \\n  decision-making,      and   \\n \\na      diffusion      model      is      formulated      as   \\n \\na      sequential      problem      where      each      denoising  \\n \\n  step      is   \\n \\na      decision-making      process.\\nTo      connect      diffusion      models      with      RL,      we      begin      with   \\n \\na      concise  \\n \\n  overview      of      RL      in      standard      entropy-regularized      MDPs      (Haarnoja      et      al.,      2017;      Neu      et      al.,      2017;</p><p> \\n \\n  Geist      et      al.,      2019;      Schulman      et      al.,      2017).</p><li>2.1      MDPs</li><p> \\n \\n  An      MDP      is      defined      as      follows:      {S,A,      {P\"*}/_5,      {r:}-9,      po}      where      S      is      the      state      space,   \\n \\nA      is      the  \\n \\n  action      space,      P;\"*      is   \\n \\na      transition      dynamic      mapping:      S   \\n \\nx   \\n \\nA   \\n \\n—      A(S),      7,   \\n \\n:      S   \\n \\nx   \\n \\nA   \\n \\n>      R      denotes  \\n \\n  reward      received      at   \\n \\nt      and      po      is      an      initial      distribution      over      S.   \\n \\nA      policy      7,   \\n \\n:      S   \\n \\n—      A(A)      is   \\n \\na      map      from  \\n \\n  any      state      s   \\n \\n€      S      to      the      distribution      over      actions.\\nThe      standard      goal      in      RL      is      to      solve</p><h2>Images</h2><p> \\n \\n \\ng   \\n \\n|   \\n \\na      ge   \\n \\nz       evi      do}      f      g,   \\n \\n2      3°      DNAs      with      high  \\n \\n  aor   \\n \\na   \\n \\n-      e      e       DNAs      io      si   \\n \\ni      og   \\n \\n+   \\n \\na      i:      cell-specificity</p><p>Figure      1:      Illustrative      examples      of      RL-based      fine-tuning,      aimed      at      optimizing      pre-trained      diffusion  \\n \\n  models      to      maximize      downstream      reward      functions.</p><p> \\n \\n  2023),      chemistry      (Jo      et      al.,      2022;      Xu      et      al.,      2022;      Hoogeboom      et      al.,      2022),      and      biology      (Avdeyev  \\n \\n  et      al.,      2023;      Stark      et      al.,      2024;      Campbell      et      al.,      2024).</p><p> \\n \\n  While      diffusion      models      exhibit      significant      power      in      capturing      the      training      data      distribution,  \\n \\n  there’s      often   \\n \\na      need      to      customize      these      models      for      particular      downstream      reward      functions.\\nFor  \\n \\n  instance,      in      computer      vision,      Stable      Diffusion      (Rombach      et      al.,      2022)      serves      as   \\n \\na      strong      backbone  \\n \\n  pre-trained      model.\\nHowever,      we      may      want      to      fine-tune      it      further      by      optimizing      downstream      reward  \\n \\n  functions      such      as      aesthetic      scores      or      human-alignment      scores      (Black      et      al.,      2023;      Fan      et      al.,      2023).\\n \\n \\n  Similarly,      in      fields      such      as      biology      and      chemistry,      various      sophisticated      diffusion      models      have  \\n \\n  been      developed      for      DNA,      RNA,      protein      sequences,      and      molecules,      effectively      modeling      biological  \\n \\n  and      chemical      spaces.\\nNonetheless,      biologists      and      chemists      typically      aim      to      optimize      specific  \\n \\n  downstream      objectives      such      as      cell-specific      expression      in      DNA      sequences      (Gosai      et      al.,      2023;      Lal  \\n \\n  et      al.,      2024;      Sarkar      et      al.,      2024),      translational      efficiency/stability      of      RNA      sequences      (Castillo-Hair  \\n \\n  and      Seelig,      2021;      Agarwal      and      Kelley,      2022),      stability/bioactivity      of      protein      sequence      (Frey      et      al.,  \\n \\n  2023;      Widatalla      et      al.,      2024)      or      QED/SA      scores      of      molecules      (Zhou      et      al.,      2019).</p><p> \\n \\n  To      achieve      this      goal,      numerous      algorithms      have      been      proposed      for      fine-tuning      diffusion      models  \\n \\n  via      reinforcement      learning      (RL)      (e.g.,      Black      et      al.      (2023);      Fan      et      al.\\n(2023);      Clark      et      al.\\n(2023);  \\n \\n  Prabhudesai      et      al.\\n(2023);      Uehara      et      al.\\n(2024)),      aiming      to      optimize      downstream      reward      functions.\\n \\n \\n  RL      is   \\n \\na      machine      learning      paradigm      where      agents      learn      to      make      sequential      decisions      to      maximize  \\n \\n  reward      signals      (Sutton      and      Barto,      2018;      Agarwal      et      al.,      2019).\\nIn      our      context,      RL      naturally      emerges  \\n \\n  as   \\n \\na      Suitable      approach      due      to      the      sequential      structure      inherent      in      diffusion      models,      where      each      time  \\n \\n  step      involves   \\n \\na      “decision”      corresponding      to      how      the      sample      is      denoised      at      that      step.\\nThis      tutorial  \\n \\n  aims      to      review      recent      works      for      readers      interested      in      understanding      the      fundamentals      of      RL-based  \\n \\n  fine-tuning      from   \\n \\na      holistic      perspective,      including      the      advantages      of      RL-based      fine-tuning      over  \\n \\n  non-RL      approaches,      the      pros      and      cons      of      different      RL-based      fine-tuning      algorithms,      the      formalized  \\n \\n  goal      of      RL-based      fine-tuning,      and      its      connections      with      related      topics      such      as      classifier      guidance.</p><p> \\n \\n  The      content      of      this      tutorial      is      primarily      divided      into      three      parts.\\nIn      addition,      as      an      implementation  \\n \\n  example,      we      also      release      the      code      that      employs      RL-based      fine-tuning      for      guided      biological      sequences  \\n \\n  (DNA/RNA)      generation      at      https://github.com/masa-ue/RLfinetuning_Diffusion_Bioseq.</p><li>1.      We      aim      to      provide   \\n \\na      comprehensive      overview      of      current      algorithms.\\nNotably,      given      the  \\n \\n  sequential      nature      of      diffusion      models,      we      can      naturally      frame      fine-tuning      as   \\n \\na      reinforcement  \\n \\n  learning      (RL)      problem      within      Markov      Decision      Processes      (MDPs),      as      detailed      in      Section   \\n \\n3       and      4.\\nTherefore,      we      can      employ      any      off-the-shelf      RL      algorithms      such      as      PPO      (Schulman  \\n \\n  et      al.,      2017),      differentiable      optimization      (direct      reward      backpropagation),      weighted      MLE  \\n \\n  (Peters      et      al.,      2010;      Peng      et      al.,      2019),      value-weighted      sampling      (close      to      classifier      guidance  \\n \\n  in      Dhariwal      and      Nichol      (2021)),      and      path      consistency      learning      (Nachum      et      al.,      2017).\\nWe  \\n \\n  discuss      these      algorithms      in      detail      in      Section      4.2      and      6.\\nInstead      of      merely      outlining      each  \\n \\n  algorithm,      we      aim      to      present      both      their      advantages      and      disadvantages      so      readers      can      select  \\n \\n  the      most      suitable      algorithms      for      their      specific      purposes.</li><li>2.      We      categorize      various      fine-tuning      scenarios      based      on      how      reward      feedback      is      acquired      in  \\n \\n  Section      7.\\nThis      distinction      is      pivotal      for      practical      algorithm      design.\\nFor      example,      if      we      can  \\n \\n  access      accurate      reward      functions,      computational      efficiency      would      become      our      primary      focus.\\n \\n \\n  However,      in      cases      where      reward      functions      are      unknown,      it      is      essential      to      learn      them      from  \\n \\n  data      with      reward      feedback,      leading      us      to      take      feedback      efficiency      and      distributional      shift      into  \\n \\n  consideration      as      well.\\nSpecifically,      when      reward      functions      need      to      be      learned      from      static  \\n \\n  offline      data      without      any      online      interactions,      we      must      address      the      issue      of      overoptimization,  \\n \\n  where      fine-tuned      models      are      misled      by      out-of-distribution      samples,      and      generate      samples  \\n \\n  with      low      genuine      rewards.\\nThis      is      crucial      because,      in      an      offline      scenario,      the      coverage      of  \\n \\n  offline      data      distribution      with      feedback      is      limited;      hence,      the      out-of-distribution      region      could  \\n \\n  be      extensive      (Uehara      et      al.,      2024).</li><li>3.      We      provide   \\n \\na      detailed      discussion      on      the      relationship      between      RL-based      fine-tuning      methods  \\n \\n  and      closely      related      methods      in      the      literature,      such      as      classifier      guidance      (Dhariwal      and      Nichol,  \\n \\n  2021)      in      Section      8,      flow-based      diffusion      models      (Liu      et      al.,      2022;      Lipman      et      al.,      2023;      Tong  \\n \\n  et      al.,      2023)      in      Section      9,      sampling      from      unnormalized      distributions      (Zhang      and      Chen,      2021)  \\n \\n  in      Section      10,      Gflownets      (Bengio      et      al.,      2023)      in      Section      6.3,      and      path      integral      control      theory  \\n \\n  (Theodorou      et      al.,      2010;      Williams      et      al.,      2017;      Kazim      et      al.,      2024)      in      Section      6.2.3.      We  \\n \\n  summarize      the      key      messages      as      follows.</li><p>¢\\n \\n   Section      6.3:      The      losses      used      in      Gflownets      are      fundamentally      equivalent      to      those      derived  \\n \\n  from   \\n \\na      specific      RL      algorithm      called      path      consistency      learning.</p><p> \\n \\n \\n¢      Section      8:      Classifier      guidance      employed      in      conditional      generation      is      regarded      as   \\n \\na       specific      RL-based      fine-tuning      method,      which      we      call      value-weighted      sampling.\\nAs  \\n \\n  formalized      in      Zhao      et      al.\\n(2024),      this      observation      indicates      that      any      off-the-shelf      RL-  \\n \\n  based      fine-tuning      algorithms      (e.g.,      PPO      and      differentiable      optimization)      can      be      applied  \\n \\n  to      conditional      generation.</p><p> \\n \\n \\n¢      Section      10:      Sampling      from      unnormalized      distributions,      often      referred      to      as      Gibbs  \\n \\n  distributions,      is      an      important      and      challenging      problem      in      diverse      domains.\\nWhile  \\n \\n  MCMC      methods      are      traditionally      used      for      this      task,      recognizing      its      similarity      to      the  \\n \\n  objectives      of      RL-based      fine-tuning      suggests      that      off-the-shelf      RL      algorithms      can      also  \\n \\n  effectively      address      the      challenge      of      sampling      from      unnormalized      distributions.</p><h2>Preliminaries</h2><li>1.1 DiffusionModels 2 2.20.\\n00 2 ee ee</li><table><th><td colSpan=1>1.1.1 Score-Based Diffusion Models (Optional)</td><td colSpan=1></td><td colSpan=1></td></th><tr><td colSpan=1>1.2 Fine-Tuning Diffusion Models withRL</td><td colSpan=1>0.00.0.</td><td colSpan=1></td></tr><tr><td colSpan=1>1.2.1. Brief Overview: Fine-tuning withRL 1.2.2 Motivation for Using RL over Non-RL Alternatives.</td><td colSpan=1></td><td colSpan=1>2.</td></tr><tr><td>Brief      Overview      of      Entropy-Regularized      MDPs</td></tr><tr><td>Fine-Tuning      Diffusion      Models      with      RL      in      Entropy      Regularized      MDPs</td></tr><tr><td>Theory      of      RL-Based      Fine-Tuning</td></tr></table><h2>RL-Based      Fine-Tuning      Algorithms      1:      Non-Distribution-Constrained      Approaches</h2><h2>RL-Based      Fine-Tuning      Algorithms      2:      Distribution-Constrained      Approaches</h2><h2>Fine-Tuning      Settings      Taxonomy</h2><h2>Connection      with      Classifier      Guidance</h2><p> \\n \\n  Connection      with      Flow-Based      Diffusion      Models  \\n \\n  10  \\n \\n  10  \\n \\n  11 12  \\n \\n  14  \\n \\n  14  \\n \\n  15  \\n \\n  16  \\n \\n  16  \\n \\n  17  \\n \\n  18  \\n \\n  19  \\n \\n  21  \\n \\n  21  \\n \\n  23  \\n \\n  24  \\n \\n  25  \\n \\n  25  \\n \\n  28  \\n \\n  28  \\n \\n  28</p><p>10      Connection      with      Sampling      from      Unnormalized      Distributions      35 10.1 Markov Chain Monte Carlo(MCMC) 2.00000 eee ene 35 10.2 RL-Based Approaches 0 2.0.0 pee ee 36</p><p>11      Closely      Related      Directions      36</p><p>12      Summary      37</p><p>1\\n \\n   Preliminaries</p><p>In      this      section,      we      outline      the      fundamentals      of      diffusion      models      and      elucidate      the      objective      of  \\n \\n  fine-tuning      them.</p><h3>1.1.      Diffusion      Models</h3><p>dels  \\n \\n  We      present      an      overview      of      denoising      diffusion      probabilistic      models      (DDPM)      (Ho      et      al.,      2020).\\n \\n \\n  For      more      details,      refer      to      Yang      et      al.\\n(2023);      Cao      et      al.\\n(2024);      Chen      et      al.\\n(2024);      Tang      and      Zhao  \\n \\n  (2024).</p><p> \\n \\n  In      diffusion      models,      the      objective      is      to      develop   \\n \\na      deep      generative      model      that      accurately      captures  \\n \\n  the      true      data      distribution.\\nSpecifically,      denoting      the      data      distribution      by      pr.\\n \\n \\n€      A(4’)      where   \\n \\n¥      is      an  \\n \\n  input      space,   \\n \\na      DDPM      aims      to      approximate      p,,.\\nusing   \\n \\na      parametric      model      structured      as 1 p(xo3      8)   \\n \\n=   \\n \\n[      rlco      O)dx1.r,      where      p(%o:7;      9)   \\n \\n=      pr+i(@7;      9)      [[      peter:      0).</p><p>t=T When   \\n \\n4      is      an      Euclidean      space      (in      R®),      the      forward      process      is      modeled      as      the      following      dynamics:</p><p>pra(er)      =N(0,1),      pe(ve-r)2;      9)   \\n \\n=      N(p(a1,      t;      8),      07      (4)   \\n \\nx      D),  \\n \\n  where      N(-,-)      denotes   \\n \\na      normal      distribution,   \\n \\nJ      is      an      identity      matrix      and      p   \\n \\n:      R¢   \\n \\nx      [0,7]   \\n \\n>      R¢.\\nIn  \\n \\n  DDPMs,      we      aim      to      obtain   \\n \\na      set      of      policies      (i.e.,      denoising      process)      {p;}{_741,      pr:   \\n \\n&   \\n \\n—      A(A&)      such  \\n \\n  that      p(x;      0)   \\n \\n©      Ppre(#o).\\nIndeed,      by      optimizing      the      variational      bound      on      the      negative      log-likelihood,  \\n \\n  we      can      derive      such   \\n \\na      set      of      policies.\\nFor      more      details,      refer      to      Section      1.1.1.  \\n \\n  Hereafter,      we      consider   \\n \\na      situation      where      we      have   \\n \\na      pre-trained      diffusion      model      that      is      already  \\n \\n  trained      on   \\n \\na      large      dataset,      such      that      the      model      can      accurately      capture      the      underlying      data      distribution.</p><p> \\n \\n  pre We      refer      to      the      pre-trained      policies      as      {p?\\n\"*(-|-)}{_7,,,      and      to      the      marginal      distribution      at   \\n \\nt   \\n \\n=   \\n \\n0       induced      by      the      pre-trained      diffusion      model      as      pp.\\nIn      other      words, 1\\n \\n    pr      (x0)   \\n \\n=      [iler)      [[°@eledderr.</p><p>t=T  \\n \\n  Remark   \\n \\n1      (Non-Euclidean      space).\\nFor      simplicity,      we      typically      assume      that      the      domain      space      is  \\n \\n  Euclidean.\\nHowever,      we      can      easily      extend      most      of      the      discussion      to   \\n \\na      more      general      space,      such      as  \\n \\n  a      Riemannian      manifold      (De      Bortoli      et      al.,      2022)      or      discrete      space      (Austin      et      al.,      2021;      Campbell  \\n \\n  et      al.,      2022;      Benton      et      al.,      2024;      Lou      et      al.,      2023).</p><p> \\n \\n  Remark   \\n \\n2      (Conditional      generative      models).\\nPre-trained      models      can      be      conditional      diffusion      models,  \\n \\n  such      as      text-to-image      diffusion      models      (Ramesh      et      al.,      2022).\\nThe      extension      is      straightforward:  \\n \\n  augmenting      the      input      spaces      of      policies      with      an      additional      space      on      which      we      want      to      condition.\\n \\n \\n  More      specifically,      by      denoting      that      space      by   \\n \\nc   \\n \\n€      C,      each      policy      becomes      p;(x¢|%1,      650)   \\n \\n:   \\n \\n&   \\n \\nx   \\n \\nC   \\n \\n>       A(X).</p><p> \\n \\n  Remark   \\n \\n3      (Extension      to      Continuous-time      diffusion      models).\\nJn      this      tutorial,      our      discussion      on  \\n \\n  fine-tuning      diffusion      models      will      be      primarily      formulated      on      the      discrete-time      formulation,      as      we      did  \\n \\n  above      Nonetheless,      much      of      our      discussion      is      also      applicable      to      continuous-time      diffusion      models,  \\n \\n  as      formalized      in      Uehara      et      al.\\n(2024)</p><h4>1.1.1      Score-Based      Diffusion      Models      (Optional)</h4><p>nal)  \\n \\n  We      briefly      discuss      how      to      train      diffusion      models      in   \\n \\na      continuous-time      framework      (Song      et      al.,  \\n \\n  2021).\\nIn      our      tutorial,      this      section      is      mainly      used      to      discuss      several      algorithms      (e.g.,      value-weighted  \\n \\n  sampling      in      Section      6.2)      and      their      relationship      with      flow-based      diffusion      models      (Section      9)      later.</p><p> \\n \\n  Therefore,      readers      may      skip      it      based      on      their      individual      needs.</p><p> \\n \\n  The      training      process      of      diffusion      models      can      be      summarized      as      follows.\\nOur      objective      is      to      train  \\n \\n  a      sequential      mapping      from   \\n \\na      known      noise      distribution      to   \\n \\na      data      distribution,      formalized      through  \\n \\n  a      stochastic      differential      equation      (SDE).\\nFirstly,      we      define   \\n \\na      forward      (fixed)      SDE      that      maps      from  \\n \\n  the      data      distribution      to      the      noise      distribution.\\nThen,   \\n \\na      time-reversal      SDE      is      expressed      as      an      SDE,  \\n \\n  which      includes      the      (unknown)      score      function.\\nNow,      by      learning      this      unknown      score      function      from  \\n \\n  the      training      data,      the      time-reversal      SDE      can      be      utilized      as   \\n \\na      generative      model.\\nHere      are      the      details.</p><p>Forward      and      time-reversal      SDE.\\nFirstly,      we      introduce   \\n \\na      forward      (a.k.a.,      reference)      Stochastic  \\n \\n  differential      equations      (SDE)      from   \\n \\n0      to      7’.\\n \\n \\nA      common      choice      is   \\n \\na      variance-preserving      (VP)      process:</p><p>t\\n \\n   €\\n \\n   [0,7];      dx,   \\n \\n=      —0.52,dt   \\n \\n+      du;,,      x9   \\n \\n~      p(x),      (1)</p><p>where      dw;      represents      standard      Brownian      motion.\\nHere      are      two      crucial      observations:</p><p>¢\\n \\n   As   \\n \\nT      approaches      oo,      the      limiting      distribution      is      \\\\\\\\\\\\\\\\V(0,      I).</p><p>¢\\n \\n   The      time-reversal      SDE      (Anderson,      1982),      which      preserves      the      marginal      distribution,      is  \\n \\n  expressed      as      follows:</p><p>t\\n \\n   €\\n \\n   [0,7];      dz,   \\n \\n=      [0.5z,   \\n \\n+      V      log      qr_s(%)|dt   \\n \\n+      du,.\\n(2) Here,      g,   \\n \\n€      A(R“)      denotes      the      marginal      distribution      at      time   \\n \\nt      induced      by      the      reference      SDE (1).\\nNotably,      the      marginal      distribution      of      zr_;      is      the      same      as      the      one      of      x;      induced      by      the  \\n \\n  reference      SDE.</p><p> \\n \\n  These      observations      suggest      that      with      sufficiently      large      T,      starting      from      V      (0,      I)      and      following      the  \\n \\n  time-reversal      SDE      (2)   \\n \\n,      we      are      able      to      sample      from      the      data      distribution      (i.e.,      p?\\n\"°)      at      the      terminal  \\n \\n  time      point      7’.</p><p>Training      score      functions      from      the      training      data.\\nNow,      the      remaining      question      is      how      to      estimate  \\n \\n  the      marginal      score      function      V      log      q(-)      in      (2).\\nThis      can      be      computed      using      the      following      principles:</p><p>°\\n \\n   We      have      V      log      (a+)   \\n \\n=      Exgxppre[Va,      log      qyo(ae   \\n \\n|      2o)],      where      q@jo      represents      the      conditional  \\n \\n  distribution      of      x;      given      Xo      induced      by      the      reference      SDE.</p><p>¢\\n \\n   The      conditional      score      function      V,,,      log      dejo      (Xe   \\n \\n|      xo)      can      be      analytically      derived      as  \\n \\n  °</p><h5>      Myo      —      Ut</h5><p> \\n \\n  {ory      ue   \\n \\n=      exp(—0.5t)xo,      {o?\\n}?\\n \\n \\n=   \\n \\n1   \\n \\n—      exp(—0.5t).\\n(3) t Subsequently,      by      defining   \\n \\na      parameterized      model      sg   \\n \\n:      R¢   \\n \\nx      [0,7]   \\n \\n>      R¢      and      utilizing      the      following weighted      regression,      the      marginal      score      is      estimated:</p><p>A\\n \\n \\n  Dore   \\n \\n=      argmin      Eywuni((0,7))      ce~aejo(w+|ar0),c0~pPr®      [A(t)      ||      Ver,      log      jo      (Le   \\n \\n|      ro)   \\n \\n_      So(£t,      t)      17)      (4)</p><p> \\n \\n  where   \\n \\n2   \\n \\n:      [0,7]   \\n \\n+      R      is   \\n \\na      weighted      function.\\n \\n \\n  Note      we      typically      use      another      parametrization      well.\\nFrom      (3),      we      can      easily      see      that      this      score      is  \\n \\n  estimated      by      introducing      networks      €9   \\n \\n:      R@   \\n \\nx      [0,7]   \\n \\n—      R®,      which      aims      to      estimate      the      noise      Bere      eO</p><p>Ore   \\n \\n=      argmin      Ey.\\nuni({0,7]),cr~p2xo+o?\\ne~N      (0,1)      ,co~pPte      [A(t)/{op}?\\nlle   \\n \\n—      €9      (1,      t)      7)   \\n \\n:      (5)</p><p>Then,      by      denoting      the      training      data      as      {a}      and      setting      A(t)   \\n \\n=      {a7      }”,      the      actual      loss      function      from the      training      data      is argmin   \\n \\n)      Ey      ai(o.r))a   \\n \\nO      peal      +e02,e~N      (0,1)      [Ile   \\n \\n—      €o(ae?,      t)      [17].\\n(6) i=1 Inference      time.\\nOnce      this      Bore      (or      Ore)      is      learned,      we      insert      it      into      the      time-reversal      SDE      (2)      and use      it      as   \\n \\na      generative      model.\\nUltimately,      with      standard      discretization,      we      obtain:</p><p>p(      xe,      t;      a)   \\n \\n=      24+      (0.54,   \\n \\n+      Sire      (,,T      —t)|(ot),      {o?(t)}?\\n \\n \\n=      (6t).</p><p>Equivalently,      this      is (21,      t;      Ore)   \\n \\n=   \\n \\n2   \\n \\n+      [0.52   \\n \\n—      1/o?\\n \\n \\nx      e%,,,(a1,T   \\n \\n—      t)]|      (6t),      (7)</p><p>where      (dt)      denotes      the      discretization      step.</p><p> \\n \\n  Equivalence      to      DDPM.\\nThe      objective      function      derived      here      is      equivalent      to      the      one      formulated  \\n \\n  based      on      variational      inference      in      the      discretized      formulation,      which      is      commonly      referred      to      as  \\n \\n  DDPMs      (Ho      et      al.,      2020).\\nIn      DDPMs      (Ho      et      al.,      2020),      we      often      see      the      following      form:</p><p> \\n \\n \\n1      1-      (ey;</p><p> \\n \\n  it;      0)   \\n \\n=      ~~   \\n \\n’      ),  \\n \\n  p(xy   \\n \\n)      Xt      Jina      ae      [om</p><h6>Vor</h6><p>When      a;   \\n \\n=   \\n \\n1   \\n \\n—      (dt),      the      above      is      equivalent      to      (7)      when      dt      goes      to   \\n \\n0      by      noting      1/,/a;   \\n \\n©      1+0.5(d¢).</p><h3>1.2      Fine-Tuning      Diffusion      Models      with      RL</h3><p>RL  \\n \\n  Importantly,      our      focus      on      RL-based      fine-tuning      distinguishes      itself      from      the      standard      fine-tuning  \\n \\n  methods.\\nStandard      fine-tuning      typically      involves      scenarios      where      we      have      pre-trained      models  \\n \\n  (e.g.,      diffusion      models)      and      new      training      data      {2,      y}.\\nIn      such      cases,      the      common      approach  \\n \\n  for      fine-tuning      is      to      retrain      diffusion      models      with      the      new      training      data      using      the      same      loss  \\n \\n  function      employed      during      pre-training.\\nIn      sharp      contrast,      RL-based      fine-tuning      directly      employs  \\n \\n  the      downstream      reward      functions      as      the      primary      optimization      objectives,      making      the      loss      functions  \\n \\n  different      from      those      used      in      pre-training.</p><p> \\n \\n  Hereafter,      we      start      with   \\n \\na      concise      overview      of      RL-based      fine-tuning.\\nThen,      before      delving      into  \\n \\n  specifics,      we      discuss      simpler      non-RL      alternatives      to      provide      motivation      for      adopting      RL-based  \\n \\n  fine-tuning.</p><li>1.2.1      Brief      Overview:      Fine-tuning      with      RL</li><p>In      this      article,      we      explore      the      fine-tuning      of      pre-trained      diffusion      models      to      optimize      downstream  \\n \\n  reward      functions      r   \\n \\n:      R¢   \\n \\n+      R.      In      domains      such      as      images,      these      backbone      diffusion      models      to      be  \\n \\n  fine-tuned      include      Stable      Diffusion      (Rombach      et      al.,      2022),      while      the      reward      functions      are      aesthetic  \\n \\n  scores      and      alignment      scores      (Clark      et      al.,      2023;      Black      et      al.,      2023;      Fan      et      al.,      2023).\\nMore      examples  \\n \\n  are      detailed      in      the      introduction.\\nThese      rewards      are      often      unknown,      necessitating      learning      from  \\n \\n  data      with      feedback:      {2      r(x™)}.\\nWe      will      explore      this      aspect      further      in      Section      7.\\nUntil      then,      we  \\n \\n  assume      r      is      known.\\n \\n \\n  Now,      readers      may      wonder      about      the      objectives      we      aim      to      achieve      during      the      fine-tuning      process.\\n \\n \\n  A      natural      approach      is      to      define      the      optimization      problem:</p><p>argmax      E,.\\n[r()|      (8)  \\n \\n  qeA(¥)  \\n \\n  where   \\n \\nq      is      initialized      with   \\n \\na      pre-trained      diffusion      model      p?\\n*®   \\n \\n€      A(2).\\nIn      this      tutorial,      we      will      detail  \\n \\n  the      procedure      of      solving      (8)      with      RL      in      the      upcoming      sections.\\nIn      essence,      we      leverage      the      fact  \\n \\n  that      diffusion      models      are      formulated      as   \\n \\na      sequential      decision-making      problem,      where      each      decision  \\n \\n  corresponds      to      how      samples      are      denoised.</p><p> \\n \\n  Although      the      above      objective      function      (8)      is      reasonable,      the      resulting      distribution      might      deviate  \\n \\n  too      much      from      the      pre-trained      diffusion      model.\\nTo      circumvent      this      issue,   \\n \\na      natural      way      is      to      add  \\n \\n  penalization      against      pre-trained      diffusion      models.\\nThen,      the      target      distribution      is      defined      as:</p><p>argmax      E,.4[r(x)]   \\n \\n—      aKL(q||p’*).\\n(9)  \\n \\n  qeA(¥) Notably,      (9)      reduces      to      the      following      distribution:</p><li>(10)</li><p>5\\n \\n   exp(r()/a)p\"\"()  \\n \\n  Pr)   \\n \\n=      Fra)      apa)</p><p> \\n \\n  Here,      the      first      term      in      (9)      corresponds      to      the      mean      reward,      which      we      want      to      optimize      in      the  \\n \\n  fine-tuning      process.\\nThe      second      term      in      (10)      serves      as   \\n \\na      penalty      term,      indicating      the      deviation      of   \\n \\nq       from      the      pre-trained      model.\\nThe      parameter   \\n \\na      controls      the      strength      of      this      regularization      term.\\nThe  \\n \\n  proper      choice      of   \\n \\na      depends      on      the      task      we      are      interested      in.</p><h6>1.2.2      Motivation      for      Using      RL      over      Non-RL      Alternatives</h6><p>ives  \\n \\n  To      achieve      our      goal      of      maximizing      downstream      reward      functions      with      diffusion      models,      readers  \\n \\n  may      question      whether      alternative      approaches      can      be      employed      apart      from      RL.\\nHere,      we      investigate  \\n \\n  these      potential      alternatives      and      explain      why      RL      approaches      may      offer      advantages      over      them.</p><p> \\n \\n  Rejection      sampling.\\nOne      approach      involves      generating      multiple      samples      from      pre-trained      diffu-  \\n \\n  sion      models      and      selecting      only      those      with      high      rewards.\\nThis      method,      called      rejection      sampling,  \\n \\n  operates      without      needing      fine-tuning.\\nHowever,      rejection      sampling      is      effective      primarily      when      the  \\n \\n  pre-trained      model      already      has   \\n \\na      high      probability      of      producing      high-reward      samples.\\nIt      resembles  \\n \\n  sampling      from   \\n \\na      prior      distribution      to      obtain      posterior      samples      (in      this      case,      high-reward      points).\\n \\n \\n  This      approach      works      efficiently      when      the      posterior      closely      matches      the      prior      but      can      become      highly  \\n \\n  inefficient      otherwise.\\nIn      contrast,      by      explicitly      updating      weight      in      diffusion      models,      RL-based      fine-  \\n \\n  tuning      allows      us      to      obtain      these      high-reward      samples,      which      are      seldom      generated      by      pre-trained  \\n \\n  models.</p><p> \\n \\n  Conditional      diffusion      models      (classifier-free      guidance).\\nIn      conditional      generative      models,      the  \\n \\n  general      goal      is      to      sample      from      p(|c),      where   \\n \\nx      is      the      output      and   \\n \\nc      denotes      the      conditioning      variable.\\n \\n \\n  For      example,      in      text-to-language      diffusion      models,   \\n \\nc      is   \\n \\na      text,      and   \\n \\nx      is      the      generated      image.\\nSimilarly,  \\n \\n  in      the      context      of      protein      engineering      for      addressing      inverse      folding      problems,      models      often      define   \\n \\nc       as      the      protein      backbone      structure      and   \\n \\nx      as      the      corresponding      amino      acid      sequence.\\nHere,      using      the  \\n \\n  training      data      {c,      ¢},      the      model      is      trained      by      using      the      loss      function:</p><p>argmin   \\n \\n»      E,uni(l0,7)2)      ~upa      +o?\\nsewN\\\\\\\\\\\\\\'(O)      [lle   \\n \\n—      co(2”,      }),      t)      I\"), t where      the      denoising      function      €g      additionally      receives      the      conditioning      information   \\n \\nc      as      input.\\nIn  \\n \\n  practice,   \\n \\na      variety      of      improvements      such      as      classifier-free      guidance      (Ho      and      Salimans,      2022)      can  \\n \\n  further      improve      the      model’s      ability      to      learn      the      conditional      distribution      p(|c).</p><p> \\n \\n  These      conditional      generative      models      can      be      used      to      optimize      down-stream      rewards      by      condi-  \\n \\n  tioning      on      the      reward      values,      then      sampling   \\n \\nx      conditioned      on      high      reward      values      (Krishnamoorthy  \\n \\n  et      al.,      2023;      Yuan      et      al.,      2023).\\nWhile      this      method      is,      in      principle,      capable      of      generating      plausible x\\n \\n   values      across   \\n \\na      range      of      reward      levels      within      the      training      data      distribution,      it      is      not      the      most  \\n \\n  effective      optimization      strategy.\\nThis      is      primarily      because      high-reward      inputs      frequently      reside  \\n \\n  in      the      tails      of      the      training      distribution      or      even      beyond      it.\\nConsequently,      this      method      may      not  \\n \\n  effectively      generate      high-reward      samples      that      lie      outside      the      training      data      distribution.\\nIn      contrast,  \\n \\n  RL-based      fine-tuning      has      the      capability      to      generate      samples      with      higher      rewards      beyond      the      training  \\n \\n  data.\\nThis      is      achieved      by      explicitly      maximizing      reward      models      learned      from      the      training      data  \\n \\n  and      leveraging      their      extrapolative      capabilities      of      reward      models,      as      theoretically      formalized      and  \\n \\n  empirically      observed      in      Uehara      et      al.\\n(2024).</p><p>Reward-weighted      training.\\nAnother      alternative      approach      is      to      use   \\n \\na      reward-weighted      version  \\n \\n  of      the      standard      training      loss      for      diffusion      models.\\nSuppose      that      we      have      data      {x      r(a)}.\\nThen, after      learning   \\n \\na      reward   \\n \\n7   \\n \\n:   \\n \\n¥   \\n \\n—      R      with      regression      from      the      data,      to      achieve      our      goal,      it      looks  \\n \\n  natural      to      use   \\n \\na      reward-weighted      version      of      the      training      loss      for      diffusion      models      (5),      1.e.,.\\na7      (4)      i)   \\n \\n2 areaun   \\n \\n»      E,uni((0,2))      peel?\\n+eo¢,enncoal      (to      ile   \\n \\n—      co(ay?,t)      |\") i\\n \\n    There      are      two      potential      drawbacks      to      this      approach.\\nFirst,      in      practice,      it      may      struggle      to      generate  \\n \\n  samples      with      higher      rewards      beyond      the      training      data.\\nAs      we      will      explain      later,      many      RL      algorithms  \\n \\n  are      more      directly      focused      on      optimizing      reward      functions,      which      are      expected      to      excel      in      obtaining  \\n \\n  samples      with      high      rewards      not      observed      in      the      original      data,      as      empirically      observed      in      Black      et      al.\\n \\n \\n  (2023).\\nSecond,      when      fine-tuning   \\n \\na      conditional      diffusion      model      p(x|c),      the      alternative      approach  \\n \\n  here      requires   \\n \\na      pair      of      {c,      «}      during      fine-tuning      to      ensure      the      validity      of      the      loss      function.\\nWhen  \\n \\n  we      only      have      data      {2      r(x      )}      but      not      {c,      2,      r(a)},      this      implies      that      we      might      need      to      solve an      inverse      problem      from   \\n \\nz      to      c,      which      can      often      be      challenging.\\nIn      contrast,      in      these      scenarios,      RL  \\n \\n  algorithms,      which      we      will      introduce      later,      can      operate      without      needing      such      pairs      {c,      x},      as  \\n \\n  long      as      we      have      learned      reward      functions      7.  \\n \\n  Finally,      it      should      be      noted      that      reward-weighted      training      technically      falls      under      the      broader  \\n \\n  category      of      RL      methods.\\nIt      shares   \\n \\na      close      connection      with      “reward-weighted      MLE”      introduced      in  \\n \\n  Section      6.1,      as      discussed      later.\\nEmploying      this      reward-weighted      MLE      helps      address      the      second  \\n \\n  concern      of      “reward-weighted      training”      mentioned      earlier.</p><p>2\\n \\n   Brief      Overview      of      Entropy-Regularized      MDPs</p><p> \\n \\n  In      this      tutorial,      we      explain      how      fine-tuning      diffusion      models      can      be      naturally      formulated      as      an      RL  \\n \\n  problem      in      entropy-regularized      MDPs.\\nThis      perspective      is      natural      because      RL      involves      sequential  \\n \\n  decision-making,      and   \\n \\na      diffusion      model      is      formulated      as   \\n \\na      sequential      problem      where      each      denoising  \\n \\n  step      is   \\n \\na      decision-making      process.\\nTo      connect      diffusion      models      with      RL,      we      begin      with   \\n \\na      concise  \\n \\n  overview      of      RL      in      standard      entropy-regularized      MDPs      (Haarnoja      et      al.,      2017;      Neu      et      al.,      2017;</p><p> \\n \\n  Geist      et      al.,      2019;      Schulman      et      al.,      2017).</p><li>2.1      MDPs</li><p> \\n \\n  An      MDP      is      defined      as      follows:      {S,A,      {P\"*}/_5,      {r:}-9,      po}      where      S      is      the      state      space,   \\n \\nA      is      the  \\n \\n  action      space,      P;\"*      is   \\n \\na      transition      dynamic      mapping:      S   \\n \\nx   \\n \\nA   \\n \\n—      A(S),      7,   \\n \\n:      S   \\n \\nx   \\n \\nA   \\n \\n>      R      denotes  \\n \\n  reward      received      at   \\n \\nt      and      po      is      an      initial      distribution      over      S.   \\n \\nA      policy      7,   \\n \\n:      S   \\n \\n—      A(A)      is   \\n \\na      map      from  \\n \\n  any      state      s   \\n \\n€      S      to      the      distribution      over      actions.\\nThe      standard      goal      in      RL      is      to      solve</p><h3>1.1.      Diffusion      Models</h3><p>dels  \\n \\n  We      present      an      overview      of      denoising      diffusion      probabilistic      models      (DDPM)      (Ho      et      al.,      2020).\\n \\n \\n  For      more      details,      refer      to      Yang      et      al.\\n(2023);      Cao      et      al.\\n(2024);      Chen      et      al.\\n(2024);      Tang      and      Zhao  \\n \\n  (2024).</p><p> \\n \\n  In      diffusion      models,      the      objective      is      to      develop   \\n \\na      deep      generative      model      that      accurately      captures  \\n \\n  the      true      data      distribution.\\nSpecifically,      denoting      the      data      distribution      by      pr.\\n \\n \\n€      A(4’)      where   \\n \\n¥      is      an  \\n \\n  input      space,   \\n \\na      DDPM      aims      to      approximate      p,,.\\nusing   \\n \\na      parametric      model      structured      as 1 p(xo3      8)   \\n \\n=   \\n \\n[      rlco      O)dx1.r,      where      p(%o:7;      9)   \\n \\n=      pr+i(@7;      9)      [[      peter:      0).</p><p>t=T When   \\n \\n4      is      an      Euclidean      space      (in      R®),      the      forward      process      is      modeled      as      the      following      dynamics:</p><p>pra(er)      =N(0,1),      pe(ve-r)2;      9)   \\n \\n=      N(p(a1,      t;      8),      07      (4)   \\n \\nx      D),  \\n \\n  where      N(-,-)      denotes   \\n \\na      normal      distribution,   \\n \\nJ      is      an      identity      matrix      and      p   \\n \\n:      R¢   \\n \\nx      [0,7]   \\n \\n>      R¢.\\nIn  \\n \\n  DDPMs,      we      aim      to      obtain   \\n \\na      set      of      policies      (i.e.,      denoising      process)      {p;}{_741,      pr:   \\n \\n&   \\n \\n—      A(A&)      such  \\n \\n  that      p(x;      0)   \\n \\n©      Ppre(#o).\\nIndeed,      by      optimizing      the      variational      bound      on      the      negative      log-likelihood,  \\n \\n  we      can      derive      such   \\n \\na      set      of      policies.\\nFor      more      details,      refer      to      Section      1.1.1.  \\n \\n  Hereafter,      we      consider   \\n \\na      situation      where      we      have   \\n \\na      pre-trained      diffusion      model      that      is      already  \\n \\n  trained      on   \\n \\na      large      dataset,      such      that      the      model      can      accurately      capture      the      underlying      data      distribution.</p><p> \\n \\n  pre We      refer      to      the      pre-trained      policies      as      {p?\\n\"*(-|-)}{_7,,,      and      to      the      marginal      distribution      at   \\n \\nt   \\n \\n=   \\n \\n0       induced      by      the      pre-trained      diffusion      model      as      pp.\\nIn      other      words, 1\\n \\n    pr      (x0)   \\n \\n=      [iler)      [[°@eledderr.</p><p>t=T  \\n \\n  Remark   \\n \\n1      (Non-Euclidean      space).\\nFor      simplicity,      we      typically      assume      that      the      domain      space      is  \\n \\n  Euclidean.\\nHowever,      we      can      easily      extend      most      of      the      discussion      to   \\n \\na      more      general      space,      such      as  \\n \\n  a      Riemannian      manifold      (De      Bortoli      et      al.,      2022)      or      discrete      space      (Austin      et      al.,      2021;      Campbell  \\n \\n  et      al.,      2022;      Benton      et      al.,      2024;      Lou      et      al.,      2023).</p><p> \\n \\n  Remark   \\n \\n2      (Conditional      generative      models).\\nPre-trained      models      can      be      conditional      diffusion      models,  \\n \\n  such      as      text-to-image      diffusion      models      (Ramesh      et      al.,      2022).\\nThe      extension      is      straightforward:  \\n \\n  augmenting      the      input      spaces      of      policies      with      an      additional      space      on      which      we      want      to      condition.\\n \\n \\n  More      specifically,      by      denoting      that      space      by   \\n \\nc   \\n \\n€      C,      each      policy      becomes      p;(x¢|%1,      650)   \\n \\n:   \\n \\n&   \\n \\nx   \\n \\nC   \\n \\n>       A(X).</p><p> \\n \\n  Remark   \\n \\n3      (Extension      to      Continuous-time      diffusion      models).\\nJn      this      tutorial,      our      discussion      on  \\n \\n  fine-tuning      diffusion      models      will      be      primarily      formulated      on      the      discrete-time      formulation,      as      we      did  \\n \\n  above      Nonetheless,      much      of      our      discussion      is      also      applicable      to      continuous-time      diffusion      models,  \\n \\n  as      formalized      in      Uehara      et      al.\\n(2024)</p><h4>1.1.1      Score-Based      Diffusion      Models      (Optional)</h4><p>nal)  \\n \\n  We      briefly      discuss      how      to      train      diffusion      models      in   \\n \\na      continuous-time      framework      (Song      et      al.,  \\n \\n  2021).\\nIn      our      tutorial,      this      section      is      mainly      used      to      discuss      several      algorithms      (e.g.,      value-weighted  \\n \\n  sampling      in      Section      6.2)      and      their      relationship      with      flow-based      diffusion      models      (Section      9)      later.</p><p> \\n \\n  Therefore,      readers      may      skip      it      based      on      their      individual      needs.</p><p> \\n \\n  The      training      process      of      diffusion      models      can      be      summarized      as      follows.\\nOur      objective      is      to      train  \\n \\n  a      sequential      mapping      from   \\n \\na      known      noise      distribution      to   \\n \\na      data      distribution,      formalized      through  \\n \\n  a      stochastic      differential      equation      (SDE).\\nFirstly,      we      define   \\n \\na      forward      (fixed)      SDE      that      maps      from  \\n \\n  the      data      distribution      to      the      noise      distribution.\\nThen,   \\n \\na      time-reversal      SDE      is      expressed      as      an      SDE,  \\n \\n  which      includes      the      (unknown)      score      function.\\nNow,      by      learning      this      unknown      score      function      from  \\n \\n  the      training      data,      the      time-reversal      SDE      can      be      utilized      as   \\n \\na      generative      model.\\nHere      are      the      details.</p><p>Forward      and      time-reversal      SDE.\\nFirstly,      we      introduce   \\n \\na      forward      (a.k.a.,      reference)      Stochastic  \\n \\n  differential      equations      (SDE)      from   \\n \\n0      to      7’.\\n \\n \\nA      common      choice      is   \\n \\na      variance-preserving      (VP)      process:</p><p>t\\n \\n   €\\n \\n   [0,7];      dx,   \\n \\n=      —0.52,dt   \\n \\n+      du;,,      x9   \\n \\n~      p(x),      (1)</p><p>where      dw;      represents      standard      Brownian      motion.\\nHere      are      two      crucial      observations:</p><p>¢\\n \\n   As   \\n \\nT      approaches      oo,      the      limiting      distribution      is      \\\\\\\\\\\\\\\\V(0,      I).</p><p>¢\\n \\n   The      time-reversal      SDE      (Anderson,      1982),      which      preserves      the      marginal      distribution,      is  \\n \\n  expressed      as      follows:</p><p>t\\n \\n   €\\n \\n   [0,7];      dz,   \\n \\n=      [0.5z,   \\n \\n+      V      log      qr_s(%)|dt   \\n \\n+      du,.\\n(2) Here,      g,   \\n \\n€      A(R“)      denotes      the      marginal      distribution      at      time   \\n \\nt      induced      by      the      reference      SDE (1).\\nNotably,      the      marginal      distribution      of      zr_;      is      the      same      as      the      one      of      x;      induced      by      the  \\n \\n  reference      SDE.</p><p> \\n \\n  These      observations      suggest      that      with      sufficiently      large      T,      starting      from      V      (0,      I)      and      following      the  \\n \\n  time-reversal      SDE      (2)   \\n \\n,      we      are      able      to      sample      from      the      data      distribution      (i.e.,      p?\\n\"°)      at      the      terminal  \\n \\n  time      point      7’.</p><p>Training      score      functions      from      the      training      data.\\nNow,      the      remaining      question      is      how      to      estimate  \\n \\n  the      marginal      score      function      V      log      q(-)      in      (2).\\nThis      can      be      computed      using      the      following      principles:</p><p>°\\n \\n   We      have      V      log      (a+)   \\n \\n=      Exgxppre[Va,      log      qyo(ae   \\n \\n|      2o)],      where      q@jo      represents      the      conditional  \\n \\n  distribution      of      x;      given      Xo      induced      by      the      reference      SDE.</p><p>¢\\n \\n   The      conditional      score      function      V,,,      log      dejo      (Xe   \\n \\n|      xo)      can      be      analytically      derived      as  \\n \\n  °</p><h5>      Myo      —      Ut</h5><p> \\n \\n  {ory      ue   \\n \\n=      exp(—0.5t)xo,      {o?\\n}?\\n \\n \\n=   \\n \\n1   \\n \\n—      exp(—0.5t).\\n(3) t Subsequently,      by      defining   \\n \\na      parameterized      model      sg   \\n \\n:      R¢   \\n \\nx      [0,7]   \\n \\n>      R¢      and      utilizing      the      following weighted      regression,      the      marginal      score      is      estimated:</p><p>A\\n \\n \\n  Dore   \\n \\n=      argmin      Eywuni((0,7))      ce~aejo(w+|ar0),c0~pPr®      [A(t)      ||      Ver,      log      jo      (Le   \\n \\n|      ro)   \\n \\n_      So(£t,      t)      17)      (4)</p><p> \\n \\n  where   \\n \\n2   \\n \\n:      [0,7]   \\n \\n+      R      is   \\n \\na      weighted      function.\\n \\n \\n  Note      we      typically      use      another      parametrization      well.\\nFrom      (3),      we      can      easily      see      that      this      score      is  \\n \\n  estimated      by      introducing      networks      €9   \\n \\n:      R@   \\n \\nx      [0,7]   \\n \\n—      R®,      which      aims      to      estimate      the      noise      Bere      eO</p><p>Ore   \\n \\n=      argmin      Ey.\\nuni({0,7]),cr~p2xo+o?\\ne~N      (0,1)      ,co~pPte      [A(t)/{op}?\\nlle   \\n \\n—      €9      (1,      t)      7)   \\n \\n:      (5)</p><p>Then,      by      denoting      the      training      data      as      {a}      and      setting      A(t)   \\n \\n=      {a7      }”,      the      actual      loss      function      from the      training      data      is argmin   \\n \\n)      Ey      ai(o.r))a   \\n \\nO      peal      +e02,e~N      (0,1)      [Ile   \\n \\n—      €o(ae?,      t)      [17].\\n(6) i=1 Inference      time.\\nOnce      this      Bore      (or      Ore)      is      learned,      we      insert      it      into      the      time-reversal      SDE      (2)      and use      it      as   \\n \\na      generative      model.\\nUltimately,      with      standard      discretization,      we      obtain:</p><p>p(      xe,      t;      a)   \\n \\n=      24+      (0.54,   \\n \\n+      Sire      (,,T      —t)|(ot),      {o?(t)}?\\n \\n \\n=      (6t).</p><p>Equivalently,      this      is (21,      t;      Ore)   \\n \\n=   \\n \\n2   \\n \\n+      [0.52   \\n \\n—      1/o?\\n \\n \\nx      e%,,,(a1,T   \\n \\n—      t)]|      (6t),      (7)</p><p>where      (dt)      denotes      the      discretization      step.</p><p> \\n \\n  Equivalence      to      DDPM.\\nThe      objective      function      derived      here      is      equivalent      to      the      one      formulated  \\n \\n  based      on      variational      inference      in      the      discretized      formulation,      which      is      commonly      referred      to      as  \\n \\n  DDPMs      (Ho      et      al.,      2020).\\nIn      DDPMs      (Ho      et      al.,      2020),      we      often      see      the      following      form:</p><p> \\n \\n \\n1      1-      (ey;</p><p> \\n \\n  it;      0)   \\n \\n=      ~~   \\n \\n’      ),  \\n \\n  p(xy   \\n \\n)      Xt      Jina      ae      [om</p><h6>Vor</h6><p>When      a;   \\n \\n=   \\n \\n1   \\n \\n—      (dt),      the      above      is      equivalent      to      (7)      when      dt      goes      to   \\n \\n0      by      noting      1/,/a;   \\n \\n©      1+0.5(d¢).</p><h4>1.1.1      Score-Based      Diffusion      Models      (Optional)</h4><p>nal)  \\n \\n  We      briefly      discuss      how      to      train      diffusion      models      in   \\n \\na      continuous-time      framework      (Song      et      al.,  \\n \\n  2021).\\nIn      our      tutorial,      this      section      is      mainly      used      to      discuss      several      algorithms      (e.g.,      value-weighted  \\n \\n  sampling      in      Section      6.2)      and      their      relationship      with      flow-based      diffusion      models      (Section      9)      later.</p><p> \\n \\n  Therefore,      readers      may      skip      it      based      on      their      individual      needs.</p><p> \\n \\n  The      training      process      of      diffusion      models      can      be      summarized      as      follows.\\nOur      objective      is      to      train  \\n \\n  a      sequential      mapping      from   \\n \\na      known      noise      distribution      to   \\n \\na      data      distribution,      formalized      through  \\n \\n  a      stochastic      differential      equation      (SDE).\\nFirstly,      we      define   \\n \\na      forward      (fixed)      SDE      that      maps      from  \\n \\n  the      data      distribution      to      the      noise      distribution.\\nThen,   \\n \\na      time-reversal      SDE      is      expressed      as      an      SDE,  \\n \\n  which      includes      the      (unknown)      score      function.\\nNow,      by      learning      this      unknown      score      function      from  \\n \\n  the      training      data,      the      time-reversal      SDE      can      be      utilized      as   \\n \\na      generative      model.\\nHere      are      the      details.</p><p>Forward      and      time-reversal      SDE.\\nFirstly,      we      introduce   \\n \\na      forward      (a.k.a.,      reference)      Stochastic  \\n \\n  differential      equations      (SDE)      from   \\n \\n0      to      7’.\\n \\n \\nA      common      choice      is   \\n \\na      variance-preserving      (VP)      process:</p><p>t\\n \\n   €\\n \\n   [0,7];      dx,   \\n \\n=      —0.52,dt   \\n \\n+      du;,,      x9   \\n \\n~      p(x),      (1)</p><p>where      dw;      represents      standard      Brownian      motion.\\nHere      are      two      crucial      observations:</p><p>¢\\n \\n   As   \\n \\nT      approaches      oo,      the      limiting      distribution      is      \\\\\\\\\\\\\\\\V(0,      I).</p><p>¢\\n \\n   The      time-reversal      SDE      (Anderson,      1982),      which      preserves      the      marginal      distribution,      is  \\n \\n  expressed      as      follows:</p><p>t\\n \\n   €\\n \\n   [0,7];      dz,   \\n \\n=      [0.5z,   \\n \\n+      V      log      qr_s(%)|dt   \\n \\n+      du,.\\n(2) Here,      g,   \\n \\n€      A(R“)      denotes      the      marginal      distribution      at      time   \\n \\nt      induced      by      the      reference      SDE (1).\\nNotably,      the      marginal      distribution      of      zr_;      is      the      same      as      the      one      of      x;      induced      by      the  \\n \\n  reference      SDE.</p><p> \\n \\n  These      observations      suggest      that      with      sufficiently      large      T,      starting      from      V      (0,      I)      and      following      the  \\n \\n  time-reversal      SDE      (2)   \\n \\n,      we      are      able      to      sample      from      the      data      distribution      (i.e.,      p?\\n\"°)      at      the      terminal  \\n \\n  time      point      7’.</p><p>Training      score      functions      from      the      training      data.\\nNow,      the      remaining      question      is      how      to      estimate  \\n \\n  the      marginal      score      function      V      log      q(-)      in      (2).\\nThis      can      be      computed      using      the      following      principles:</p><p>°\\n \\n   We      have      V      log      (a+)   \\n \\n=      Exgxppre[Va,      log      qyo(ae   \\n \\n|      2o)],      where      q@jo      represents      the      conditional  \\n \\n  distribution      of      x;      given      Xo      induced      by      the      reference      SDE.</p><p>¢\\n \\n   The      conditional      score      function      V,,,      log      dejo      (Xe   \\n \\n|      xo)      can      be      analytically      derived      as  \\n \\n  °</p><h5>      Myo      —      Ut</h5><p> \\n \\n  {ory      ue   \\n \\n=      exp(—0.5t)xo,      {o?\\n}?\\n \\n \\n=   \\n \\n1   \\n \\n—      exp(—0.5t).\\n(3) t Subsequently,      by      defining   \\n \\na      parameterized      model      sg   \\n \\n:      R¢   \\n \\nx      [0,7]   \\n \\n>      R¢      and      utilizing      the      following weighted      regression,      the      marginal      score      is      estimated:</p><p>A\\n \\n \\n  Dore   \\n \\n=      argmin      Eywuni((0,7))      ce~aejo(w+|ar0),c0~pPr®      [A(t)      ||      Ver,      log      jo      (Le   \\n \\n|      ro)   \\n \\n_      So(£t,      t)      17)      (4)</p><p> \\n \\n  where   \\n \\n2   \\n \\n:      [0,7]   \\n \\n+      R      is   \\n \\na      weighted      function.\\n \\n \\n  Note      we      typically      use      another      parametrization      well.\\nFrom      (3),      we      can      easily      see      that      this      score      is  \\n \\n  estimated      by      introducing      networks      €9   \\n \\n:      R@   \\n \\nx      [0,7]   \\n \\n—      R®,      which      aims      to      estimate      the      noise      Bere      eO</p><p>Ore   \\n \\n=      argmin      Ey.\\nuni({0,7]),cr~p2xo+o?\\ne~N      (0,1)      ,co~pPte      [A(t)/{op}?\\nlle   \\n \\n—      €9      (1,      t)      7)   \\n \\n:      (5)</p><p>Then,      by      denoting      the      training      data      as      {a}      and      setting      A(t)   \\n \\n=      {a7      }”,      the      actual      loss      function      from the      training      data      is argmin   \\n \\n)      Ey      ai(o.r))a   \\n \\nO      peal      +e02,e~N      (0,1)      [Ile   \\n \\n—      €o(ae?,      t)      [17].\\n(6) i=1 Inference      time.\\nOnce      this      Bore      (or      Ore)      is      learned,      we      insert      it      into      the      time-reversal      SDE      (2)      and use      it      as   \\n \\na      generative      model.\\nUltimately,      with      standard      discretization,      we      obtain:</p><p>p(      xe,      t;      a)   \\n \\n=      24+      (0.54,   \\n \\n+      Sire      (,,T      —t)|(ot),      {o?(t)}?\\n \\n \\n=      (6t).</p><p>Equivalently,      this      is (21,      t;      Ore)   \\n \\n=   \\n \\n2   \\n \\n+      [0.52   \\n \\n—      1/o?\\n \\n \\nx      e%,,,(a1,T   \\n \\n—      t)]|      (6t),      (7)</p><p>where      (dt)      denotes      the      discretization      step.</p><p> \\n \\n  Equivalence      to      DDPM.\\nThe      objective      function      derived      here      is      equivalent      to      the      one      formulated  \\n \\n  based      on      variational      inference      in      the      discretized      formulation,      which      is      commonly      referred      to      as  \\n \\n  DDPMs      (Ho      et      al.,      2020).\\nIn      DDPMs      (Ho      et      al.,      2020),      we      often      see      the      following      form:</p><p> \\n \\n \\n1      1-      (ey;</p><p> \\n \\n  it;      0)   \\n \\n=      ~~   \\n \\n’      ),  \\n \\n  p(xy   \\n \\n)      Xt      Jina      ae      [om</p><h6>Vor</h6><p>When      a;   \\n \\n=   \\n \\n1   \\n \\n—      (dt),      the      above      is      equivalent      to      (7)      when      dt      goes      to   \\n \\n0      by      noting      1/,/a;   \\n \\n©      1+0.5(d¢).</p><h5>      Myo      —      Ut</h5><p> \\n \\n  {ory      ue   \\n \\n=      exp(—0.5t)xo,      {o?\\n}?\\n \\n \\n=   \\n \\n1   \\n \\n—      exp(—0.5t).\\n(3) t Subsequently,      by      defining   \\n \\na      parameterized      model      sg   \\n \\n:      R¢   \\n \\nx      [0,7]   \\n \\n>      R¢      and      utilizing      the      following weighted      regression,      the      marginal      score      is      estimated:</p><p>A\\n \\n \\n  Dore   \\n \\n=      argmin      Eywuni((0,7))      ce~aejo(w+|ar0),c0~pPr®      [A(t)      ||      Ver,      log      jo      (Le   \\n \\n|      ro)   \\n \\n_      So(£t,      t)      17)      (4)</p><p> \\n \\n  where   \\n \\n2   \\n \\n:      [0,7]   \\n \\n+      R      is   \\n \\na      weighted      function.\\n \\n \\n  Note      we      typically      use      another      parametrization      well.\\nFrom      (3),      we      can      easily      see      that      this      score      is  \\n \\n  estimated      by      introducing      networks      €9   \\n \\n:      R@   \\n \\nx      [0,7]   \\n \\n—      R®,      which      aims      to      estimate      the      noise      Bere      eO</p><p>Ore   \\n \\n=      argmin      Ey.\\nuni({0,7]),cr~p2xo+o?\\ne~N      (0,1)      ,co~pPte      [A(t)/{op}?\\nlle   \\n \\n—      €9      (1,      t)      7)   \\n \\n:      (5)</p><p>Then,      by      denoting      the      training      data      as      {a}      and      setting      A(t)   \\n \\n=      {a7      }”,      the      actual      loss      function      from the      training      data      is argmin   \\n \\n)      Ey      ai(o.r))a   \\n \\nO      peal      +e02,e~N      (0,1)      [Ile   \\n \\n—      €o(ae?,      t)      [17].\\n(6) i=1 Inference      time.\\nOnce      this      Bore      (or      Ore)      is      learned,      we      insert      it      into      the      time-reversal      SDE      (2)      and use      it      as   \\n \\na      generative      model.\\nUltimately,      with      standard      discretization,      we      obtain:</p><p>p(      xe,      t;      a)   \\n \\n=      24+      (0.54,   \\n \\n+      Sire      (,,T      —t)|(ot),      {o?(t)}?\\n \\n \\n=      (6t).</p><p>Equivalently,      this      is (21,      t;      Ore)   \\n \\n=   \\n \\n2   \\n \\n+      [0.52   \\n \\n—      1/o?\\n \\n \\nx      e%,,,(a1,T   \\n \\n—      t)]|      (6t),      (7)</p><p>where      (dt)      denotes      the      discretization      step.</p><p> \\n \\n  Equivalence      to      DDPM.\\nThe      objective      function      derived      here      is      equivalent      to      the      one      formulated  \\n \\n  based      on      variational      inference      in      the      discretized      formulation,      which      is      commonly      referred      to      as  \\n \\n  DDPMs      (Ho      et      al.,      2020).\\nIn      DDPMs      (Ho      et      al.,      2020),      we      often      see      the      following      form:</p><p> \\n \\n \\n1      1-      (ey;</p><p> \\n \\n  it;      0)   \\n \\n=      ~~   \\n \\n’      ),  \\n \\n  p(xy   \\n \\n)      Xt      Jina      ae      [om</p><h6>Vor</h6><p>When      a;   \\n \\n=   \\n \\n1   \\n \\n—      (dt),      the      above      is      equivalent      to      (7)      when      dt      goes      to   \\n \\n0      by      noting      1/,/a;   \\n \\n©      1+0.5(d¢).</p><h6>Vor</h6><p>When      a;   \\n \\n=   \\n \\n1   \\n \\n—      (dt),      the      above      is      equivalent      to      (7)      when      dt      goes      to   \\n \\n0      by      noting      1/,/a;   \\n \\n©      1+0.5(d¢).</p><h3>1.2      Fine-Tuning      Diffusion      Models      with      RL</h3><p>RL  \\n \\n  Importantly,      our      focus      on      RL-based      fine-tuning      distinguishes      itself      from      the      standard      fine-tuning  \\n \\n  methods.\\nStandard      fine-tuning      typically      involves      scenarios      where      we      have      pre-trained      models  \\n \\n  (e.g.,      diffusion      models)      and      new      training      data      {2,      y}.\\nIn      such      cases,      the      common      approach  \\n \\n  for      fine-tuning      is      to      retrain      diffusion      models      with      the      new      training      data      using      the      same      loss  \\n \\n  function      employed      during      pre-training.\\nIn      sharp      contrast,      RL-based      fine-tuning      directly      employs  \\n \\n  the      downstream      reward      functions      as      the      primary      optimization      objectives,      making      the      loss      functions  \\n \\n  different      from      those      used      in      pre-training.</p><p> \\n \\n  Hereafter,      we      start      with   \\n \\na      concise      overview      of      RL-based      fine-tuning.\\nThen,      before      delving      into  \\n \\n  specifics,      we      discuss      simpler      non-RL      alternatives      to      provide      motivation      for      adopting      RL-based  \\n \\n  fine-tuning.</p><li>1.2.1      Brief      Overview:      Fine-tuning      with      RL</li><p>In      this      article,      we      explore      the      fine-tuning      of      pre-trained      diffusion      models      to      optimize      downstream  \\n \\n  reward      functions      r   \\n \\n:      R¢   \\n \\n+      R.      In      domains      such      as      images,      these      backbone      diffusion      models      to      be  \\n \\n  fine-tuned      include      Stable      Diffusion      (Rombach      et      al.,      2022),      while      the      reward      functions      are      aesthetic  \\n \\n  scores      and      alignment      scores      (Clark      et      al.,      2023;      Black      et      al.,      2023;      Fan      et      al.,      2023).\\nMore      examples  \\n \\n  are      detailed      in      the      introduction.\\nThese      rewards      are      often      unknown,      necessitating      learning      from  \\n \\n  data      with      feedback:      {2      r(x™)}.\\nWe      will      explore      this      aspect      further      in      Section      7.\\nUntil      then,      we  \\n \\n  assume      r      is      known.\\n \\n \\n  Now,      readers      may      wonder      about      the      objectives      we      aim      to      achieve      during      the      fine-tuning      process.\\n \\n \\n  A      natural      approach      is      to      define      the      optimization      problem:</p><p>argmax      E,.\\n[r()|      (8)  \\n \\n  qeA(¥)  \\n \\n  where   \\n \\nq      is      initialized      with   \\n \\na      pre-trained      diffusion      model      p?\\n*®   \\n \\n€      A(2).\\nIn      this      tutorial,      we      will      detail  \\n \\n  the      procedure      of      solving      (8)      with      RL      in      the      upcoming      sections.\\nIn      essence,      we      leverage      the      fact  \\n \\n  that      diffusion      models      are      formulated      as   \\n \\na      sequential      decision-making      problem,      where      each      decision  \\n \\n  corresponds      to      how      samples      are      denoised.</p><p> \\n \\n  Although      the      above      objective      function      (8)      is      reasonable,      the      resulting      distribution      might      deviate  \\n \\n  too      much      from      the      pre-trained      diffusion      model.\\nTo      circumvent      this      issue,   \\n \\na      natural      way      is      to      add  \\n \\n  penalization      against      pre-trained      diffusion      models.\\nThen,      the      target      distribution      is      defined      as:</p><p>argmax      E,.4[r(x)]   \\n \\n—      aKL(q||p’*).\\n(9)  \\n \\n  qeA(¥) Notably,      (9)      reduces      to      the      following      distribution:</p><li>(10)</li><p>5\\n \\n   exp(r()/a)p\"\"()  \\n \\n  Pr)   \\n \\n=      Fra)      apa)</p><p> \\n \\n  Here,      the      first      term      in      (9)      corresponds      to      the      mean      reward,      which      we      want      to      optimize      in      the  \\n \\n  fine-tuning      process.\\nThe      second      term      in      (10)      serves      as   \\n \\na      penalty      term,      indicating      the      deviation      of   \\n \\nq       from      the      pre-trained      model.\\nThe      parameter   \\n \\na      controls      the      strength      of      this      regularization      term.\\nThe  \\n \\n  proper      choice      of   \\n \\na      depends      on      the      task      we      are      interested      in.</p><h6>1.2.2      Motivation      for      Using      RL      over      Non-RL      Alternatives</h6><p>ives  \\n \\n  To      achieve      our      goal      of      maximizing      downstream      reward      functions      with      diffusion      models,      readers  \\n \\n  may      question      whether      alternative      approaches      can      be      employed      apart      from      RL.\\nHere,      we      investigate  \\n \\n  these      potential      alternatives      and      explain      why      RL      approaches      may      offer      advantages      over      them.</p><p> \\n \\n  Rejection      sampling.\\nOne      approach      involves      generating      multiple      samples      from      pre-trained      diffu-  \\n \\n  sion      models      and      selecting      only      those      with      high      rewards.\\nThis      method,      called      rejection      sampling,  \\n \\n  operates      without      needing      fine-tuning.\\nHowever,      rejection      sampling      is      effective      primarily      when      the  \\n \\n  pre-trained      model      already      has   \\n \\na      high      probability      of      producing      high-reward      samples.\\nIt      resembles  \\n \\n  sampling      from   \\n \\na      prior      distribution      to      obtain      posterior      samples      (in      this      case,      high-reward      points).\\n \\n \\n  This      approach      works      efficiently      when      the      posterior      closely      matches      the      prior      but      can      become      highly  \\n \\n  inefficient      otherwise.\\nIn      contrast,      by      explicitly      updating      weight      in      diffusion      models,      RL-based      fine-  \\n \\n  tuning      allows      us      to      obtain      these      high-reward      samples,      which      are      seldom      generated      by      pre-trained  \\n \\n  models.</p><p> \\n \\n  Conditional      diffusion      models      (classifier-free      guidance).\\nIn      conditional      generative      models,      the  \\n \\n  general      goal      is      to      sample      from      p(|c),      where   \\n \\nx      is      the      output      and   \\n \\nc      denotes      the      conditioning      variable.\\n \\n \\n  For      example,      in      text-to-language      diffusion      models,   \\n \\nc      is   \\n \\na      text,      and   \\n \\nx      is      the      generated      image.\\nSimilarly,  \\n \\n  in      the      context      of      protein      engineering      for      addressing      inverse      folding      problems,      models      often      define   \\n \\nc       as      the      protein      backbone      structure      and   \\n \\nx      as      the      corresponding      amino      acid      sequence.\\nHere,      using      the  \\n \\n  training      data      {c,      ¢},      the      model      is      trained      by      using      the      loss      function:</p><p>argmin   \\n \\n»      E,uni(l0,7)2)      ~upa      +o?\\nsewN\\\\\\\\\\\\\\'(O)      [lle   \\n \\n—      co(2”,      }),      t)      I\"), t where      the      denoising      function      €g      additionally      receives      the      conditioning      information   \\n \\nc      as      input.\\nIn  \\n \\n  practice,   \\n \\na      variety      of      improvements      such      as      classifier-free      guidance      (Ho      and      Salimans,      2022)      can  \\n \\n  further      improve      the      model’s      ability      to      learn      the      conditional      distribution      p(|c).</p><p> \\n \\n  These      conditional      generative      models      can      be      used      to      optimize      down-stream      rewards      by      condi-  \\n \\n  tioning      on      the      reward      values,      then      sampling   \\n \\nx      conditioned      on      high      reward      values      (Krishnamoorthy  \\n \\n  et      al.,      2023;      Yuan      et      al.,      2023).\\nWhile      this      method      is,      in      principle,      capable      of      generating      plausible x\\n \\n   values      across   \\n \\na      range      of      reward      levels      within      the      training      data      distribution,      it      is      not      the      most  \\n \\n  effective      optimization      strategy.\\nThis      is      primarily      because      high-reward      inputs      frequently      reside  \\n \\n  in      the      tails      of      the      training      distribution      or      even      beyond      it.\\nConsequently,      this      method      may      not  \\n \\n  effectively      generate      high-reward      samples      that      lie      outside      the      training      data      distribution.\\nIn      contrast,  \\n \\n  RL-based      fine-tuning      has      the      capability      to      generate      samples      with      higher      rewards      beyond      the      training  \\n \\n  data.\\nThis      is      achieved      by      explicitly      maximizing      reward      models      learned      from      the      training      data  \\n \\n  and      leveraging      their      extrapolative      capabilities      of      reward      models,      as      theoretically      formalized      and  \\n \\n  empirically      observed      in      Uehara      et      al.\\n(2024).</p><p>Reward-weighted      training.\\nAnother      alternative      approach      is      to      use   \\n \\na      reward-weighted      version  \\n \\n  of      the      standard      training      loss      for      diffusion      models.\\nSuppose      that      we      have      data      {x      r(a)}.\\nThen, after      learning   \\n \\na      reward   \\n \\n7   \\n \\n:   \\n \\n¥   \\n \\n—      R      with      regression      from      the      data,      to      achieve      our      goal,      it      looks  \\n \\n  natural      to      use   \\n \\na      reward-weighted      version      of      the      training      loss      for      diffusion      models      (5),      1.e.,.\\na7      (4)      i)   \\n \\n2 areaun   \\n \\n»      E,uni((0,2))      peel?\\n+eo¢,enncoal      (to      ile   \\n \\n—      co(ay?,t)      |\") i\\n \\n    There      are      two      potential      drawbacks      to      this      approach.\\nFirst,      in      practice,      it      may      struggle      to      generate  \\n \\n  samples      with      higher      rewards      beyond      the      training      data.\\nAs      we      will      explain      later,      many      RL      algorithms  \\n \\n  are      more      directly      focused      on      optimizing      reward      functions,      which      are      expected      to      excel      in      obtaining  \\n \\n  samples      with      high      rewards      not      observed      in      the      original      data,      as      empirically      observed      in      Black      et      al.\\n \\n \\n  (2023).\\nSecond,      when      fine-tuning   \\n \\na      conditional      diffusion      model      p(x|c),      the      alternative      approach  \\n \\n  here      requires   \\n \\na      pair      of      {c,      «}      during      fine-tuning      to      ensure      the      validity      of      the      loss      function.\\nWhen  \\n \\n  we      only      have      data      {2      r(x      )}      but      not      {c,      2,      r(a)},      this      implies      that      we      might      need      to      solve an      inverse      problem      from   \\n \\nz      to      c,      which      can      often      be      challenging.\\nIn      contrast,      in      these      scenarios,      RL  \\n \\n  algorithms,      which      we      will      introduce      later,      can      operate      without      needing      such      pairs      {c,      x},      as  \\n \\n  long      as      we      have      learned      reward      functions      7.  \\n \\n  Finally,      it      should      be      noted      that      reward-weighted      training      technically      falls      under      the      broader  \\n \\n  category      of      RL      methods.\\nIt      shares   \\n \\na      close      connection      with      “reward-weighted      MLE”      introduced      in  \\n \\n  Section      6.1,      as      discussed      later.\\nEmploying      this      reward-weighted      MLE      helps      address      the      second  \\n \\n  concern      of      “reward-weighted      training”      mentioned      earlier.</p><p>2\\n \\n   Brief      Overview      of      Entropy-Regularized      MDPs</p><p> \\n \\n  In      this      tutorial,      we      explain      how      fine-tuning      diffusion      models      can      be      naturally      formulated      as      an      RL  \\n \\n  problem      in      entropy-regularized      MDPs.\\nThis      perspective      is      natural      because      RL      involves      sequential  \\n \\n  decision-making,      and   \\n \\na      diffusion      model      is      formulated      as   \\n \\na      sequential      problem      where      each      denoising  \\n \\n  step      is   \\n \\na      decision-making      process.\\nTo      connect      diffusion      models      with      RL,      we      begin      with   \\n \\na      concise  \\n \\n  overview      of      RL      in      standard      entropy-regularized      MDPs      (Haarnoja      et      al.,      2017;      Neu      et      al.,      2017;</p><p> \\n \\n  Geist      et      al.,      2019;      Schulman      et      al.,      2017).</p><li>2.1      MDPs</li><p> \\n \\n  An      MDP      is      defined      as      follows:      {S,A,      {P\"*}/_5,      {r:}-9,      po}      where      S      is      the      state      space,   \\n \\nA      is      the  \\n \\n  action      space,      P;\"*      is   \\n \\na      transition      dynamic      mapping:      S   \\n \\nx   \\n \\nA   \\n \\n—      A(S),      7,   \\n \\n:      S   \\n \\nx   \\n \\nA   \\n \\n>      R      denotes  \\n \\n  reward      received      at   \\n \\nt      and      po      is      an      initial      distribution      over      S.   \\n \\nA      policy      7,   \\n \\n:      S   \\n \\n—      A(A)      is   \\n \\na      map      from  \\n \\n  any      state      s   \\n \\n€      S      to      the      distribution      over      actions.\\nThe      standard      goal      in      RL      is      to      solve</p><h6>1.2.2      Motivation      for      Using      RL      over      Non-RL      Alternatives</h6><p>ives  \\n \\n  To      achieve      our      goal      of      maximizing      downstream      reward      functions      with      diffusion      models,      readers  \\n \\n  may      question      whether      alternative      approaches      can      be      employed      apart      from      RL.\\nHere,      we      investigate  \\n \\n  these      potential      alternatives      and      explain      why      RL      approaches      may      offer      advantages      over      them.</p><p> \\n \\n  Rejection      sampling.\\nOne      approach      involves      generating      multiple      samples      from      pre-trained      diffu-  \\n \\n  sion      models      and      selecting      only      those      with      high      rewards.\\nThis      method,      called      rejection      sampling,  \\n \\n  operates      without      needing      fine-tuning.\\nHowever,      rejection      sampling      is      effective      primarily      when      the  \\n \\n  pre-trained      model      already      has   \\n \\na      high      probability      of      producing      high-reward      samples.\\nIt      resembles  \\n \\n  sampling      from   \\n \\na      prior      distribution      to      obtain      posterior      samples      (in      this      case,      high-reward      points).\\n \\n \\n  This      approach      works      efficiently      when      the      posterior      closely      matches      the      prior      but      can      become      highly  \\n \\n  inefficient      otherwise.\\nIn      contrast,      by      explicitly      updating      weight      in      diffusion      models,      RL-based      fine-  \\n \\n  tuning      allows      us      to      obtain      these      high-reward      samples,      which      are      seldom      generated      by      pre-trained  \\n \\n  models.</p><p> \\n \\n  Conditional      diffusion      models      (classifier-free      guidance).\\nIn      conditional      generative      models,      the  \\n \\n  general      goal      is      to      sample      from      p(|c),      where   \\n \\nx      is      the      output      and   \\n \\nc      denotes      the      conditioning      variable.\\n \\n \\n  For      example,      in      text-to-language      diffusion      models,   \\n \\nc      is   \\n \\na      text,      and   \\n \\nx      is      the      generated      image.\\nSimilarly,  \\n \\n  in      the      context      of      protein      engineering      for      addressing      inverse      folding      problems,      models      often      define   \\n \\nc       as      the      protein      backbone      structure      and   \\n \\nx      as      the      corresponding      amino      acid      sequence.\\nHere,      using      the  \\n \\n  training      data      {c,      ¢},      the      model      is      trained      by      using      the      loss      function:</p><p>argmin   \\n \\n»      E,uni(l0,7)2)      ~upa      +o?\\nsewN\\\\\\\\\\\\\\'(O)      [lle   \\n \\n—      co(2”,      }),      t)      I\"), t where      the      denoising      function      €g      additionally      receives      the      conditioning      information   \\n \\nc      as      input.\\nIn  \\n \\n  practice,   \\n \\na      variety      of      improvements      such      as      classifier-free      guidance      (Ho      and      Salimans,      2022)      can  \\n \\n  further      improve      the      model’s      ability      to      learn      the      conditional      distribution      p(|c).</p><p> \\n \\n  These      conditional      generative      models      can      be      used      to      optimize      down-stream      rewards      by      condi-  \\n \\n  tioning      on      the      reward      values,      then      sampling   \\n \\nx      conditioned      on      high      reward      values      (Krishnamoorthy  \\n \\n  et      al.,      2023;      Yuan      et      al.,      2023).\\nWhile      this      method      is,      in      principle,      capable      of      generating      plausible x\\n \\n   values      across   \\n \\na      range      of      reward      levels      within      the      training      data      distribution,      it      is      not      the      most  \\n \\n  effective      optimization      strategy.\\nThis      is      primarily      because      high-reward      inputs      frequently      reside  \\n \\n  in      the      tails      of      the      training      distribution      or      even      beyond      it.\\nConsequently,      this      method      may      not  \\n \\n  effectively      generate      high-reward      samples      that      lie      outside      the      training      data      distribution.\\nIn      contrast,  \\n \\n  RL-based      fine-tuning      has      the      capability      to      generate      samples      with      higher      rewards      beyond      the      training  \\n \\n  data.\\nThis      is      achieved      by      explicitly      maximizing      reward      models      learned      from      the      training      data  \\n \\n  and      leveraging      their      extrapolative      capabilities      of      reward      models,      as      theoretically      formalized      and  \\n \\n  empirically      observed      in      Uehara      et      al.\\n(2024).</p><p>Reward-weighted      training.\\nAnother      alternative      approach      is      to      use   \\n \\na      reward-weighted      version  \\n \\n  of      the      standard      training      loss      for      diffusion      models.\\nSuppose      that      we      have      data      {x      r(a)}.\\nThen, after      learning   \\n \\na      reward   \\n \\n7   \\n \\n:   \\n \\n¥   \\n \\n—      R      with      regression      from      the      data,      to      achieve      our      goal,      it      looks  \\n \\n  natural      to      use   \\n \\na      reward-weighted      version      of      the      training      loss      for      diffusion      models      (5),      1.e.,.\\na7      (4)      i)   \\n \\n2 areaun   \\n \\n»      E,uni((0,2))      peel?\\n+eo¢,enncoal      (to      ile   \\n \\n—      co(ay?,t)      |\") i\\n \\n    There      are      two      potential      drawbacks      to      this      approach.\\nFirst,      in      practice,      it      may      struggle      to      generate  \\n \\n  samples      with      higher      rewards      beyond      the      training      data.\\nAs      we      will      explain      later,      many      RL      algorithms  \\n \\n  are      more      directly      focused      on      optimizing      reward      functions,      which      are      expected      to      excel      in      obtaining  \\n \\n  samples      with      high      rewards      not      observed      in      the      original      data,      as      empirically      observed      in      Black      et      al.\\n \\n \\n  (2023).\\nSecond,      when      fine-tuning   \\n \\na      conditional      diffusion      model      p(x|c),      the      alternative      approach  \\n \\n  here      requires   \\n \\na      pair      of      {c,      «}      during      fine-tuning      to      ensure      the      validity      of      the      loss      function.\\nWhen  \\n \\n  we      only      have      data      {2      r(x      )}      but      not      {c,      2,      r(a)},      this      implies      that      we      might      need      to      solve an      inverse      problem      from   \\n \\nz      to      c,      which      can      often      be      challenging.\\nIn      contrast,      in      these      scenarios,      RL  \\n \\n  algorithms,      which      we      will      introduce      later,      can      operate      without      needing      such      pairs      {c,      x},      as  \\n \\n  long      as      we      have      learned      reward      functions      7.  \\n \\n  Finally,      it      should      be      noted      that      reward-weighted      training      technically      falls      under      the      broader  \\n \\n  category      of      RL      methods.\\nIt      shares   \\n \\na      close      connection      with      “reward-weighted      MLE”      introduced      in  \\n \\n  Section      6.1,      as      discussed      later.\\nEmploying      this      reward-weighted      MLE      helps      address      the      second  \\n \\n  concern      of      “reward-weighted      training”      mentioned      earlier.</p><p>2\\n \\n   Brief      Overview      of      Entropy-Regularized      MDPs</p><p> \\n \\n  In      this      tutorial,      we      explain      how      fine-tuning      diffusion      models      can      be      naturally      formulated      as      an      RL  \\n \\n  problem      in      entropy-regularized      MDPs.\\nThis      perspective      is      natural      because      RL      involves      sequential  \\n \\n  decision-making,      and   \\n \\na      diffusion      model      is      formulated      as   \\n \\na      sequential      problem      where      each      denoising  \\n \\n  step      is   \\n \\na      decision-making      process.\\nTo      connect      diffusion      models      with      RL,      we      begin      with   \\n \\na      concise  \\n \\n  overview      of      RL      in      standard      entropy-regularized      MDPs      (Haarnoja      et      al.,      2017;      Neu      et      al.,      2017;</p><p> \\n \\n  Geist      et      al.,      2019;      Schulman      et      al.,      2017).</p><li>2.1      MDPs</li><p> \\n \\n  An      MDP      is      defined      as      follows:      {S,A,      {P\"*}/_5,      {r:}-9,      po}      where      S      is      the      state      space,   \\n \\nA      is      the  \\n \\n  action      space,      P;\"*      is   \\n \\na      transition      dynamic      mapping:      S   \\n \\nx   \\n \\nA   \\n \\n—      A(S),      7,   \\n \\n:      S   \\n \\nx   \\n \\nA   \\n \\n>      R      denotes  \\n \\n  reward      received      at   \\n \\nt      and      po      is      an      initial      distribution      over      S.   \\n \\nA      policy      7,   \\n \\n:      S   \\n \\n—      A(A)      is   \\n \\na      map      from  \\n \\n  any      state      s   \\n \\n€      S      to      the      distribution      over      actions.\\nThe      standard      goal      in      RL      is      to      solve</p><h1>T</h1><p>So      ri(si,      2)      (11)  \\n \\n  argmax      Ey      ,,}  \\n \\n  {re}  \\n \\n  In      entropy-regularized      MDPs,      we      consider      the      following      regularized      objective      instead:</p><p>t=0  \\n \\n  where      E;,,}[-]      is      the      expectation      induced      both      policy   \\n \\n7      and      the      transition      dynamics      as      follows:  \\n \\n  So   \\n \\n~      Po,      do   \\n \\n~      To(-|80),      51   \\n \\n~      P§\"(-|S0,@0),°+:.\\nAs      we      will      soon      detail      in      the      next      section  \\n \\n  (Section      3),      diffusion      models      can      naturally      be      framed      as      MDPs      as      each      policy      corresponds      to   \\n \\na       denoising      process      in      diffusion      models.</p><p>T\\n \\n \\n  {iy}   \\n \\n=      argmax      Ein)      Son      (51,      a4)   \\n \\n—      AKL(m:(-|s¢),      7:      (-]54))      (12) mt}      t—0  \\n \\n  where      7’   \\n \\n:      S      —>      A(A)      is   \\n \\na      certain      reference      policy.\\nThe      arg      max      solution      is      often      called   \\n \\na      set      of      soft  \\n \\n  optimal      policies.\\nCompared      to   \\n \\na      standard      objective      (11),      here      we      add      KL      terms      against      reference  \\n \\n  policies.\\nThis      addition      aims      to      ensure      that      soft      optimal      policies      closely      align      with      the      reference  \\n \\n  policies.\\nIn      the      context      of      fine-tuning      diffusion      models,      these      reference      policies      correspond      to  \\n \\n  the      pre-trained      diffusion      models,      as      we      aim      to      maintain      similarity      between      the      fine-tuned      and  \\n \\n  pre-trained      models.</p><p> \\n \\n  This      entropy-regularized      objective      in      (12)      has      been      widely      employed      in      RL      literature      due      to  \\n \\n  several      benefits      (Levine,      2018).\\nFor      instance,      in      online      RL,      it      is      known      that      these      policies      have  \\n \\n  good      exploration      properties      by      setting      reference      policies      as      uniform      policies      (Fox      et      al.,      2015;</p><p> \\n \\n  Haarnoja      et      al.,      2017).\\nIn      offline      RL,      Wu      et      al.\\n(2019)      suggests      using      these      policies      as      conservative  \\n \\n  policies      by      setting      reference      policies      close      to      behavior      policies      (policies      used      to      collect      offline      data).\\n \\n \\n  Additionally,      in      inverse      RL,      this      soft      optimal      policy      is      used      as      an      expert      policy      in      scenarios      where  \\n \\n  rewards      are      unobservable,      only      trajectories      from      expert      policies      are      available      (typically      referred      to  \\n \\n  as      maximum      entropy      RL      as      Ziebart      et      al.      (2008);      Wulfmeier      et      al.\\n(2015);      Finn      et      al.\\n(2016)).</p><h6>2.2      Key      Concepts:      Soft      Q-functions,      Soft      Bellman      Equations</h6><p>ions.</p><p> \\n \\n  The      crucial      question      in      RL      is      how      to      devise      algorithms      that      effectively      solve      the      optimization  \\n \\n  problem      (12).\\nThese      algorithms      are      later      used      as      fine-tuning      algorithms      of      diffusion      models.\\nTo      see  \\n \\n  these      algorithms,      we      rely      on      several      critical      concepts      in      entropy-regularized      MDPs.\\nSpecifically,  \\n \\n  soft-optimal      policies      (i.e.,      solutions      to      (12))      can      be      expressed      analytically      as   \\n \\na      blend      of      soft      Q-  \\n \\n  functions      and      reference      policies.\\nFurthermore,      these      soft      Q-functions      are      defined      as      solutions      to  \\n \\n  equations      known      as      soft      Bellman      equations.\\nWe      elaborate      on      these      foundational      concepts      below.</p><p>Soft      Q-functions      and      soft      optimal      policies.\\nSoft      optimal      policies      are      expressed      as   \\n \\na      blend      of      soft  \\n \\n  Q-functions      and      reference      policies.\\nTo      see      it,      we      define      the      soft      Q-function      as      follows:</p><p>T\\n \\n \\n  q(se,      a)   \\n \\n=      Egnsy      So      ral      ($4,      4%)   \\n \\n—      OBL      (m1      (-|8e41)      ley      1(-/Se41))      [S45      Ge   \\n \\n|   \\n \\n-      (13) k=t Then,      by      comparing      (13)      and      (12),      we      clearly      have m\\n \\n   =\\n \\n   argmax      Eq,~n(s,)[Ge(se,      a)   \\n \\n—      AKL(a(-|5¢)|]77(-   \\n \\n|      S2)|s¢].\\n(14)  \\n \\n  TE[XA(X)] Hence,      by      calculating      the      above      explicitly,   \\n \\na      soft      optimal      policy      in      (12)      is      described      as      follows:</p><p> \\n \\n  we)      og      _£xBlan(s.)      fant)  \\n \\n  mi      CIS)   \\n \\n&      Fe      qi(s,      a)      (ayn      (als)da      as)  \\n \\n  Soft      Bellman      equations.\\nWe      have      already      defined      soft      Q-functions      in      (13).\\nHowever,      this      form  \\n \\n  includes      the      soft      optimal      policies.\\nActually,      without      using      soft      optimal      policies,      the      soft      Q-function  \\n \\n  satisfies      the      following      recursive      equation      (a.k.a.      soft      Bellman      equation):</p><p>4(      St,      At)   \\n \\n=      Epps}      [rosuay)   \\n \\n+      alog   \\n \\n{   \\n \\n[      explara(siara)/a}n{(alsis)aa}   \\n \\n|      sia    \\n.\\n     (16)</p><p>This      is      proven      by      noting      we      recursively      have a\\n \\n   (Se,      ar)   \\n \\n=      Bers}      [re(      Se,      at)   \\n \\n+      G41      St41;      di41)   \\n \\n_      akKL      (74,4      (-|Se41),      Tai      (-|Se41))[Se,      ar|</p><p>By      substituting      (15)      into      the      above,      we      obtain      the      soft      Bellman      equation      (16).</p><p> \\n \\n  Soft      value      functions.\\nSo      far,      we      have      defined      the      soft      Q-functions,      which      depend      on      both      states  \\n \\n  and      actions.\\nWe      can      now      introduce   \\n \\na      related      concept      that      depends      solely      on      states,      termed      the      soft  \\n \\n  value      function.\\nThe      soft      value      function      is      defined      as      follows:</p><li>v4      (Sz)   \\n \\n=      tary T Tk(Sk;      Qk)   \\n \\n—      onto)</li><p>k=t Then,      the      soft      optimal      policy      in      (14)      is      also      written      as r*(-|s)   \\n \\nx      exp(q(s,      -)/o)m      Cs)  \\n \\n  ‘      exp(u;(s)/a) (17)</p><p>because      we      have exp      (ee)   \\n \\n=   \\n \\n/      exp      (a   \\n \\n)      m(a   \\n \\n|      s)da.</p><p>Then,      substituting      the      above      in      the      soft      Bellman      equation      (16),      it      is      written      as a\\n \\n   (Se,      at)   \\n \\n=      Egrs}      [7      (Se;      at)   \\n \\n+      p41      (Se41)|S¢,      ar).</p><p> \\n \\n  Algorithms      in      entropy-regularized      MDPs.\\nAs      outlined      in      Levine      (2018),      to      solve      (12),      various  \\n \\n  well-known      algorithms      exist      in      the      literature      on      RL.\\nThe      abovementioned      concepts      are      useful      in  \\n \\n  constructing      these      algorithms.\\nThese      include      policy      gradients,      which      gradually      optimize   \\n \\na      policy  \\n \\n  using   \\n \\na      policy      neural      network;      soft      Q-learning      algorithms,      which      utilize      the      soft-Bellman      equation  \\n \\n  and      approximate      the      soft-value      function      with   \\n \\na      value      neural      network;      and      soft      actor-critic      algorithms  \\n \\n  that      leverage      both      policy      and      value      neural      networks.\\nWe      will      explore      how      these      algorithms      can      be  \\n \\n  applied      in      the      context      of      diffusion      models      shortly      in      Section      4.2      and      6.</p><p>3\\n \\n   Fine-Tuning      Diffusion      Models      with      RL      in      Entropy      Regular- ized      MDPs</p><p> \\n \\n  In      this      section,      as      done      in      Fan      et      al.\\n(2023);      Black      et      al.\\n(2023);      Uehara      et      al.\\n(2024),      we      illustrate  \\n \\n  how      fine-tuning      can      be      formulated      as      an      RL      problem      in      soft-entropy      regularized      MDPs,      where      each  \\n \\n  PT   \\n \\nx      PT-1      rp_9      PT-2      P2      r1      Pl      Lo  \\n \\n  LT      T-1</p><p>Figure      2:      Formulating      fine-tuning      in      diffusion      models      using      MDPs.</p><p> \\n \\n  denoising      step      of      diffusion      models      corresponds      to   \\n \\na      policy      in      RL.\\nFinally,      we      outline   \\n \\na      specific      RL  \\n \\n  problem      of      interest      in      our      context.</p><p> \\n \\n  To      cast      fine-tuning      diffusion      models      as      an      RL      problem,      we      start      with      defining      the      following  \\n \\n  MDP:</p><p>The      state      space      S      and      action      space   \\n \\nA      correspond      to      the      input      space      1.</p><p>The      transition      dynamics      at      time   \\n \\nt      (i.e.,      P;)      is      an      identity      map      6(5;4;   \\n \\n=      a;).</p><p>The      reward      at      time   \\n \\nt   \\n \\n€      |0,---   \\n \\n,      7]      (i.e.,      r¢)      is      provided      only      at   \\n \\nT      as      r      (down-stream      reward  \\n \\n  function);      but   \\n \\n0      at      other      time      steps.</p><p>The      policy      at      time   \\n \\nt      (i.e,      7)      corresponds      to      pr41;_,:   \\n \\n¥   \\n \\n>      A(X).</p><p>The      initial      distribution      at      time   \\n \\n0      corresponds      to      pr,   \\n \\n€      A(4).\\nWith      slight      abuse      of      notation,  \\n \\n  we      often      denote      it      by      pr41(-|-),      while      this      is      just      pr+4(-).</p><p> \\n \\n  The      reference      policy      at   \\n \\nt      (i.e.,      7;)      corresponds      to   \\n \\na      denoising      process      in      the      pre-trained      model  \\n \\n  pre  \\n \\n  Pr+i-t We      list      several      things      to      note.</p><p> \\n \\n \\n¢      We      reverse      the      time-evolving      process      to      adhere      to      the      standard      notation      in      diffusion      models,  \\n \\n  i.e.,      from   \\n \\nt   \\n \\n=      T\\\\\\\\\\\\\\'      tot   \\n \\n=      0.\\nHence,      s;      in      standard      MDPs      corresponds      to      x74      _;      in      diffusion  \\n \\n  models.</p><p>¢\\n \\n   In      our      context,      unlike      standard      RL      scenarios,      the      transition      dynamics      are      known.</p><p>Key      RL      Problem.\\nNow,      by      reformulating      the      original      objective      of      standard      RL      into      our      contexts,  \\n \\n  the      objective      function      in      (12)      reduces      to      the      following:</p><p> \\n \\n  {pi}:   \\n \\n=      argmax      Eg,   \\n \\n3      (r(x)   \\n \\na      Deep      Epp      [KL      (pe(-|24)      lve      (-|ee))]      8)  \\n \\n  {pie      [RES      A(RY)]      Hay  \\n \\n  Reward      KL      penalty where      the      expectation      E,,,,;[-]      is      taken      with      respect      to      Tierys      Di(@r-1|@1),      Le.\\n@r   \\n \\n~      prai(-),      7-1   \\n \\n~       pr-i(-   \\n \\n|      &r-1),r_-2   \\n \\n~      pr—a(-   \\n \\n|      er_2),--+.\\nIn      this      article,      we      set      this      as      an      objective      function      in  \\n \\n  fine-tuning      diffusion      models.\\nThis      objective      is      natural      as      it      seeks      to      optimize      sequential      denoising  \\n \\n  processes      to      maximize      downstream      rewards      while      maintaining      proximity      to      pre-trained      models.</p><p> \\n \\n  Subsequently,      we      investigate      several      algorithms      to      solve      (18).\\nBefore      discussing      these      algorithms,  \\n \\n  we      summarize      several      key      theoretical      properties      that      will      aid      their      derivation.</p><p>4\\n \\n   Theory      of      RL-Based      Fine-Tuning</p><p> \\n \\n  So      far,      we      have      introduced   \\n \\na      certain      RL      problem      (i.e.,      (18))      as   \\n \\na      fine-tuning      diffusion      model.\\nIn  \\n \\n  this      section,      we      explain      that      solving      this      RL      problem      allows      us      to      achieve      the      target      distribution  \\n \\n  discussed      in      Section      1.2.1.\\nAdditionally,      we      present      several      important      theoretical      properties,      such  \\n \\n  as      the      analytical      form      of      marginal      distributions      and      posterior      distributions      induced      by      fine-tuned  \\n \\n  models.\\nThis      formulation      is      also      instrumental      in      introducing      several      algorithms      (reward-weighted  \\n \\n  MLE,      value-weighted      sampling,      and      path      consistency      learning      in      Section      6),      and      establishing  \\n \\n  connections      with      related      areas      (classifier      guidance      in      Section      8,      and      flow-based      diffusion      models      in  \\n \\n  Section      9).\\nWe      start      with      several      key      concepts.</p><li>4.1      Key      Concepts:      Soft      Value      functions      and      Soft      Bellman      Equations.</li><p> \\n \\n  Now,      reflecting      on      how      soft      optimal      policies      are      expressed      using      soft      value      functions      in      Section   \\n \\n2      in  \\n \\n  the      context      of      standard      RL      problems,      we      derive      several      important      concepts      applicable      to      fine-tuning  \\n \\n  diffusion      models.\\nThese      concepts      are      later      useful      in      constructing      algorithms      to      solve      our      RL      problem  \\n \\n  (18).</p><p>      Firstly,      as      we      see      in      (15),      soft-optimal      policies      are      characterized      as: Pe      (-|e1) expt      a(/aypl(      a9       f      exp(vr-a(ve-1)/@)      pe      (ar-1      |      v4)dx1-1 where      soft-value      functions      are      defined      as ve(es)      =      Egppy[r(@o)      —      oD      KL(pe(-|ere)      lle      Cle)      |e), 1       (a1,      t1-1)      =      Egsy(r(@o)      —      a      $5      KL(pe(-|e)      Pee      (len)      [es      era]      =      era      (1-1). k=t+1 Secondly,      as      we      see      in      (16),      the      soft-value      functions      are      also      recursively      defined      by      the      soft       Bellman      equations:       7      (22)      _      fexp      (uae)      De      (X41      |      v,)dx,-1      (t      =T+1,---      ,1), vo(%o)      =      (x0). (20)</p><p>Now      substituting      the      above      in      (19),      we      obtain sg.y   \\n \\n=      ex      (trea      )/a)eP\"C   \\n \\n|      2)  \\n \\n  PEC|e)      exp(v;      (x)      /a)      :</p><p> \\n \\n  As      mentioned      earlier,      these      soft      value      functions      and      their      recursive      form      will      later      serve      as      the  \\n \\n  basis      for      constructing      several      concrete      fine-tuning      algorithms      (such      as      reward-weighted      MLE      and  \\n \\n  value-weighted      sampling      in      Section      6).</p><h6>2.2      Key      Concepts:      Soft      Q-functions,      Soft      Bellman      Equations</h6><p>ions.</p><p> \\n \\n  The      crucial      question      in      RL      is      how      to      devise      algorithms      that      effectively      solve      the      optimization  \\n \\n  problem      (12).\\nThese      algorithms      are      later      used      as      fine-tuning      algorithms      of      diffusion      models.\\nTo      see  \\n \\n  these      algorithms,      we      rely      on      several      critical      concepts      in      entropy-regularized      MDPs.\\nSpecifically,  \\n \\n  soft-optimal      policies      (i.e.,      solutions      to      (12))      can      be      expressed      analytically      as   \\n \\na      blend      of      soft      Q-  \\n \\n  functions      and      reference      policies.\\nFurthermore,      these      soft      Q-functions      are      defined      as      solutions      to  \\n \\n  equations      known      as      soft      Bellman      equations.\\nWe      elaborate      on      these      foundational      concepts      below.</p><p>Soft      Q-functions      and      soft      optimal      policies.\\nSoft      optimal      policies      are      expressed      as   \\n \\na      blend      of      soft  \\n \\n  Q-functions      and      reference      policies.\\nTo      see      it,      we      define      the      soft      Q-function      as      follows:</p><p>T\\n \\n \\n  q(se,      a)   \\n \\n=      Egnsy      So      ral      ($4,      4%)   \\n \\n—      OBL      (m1      (-|8e41)      ley      1(-/Se41))      [S45      Ge   \\n \\n|   \\n \\n-      (13) k=t Then,      by      comparing      (13)      and      (12),      we      clearly      have m\\n \\n   =\\n \\n   argmax      Eq,~n(s,)[Ge(se,      a)   \\n \\n—      AKL(a(-|5¢)|]77(-   \\n \\n|      S2)|s¢].\\n(14)  \\n \\n  TE[XA(X)] Hence,      by      calculating      the      above      explicitly,   \\n \\na      soft      optimal      policy      in      (12)      is      described      as      follows:</p><p> \\n \\n  we)      og      _£xBlan(s.)      fant)  \\n \\n  mi      CIS)   \\n \\n&      Fe      qi(s,      a)      (ayn      (als)da      as)  \\n \\n  Soft      Bellman      equations.\\nWe      have      already      defined      soft      Q-functions      in      (13).\\nHowever,      this      form  \\n \\n  includes      the      soft      optimal      policies.\\nActually,      without      using      soft      optimal      policies,      the      soft      Q-function  \\n \\n  satisfies      the      following      recursive      equation      (a.k.a.      soft      Bellman      equation):</p><p>4(      St,      At)   \\n \\n=      Epps}      [rosuay)   \\n \\n+      alog   \\n \\n{   \\n \\n[      explara(siara)/a}n{(alsis)aa}   \\n \\n|      sia    \\n.\\n     (16)</p><p>This      is      proven      by      noting      we      recursively      have a\\n \\n   (Se,      ar)   \\n \\n=      Bers}      [re(      Se,      at)   \\n \\n+      G41      St41;      di41)   \\n \\n_      akKL      (74,4      (-|Se41),      Tai      (-|Se41))[Se,      ar|</p><p>By      substituting      (15)      into      the      above,      we      obtain      the      soft      Bellman      equation      (16).</p><p> \\n \\n  Soft      value      functions.\\nSo      far,      we      have      defined      the      soft      Q-functions,      which      depend      on      both      states  \\n \\n  and      actions.\\nWe      can      now      introduce   \\n \\na      related      concept      that      depends      solely      on      states,      termed      the      soft  \\n \\n  value      function.\\nThe      soft      value      function      is      defined      as      follows:</p><li>v4      (Sz)   \\n \\n=      tary T Tk(Sk;      Qk)   \\n \\n—      onto)</li><p>k=t Then,      the      soft      optimal      policy      in      (14)      is      also      written      as r*(-|s)   \\n \\nx      exp(q(s,      -)/o)m      Cs)  \\n \\n  ‘      exp(u;(s)/a) (17)</p><p>because      we      have exp      (ee)   \\n \\n=   \\n \\n/      exp      (a   \\n \\n)      m(a   \\n \\n|      s)da.</p><p>Then,      substituting      the      above      in      the      soft      Bellman      equation      (16),      it      is      written      as a\\n \\n   (Se,      at)   \\n \\n=      Egrs}      [7      (Se;      at)   \\n \\n+      p41      (Se41)|S¢,      ar).</p><p> \\n \\n  Algorithms      in      entropy-regularized      MDPs.\\nAs      outlined      in      Levine      (2018),      to      solve      (12),      various  \\n \\n  well-known      algorithms      exist      in      the      literature      on      RL.\\nThe      abovementioned      concepts      are      useful      in  \\n \\n  constructing      these      algorithms.\\nThese      include      policy      gradients,      which      gradually      optimize   \\n \\na      policy  \\n \\n  using   \\n \\na      policy      neural      network;      soft      Q-learning      algorithms,      which      utilize      the      soft-Bellman      equation  \\n \\n  and      approximate      the      soft-value      function      with   \\n \\na      value      neural      network;      and      soft      actor-critic      algorithms  \\n \\n  that      leverage      both      policy      and      value      neural      networks.\\nWe      will      explore      how      these      algorithms      can      be  \\n \\n  applied      in      the      context      of      diffusion      models      shortly      in      Section      4.2      and      6.</p><p>3\\n \\n   Fine-Tuning      Diffusion      Models      with      RL      in      Entropy      Regular- ized      MDPs</p><p> \\n \\n  In      this      section,      as      done      in      Fan      et      al.\\n(2023);      Black      et      al.\\n(2023);      Uehara      et      al.\\n(2024),      we      illustrate  \\n \\n  how      fine-tuning      can      be      formulated      as      an      RL      problem      in      soft-entropy      regularized      MDPs,      where      each  \\n \\n  PT   \\n \\nx      PT-1      rp_9      PT-2      P2      r1      Pl      Lo  \\n \\n  LT      T-1</p><p>Figure      2:      Formulating      fine-tuning      in      diffusion      models      using      MDPs.</p><p> \\n \\n  denoising      step      of      diffusion      models      corresponds      to   \\n \\na      policy      in      RL.\\nFinally,      we      outline   \\n \\na      specific      RL  \\n \\n  problem      of      interest      in      our      context.</p><p> \\n \\n  To      cast      fine-tuning      diffusion      models      as      an      RL      problem,      we      start      with      defining      the      following  \\n \\n  MDP:</p><p>The      state      space      S      and      action      space   \\n \\nA      correspond      to      the      input      space      1.</p><p>The      transition      dynamics      at      time   \\n \\nt      (i.e.,      P;)      is      an      identity      map      6(5;4;   \\n \\n=      a;).</p><p>The      reward      at      time   \\n \\nt   \\n \\n€      |0,---   \\n \\n,      7]      (i.e.,      r¢)      is      provided      only      at   \\n \\nT      as      r      (down-stream      reward  \\n \\n  function);      but   \\n \\n0      at      other      time      steps.</p><p>The      policy      at      time   \\n \\nt      (i.e,      7)      corresponds      to      pr41;_,:   \\n \\n¥   \\n \\n>      A(X).</p><p>The      initial      distribution      at      time   \\n \\n0      corresponds      to      pr,   \\n \\n€      A(4).\\nWith      slight      abuse      of      notation,  \\n \\n  we      often      denote      it      by      pr41(-|-),      while      this      is      just      pr+4(-).</p><p> \\n \\n  The      reference      policy      at   \\n \\nt      (i.e.,      7;)      corresponds      to   \\n \\na      denoising      process      in      the      pre-trained      model  \\n \\n  pre  \\n \\n  Pr+i-t We      list      several      things      to      note.</p><p> \\n \\n \\n¢      We      reverse      the      time-evolving      process      to      adhere      to      the      standard      notation      in      diffusion      models,  \\n \\n  i.e.,      from   \\n \\nt   \\n \\n=      T\\\\\\\\\\\\\\'      tot   \\n \\n=      0.\\nHence,      s;      in      standard      MDPs      corresponds      to      x74      _;      in      diffusion  \\n \\n  models.</p><p>¢\\n \\n   In      our      context,      unlike      standard      RL      scenarios,      the      transition      dynamics      are      known.</p><p>Key      RL      Problem.\\nNow,      by      reformulating      the      original      objective      of      standard      RL      into      our      contexts,  \\n \\n  the      objective      function      in      (12)      reduces      to      the      following:</p><p> \\n \\n  {pi}:   \\n \\n=      argmax      Eg,   \\n \\n3      (r(x)   \\n \\na      Deep      Epp      [KL      (pe(-|24)      lve      (-|ee))]      8)  \\n \\n  {pie      [RES      A(RY)]      Hay  \\n \\n  Reward      KL      penalty where      the      expectation      E,,,,;[-]      is      taken      with      respect      to      Tierys      Di(@r-1|@1),      Le.\\n@r   \\n \\n~      prai(-),      7-1   \\n \\n~       pr-i(-   \\n \\n|      &r-1),r_-2   \\n \\n~      pr—a(-   \\n \\n|      er_2),--+.\\nIn      this      article,      we      set      this      as      an      objective      function      in  \\n \\n  fine-tuning      diffusion      models.\\nThis      objective      is      natural      as      it      seeks      to      optimize      sequential      denoising  \\n \\n  processes      to      maximize      downstream      rewards      while      maintaining      proximity      to      pre-trained      models.</p><p> \\n \\n  Subsequently,      we      investigate      several      algorithms      to      solve      (18).\\nBefore      discussing      these      algorithms,  \\n \\n  we      summarize      several      key      theoretical      properties      that      will      aid      their      derivation.</p><p>4\\n \\n   Theory      of      RL-Based      Fine-Tuning</p><p> \\n \\n  So      far,      we      have      introduced   \\n \\na      certain      RL      problem      (i.e.,      (18))      as   \\n \\na      fine-tuning      diffusion      model.\\nIn  \\n \\n  this      section,      we      explain      that      solving      this      RL      problem      allows      us      to      achieve      the      target      distribution  \\n \\n  discussed      in      Section      1.2.1.\\nAdditionally,      we      present      several      important      theoretical      properties,      such  \\n \\n  as      the      analytical      form      of      marginal      distributions      and      posterior      distributions      induced      by      fine-tuned  \\n \\n  models.\\nThis      formulation      is      also      instrumental      in      introducing      several      algorithms      (reward-weighted  \\n \\n  MLE,      value-weighted      sampling,      and      path      consistency      learning      in      Section      6),      and      establishing  \\n \\n  connections      with      related      areas      (classifier      guidance      in      Section      8,      and      flow-based      diffusion      models      in  \\n \\n  Section      9).\\nWe      start      with      several      key      concepts.</p><li>4.1      Key      Concepts:      Soft      Value      functions      and      Soft      Bellman      Equations.</li><p> \\n \\n  Now,      reflecting      on      how      soft      optimal      policies      are      expressed      using      soft      value      functions      in      Section   \\n \\n2      in  \\n \\n  the      context      of      standard      RL      problems,      we      derive      several      important      concepts      applicable      to      fine-tuning  \\n \\n  diffusion      models.\\nThese      concepts      are      later      useful      in      constructing      algorithms      to      solve      our      RL      problem  \\n \\n  (18).</p><p>      Firstly,      as      we      see      in      (15),      soft-optimal      policies      are      characterized      as: Pe      (-|e1) expt      a(/aypl(      a9       f      exp(vr-a(ve-1)/@)      pe      (ar-1      |      v4)dx1-1 where      soft-value      functions      are      defined      as ve(es)      =      Egppy[r(@o)      —      oD      KL(pe(-|ere)      lle      Cle)      |e), 1       (a1,      t1-1)      =      Egsy(r(@o)      —      a      $5      KL(pe(-|e)      Pee      (len)      [es      era]      =      era      (1-1). k=t+1 Secondly,      as      we      see      in      (16),      the      soft-value      functions      are      also      recursively      defined      by      the      soft       Bellman      equations:       7      (22)      _      fexp      (uae)      De      (X41      |      v,)dx,-1      (t      =T+1,---      ,1), vo(%o)      =      (x0). (20)</p><p>Now      substituting      the      above      in      (19),      we      obtain sg.y   \\n \\n=      ex      (trea      )/a)eP\"C   \\n \\n|      2)  \\n \\n  PEC|e)      exp(v;      (x)      /a)      :</p><p> \\n \\n  As      mentioned      earlier,      these      soft      value      functions      and      their      recursive      form      will      later      serve      as      the  \\n \\n  basis      for      constructing      several      concrete      fine-tuning      algorithms      (such      as      reward-weighted      MLE      and  \\n \\n  value-weighted      sampling      in      Section      6).</p><h3>4.2      Induced      Distributions      after      Fine-Tuning</h3><p>ning Now,      with      the      above      preparation,      we      can      show      that      the      induced      distribution      derived      by      this      soft  \\n \\n  optimal      policy      is      actually      equal      to      our      target      distribution.</p><p>Theorem   \\n \\n1      (Theorem   \\n \\n|      in      Uehara      et      al.      (2024)).\\nLet      p*(-)      be      an      induced      distribution      at      time   \\n \\n0 from optimal policies {pf }j_7 41) 1-e P*(%o) = Tera pt (a@1-1|@1) }dx1.r.\\nThe distribution p* is      equal      to      the      target      distribution      (10),      i.e., p*(a)   \\n \\n=      p,(2).</p><p> \\n \\n  This      theorem      states      that      after      solving      (18)      and      obtaining      soft      optimal      policies,      we      can      sample  \\n \\n  from      the      target      distribution      by      sequentially      running      policies      from      p7.,,      to      pj.\\nThus,      (18)      serves      as   \\n \\na       natural      objective      for      fine-tuning.\\nThis      fact      is      also      useful      in      deriving   \\n \\na      connection      with      classifier  \\n \\n  guidance      in      Section      8.</p><p>Marginal      distributions.\\nWe      can      derive      the      marginal      distribution      as      follows.</p><p>Theorem   \\n \\n2      (Theorem   \\n \\n2      in      Uehara      et      al.      (2024)      ).\\nLet      ps(x,)      be      the      marginal      distributions      at   \\n \\nt       induced      by      soft-optimal      policies      {p¥}}_741,      i.e.\\nDi(@0)   \\n \\n=      f(T]      Pe(@e—1zn)      }dvipir.\\nThen, x\\n \\n   _\\n \\n   exp(vi(a4)/a)pP      (zt)  \\n \\n  Pr      (xz)      —_   \\n \\nC      ’</p><p>where      v;(-)      is      the      soft-value      function.</p><p>Interestingly,      the      normalizing      constant      is      independent      of   \\n \\n¢      in      the      above      theorem.</p><p>Posterior      distributions.\\nWe      can      derive      the      posterior      distribution      as      follows.</p><p> \\n \\n  Theorem   \\n \\n3      (Theorem   \\n \\n3      in      Uehara      et      al.      (2024)).\\nDenote      the      posterior      distribution      of      x4      given  \\n \\n  24_1      for      the      distribution      induced      by      soft-optimal      policies      {py      }i_7,      by      {p*}°(-   \\n \\n|      -).      We      define      the  \\n \\n  analogous      objective      for   \\n \\na      pre-trained      policy      and      denote      it      by      p?\\\\\\\\\\\\\\'*(-   \\n \\n|      -).      Then,      we      get {p\"}}(@+|@1-1)   \\n \\n=      pe      (@4|X4-1)-</p><p> \\n \\n  This      theorem      indicates      that      after      solving      (18),      the      posterior      distribution      induced      by      pre-trained  \\n \\n  models      remains      preserved.\\nThis      property      plays      an      important      role      in      constructing      PCL      (path  \\n \\n  consistency      learning)      in      Section      6.3.  \\n \\n  Remark   \\n \\n4      (Continuous-time      formulation).\\nFor      simplicity,      our      explanation      is      generally      based      on  \\n \\n  the      discrete-time      formulation.\\nHowever,      as      training      of      diffusion      models      could      be      formulated      in  \\n \\n  the      continuous-time      formulation      (Song      et      al.,      2021),      we      can      still      extend      most      of      our      discussion      of  \\n \\n  fine-tuning      in      our      tutorial      in      the      continuous-time      formulation.\\nFor      example,      the      above      Theorems      are  \\n \\n  extended      to      the      continuous      time      formulation      in      Uehara      et      al.\\n(2024).</p><p> \\n \\n  Table      1:      Description      of      each      RL      algorithm      for      fine-tuning      diffusion      models      (note      that      value-  \\n \\n  weighted      sampling      is      technically      not   \\n \\na      fine-tuning      algorithm.)      Note      (1)      “Without      learning      value  \\n \\n  functions”      refers      to      the      capability      of      algorithms      to      directly      utilize      non-differentiable      black-box  \\n \\n  reward      feedback,      bypassing      the      necessity      to      train      differentiable      reward      functions,      (2)      “Distribution-  \\n \\n  constrained”      indicates      that      the      algorithms      are      designed      to      maintain      proximity      to      pre-trained      models.</p><p> \\n \\n  Based      on      it,      the      practical      recommendation      of      algorithms      is      summarized      in      Figure      3.</p><p>Memory      Computational      Without      learning      Distribution-  \\n \\n  efficiency   \\n \\n—      efficiency      value      functions      constrained</p><p>Soft      PPO      v      Vv</p><p>Reward      backpropagation      v      v</p><p>Reward-weighted      MLE      v      v      v</p><p>Value-weighted      sampling      v      v</p><p>Path      consistency      learning      v      v</p><p>5\\n \\n   RL-Based      Fine-Tuning      Algorithms      1:      Non-Distribution-Constrained</p><h2>Approaches</h2><p> \\n \\n  So      far,      we      have      explained      how      to      frame      fine-tuning      diffusion      models      as      the      RL      problem      in  \\n \\n  entropy-regularized      MDPs.\\nMoving      forward,      we      summarize      actual      algorithms      that      can      solve      the      RL  \\n \\n  problem      of      interest      described      by      Equation      (18).\\nIn      this      section,      we      introduce      two      algorithms:      PPO       and      direct      reward      backpropagation.\\n \\n \\n  These      algorithms      are      originally      designed      to      optimize      reward      functions      directly,      meaning      they  \\n \\n  operate      effectively      even      without      entropy      regularization      (i.e.,   \\n \\na   \\n \\n=      0).\\nConsequently,      they      are      well-  \\n \\n  suited      for      generating      samples      with      high      rewards      that      may      not      be      in      the      original      training      dataset.\\nMore  \\n \\n  distribution-constrained      algorithms      that      align      closely      with      pre-trained      models      will      be      discussed      in  \\n \\n  the      subsequent      section      (Section      4.2).\\nTherefore,      we      classify      algorithms      in      this      section      (i.e.,      PPO      and  \\n \\n  direct      reward      backpropagation)      as      non-distribution-constrained      approaches.\\nThe      whole      summary      is  \\n \\n  described      in      Table      1.</p><h3>5.1      Soft      Proximal      Policy      Optimization      (PPO)</h3><p>PPO)  \\n \\n  In      order      to      solve      Equation      (18),      Fan      et      al.\\n(2023);      Clark      et      al.\\n(2023)      propose      using      PPO      (Schulman  \\n \\n  et      al.,      2017).\\nPPO      has      been      widely      used      in      RL,      as      well      as,      in      the      literature      in      fine-tuning      LLMs,  \\n \\n  due      to      its      stability      and      simplicity.\\nIn      the      standard      context      of      RL,      this      is      especially      preferred      over  \\n \\n  Q-learning      when      the      action      space      is      high-dimensional.</p><p> \\n \\n  The      PPO      algorithm      is      described      in      Algorithm      1.\\nThis      is      an      iterative      procedure      of      updating  \\n \\n  a      parameter      7.\\nEach      iteration      comprises      two      steps:      firstly,      samples      are      generated      by      executing  \\n \\n  Algorithm   \\n \\n1      Soft      PPO</p><p> \\n \\n  1:      Require:      Pre-trained      model      {N(p(1,      t;      Opre),      77      (t))      }i_-74.1,      batch      size      m,      parameter   \\n \\na   \\n \\n€      R*,  \\n \\n  learning      rate  \\n \\n  2:      Initialize:      0;   \\n \\n=      Opre  \\n \\n  3:      for      s   \\n \\n€      [1,---      ,S]      do  \\n \\n  4:      Collect   \\n \\nm      samples      fa      (0)      }9_      from   \\n \\na      current      diffusion      model      (i.e.,      generating      by      sequen- tially      running      polices      {N(p(2z,      t;      9),      07(t))      }t_p4,      from   \\n \\nt   \\n \\n=   \\n \\nT   \\n \\n+   \\n \\n1      tot   \\n \\n=      1)  \\n \\n  5:      Update      as      follows      (several      times      if      needed):</p><h3>5.1      Soft      Proximal      Policy      Optimization      (PPO)</h3><p>PPO)  \\n \\n  In      order      to      solve      Equation      (18),      Fan      et      al.\\n(2023);      Clark      et      al.\\n(2023)      propose      using      PPO      (Schulman  \\n \\n  et      al.,      2017).\\nPPO      has      been      widely      used      in      RL,      as      well      as,      in      the      literature      in      fine-tuning      LLMs,  \\n \\n  due      to      its      stability      and      simplicity.\\nIn      the      standard      context      of      RL,      this      is      especially      preferred      over  \\n \\n  Q-learning      when      the      action      space      is      high-dimensional.</p><p> \\n \\n  The      PPO      algorithm      is      described      in      Algorithm      1.\\nThis      is      an      iterative      procedure      of      updating  \\n \\n  a      parameter      7.\\nEach      iteration      comprises      two      steps:      firstly,      samples      are      generated      by      executing  \\n \\n  Algorithm   \\n \\n1      Soft      PPO</p><p> \\n \\n  1:      Require:      Pre-trained      model      {N(p(1,      t;      Opre),      77      (t))      }i_-74.1,      batch      size      m,      parameter   \\n \\na   \\n \\n€      R*,  \\n \\n  learning      rate  \\n \\n  2:      Initialize:      0;   \\n \\n=      Opre  \\n \\n  3:      for      s   \\n \\n€      [1,---      ,S]      do  \\n \\n  4:      Collect   \\n \\nm      samples      fa      (0)      }9_      from   \\n \\na      current      diffusion      model      (i.e.,      generating      by      sequen- tially      running      polices      {N(p(2z,      t;      9),      07(t))      }t_p4,      from   \\n \\nt   \\n \\n=   \\n \\nT   \\n \\n+   \\n \\n1      tot   \\n \\n=      1)  \\n \\n  5:      Update      as      follows      (several      times      if      needed):</p><h2>      A541      <—      O5      —      7      Ve      )      )      i      {le      ))</h2><p>Co,      (i)      1).\\n \\n \\ng       F(a,      xt”))   \\n \\n-      Clip      Pere      so)      l—e,l+e</p><p>where  \\n \\n  1   \\n \\nm    \\n \\nY    \\n.\\n     Je,      (i)   \\n \\n4 t=T+1      i=1 p(x,      ja;      9) pai,      |al;      As) (21)</p><h3>P(x,      |e      ;      5)</h3><p>t;      0)   \\n \\n—      t:      9Pre)      ||2  \\n \\n  F4(Xo,      Lt)   \\n \\n=      —r(ap)   \\n \\n+      ole   \\n \\n?      ae   \\n \\n’      II     .</p><p>lo=0.></p><p>KL      term 6:      end      for  \\n \\n  7;      Output:      Policy      {p.(-   \\n \\n|      0s)      Jars</p><p> \\n \\n  current      policies      to      construct      the      loss      function      (inspired      by      policy      gradient      formulation);      secondly,  \\n \\n  the      parameter   \\n \\n6      is      updated      by      computing      the      gradient      of      the      loss      function.</p><p> \\n \\n  PPO      offers      several      advantages.\\nThe      approach      is      known      for      its      stability      and      relatively      straight-  \\n \\n  forward      implementation.\\nStability      comes      from      the      conservative      parameter      updates.\\nIndeed,      PPO       builds      upon      TRPO      (Schulman      et      al.,      2015),      where      parameters      are      conservatively      updated      with   \\n \\na      KL  \\n \\n  penalty      term      (between      @,,,      and      @,)      to      prevent      significant      deviation      from      the      current      parameter.\\nThis  \\n \\n  gives      us      stability      in      the      optimization      landscape.\\nFurthermore,      in      Algorithm      1,      we      do      not      necessarily  \\n \\n  need      to      rely      on      value      functions,      although      they      could      be      useful      for      variance      reduction.\\nAs      discussed  \\n \\n  in      the      subsequent      subsection,      this      can      be      advantageous      compared      to      other      methods,      especially      since  \\n \\n  learning      value      functions      can      be      challenging      in      high-dimensional      spaces,      particularly      within      the  \\n \\n  context      of      diffusion      models.</p><h3>5.2      Direct      Reward      Backpropagation</h3><p>tion  \\n \\n  Another      standard      approach      is   \\n \\na      differentiable      optimization      (Clark      et      al.,      2023;      Prabhudesai      et      al.,  \\n \\n  2023;      Uehara      et      al.,      2024),      where      gradients      are      directly      propagated      from      reward      functions      to      update  \\n \\n  policies.</p><p> \\n \\n  The      entire      algorithm      is      detailed      in      Algorithm      2.\\nThis      reward      backpropagation      entails      an      iterative  \\n \\n  process      of      updating   \\n \\na      parameter      0.\\nEach      iteration      comprises      two      steps;      firstly,      samples      are      generated  \\n \\n  by      executing      current      policies      to      approximate      the      expectation      in      the      loss      function,      which      is      directly  \\n \\n  derived      from      (18);      second,      the      current      parameter   \\n \\n@      is      updated      by      computing      the      gradient      of      the      loss  \\n \\n  Algorithm   \\n \\n2      Reward      backpropagation</p><p>1:      Require:      Pre-trained      model      {N(p(21,      t;      pre),      77      (t))      }i_-74.1,      batch      size      m,      parameter   \\n \\na   \\n \\n€      R*,  \\n \\n  learning      rate   \\n \\n7       2:      Train   \\n \\na      differentiable      reward      function      (if      reward      feedback      is      not      differentiable)  \\n \\n  3:      Initialize:      0;   \\n \\n=      Opre  \\n \\n  4:      for      s   \\n \\n€      {1,---   \\n \\n,      S|]      do  \\n \\n  5:      Collect   \\n \\nm      samples      {a      (0)      }9_»      from   \\n \\na      current      diffusion      model      (i.e.,      generating      by      sequen- tially      running      polices      {N(p(2,      t;      9),      07(t))      }i_74,      from   \\n \\nt   \\n \\n=   \\n \\nT   \\n \\n+      1tot   \\n \\n=      1)  \\n \\n  6:      Update      6,      to      0,11:</p><p> \\n \\n  Baar      Ba   \\n \\n|      ESF      (al(B)      —c      Sp      WALee      Bt)   \\n \\n—      vas\"):      Bore?\\nYY  \\n \\n  s+l      Ss      v   \\n \\nm   \\n \\n4      r      Lo   \\n \\na      20?\\n(t)      0=0.°  \\n \\n  i=1      t=T+1 (22)</p><p>7:      end      for  \\n \\n  8:      Output:      Policy      {~(-   \\n \\n|      -;4s)      inna</p><p>function.</p><p> \\n \\n  Advantages      over      PPO.      This      approach      offers      further      simplicity      in      implementation      in   \\n \\na      case      where  \\n \\n  we      already      have   \\n \\na      pre-trained      differentiable      reward      model.\\nFurthermore,      the      training      speed      is      much  \\n \\n  faster      since      we      are      directly      back-propagating      from      rewards.</p><p> \\n \\n  Potential      disadvantages      over      PPO.      Reward      backpropagation      may      face      memory      inefficiency  \\n \\n  issues.\\nHowever,      there      are      strategies      to      mitigate      this      challenge.\\nFirstly,      implementing      gradient  \\n \\n  accumulation      can      help      to      keep   \\n \\na      large      batch      size.\\nSecondly,      as      proposed      in      DRaFT      (Clark      et      al.,  \\n \\n  2023),      propagating      rewards      backward      from      time   \\n \\n0      to      k      (k      is      an      intermediate      step      smaller      than      7’)  \\n \\n  and      updating      policies      from      k      to   \\n \\n0      can      still      yield      high      performance.\\nThirdly,      drawing      from      insights      in  \\n \\n  the      literature      on      neural      SDE/ODE      (Chen      et      al.,      2018),      more      memory-efficient      advanced      techniques  \\n \\n  such      as      adjoint      methods      could      be      helpful.</p><p> \\n \\n  Another      potential      drawback      is      the      requirement      for      “differentiable”      reward      functions.\\nOften,  \\n \\n  reward      functions      are      obtained      in   \\n \\na      non-differentiable      black-box      way      (e.g.,      computational      feedback  \\n \\n  derived      from      physical      simulations).\\nIn      such      scenarios,      using      direct      backpropagation      necessitates  \\n \\n  the      learning      of      differentiable      reward      functions      even      if      accurate      reward      feedback      is      available.\\nThis  \\n \\n  learning      step      can      pose      challenges      as      it      involves      data      collection      and      constructing      suitable      reward  \\n \\n  models.</p><p>6\\n \\n   RL-Based      Fine-Tuning      Algorithms      2:      Distribution-Constrained</p><h3>P(x,      |e      ;      5)</h3><p>t;      0)   \\n \\n—      t:      9Pre)      ||2  \\n \\n  F4(Xo,      Lt)   \\n \\n=      —r(ap)   \\n \\n+      ole   \\n \\n?      ae   \\n \\n’      II     .</p><p>lo=0.></p><p>KL      term 6:      end      for  \\n \\n  7;      Output:      Policy      {p.(-   \\n \\n|      0s)      Jars</p><p> \\n \\n  current      policies      to      construct      the      loss      function      (inspired      by      policy      gradient      formulation);      secondly,  \\n \\n  the      parameter   \\n \\n6      is      updated      by      computing      the      gradient      of      the      loss      function.</p><p> \\n \\n  PPO      offers      several      advantages.\\nThe      approach      is      known      for      its      stability      and      relatively      straight-  \\n \\n  forward      implementation.\\nStability      comes      from      the      conservative      parameter      updates.\\nIndeed,      PPO       builds      upon      TRPO      (Schulman      et      al.,      2015),      where      parameters      are      conservatively      updated      with   \\n \\na      KL  \\n \\n  penalty      term      (between      @,,,      and      @,)      to      prevent      significant      deviation      from      the      current      parameter.\\nThis  \\n \\n  gives      us      stability      in      the      optimization      landscape.\\nFurthermore,      in      Algorithm      1,      we      do      not      necessarily  \\n \\n  need      to      rely      on      value      functions,      although      they      could      be      useful      for      variance      reduction.\\nAs      discussed  \\n \\n  in      the      subsequent      subsection,      this      can      be      advantageous      compared      to      other      methods,      especially      since  \\n \\n  learning      value      functions      can      be      challenging      in      high-dimensional      spaces,      particularly      within      the  \\n \\n  context      of      diffusion      models.</p><h3>5.2      Direct      Reward      Backpropagation</h3><p>tion  \\n \\n  Another      standard      approach      is   \\n \\na      differentiable      optimization      (Clark      et      al.,      2023;      Prabhudesai      et      al.,  \\n \\n  2023;      Uehara      et      al.,      2024),      where      gradients      are      directly      propagated      from      reward      functions      to      update  \\n \\n  policies.</p><p> \\n \\n  The      entire      algorithm      is      detailed      in      Algorithm      2.\\nThis      reward      backpropagation      entails      an      iterative  \\n \\n  process      of      updating   \\n \\na      parameter      0.\\nEach      iteration      comprises      two      steps;      firstly,      samples      are      generated  \\n \\n  by      executing      current      policies      to      approximate      the      expectation      in      the      loss      function,      which      is      directly  \\n \\n  derived      from      (18);      second,      the      current      parameter   \\n \\n@      is      updated      by      computing      the      gradient      of      the      loss  \\n \\n  Algorithm   \\n \\n2      Reward      backpropagation</p><p>1:      Require:      Pre-trained      model      {N(p(21,      t;      pre),      77      (t))      }i_-74.1,      batch      size      m,      parameter   \\n \\na   \\n \\n€      R*,  \\n \\n  learning      rate   \\n \\n7       2:      Train   \\n \\na      differentiable      reward      function      (if      reward      feedback      is      not      differentiable)  \\n \\n  3:      Initialize:      0;   \\n \\n=      Opre  \\n \\n  4:      for      s   \\n \\n€      {1,---   \\n \\n,      S|]      do  \\n \\n  5:      Collect   \\n \\nm      samples      {a      (0)      }9_»      from   \\n \\na      current      diffusion      model      (i.e.,      generating      by      sequen- tially      running      polices      {N(p(2,      t;      9),      07(t))      }i_74,      from   \\n \\nt   \\n \\n=   \\n \\nT   \\n \\n+      1tot   \\n \\n=      1)  \\n \\n  6:      Update      6,      to      0,11:</p><p> \\n \\n  Baar      Ba   \\n \\n|      ESF      (al(B)      —c      Sp      WALee      Bt)   \\n \\n—      vas\"):      Bore?\\nYY  \\n \\n  s+l      Ss      v   \\n \\nm   \\n \\n4      r      Lo   \\n \\na      20?\\n(t)      0=0.°  \\n \\n  i=1      t=T+1 (22)</p><p>7:      end      for  \\n \\n  8:      Output:      Policy      {~(-   \\n \\n|      -;4s)      inna</p><p>function.</p><p> \\n \\n  Advantages      over      PPO.      This      approach      offers      further      simplicity      in      implementation      in   \\n \\na      case      where  \\n \\n  we      already      have   \\n \\na      pre-trained      differentiable      reward      model.\\nFurthermore,      the      training      speed      is      much  \\n \\n  faster      since      we      are      directly      back-propagating      from      rewards.</p><p> \\n \\n  Potential      disadvantages      over      PPO.      Reward      backpropagation      may      face      memory      inefficiency  \\n \\n  issues.\\nHowever,      there      are      strategies      to      mitigate      this      challenge.\\nFirstly,      implementing      gradient  \\n \\n  accumulation      can      help      to      keep   \\n \\na      large      batch      size.\\nSecondly,      as      proposed      in      DRaFT      (Clark      et      al.,  \\n \\n  2023),      propagating      rewards      backward      from      time   \\n \\n0      to      k      (k      is      an      intermediate      step      smaller      than      7’)  \\n \\n  and      updating      policies      from      k      to   \\n \\n0      can      still      yield      high      performance.\\nThirdly,      drawing      from      insights      in  \\n \\n  the      literature      on      neural      SDE/ODE      (Chen      et      al.,      2018),      more      memory-efficient      advanced      techniques  \\n \\n  such      as      adjoint      methods      could      be      helpful.</p><p> \\n \\n  Another      potential      drawback      is      the      requirement      for      “differentiable”      reward      functions.\\nOften,  \\n \\n  reward      functions      are      obtained      in   \\n \\na      non-differentiable      black-box      way      (e.g.,      computational      feedback  \\n \\n  derived      from      physical      simulations).\\nIn      such      scenarios,      using      direct      backpropagation      necessitates  \\n \\n  the      learning      of      differentiable      reward      functions      even      if      accurate      reward      feedback      is      available.\\nThis  \\n \\n  learning      step      can      pose      challenges      as      it      involves      data      collection      and      constructing      suitable      reward  \\n \\n  models.</p><p>6\\n \\n   RL-Based      Fine-Tuning      Algorithms      2:      Distribution-Constrained</p><h2>Approaches</h2><p> \\n \\n  In      this      section,      following      Section      4.2,      we      present      three      additional      algorithms      (reward-weighted  \\n \\n  MLE,      value-weighted      sampling,      and      path      consistency      learning)      aimed      at      solving      the      RL      problem  \\n \\n  of      interest      defined      by      Equation      (18).\\nIn      the      context      of      standard      RL,      these      algorithms      are      tailored  \\n \\n  to      align      closely      with      reference      policies,      specifically      pre-trained      diffusion      models      in      our      context.\\n \\n \\n  Formally,      indeed,      all      algorithms      in      this      section      are      not      well-defined      when   \\n \\na   \\n \\n=   \\n \\n0      (1.e.,      without  \\n \\n  entropy      regularization).\\nHence,      we      categorize      these      three      algorithms      as      distribution-constrained  \\n \\n  approaches.</p><p> \\n \\n  The      algorithms      in      this      section      excel      in      preserving      the      characteristics      of      pre-trained      diffusion  \\n \\n  models.\\nPractically,      this      property      becomes      especially      crucial      when      reward      functions      are      learned      from  \\n \\n  training      data,      and      we      want      to      avoid      being      fooled      by      distribution      samples      (a.k.a.      overoptimization      as  \\n \\n  detailed      in      Section      7.3).\\nHowever,      as   \\n \\na      caveat,      this      property      might      also      pose      challenges      in      effectively  \\n \\n  generating      high-reward      samples      beyond      the      training      data.\\nThis      implies      that      these      approaches      may  \\n \\n  not      be      suitable      when      accurate      reward      feedback      is      readily      available      without      learning.\\nHence,      we  \\n \\n  generally      recommend      using      them      when      reward      functions      are      unknown.</p><h3>6.1      Reward-Weighted      MLE</h3><p>MLE  \\n \\n  Here,      we      elucidate      an      approach      based      on      reward-weighted      MLE      (Peters      et      al.,      2010),   \\n \\na      technique  \\n \\n  commonly      employed      in      offline      RL      (Peng      et      al.,      2019).\\nWhile      Fan      et      al.\\n(2023,      Algorithm      2)      and  \\n \\n  Zhang      and      Xu      (2023)      propose      variations      of      reward-weighted      MLE      for      diffusion      models,      the      specific  \\n \\n  formulation      of      reward-weighted      MLE      discussed      here      does      not      seem      to      have      been      explicitly      detailed  \\n \\n  previously.\\nTherefore,      unlike      the      previous      section,      we      start      by      outlining      the      detailed      rationale      for  \\n \\n  this      approach.\\nSubsequently,      we      provide   \\n \\na      comprehensive      explanation      of      the      algorithm.\\nFinally,      we  \\n \\n  delve      into      its      connection      with      the      original      training      loss      of      diffusion      models.</p><p>Motivation.\\nFirst,      from      Theorem      2,      recall      the      form      of      the      optimal      policy      p¥(x_1|x,):</p><p>exp(v¢—-1      (+1)      /a:)      pe      (a1      |) exp(u;(x;)/@)  \\n \\n  Py      (Lt-1|2t)   \\n \\n= Now,      we      have:</p><p>p=      argmin      Ey,xu,[KL(p;      (-|2+)||Pe(-|2+))|  \\n \\n  pt:X      A(X) where      u,   \\n \\n€      A(%)      is   \\n \\na      roll-in      distribution      encompassing      the      entire      space      V      This      can      be      reformulated  \\n \\n  as      value-weighted      MLE      as      follows.</p><p>Lemma   \\n \\n1      (Value-weighted      MLE).\\nWhen      II,   \\n \\n=      [¥   \\n \\n—      A(4)],      the      policy      pt      is      equal      to</p><p>Ut—-1\\\\\\\\\\\\\\\\      Tt-1  \\n \\n  PC)   \\n \\n=      Ar      gMAX      Bey,      sph      (ne)      seeme      lex»      (eaGey      ’)      log      r(e-aley)     .</p><p>t\\n \\n   t\\n \\n    This      lemma      illustrates      that      if      v,-;      is      known,      py      can      be      estimated      using      weighted      maximum  \\n \\n  likelihood      estimation      (MLE).\\nWhile      this      formulation      is      commonly      used      in      standard      RL      (Peng      et      al.,  \\n \\n  2019),      in      our      context      of      fine-tuning      diffusion      models,      learning   \\n \\na      value      function      is      often      challenging.\\n \\n \\n  Interestingly,      this      reward-weighted      MLE      can      be      performed      without      directly      estimating      the      soft      value  \\n \\n  function      after      proper      reformulation.\\nTo      demonstrate      this,      let’s      utilize      the      following      lemma:</p><h3>6.1      Reward-Weighted      MLE</h3><p>MLE  \\n \\n  Here,      we      elucidate      an      approach      based      on      reward-weighted      MLE      (Peters      et      al.,      2010),   \\n \\na      technique  \\n \\n  commonly      employed      in      offline      RL      (Peng      et      al.,      2019).\\nWhile      Fan      et      al.\\n(2023,      Algorithm      2)      and  \\n \\n  Zhang      and      Xu      (2023)      propose      variations      of      reward-weighted      MLE      for      diffusion      models,      the      specific  \\n \\n  formulation      of      reward-weighted      MLE      discussed      here      does      not      seem      to      have      been      explicitly      detailed  \\n \\n  previously.\\nTherefore,      unlike      the      previous      section,      we      start      by      outlining      the      detailed      rationale      for  \\n \\n  this      approach.\\nSubsequently,      we      provide   \\n \\na      comprehensive      explanation      of      the      algorithm.\\nFinally,      we  \\n \\n  delve      into      its      connection      with      the      original      training      loss      of      diffusion      models.</p><p>Motivation.\\nFirst,      from      Theorem      2,      recall      the      form      of      the      optimal      policy      p¥(x_1|x,):</p><p>exp(v¢—-1      (+1)      /a:)      pe      (a1      |) exp(u;(x;)/@)  \\n \\n  Py      (Lt-1|2t)   \\n \\n= Now,      we      have:</p><p>p=      argmin      Ey,xu,[KL(p;      (-|2+)||Pe(-|2+))|  \\n \\n  pt:X      A(X) where      u,   \\n \\n€      A(%)      is   \\n \\na      roll-in      distribution      encompassing      the      entire      space      V      This      can      be      reformulated  \\n \\n  as      value-weighted      MLE      as      follows.</p><p>Lemma   \\n \\n1      (Value-weighted      MLE).\\nWhen      II,   \\n \\n=      [¥   \\n \\n—      A(4)],      the      policy      pt      is      equal      to</p><p>Ut—-1\\\\\\\\\\\\\\\\      Tt-1  \\n \\n  PC)   \\n \\n=      Ar      gMAX      Bey,      sph      (ne)      seeme      lex»      (eaGey      ’)      log      r(e-aley)     .</p><p>t\\n \\n   t\\n \\n    This      lemma      illustrates      that      if      v,-;      is      known,      py      can      be      estimated      using      weighted      maximum  \\n \\n  likelihood      estimation      (MLE).\\nWhile      this      formulation      is      commonly      used      in      standard      RL      (Peng      et      al.,  \\n \\n  2019),      in      our      context      of      fine-tuning      diffusion      models,      learning   \\n \\na      value      function      is      often      challenging.\\n \\n \\n  Interestingly,      this      reward-weighted      MLE      can      be      performed      without      directly      estimating      the      soft      value  \\n \\n  function      after      proper      reformulation.\\nTo      demonstrate      this,      let’s      utilize      the      following      lemma:</p><h2>Algorithm      3      Reward-weighed      MLE</h2><p>:\\n \\n   Require:      Pre-trained      model      {N(p(+,      t;      Opre),      07      (t))      }i-741,      batch      size      m,      parameter   \\n \\na   \\n \\n€      Rt, learning      rate   \\n \\n7 — 2:      Initialize:      6;   \\n \\n=      Opre  \\n \\n  3:      for      s   \\n \\n€      [1,---      ,S]      do 4.      fork   \\n \\n€      [2      +1,---      ,1]      do  \\n \\n  5:      Collect   \\n \\nm      samples      {aye      from</p><p> \\n \\n \\na      policy      pr+i(:   \\n \\n|      -3Os)i-°+      Peer|5      9s),      PP      Cl).\\nPr      Cl).\\n \\n \\n  6:      end      for  \\n \\n  7:      Update      @,      to      6,1;      as      follows:</p><p>1\\n \\n   m\\n \\n   (i,t)      (i,t)      (i,t)      4,   \\n \\n2       r(x      XL,   \\n \\n1   \\n \\n—      play’,      t3   \\n \\n0 t=T+1      i=1   \\n \\no      tor}</p><p>8:      end      for  \\n \\n  9:      Output:      Policy      {p:(-   \\n \\n|      39s)      }ir41</p><p>Lemma   \\n \\n2      (Characterization      of      soft      optimal      value      functions).</p><p>v,(@e)      ro0) exp      (ee)   \\n \\n~      Bey      mpP      (a1),      tr_1      pe,      (et)      lex»   \\n \\n(   \\n \\na   \\n \\n-     .</p><p>Recall      Eg,py|-|24]      means      E       zo~pP\"®      (1)      ae-1p?™S      (e)      [|e]:</p><p>Proof.\\nThis      is      obtained      by      recursively      using      the      soft-Bellman      equation      (20):</p><p>en      (22D)   \\n \\n&      few      (2%)      spears      ates   \\n \\n=      >=      Bie      fxn      (2) (23)</p><h1>O</h1><p>Algorithm.\\nNow,      we      are      ready      to      present      the      algorithm.\\nBy      combining      Lemma   \\n \\n|      and      Lemma      2,  \\n \\n  we      obtain      the      following.</p><p>Lemma   \\n \\n3      (Reward-weighted      MLE).\\nWhen      Il,   \\n \\n=      [¥   \\n \\n>      A(X)], *\\n \\n   r(x)</p><p>Pye   \\n \\n=      ATBMAX      Ep   \\n \\ny      mph      (1)y--      ee      a~pP      (este      exw   \\n \\n(   \\n \\na   \\n \\n)      tor      r(eale)   \\n \\n,      (25) Pt   \\n \\nt       Proof.\\nUsing      Lemma      2,      we      have Up—-1(Lt-1)  \\n \\n  By,      ywpPP      (-[ae),cevur      lexp   \\n \\n(   \\n \\na   \\n \\n)      log      r(esale)</p><p>r(x)  \\n \\n  The      rest      of      the      proof      is      obvious      by      using      Lemma      1.   \\n \\nO 0  \\n \\n  =      Ba      spledirnn      [Bp      [exp      (TE)      joa]      town      (esal)</p><p>r(x) =\\n \\n   Legg?\"\\n(01)      yeep\",      (ae)      eewur      exw      (a)      log      r(es-al)      :</p><p> \\n \\n  Then,      after      approximating      the      expectation      in      (25),      by      using   \\n \\na      Gaussian      policy      class      with      the  \\n \\n  mean      parameterized      by      neural      networks      as   \\n \\na      policy      class      II;,      we      can      estimate      p;.\\nFinally,      the      entire  \\n \\n  algorithm      is      described      in      Algorithm      3.  \\n \\n  Here,      we      give      two      remarks.\\nThis      is      an      off-policy      algorithm.\\nHence,      we      can      use      any      roll-in  \\n \\n  policies      as      wu;      in      Lemma      3.\\nIn      Algorithm      3,      we      use      the      current      policy      as   \\n \\na      roll-in      policy.\\nAdditionally,  \\n \\n  in      Algorithm      3,      the      loss      function      (24)      is      derived      by      recalling      that,      up      to      constant,   \\n \\n—      log      p;(2_1|2:)  \\n \\n  is      equal      to   \\n \\nA      ;</p><p> \\n \\n  Advantages      and      potential      disadvantages.\\nLike      PPO      in      Section      5.1,      this      approach      is      expected      to  \\n \\n  be      memory      efficient,      and      does      not      require      learning      differentiable      reward      functions,      which      can      often  \\n \\n  be      challenging.\\nHowever,      compared      to      direct      reward      backpropagation      Section      5.2,      it      might      not      be  \\n \\n  as      computationally      efficient.\\nFurthermore,      unlike      PPO,      this      algorithm      is      not      effective      when   \\n \\na   \\n \\n=      0,  \\n \\n  potentially      limiting      its      ability      to      generate      samples      with      extremely      high      rewards.</p><h2>6.1.1      Relation      with      Loss      Functions      for      the      Original      Training      Objective</h2><p>tive In      this      subsection,      we      explore      the      connection      with      the      original      loss      function      of      pre-trained      diffusion  \\n \\n  models.\\nTo      see      that,      as      we      see      in      Section      1.1.1,      recall ol)   \\n \\n=      al   \\n \\n4      (0.50,   \\n \\n—      1/o%eg,,,      (a1,   \\n \\nT   \\n \\n—      t)](5t)      +e?\\n02,      €)   \\n \\n~      N(0,1).</p><p> \\n \\n \\n/       \\\\\\\\\\\\\\\\-  \\n \\n  (al?\\nt;Opre) Then,      the      loss      function      (24)      in      reward-weighted      MLE      reduces      to</p><table><th><td colSpan=1>      =      r(x”)       A,      —      Vo      S>      Ss\"      exp      |      ~~      —       t=T+1      i=1</td><td colSpan=1>      .      _      ;      2       (i,t)      (ay,      Tt;      Opre)      _      eal,      T      -t;      0)       ey      _       {or}?</td><td colSpan=1>|o=0</td></th><tr><td colSpan=1></td><td colSpan=1>      2</td><td colSpan=1>      (26)</td></tr></table><p>This      objective      function      closely      resembles      the      reward-weighted      version      of      the      loss      function      (5)      used  \\n \\n  for      training      pre-trained      diffusion      models.</p><h3>6.2      Value-Weighted      Sampling</h3><p>ling  \\n \\n  Thus      far,      we      have      discussed      methods      for      fine-tuning      pre-trained      diffusion      models.\\nNow,      let’s      delve  \\n \\n  into      an      alternative      approach      during      inference      that      aims      to      sample      from      the      target      distribution      p,.\\n \\n \\n  without      explicitly      fine-tuning      the      diffusion      models.\\nIn      essence,      this      approach      involves      incorporating  \\n \\n  gradients      of      value      functions      during      inference      alongside      the      denoising      process      in      pre-trained      diffusion  \\n \\n  models.\\nHence,      we      refer      to      this      approach      as      value-weighted      sampling.\\nWhile      it      seems      that      this  \\n \\n  method      has      not      been      explicitly      formalized      in      previous      literature,      the      value-weighted      sampling  \\n \\n  closely      connects      with      classifier      guidance,      as      discussed      in      Section      8.1.  \\n \\n  Before      delving      into      the      algorithmic      details,      we      outline      the      motivation.\\nSubsequently,      we      present  \\n \\n  the      concrete      algorithm      and      discuss      its      advantages—specifically,      its      capability      to      operate      without      the  \\n \\n  necessity      of      fine-tuning      diffusion      models—and      its      disadvantage,      which      involves      the      need      to      obtain  \\n \\n  differentiable      value      functions.</p><p>Algorithm   \\n \\n4      Value-weighted      sampling 1:      Require:      Pre-trained      model      {p?\\n\"*(2,_1|r1)      }:   \\n \\n=      {N(0(      22,      t;      Opre),      07(t))      be.\\n \\n \\n  2:      Estimate      v   \\n \\n:   \\n \\n¥   \\n \\nx      [0,7]   \\n \\n—      R      and      denote      it      by      i(.,      -) ¢\\n \\n   (1)      Monte-Carlo      approach      in      (28) e      (2)      Value      iteration      approach      (Soft      Q-learning)      in      (29)      in      Section      6.2.1 ¢\\n \\n   (3)      Approximation      using      Tweedie’s      formula      in      Section      6.2.2 3:      fort   \\n \\n€      [7      +1,---   \\n \\n,      1}      do  \\n \\n  4:      Set.\\na?\\n \\n \\nt      Ve0   \\n \\nx      xL=Lt  \\n \\n  P(t,      t)   \\n \\n=   \\n \\n(   \\n \\n)      \\\\\\\\\\\\\\\\   \\n \\nu   \\n \\n+      p(xz,t      Are)</p><p>5:      end      for  \\n \\n  6:      Output:      {N(p      O(      Le,      t),o7(t))      ery</p><p> \\n \\n  Motivation.\\nConsidering   \\n \\na      Gaussian      policy      7:1   \\n \\n~      N(/(2:,t),07(t)),      we      aim      to      determine  \\n \\n  P(x4,t;      0)      such      that      N(A(2x;,      t;      0),      0?(t))      closely      approximates      p*(-|x,).\\nHere,      typically,      we      have  \\n \\n  p(xz,t;0)   \\n \\n=      a,   \\n \\n+      (t)g(a;,t)      and      o?(t)   \\n \\n=      g(t)(ot)      for      certain      function   \\n \\ng   \\n \\n:   \\n \\nY   \\n \\nx      [0,7]   \\n \\n>      R¢  \\n \\n  and   \\n \\ng   \\n \\n:      [0,7]   \\n \\n—      R,      as      we      have      explained      in      Section      1.1.1.\\nThen,      using   \\n \\nx      as      equality      up      to      the  \\n \\n  normalizing      constant, Piles      |)      exp(eralar-s)/a)      exp      (BEE      OI)</p><p>2\\n \\n   exp(vy(am)   \\n \\n+      Ver(      ee)»      {a1   \\n \\n—      3)      exp      (—      “2      Clr      uF)</p><p>_\\n \\n   lve1   \\n \\n—      ae   \\n \\n—      (58)      9(t)      Vora)      /o   \\n \\n—      (6t)9(ae,      t))      |?\\n \\n \\n  esp      (-      0.59(#)      (6t)      )</p><h4>Hence,      we      can      approximate:</h4><p> \\n \\n  o°(t)      Vu;      (a4)  \\n \\n  a   \\n \\n+      p(      xz,      ti      Ore):      (27)  \\n \\n  P(x,      t)      od</p><p>Remark      5.\\nNote      while      the      above      derivation      is      heuristic,      it      is      formalized      in      Uehara      et      al.\\n(2024,  \\n \\n  Lemma      2)      in      the      continuous-time      formulation.</p><p> \\n \\n  Algorithm.\\nUtilizing      (27),      the      entire      algorithm      of      value-weighted      sampling      is      outlined      in      Algo-  \\n \\n  rithm      4.\\nThis      method      doesn’t      involve      updating      parameters      in   \\n \\na      diffusion      model;      hence,      it’s      not   \\n \\na       fine-tuning      method.</p><p> \\n \\n  Note      that      in      this      algorithm,      we      require   \\n \\na      form      of      v;(-)      to      compute      gradients.\\nThis      value      function  \\n \\n  can      be      estimated      through      regression      using      the      characterization      described      in      Lemma      2.\\nHere,      inspired  \\n \\n  by      Lemma      2,      we      use      the      following      loss      function:</p><p>1\\n \\n   om      (i,t)      (i,t)   \\n \\n°       o(2)   \\n \\n=      argmin      S-      S-      {es      (me)   \\n \\n—      exp   \\n \\n|   \\n \\n,      (28)</p><p>h:[R4,{0,T]]-R      t=T+1      i=1</p><p>where      the      data      is      collected      in      an      off-policy      way.\\nLater,      we      explain      two      alternative      approaches      in  \\n \\n  Section      6.2.1      and      Section      6.2.2.</p><p> \\n \\n  Advantages      and      potential      disadvantages.\\nThe      value-weighted      sampling      is      straightforward      and  \\n \\n  less      memory-intensive      since      it      avoids      fine-tuning.\\nTherefore,      it      presents      an      appealing      simple      option  \\n \\n  if      it      performs      well.\\nIndeed,      in      various      inverse      problems,      such      as      inpainting,      super-resolution,      and  \\n \\n  colorization,      setting      r()      as   \\n \\na      likelihood      of      the      measurement      model   \\n \\ny   \\n \\n=      g(a)      +e      (where   \\n \\ny      represents  \\n \\n  actual      measurements,   \\n \\n€      denotes      noise,      and   \\n \\ng      defines      the      measurement      function)      has      proven      highly  \\n \\n  successful      (Chung      et      al.,      2022;      Bansal      et      al.,      2023;      Chung      et      al.,      2022).</p><p> \\n \\n  The      potential      drawback      compared      to      fine-tuning      algorithms      so      far      (i.e.,      PPO      and      reward-  \\n \\n  weighted      MLE)      is      the      necessity      to      learn      differetiable      soft      value      functions      like      direct      reward      back-  \\n \\n  propagation.\\nThis      learning      process      is      often      not      straightforward      as      explained      in      Section      5.2.\\nThe  \\n \\n  previously      discussed      fine-tuning      algorithms      (i.e.,      PPO,      reward-weighted      MLE)      circumvent      this  \\n \\n  requirement      by      obtaining      rewards      to      go      via      Monte      Carlo      approaches,      thereby      avoiding      the      need      for  \\n \\n  soft      value      functions.</p><h2>6.2.1      Soft      Q-learning</h2><p>ning  \\n \\n  We      have      elucidated      that      leveraging      Lemma      2,      we      can      estimate      soft      value      functions      v;(-)      based  \\n \\n  on      Equation      (28)      in   \\n \\na      Monte      Carlo      way.\\nSubsequently,      these      soft      value      functions      are      used      in  \\n \\n  value-weighted      sampling      to      sample      from      the      target      distribution      p,.\\nAlternatively,      there      is      another  \\n \\n  method      that      involves      using      soft      Bellman      equations      to      estimate      soft      value      functions      v;(-).\\nThis  \\n \\n  technique      is      commonly      called      soft      Q-learning      in      the      context      of      standard      RL.</p><p> \\n \\n  First,      recalling      the      soft      Bellman      equations      in      (16),      we      have en      (22)   \\n \\n=      fey      (=)      pa      nd</p><p>Taking      the      logarithm,      we      obtain u(ay)   \\n \\n=      alog      f      exp      (eaen)      De      (@4-1   \\n \\n|      @4)dxy-1.</p><p>a</p><h3>Hence,</h3><p> \\n \\n \\n_   \\n \\n:      (xt)      Up-1(Lt-1)   \\n \\n°       vu   \\n \\n=      argmin      Ey,.u,   \\n \\n|   \\n \\n4      ——   \\n \\n—      log   \\n \\n|      exp   \\n \\n|      ————   \\n \\n]      Dpre(@e-1]      21)      da4_1  \\n \\n  a   \\n \\na h:X->R  \\n \\n  where      u,   \\n \\n€      A(%)      is      any      roll-in      distribution      that      covers      the      entire      space      V.      Using      this      relation      and  \\n \\n  replacing      the      expectation      E,,,.,,,      with      empirical      approximation,      we      are      able      to      estimate      soft      value  \\n \\n  functions      v;      in   \\n \\na      recursive      manner:</p><p> \\n \\n \\n1   \\n \\nm      ~(j—1)   \\n \\n°       aj   \\n \\n;   \\n \\ni      Ur      Ut   \\n \\ny       {6      (x)},   \\n \\n—       argmin   \\n \\ny      S      {ht   \\n \\n)   \\n \\n—      log   \\n \\n/      exp      (eee)      Ppre(@1—1|}      yan] h:      [R¢,[0,7]]|-R      t=T+1      i=1</p><p>where      the      data      is      collected      in      an      off-policy      way.</p><p> \\n \\n  Remark      6.\\nAlthough      soft      Q-learning      is      widely      used      in      standard      RL      (Schulman      et      al.,      2017),      it  \\n \\n  cannot      be      directly      applied      to      our      fine-tuning      context      without      resorting      to      value-weighted      sampling  \\n \\n  or      value-weighted      MLE.\\nThis      is      because,      even      if      we      estimate      soft-value      functions      as      %;,      substituting  \\n \\n  Uz      with      0,      in      the      soft-optimal      policy      results      in      an      unnormalized      policy.<ul><li>6.2.2.\\nApproximation      using      Tweedie’s      formula</li></ul></p><p> \\n \\n  So      far,      we      have      explained      two      approaches:   \\n \\na      Monte      Carlo      approach      and   \\n \\na      value      iteration      approach      to  \\n \\n  estimate      soft      value      functions.\\nHowever,      learning      value      functions      in      (28)      can      still      be      often      challenging  \\n \\n  in      practice.\\nTherefore,      we      can      employ      approximation      strategies      inspired      by      recent      literature      on  \\n \\n  classifier      guidance      (e.g.,      reconstruction      guidance      (Ho      et      al.,      2022),      manifold      constrained      gradients  \\n \\n  (Chung      et      al.,      2022),      universal      guidance      (Bansal      et      al.,      2023),      and      diffusion      posterior      sampling  \\n \\n  (Chung      et      al.,      2022)).</p><p> \\n \\n  Specifically,      we      adopt      the      following      approximation:</p><p>ur(xz)   \\n \\n=      alog      Egpry      lex      (“)      oy   \\n \\n=      alog      (/      exp      (om)      p      (colar      (30)</p><p>~\\n \\n   alog      (exw      (ceo)   \\n \\n)   \\n \\n»       £o(t1)   \\n \\n=      Egprey|xo   \\n \\n|      xe],      (31) a\\n \\n    =\\n \\n   1r(Xo(2t)).</p><p> \\n \\n  Here,      we      replace      the      integration      in      (30)      with   \\n \\na      Dirac      delta      distribution      with      the      posterior      mean.\\n \\n \\n  Importantly,      we      can      calculate      Zo      (x;)   \\n \\n=      E,»*-[xo   \\n \\n|      x4]      using      the      pre-trained      (score-based)      diffusion  \\n \\n  model      based      on      Tweedie’s      formula:</p><table><tr><td colSpan=1>      O12</td><td colSpan=1><p>]\\n \\n \\n  E,pre      [xo   \\n \\n|      x1   \\n \\n—      Ut   \\n \\n+      {or}      V      0g      Ge(&e)  \\n \\n  Le</p></td></tr></table><p>Recall      that      the      notation      p/?,      0?,      q,      are      defined      in      (3).\\nFinally,      by      recalling      V      log   \\n \\n@   \\n \\n=      Si      ore      (x;,t)      in  \\n \\n  score-based      diffusion      models,      we      can      approximate      Vv;(a)      with</p><h4>Le</h4><p> \\n \\n  and      plug      it      into      Algorithm   \\n \\n4      (i.e.,      value-weighted      sampling).\\n \\n \\n  As      mentioned      earlier,      this      approach      has      been      practically      widely      used      in      the      context      of      classifier  \\n \\n  guidance.\\nDespite      its      simplicity,      the      approximation      error      can      be      significant,      as      it      can      not      be  \\n \\n  diminished      even      with   \\n \\na      large      sample      size      because      the      discrepancy      from      (30)      to      (31)      is      not      merely   \\n \\na       statistical      error      that      diminishes      with      increasing      sample      size.</p><li>6.2.3.\\nZeroth-Order      Guidance      using      Path      Integral      Control</li><p> \\n \\n  In      the      previous      subsection      (Section      6.2.2),      we      discussed      an      approximation      technique      to      bypass      the  \\n \\n  necessity      of      learning      explicit      value      functions.\\nAnother      approach      to      bypass      this      requirement      is      to      use  \\n \\n  control-based      techniques      for      obtaining      soft-optimal      policies,      a.k.a.,      path      integral      control      (Theodorou  \\n \\n  et      al.,      2010;      Williams      et      al.,      2017;      Kazim      et      al.,      2024).</p><p> \\n \\n  Recall      that      we      have      p(x;,t;      6)   \\n \\n=      2;   \\n \\n+      (dt)g(x+,      t)      and      o°(t)   \\n \\n=      g(t)(dt),      the      motivation      behind this      approach      is      as      follows.\\nInitially,      we      have:</p><h5>Vo,      exp(v(#1)/@)</h5><p>=\\n \\n   Egppreyexp(vr_1      (22-1)      /@)   \\n \\n|      24]      (Soft      Bellman      equation) _\\n \\n   (1   \\n \\n—   \\n \\nx   \\n \\n—      (Ot)      G(a2,      t))?\\n=\\n \\n   Vz»,   \\n \\n{   \\n \\n[      explatena/ay)      exp      (-      0-5a(t)      (ot)   \\n \\n)      in,-}</p><p>=\\n \\n   Byes      [ese      a(era/a))      {ea      ~~      (61)g(nn,)}      EE      Cah</p><p>~\\n \\n   arp      Buty      lexr(era(ea/a))      Gera   \\n \\n—      21   \\n \\n~      (8t)ale      plod</p><p>_t   \\n \\n1       Ht)      ot)      Eg,prey      [E      ppp}      [exp(r(2o)/a)   \\n \\n|      T-1]      e|24|   \\n \\n=      Woon)      [exp(r(xo/a))e|x¢]      ;</p><h5>Now,      we      obtain:</h5><p>o7(t)Vnv(ae)   \\n \\n1      Eqppry      lexp(r(wo)/a)      eel    \\n \\nQ   \\n \\n~   \\n \\na   \\n \\n*      Egyprey      lexp(r(2o)/a))      |x]      (32)</p><p> \\n \\n  Importantly,      this      approach      does      not      require      any      differentiation.\\nTherefore,      by      running      policies  \\n \\n  from      pre-trained      models      and      simply      using      Monte      Carlo      approximation      in      both      the      denominator  \\n \\n  and      numerator,      we      can      estimate      the      above      quantity      (the      right-hand      side      of      (32))      without      making  \\n \\n  any      models      (classifiers),      unlike      classifier      guidance.\\nNote      while      this      approach      is      widely      used      in      the  \\n \\n  control      community,      it      may      not      be      feasible      for      diffusion      models      due      to      the      high      dimensionality      of  \\n \\n  the      input      space.</p><h3>6.3      Path      Consistency      Learning      (Losses      Often      Used      in      Gflownets)</h3><p>ets)  \\n \\n  Now,      we      explain      how      to      apply      path      consistency      learning      (PCL)      (Nachum      et      al.,      2017)      to      fine-  \\n \\n  tune      diffusion      models.\\nIn      the      Gflownets      literature      (Bengio      et      al.,      2023),      it      seems      that      this      variant      is  \\n \\n  utilized      as      either   \\n \\na      detailed      balance      or   \\n \\na      trajectory      balance      loss,      as      discussed      in      Mohammadpour  \\n \\n  et      al.\\n(2023);      Tiapkin      et      al.\\n(2023);      Deleu      et      al.\\n(2024).\\nHowever,      to      the      best      of      our      knowledge,  \\n \\n  the      precise      formulation      of      path      consistency      learning      in      the      context      of      fine-tuning      diffusion      models  \\n \\n  has      not      been      established.\\nTherefore,      we      start      by      elucidating      the      rationale      of      PCL.\\nSubsequently,  \\n \\n  we      provide   \\n \\na      comprehensive      explanation      of      the      PCL.\\nFinally,      we      discuss      its      connection      with      the  \\n \\n  literature      on      Gflownets.</p><p>Motivation.\\nHere,      we      present      the      fundamental      principles      of      the      PCL.\\nTo      start      with,      we      prove      the  \\n \\n  following      lemma,      which      characterizes      soft-value      functions      and      soft-optimal      policies      recursively.</p><p>Algorithm   \\n \\n5      Path      Consistency      Learning      (Training      with      detailed      balance      loss)  \\n \\n  1:      Require:      Diffusion-model      {N(p(2,t;),07(t))}i-74,,  \\n \\n  -pre-trained   \\n \\n=      model  \\n \\n  {N      (p(t,      t;      Opre),      07      (t))      }i_744,      batch      size      m,      parameter   \\n \\na   \\n \\n€      Rt,      learning      rate   \\n \\n7       2:      Set   \\n \\na      model      {v;(-;@)}      to      learn      optimal      soft      value      function,      and   \\n \\na      model      {p;(-|-;@)}      to      learn  \\n \\n  optimal      polices.\\n \\n \\n  3:      Initialize:      0;   \\n \\n=      Opre  \\n \\n  4:      for      s   \\n \\n=      {1,---      ,S}do  \\n \\n  5:      Collect   \\n \\nm      samples      {a      (0)      }?_7,,      from   \\n \\na      current      diffusion      model      (i.e.,      generating      by      sequen- tially      running      polices      {N(p(2z,      t;      8),      07(t))      }i_-74,      from   \\n \\nt   \\n \\n=   \\n \\nT      to   \\n \\nt   \\n \\n=      0)  \\n \\n  6:      Setuo=r</p><li>(i)   \\n \\n?       a!\\n \\n \\n3   \\n \\na   \\n \\n4      Up—-1(@      13      ds)      re  \\n \\n  bs41   \\n \\n—      bs   \\n \\n—      Vo   \\n \\n3   \\n \\na      9)   \\n \\n+      log      p:(a\\\\\\\\\\\\\\\\,      |a\\\\\\\\\\\\\\\\;0,)   \\n \\n-      EP   \\n \\n—      Jog      rl      >}      Ibe:</li><p>a\\n \\n \\n  t=T+1      i=1  \\n \\n  (c}      ite)      ()      via)      43s)   \\n \\n;       Os41   \\n \\n<      Os   \\n \\n—      Vo   \\n \\n3      TE      velee      ids)      —S<   \\n \\n+      log      pe(ay      2s      |p      0)      —§      log      pee      (ar?\\nifort?\\nJe      lass</p><p>t=T+1      i=1 7:      end      for  \\n \\n  8:      Output:      {p,(x1-1      [215      Os)      fe</p><h3>Lemma      4      (1-step      Consistency      Equation).</h3><li>(2)   \\n \\n+      log      pi      (aa      |a1)   \\n \\n=      (eaeen)   \\n \\n+      log      pp      (x+-1|2+)      (33)</li><p> \\n \\n  Proof.\\nWe      consider      the      marginal      distribution      with      respect      to      x,      and      2;_,      induced      by      the      soft-  \\n \\n  optimal      policy,      and      denote      it      [(z;,      2:1)   \\n \\n€      A(X   \\n \\nx      X).\\nThen,      it      is      clearly      I(x,)l(a,_1   \\n \\n|      x)   \\n \\n=       I(x4-1)l      (xz   \\n \\n|      a1).\\nNow,      from      Theorem   \\n \\n2      that      characterizes      marginal      distributions,      and      Theorem   \\n \\n3       that      characterizes      posterior      distributions,      this      results      in      :</p><p>1\\n \\n   UE      L      re   \\n \\nx   \\n \\n1      Otte      me  \\n \\n  exp   \\n \\n(   \\n \\ni      2)      Pe      (xt)   \\n \\nx      pe      (®e-1|@e)   \\n \\n=   \\n \\nB      exp      (faeev)      Pra      (@1-1)   \\n \\nX      Pe      (1/01) a\\n \\n   —_-_——”’   \\n \\nC   \\n \\na      ~~  \\n \\n  C  \\n \\n  we  \\n \\n  Optimal      policy  \\n \\n  “~   \\n \\n“      Posterior      distribution  \\n \\n  Marginal      distribution      at   \\n \\nt      Marginal      distribution      at      t-1 Rearranging      yields:</p><p>1\\n \\n   UL(&   \\n \\n1      Up—1      (Le      re  \\n \\n  G      exp   \\n \\n(      ))   \\n \\nx      Di      (@-1|%4)   \\n \\n=   \\n \\nG      exp      (eaeen)   \\n \\nx      pe      (@4-1|24)</p><p>Taking      the      logarithm,      the      statement      is      concluded.\\nO</p><p>Algorithm.\\nBeing      motivated      by      the      relation      in      (33),      after      initializing      v9   \\n \\n=      r,      we      obtain      the  \\n \\n  recursive      equation:</p><p>2 Olen      U_-1      (Lt  \\n \\n  (pp)   \\n \\n=      argmin   \\n \\n—      Egany   \\n \\n|      oO   \\n \\n4      tog   \\n \\ng      (arp      ng)   \\n \\n—      PEED)   \\n \\n—      dog      pP\"      (ya      er) g):%3R,g2):¥      A(X)   \\n \\na   \\n \\no U   \\n \\n_   \\n \\nx   \\n \\n_      re      re  \\n \\n  =      (MEY)      tog      peal      altiansa)      +++      Flog      pe      (esl),  \\n \\n  which      is      an      extension      of      (33).\\nThe      loss      function      based      on      the      above      k-step      consistency      equation  \\n \\n  could      make      training      faster      without      learning      value      functions      at      every      time      point,      as      noted      in      the  \\n \\n  literature      in      PCL.\\nIn      the      extreme      case      (i.e.,      when      we      recursively      apply      it      with   \\n \\nt   \\n \\n=      7’),      we      obtain      the  \\n \\n  following.</p><li>(34)  \\n \\n  where      u,   \\n \\n€      A(X)      is      any      exploratory      roll-in      distribution.\\nBased      on      this      algorithm,      we      outline      the  \\n \\n  entire      algorithm      in      Algorithm      5.  \\n \\n  We      make      several      important      remarks      regarding      Algorithm      5.\\nFirstly,      while      we      use      on-policy      data  \\n \\n  collection,      technically,      any      policy      can      be      used      in      this      off-policy      algorithm,      like      reward-weighted  \\n \\n  MLE.\\nSecondly,      in      practice,      it      might      be      preferable      to      utilize   \\n \\na      sub-trajectory      from      x;      to      x;_,      based  \\n \\n  on      the      following      expression:</li><p>UFZ  \\n \\n  log      pi_      pa      (Gee      |Vt—ng1)   \\n \\n+      +++   \\n \\n+      log      pi      (4-1      |a4)   \\n \\n+      (2)</p><h3>Corollary      1      (7-step      consistency).</h3><p>log      pf      (xo|21)   \\n \\n+      +++   \\n \\n+      log      pp(wr-a|er)   \\n \\n+      log      pp      (x7)      (35) r(x      re      re      re  \\n \\n  —      (=)   \\n \\n+      log      p?\\n*(xo|v1)      +--+   \\n \\n+      log      pr\"      (ar-i|er)   \\n \\n+      log      pe      (xr).</p><p>Proof.\\nWe      consider      the      marginal      distribution      with      respect      to      7,      +--+   \\n \\n,   \\n \\n%      induced      by      the      soft-optimal  \\n \\n  policy,      and      denote      it      I(vp,---      ,--+      ,%o)   \\n \\n€   \\n \\n¥      X--+   \\n \\nx      X.      We      have Uar)l(ar—1   \\n \\n|      Lr)      tee      I(xo|21)   \\n \\n=      U(x      )l(a4   \\n \\n|      Xo)      te      (ar   \\n \\n|      LT-1)      (36) From      Theorem   \\n \\n2      that      characterized      marginal      distributions,      and      Theorem   \\n \\n3      that      characterize      posterior  \\n \\n  distributions,      the      left      hand      side      of      (36)      is      equal      to *\\n \\n   *\\n \\n   *</p><p>Pr      (xr)   \\n \\nx      pp      (r|er-1)      X-++   \\n \\nx      pi      (Xo|@1) Marginal      distribution      atT      Optimal      policy      at   \\n \\nT   \\n \\n—   \\n \\n1      Optimal      policy      at   \\n \\n1 and      the      right-hand      side      in      (36)      is      equal      to exp(r      (xo)      /a)      pre      pre      pre  \\n \\n  aq      Po      (xo)   \\n \\nX      po      (a1   \\n \\n|      Zo)      X+++   \\n \\nX      prey      (@r   \\n \\n|      er-1)-.\\n \\n \\n|      ~~  \\n \\n  Marginal      distribution      at   \\n \\n0      Posterior      distribution      Posterior      distribution By      rearranging      the      term,      we      obtain      (35).\\nO</p><p> \\n \\n  Comparison      with      Gflownets.\\nIn      the      Gflownets      literature,      similar      losses      are      used.\\nFor      instance,  \\n \\n  the      loss      derived      from      (33)      or      (35)      is      commonly      known      as   \\n \\na      detailed      balance      loss      (Bengio      et      al.,  \\n \\n  2023)      or   \\n \\na      trajectory      loss      (Malkin      et      al.,      2022),      respectively.</p><p> \\n \\n  Note      in      general,      the      literature      in      Gflownets      primarily      focuses      on      sampling      from      unnormalized  \\n \\n  models      (distributions      proportional      to      exp(r(a))).\\nHence,      reference      policies      (i.e,      {p?\"\"}\\n \\n \\n)      or      latent  \\n \\n  states      (1.e.,      %7.;      before      x9)      are      introduced      without      relying      on      pre-trained      diffusion      models.\\nIn  \\n \\n  contrast,      in      our      context,      we      use      policies      derived      from      pre-trained      diffusion      models      as      reference  \\n \\n  policies,      leveraging      them      as      our      prior      knowledge.</p><p>7\\n \\n   Fine-Tuning      Settings      Taxonomy</p><p> \\n \\n  So      far,      we      implicitly      assume      we      have      access      to      reward      functions.\\nHowever,      these      functions      are      often  \\n \\n  unknown      and      need      to      be      learned      from      data.\\nWe      classify      several      settings      in      terms      of      whether      reward  \\n \\n  functions      are      available      or,      if      not,      how      they      could      be      learned.\\nThis      section      is      summarized      in      Figure      3.</p><h4>Direct      backpropagation      (Sec.      5.2)       _       Differentiable</h4><p>_\\n \\n   ——_      Non-differentiable PP      5.1</p><h4>Accurate      rewards      O      (Sec.      5.1)</h4><p> \\n \\n  feedback   \\n \\n;    \\n \\n—      Reward-weighted      MLE      (Sec.      6.1  \\n \\n  Xo      Unknown      reward      Memory      effi-      oe      eward-welgnte      (Sec.      6.1) unctions      —_\\\\\\\\\\\\\\\\_—      ciency</p><h4>No      fine-tuning      Computational</h4><p>efficiency —\\n \\n    NN      Direct      backpropagation      (Sec.      5.2)</p><h5>Value-weighted      sampling      (Sec.      6.2)</h5><p>Figure      3:      Practical      recommendation      of      RL-based      algorithms      to      optimize      downstream      reward  \\n \\n  functions.</p><h6>7.1      Fine-Tuning      with      Known,      Differentiable      Reward      Functions</h6><p>ions  \\n \\n  When      accurate      differentiable      reward      functions      are      available      (e.g.,      standard      in      computer      vision      tasks),  \\n \\n  our      primary      focus      is      typically      on      computational      and      memory      efficiency.\\nIn      general,      we      advocate      for  \\n \\n  employing      reward      backpropagation      (c.f.      Algorithm      2)      for      its      computational      efficiency.\\nAlternatively,  \\n \\n  if      memory      efficiency      is   \\n \\na      significant      concern,      we      suggest      using      PPO      due      to      its      stability.</p><h6>7.2      Fine-Tuning      with      Black-Box      Reward      Feedback</h6><p>back  \\n \\n  Here,      we      explore      scenarios      where      we      have      access      to      accurate      but      non-differentiable      (black-box)  \\n \\n  reward      feedback,      often      found      in      scientific      simulations.\\nThe      emphasis      still      remains      primarily      on  \\n \\n  computational      and      memory      efficiency.\\nIn      such      scenarios,      due      to      the      non-differentiability      of      reward  \\n \\n  feedback,      we      recommend      using      PPO      or      reward-weighted      MLE      for      the      following      reasons.</p><p> \\n \\n  In      the      preceding      section,      we      advocated      for      the      use      of      reward      backpropagation      due      to      its      com-  \\n \\n  putational      advantages.\\nHowever,      when      dealing      with      non-differentiable      feedback,      the      advantage  \\n \\n  of      reward      backpropagation      may      diminish.\\nThis      is      because      learning      from      such      feedback      requires  \\n \\n  learning      differentiable      reward      functions      to      make      the      algorithm      work      as      we      discuss      in      Section      5.2.  \\n \\n  However,      obtaining   \\n \\na      differentiable      reward      function      can      be      challenging      in      many      domains.\\nFor  \\n \\n  example,      in      molecular      property      prediction,      molecular      fingerprints      are      informative      features,      and  \\n \\n  accurate      reward      functions      can      be      derived      by      training      simple      neural      networks      on      these      features  \\n \\n  (Pattanaik      and      Coley,      2020).\\nYet,      these      mappings,      grounded      in      prior      scientific      understanding,      are  \\n \\n  non-differentiable,      thereby      restricting      their      applicability.</p><p> \\n \\n  In      contrast,      PPO      and      reward-weighted      MLE      allow      for      policy      updates      without      the      explicit  \\n \\n  requirement      to      learn   \\n \\na      differentiable      reward      function,      offering   \\n \\na      significant      advantage      over      reward  \\n \\n  backpropagation.</p><h6>7.3      Fine-Tuning      with      Unknown      Rewards      Functions</h6><p>ions  \\n \\n  When      reward      functions      (or      computational      proxies      in      Section      7.2)      are      unavailable,      we      must      learn  \\n \\n  from      data      with      reward      feedback.\\nIn      such      cases,      compared      to      the      previous      scenarios,      two      important  \\n \\n  considerations      arise:</p><p>¢\\n \\n   Not      only      computational      or      memory      efficiency      but      also      feedback      efficiency      (i.e.,      sample  \\n \\n  efficiency      regarding      reward      feedback)      is      crucial.</p><p> \\n \\n \\n¢      Given      that      learned      reward      feedback      may      not      generalize      well      outside      the      training      data      distri-  \\n \\n  butions,      it      is      essential      to      constrain      fine-tuned      models      to      prevent      significant      divergence      from  \\n \\n  diffusion      models.\\nIn      these      situations,      not      only      soft      PPO      and      direct      backpropagation      but      also  \\n \\n  methods      discussed      in      Section      6,      such      as      reward-weighted      MLE,      can      be      particularly      effective.</p><p>Further,      the      scenario      involving      unknown      reward      functions      can      be      broadly      categorized      into      two      cases,  \\n \\n  which      we      will      discuss      in      more      detail.</p><p>Offline      scenario      (Figure      4).\\nIn      many      scenarios,      even  \\n \\n  in      cases      where      reward      functions      are      unknown,      we      of-  \\n \\n.     .</p><p>Offline      data      with ten      have      access      to      offline      data      with      reward      feedback      reward      feedback</p><li>(i)      (i)   \\n \\nO   \\n \\n;   \\n \\ni       a”      ra  \\n \\n.      ne      straightforward      approach      is      to</li><p> \\n \\n \\n{   \\n \\n,   \\n \\n(   \\n \\n)    \\n.\\n    .\\n \\n   g\\n \\n   PP  \\n \\n.      <=>      Learning      reward  \\n \\n  perform      regression      with      neural      networks      and      build   \\n \\na      models  \\n \\n  learned      regressor      *.\\nHowever,      this      learned      reward      func-      SS  \\n \\n  tion   \\n \\n*      might      not      generalize      well      beyond      the      distribution      x)  \\n \\n  of      the      offline      data.\\nIn      regions      outside      this      distribution,   \\n \\n7       could      assign   \\n \\na      high      value      even      when      the      actual      reward  \\n \\n  r      is      low      due      to      high      uncertainty.\\nConsequently,      we  \\n \\n  could      be      easily      misled      by      out-of-distribution      samples  \\n \\n  if      we      solely      optimize      7.\\n|Fine-tuning      diffusion      models   \\n \\n|       One      approach      to      alleviate      this      issue      is      to      adopt   \\n \\na       pessimistic      (i.e.,      conservative)      strategy,      as      proposed      in      Figure      4:      Offline      scenario  \\n \\n  Uehara      et      al.\\n(2024).\\nThis      technique      has      been      widely      Collecting      data]   \\n \\nY      Learning      reward  \\n \\n  applied      in      the      field      of      offline      RL      (Levine,      2018).\\nThe      aa  \\n \\n  main      idea      is      to      penalize   \\n \\n7      outside      the      distributions      us-  \\n \\n  ing      techniques      such      as      adding      an      explicit      penalty      term  \\n \\n  (Yu      et      al.,      2020;      Chang      et      al.,      2021),      bootstrapping,  \\n \\n  or      more      sophisticated      methods      (Kumar      et      al.,      2020;  \\n \\n  Xie      et      al.,      2021;      Rigter      et      al.,      2022;      Uehara      and      Sun,  \\n \\n  2021).\\nBy      doing      so,      we      can      avoid      being      fooled      by  \\n \\n  out-to-distribution      regions      while      still      benefiting      from  \\n \\n  the      extrapolation      capabilities      of      reward      models.</p><p>S—      diffusion      models      |</p><p>Figure      5:      Online      scenario</p><p> \\n \\n  Online      scenario      (Figure      5).\\nConsider      situations  \\n \\n  where      reward      functions      are      unknown,      but      we      can      gather      data      online.\\nThese      scenarios      are      often  \\n \\n  referred      to      as   \\n \\na      lab-in-the-loop      setting.</p><p> \\n \\n  In      such      scenarios,      Uehara      et      al.\\n(2024)      proposes      an      iterative      procedure      comprising      three      steps:</p><li>(1)      collecting      feedback      data      by      exploring      new      areas      (i.e.,      obtaining   \\n \\nx      from      current      diffusion      models  \\n \\n  and      corresponding      r(x)),      (2)      learning      the      reward      function      from      the      collected      feedback      data,      and</li><li>(3)      fine-tuning      current      diffusion      models      by      maximizing      the      learned      reward      function      enhanced      by  \\n \\n  an      optimistic      bonus      term,      using      algorithms      discussed      in      Section      4.2.\\nIn      contrast      to      standard      online  \\n \\n  PPO,      where      feedback      data      is      directly      used      in      the      fine-tuning      process,      actual      reward      feedback      is  \\n \\n  only      used      in      step      (2)      and      not      in      step      (3).\\nAn      additional      key      aspect      is      the      use      of      an      optimistic  \\n \\n  reward      function      that      encourages      exploration      beyond      the      distributions      of      the      current      diffusion      model.\\n \\n \\n  This      deliberate      separation      between      reward      learning      and      fine-tuning      steps,      coupled      with      the      use      of  \\n \\n  optimism,      significantly      enhances      feedback      efficiency.</li><p>8\\n \\n   Connection      with      Classifier      Guidance</p><p> \\n \\n  Classifier      guidance      (Dhariwal      and      Nichol,      2021)      is      commonly      used      for      conditional      generation      in  \\n \\n  diffusion      models.\\nInterestingly,      RL-based      methods      share   \\n \\na      close      connection      with      classifier      guidance.\\n \\n \\n  More      specifically,      in      this      section,      following      Zhao      et      al.\\n(2024),      we      elucidate      how      RL-based      methods  \\n \\n  can      be      applied      to      conditional      generation.\\nFurthermore,      from      this      unified      viewpoint,      we      highlight  \\n \\n  that      classifier      guidance      is      seen      as   \\n \\na      value-weighted      sampling      in      Section      4.2.</p><h6>8.1      Classfier      Guidance</h6><p>ance  \\n \\n  In      this      section,      we      first      introduce      classifier      guidance.\\nClassifier      guidance      utilizes      an      unconditional  \\n \\n  pre-trained      model      for      conditional      generation.\\nMore      specifically,      we      consider   \\n \\na      scenario      where      we  \\n \\n  have   \\n \\na      pre-trained      diffusion      model,      which      enables      us      to      sample      from      p?\\n\"*(x),      and      data      with      feedback  \\n \\n  {x,y},      where   \\n \\ny      represents      the      new      label      we      want      to      condition      on.\\nOur      objective      here      is      to      sample  \\n \\n  from      p(-|y)   \\n \\n€      [Y   \\n \\n>      ACR), P(-   \\n \\n|      y)   \\n \\nx      py(yl-)pP*(-).\\n(37)</p><p>where      p,   \\n \\n:   \\n \\nX      —>      A(Y)      denotes      the      conditional      distribution      of   \\n \\ny      given      x.</p><p>Motivation.\\nIn      classifier      guidance,      we      consider      the      following      policy:</p><p>pe      (-lare,      y)   \\n \\n=   \\n \\nN      (p(22,      t;      Opre)   \\n \\n+      a(t)      Ve      log      Vy      (Xt;      t),      a*(t))   \\n \\n’ dy(x,t)   \\n \\n=      Egy.\\nyap,      (-leo)      L(Y   \\n \\n=      y)|a,   \\n \\n=      2].</p><p>Under      the      continuous-time      formulation      of      diffusion      models,      applying      Doob’s      h-transform      (Rogers  \\n \\n  and      Williams,      2000),      it      is      shown      that      we      can      sample      from      p(x|y)      by      sequentially      running      {p7\"\\\\\\\\\\\\\\'(-|7+,      y)      }i-r41-</p><p>Algorithm.\\nSpecifically,      the      algorithm      comprises      two      steps.\\nIt’s      important      to      note      that      this      is      not  \\n \\n  a      fine-tuning      method      but      rather      an      inference-time      technique      as      we      freeze      the      pre-trained      model.</p><li>1. Learning      q,(x,t)      through      regression      by      gathering      data      containing      {z;,      y}.\\nHere,      for      each  \\n \\n  (Xo,      y),      using   \\n \\na      forward      policy      for      the      pre-training      (from   \\n \\n0      to      T),      we      obtain      2;      starting      from</li><p>Lo.</p><li>2. During      inference,      sequentially      execute      {p2\"(-|a,,      y)      }1_>   \\n \\n1      by      leverage      the      fact      that      compared</li><p>gui to      the      policy      in      pre-trained      diffusion      models,      the      mean      of      {p?\\n\"\\\\\\\\\\\\\\'(-|71,      y)      }i-74,      is      just      shifted  \\n \\n  with      an      additional      term      V,,      log      q,(2,      t).</p><p> \\n \\n  Notably,      the      method      described      above      can      be      reduced      to      value-weighted      sampling      (cf.      Sec-  \\n \\n  tion      6.2),      when      r(x,      y)   \\n \\n=      log      p(y|x).\\nInspired      by      such      an      observation,      we      apply      an      RL-based  \\n \\n  fine-tuning      method      for      conditional      generation      as      below.</p><h7>8.2.      RL-Based      Fine-Tuning      for      Conditional      Generation</h7><p>tion  \\n \\n  In      this      section,      following      Zhao      et      al.\\n(2024),      we      explain      an      RL-based      fine-tuning      approach      for  \\n \\n  conditional      generation.\\nThe      core      idea      is      that      setting      r(x,      y)   \\n \\n=      log      p(y|x)      allows      us      to      sample      from  \\n \\n  the      target      conditional      distribution      (37),      drawing      on      the      insights      discussed      in      Section      4.2.  \\n \\n  More      precisely,      introducing   \\n \\na      distribution      over   \\n \\ny      as      q(-),      we      formulate      the      following      RL      problem:</p><p>{Di      ft   \\n \\n=      argmax      Egy,      (-\\\\\\\\\\\\\\\\-y)}.yott      log      p(y|xo)   \\n \\n—      NeopyaKL(pe(-|e,      y)      ||P      (late).\\n \\n \\n  {prE[(V,.X)PA(A)]      Heyy (38)</p><p> \\n \\n  Compared      to      the      objective      function      (18)      in      Section      4.2,      the      main      differences      are:      (1)      policy      space  \\n \\n  is      expanded      to      accommodate      additional      control      (i.e.,      y):      pz(-|-,-)   \\n \\nC      [R¢   \\n \\nx   \\n \\nY   \\n \\n+      A(R®)],      (2)      the  \\n \\n  fine-tuning      objectives      are      set      as      log-likelihoods:      log      p(y|z).\\nThen,      following      Theorem      1,      we      can  \\n \\n  establish      the      following      result.</p><p>Corollary      2.\\nThe      distribution      induced      by      {p*}1_74      (ue.   \\n \\n[      {Teri      Ph      (ae-1   \\n \\n|      te,      y)}deur      is      the target      conditional      distribution      (37).</p><p> \\n \\n  The      above      corollary      suggests      that      for      conditional      generation,      we      can      utilize      various      off-the-shelf  \\n \\n  algorithms      discussed      in      Section      4.2      and      6,      such      as      PPO,      reward      backpropagation,      and      reward-  \\n \\n  weighted      MLE,      as      well      as      value-weighted      sampling.</p><p>9\\n \\n   Connection      with      Flow-Based      Diffusion      Models</p><p> \\n \\n  We      elucidate      the      close      relationship      between      bridge      matching      and      fine-tuning,      as      summarized      in  \\n \\n  Table      2.\\nBridge      (flow)      matching      (Liu      et      al.,      2022;      Tong      et      al.,      2023;      Lipman      et      al.,      2023;      Liu      et      al.,  \\n \\n  2022;      Shi      et      al.,      2024;      Albergo      and      Vanden-Eijnden,      2022),      has      recently      gained      popularity      as   \\n \\na       distinct      type      of      diffusion      model      from      score-matching-based      ones.\\nThese      works      suggest      that      training  \\n \\n  and      inference      speed      will      be      accelerated      due      to      the      ability      to      define   \\n \\na      more      direct      trajectory      from  \\n \\n  noise      to      data      distribution      than      score-matching-based      diffusion      models.</p><p> \\n \\n  The      connection      between      bridge      matching      and      fine-tuning      becomes      evident      in      the      continuous-  \\n \\n  time      formulation.\\nTo      begin,      we      first      provide   \\n \\na      brief      overview      of      the      continuous-time      formulation      of  \\n \\n  score-matching-based      diffusion      models.</p><p> \\n \\n  Table      2:      Comparison      between      training      score/flow-based      diffusion      models      and      fine-tuning.\\nWe  \\n \\n  optimize      the      parameter   \\n \\n6      in      P’,      which      represents      the      induced      distribution      over      trajectories      associated  \\n \\n  with      6.</p><h8>Loss      Note      of      Q</h8><p>Score-based      KL(Q\\\\\\\\\\\\\\'™¢|P?)      Qtime:      Time-reversal      of      reference      SDE      (reference training      SDE      is      defined      from   \\n \\n0      to      T’      where      the      initial      dis-  \\n \\n  tribute      at   \\n \\n0      is      data      distribution)</p><p> \\n \\n  Flow-based      KL(Q*°>      ||P?)      @Q*°°h:      Coupling      between      the      reference      SDE      from  \\n \\n  training   \\n \\nT      to   \\n \\n0      conditioning      at   \\n \\n0      (dist.      over      x.;|7o)      and  \\n \\n  a      data      distribution      at      time   \\n \\n0      (dis.      over      x).\\nNote  \\n \\n  that      reference      SDE      is      defined      from      7’      to      0.</p><p> \\n \\n  Fine-tuning      KL(P?||Q*°)      @*:      Coupling      between      the      pre-trained      SDE  \\n \\n  from   \\n \\nT      to   \\n \\n0      conditioned      on   \\n \\n0      (dist.      over      x7.1|X0)  \\n \\n  and   \\n \\na      data      distribution      at      time   \\n \\n0      (dis.      over      Xo) 9.1      Continuous      Time      Formulation      of      Score-Based      diffusion      models</p><p> \\n \\n  We      provide   \\n \\na      brief      overview      of      the      continuous-time      formulation      of      diffusion      models      (Song      et      al.,  \\n \\n  2021),      further      expanding      Section      1.1.1.\\nIn      this      formulation,      we      initially      define      the      forward      reference  \\n \\n  SDE,      spanning      from      time   \\n \\n0      to      7’      (e.g.,      variance      exploding      (VE)      process      or      variance      preserving      (VP)  \\n \\n  process),      by      setting      an      initial      distribution      at   \\n \\n0      as      the      data      distribution.\\nAdditionally,      the      reference  \\n \\n  SDE      is      tailored      to      converge      to      an      easy      distribution      at      time      7’,      such      as      the      normal      distribution.</p><p> \\n \\n  Subsequently,      we      consider      the      time-reversal      SDE      (Anderson,      1982),      progressing      from      T\\\\\\\\\\\\\\'      to      0.\\nIf  \\n \\n  we      could      learn      this      time-reversal      SDE,      it      would      enable      us      to      sample      from      the      data      distribution      by  \\n \\n  starting      with      the      easy      initial      distribution      (at      7’)      and      following      the      time-reversal      SDE      from   \\n \\nT      to   \\n \\n0      at  \\n \\n  inference      time.</p><p> \\n \\n  Training.\\nNow,      the      remaining      question      is      how      to      learn      this      time-reversal      SDE.\\nHere,      by      repre-  \\n \\n  senting      the      induced      distribution      over      trajectories      from   \\n \\nT      to   \\n \\n0      with      this      time-reversal      SDE      as      Q\\\\\\\\\\\\\\'™®,  \\n \\n  our      goal      is      to      train   \\n \\na      new      SDE,      parameterized      by   \\n \\n@      in      the      neural      network,      such      that      the      induced  \\n \\n  distribution      P®      over      trajectories      associated      with   \\n \\n6      aligns      with      Q\"™*.\\nSpecifically,      the      objective      is      to minimize      the      following      losses:</p><p> \\n \\n  argmin      KL(Q\"™||P*).\\n(39)  \\n \\n  0  \\n \\n  With      some      algebra,      leveraging      the      observation      that      the      time-reversal      SDE      comprises      the      original  \\n \\n  forward      SDE      and      the      scoring      term      of      the      marginal      distribution      (Anderson,      1982)      as      in      (2),      Song  \\n \\n  et      al.\\n(2021)      demonstrates      that      this      KL      loss      (39)      is      approximated      as Dore   \\n \\n=      argmin      Eeuni((0,7))01~qyo0(weleo)co~pP\"e      Fe      Fl      Vey      log      jo      (Xt   \\n \\n|      Xo)   \\n \\n_      so(Xt,      t)|I?\\n],      (40)  \\n \\n  which      is      equal      to      (4)      where      the      weight      A(t)      is      {o?\\n}?\\n(for      notation,      refer      to      Section      1.1.1).</p><h9>9.2      Flow-Based      (Bridge-Based)      Diffusion      Models</h9><p>dels We      adopt      the      explanation      outlined      in      Liu      et      al.\\n(2022).\\nIn      this      framework,      we      begin      by      considering  \\n \\n  a      reference      SDE      (from      T\\\\\\\\\\\\\\'      to      0),      x;   \\n \\n=      zr_;      where      z;      is      defined      as      Brownian      motion:</p><p>dz,   \\n \\n=      dwt,      te      (0,      7).\\n(41)</p><p> \\n \\n  Subsequently,      we      consider      the      coupling      of      the      reference      SDE      conditioned      on      state      at      time   \\n \\n0      (i.e.,  \\n \\n  distribution      of      xo.7      conditional      on      x)      and      the      data      distribution      at      time      0,      denoting      the      resulting  \\n \\n  distribution      as      Q*°?,      Informally,      this      Q*°°?\\nis      characterized      as      follows.</p><p>Informal      characterization      of      Q*”.\\nWhen      discretizing      the      time      step,      by      denoting      the      distribution  \\n \\n  induced      by      the      reference      SDE      as      p\"(x7,--+   \\n \\n,      29),      we      define QF      (a7,      tae   \\n \\n,      20)   \\n \\n=      p(x,      ay   \\n \\n|      r9)pP\"°      (x9).</p><p>The      distribution      Q*”      above      is   \\n \\na      limiting      distribution      obtained      by      making      the      discretization      step small.</p><p>Training.\\nIn      bridge-based      diffusion      models,      our      objective      is      to      train   \\n \\na      new      SDE,      parametrized      by  \\n \\n  6      (e.g.,   \\n \\na      neural      network),      to      align      the      induced      distribution      P®      with      Q*°?.\\nSpecifically,      bridge-based training      aims      to      minimize aremin      KL(Q*>||P’).\\n(42)  \\n \\n  0  \\n \\n  While      the      method      to      minimize      the      above      is      currently      unclear,      we      can      derive   \\n \\na      trainable      objective  \\n \\n  through      algebraic      manipulation.\\nInitially,      we      observe      that      the      reference      SDE      (41),      conditioned      on  \\n \\n  state      xo      at      time   \\n \\n0      (zr   \\n \\n=      D),      is      expressed      as:</p><p>b\\n \\n \\n  b—      2;</p><p>t\\n \\n   €\\n \\n   [0,T];dz?\\n \\n \\n=      g?(z?,t)dt      +du,,  \\n \\n  g(z?,t)   \\n \\n=   \\n \\na       (43)</p><p>using      the      seminal      “Doob’s      h-transform”      (Rogers      and      Williams,      2000).\\nThis      SDE      is      commonly  \\n \\n  known      as      the      Brownian      bridge.\\nNow,      we      consider   \\n \\na      new      SDE      parameterized      by      0:</p><p>t\\n \\n   €\\n \\n   [0,7];      dz   \\n \\n=      s(z,t;      @)dt   \\n \\n+      dw.\\n(44)</p><p>Then,      we      can      demonstrate      that      minimizing      the      KL      loss      in      (42)      is      equivalent      to argmin      E,W      unil0,7],1~4\"°      cop?\\n[Ilg”      (2,      t)   \\n \\n_      s(21,      t;      0)      M3]      (45)</p><p>where      q;°      denotes      the      induced      distribution      at      time   \\n \\nt      following      SDE      (43)      conditioned      on      xo,      and  \\n \\n  s:      Xx      |0,T]   \\n \\n>   \\n \\n&      is      the      model      we      aim      to      optimize.</p><p> \\n \\n  Flow-based      diffusion      models.\\nThe      aforementioned      formulation      has      been      originally      proposed      as   \\n \\na       bridge-based      SDE      (Liu      et      al.,      2022)      because      the      conditional      SDE      is      often      referred      to      as   \\n \\na      Brownian  \\n \\n  bridge.\\nFlow-based      diffusion      models      proposed      in      Lipman      et      al.\\n(2023);      Tong      et      al.\\n(2023)      share   \\n \\na       close      relationship      with      bridge-based      diffusion      models.\\nThese      flow-based      models      use   \\n \\na      target      SDE (43)      and      an      SDE      with   \\n \\na      parameter      (44)      without      stochastic      terms      (1.e.,      no      dw,)      and      aim      to      minimize  \\n \\n  the      loss      function      (45).</p><p> \\n \\n  Comparison      with      score-based      diffusion      models.\\nBoth      score-based      diffusion      models      and      bridge-  \\n \\n  based      diffusion      models      aim      to      minimize      KL      divergences.\\nDespite      its      similarity,      in      contrast      to  \\n \\n  score-based      diffusion      models,      bridge-based      diffusion      models      target      Q*°>      rather      than      the      time-  \\n \\n  reversal      SDE      Q\"™®.\\nThe      distribution      Q*”      is      often      considered      preferable      because      it      can      impose      an  \\n \\n  SDE      that      navigates      efficiently      from   \\n \\n0      to      7’      (e.g.,   \\n \\na      Brownian      bridge      with      minimal      noise).\\nThis      can  \\n \\n  expedite      the      learning      process      since      we      lack      direct      control      over      the      time-reversal      SDE      in      score-based  \\n \\n  diffusion      models.</p><h10>9.3.      Connection      with      RL-Based      Fine-Tuning</h10><p>ning  \\n \\n  RL-based      fine-tuning      shares   \\n \\na      close      relationship      with      flow-based      training.\\nTo      illustrate      this,      consider  \\n \\n  the      reference      SDE      (from      7’      to      0)      following      the      pre-trained      diffusion      model.\\nThen,      akin      to      flow-  \\n \\n  based      training,      we      consider      the      coupling      of      the      reference      SDE      conditioned      on      state      at      time   \\n \\n0       (i.e.,      distribution      over      xo-r      conditioned      on      x9)      and      the      target      distribution      at      time   \\n \\n0      (i.e.,      p,(-)   \\n \\n«       exp(r(-)/a)p?\\n\"°(-)),      denoting      the      induced      distribution      as      Q*™.</p><p>Informal      characterization      of      Q™°*.\\nInformally,      Q*\"°      is      introduced      as      follows.\\nWhen      discretizing  \\n \\n  the      time      step,      by      denoting      the      distribution      induced      by      the      reference      SDE      as      p™\\\\\\\\\\\\\\'(xp,-+-   \\n \\n,      9),      we define Qi\"      (xr,   \\n \\na   \\n \\n,      0)   \\n \\n=      p(x,      nr      Ly   \\n \\n|      L0)Pr      (Xo).</p><p>The      distribution      Q*°      above      is   \\n \\na      limiting      distribution      obtained      by      making      the      discretization      step small.</p><p>Training.\\nNow,      we      introduce   \\n \\na      new      SDE      parameterized      by   \\n \\n@      such      as      an      neural      network      to      align  \\n \\n  the      induced      distribution      P’      with      Q®\"*.\\nActually,      in      the      continuous      formulation,      the      RL-problem      (18) in      Section      4.2      is      equal      to      solving argmin      KL(P?||Q\"™).\\n \\n \\n  0 This      formalization      has      been      presented      in      Uehara      et      al.\\n(2024).\\nIntuitively,      as      we      see      in      Theorem      3,  \\n \\n  this      is      expected      because      we      see      that      the      posterior      distribution      induced      by      pre-trained      models:</p><p>ref   \\n \\n(      PD      (ar,-++      21   \\n \\n|      to)      (=      vey      (er|er_-1)p      Pp      (ar-1   \\n \\n|      er_2)      +++      pp\"      (x1   \\n \\n|      20), remains      preserved      after      fine-tuning      based      on      the      RL-formulation      (18).</p><p> \\n \\n  Comparison      with      flow-based      training.\\nIn      contrast      to      flow-based      training,      RL-based      fine-tuning  \\n \\n  minimizes      the      inverse      KL      divergence      rather      than      the      KL      divergence.\\nIntuitively,      this      is      because,  \\n \\n  unlike      Qt™*      or      Q*>   \\n \\n,      we      cannot      sample      from      Q*”      (recall      that   \\n \\na      marginal      distribution      at      time   \\n \\n0      in  \\n \\n  Qi,      i.e.,      p,,      is      unnormalized);      on      the      other      hand,      the      marginal      distributions      at      time   \\n \\n0      in      both      Q\\\\\\\\\\\\\\'™*  \\n \\n  and      Q*°°>      are      data      distributions.</p><p> \\n \\n  Remark   \\n \\n7      (Extension      to      f-divergence).\\nEach      of      these      methods      can      be      technically      extended      by  \\n \\n  incorporating      more      general      divergences      beyond      KL      divergence.\\nFor      instance,      in      the      context      of  \\n \\n  fine-tuning,      please      see      Tang      (2024).</p><p>10      Connection      with      Sampling      from      Unnormalized      Distributions</p><p> \\n \\n  Numerous      studies      delve      into      sampling      from      an      unnormalized      distribution      proportional      to      exp(r()),  \\n \\n  commonly      known      as      the      Gibbs      distribution,      extensively      discussed      in      the      computational      statistics  \\n \\n  and      statistical      physics      community      (Robert,      2014).\\nThis      issue      is      pertinent      to      our      work,      as      during  \\n \\n  fine-tuning,      the      target      distribution      is      also      formulated      in      the      Gibbs      distribution      form      in      (10),      which      is  \\n \\n  proportional      to      exp(r(a))p?\\n\"*      (x).</p><p> \\n \\n  The      literature      employs      two      main      approaches      to      tackle      this      challenge:      Markov      Chain      Monte  \\n \\n  Carlo      (MCMC)      and      RL-based      methods.\\nIn      particular,      the      RL-based      diffusion      model      fine-tuning  \\n \\n  we      have      discussed      so      far      is      closely      related      to      the      latter      approach.\\nIn      this      section,      we      explain      and  \\n \\n  compare      these      two      approaches.</p><h10>10.1      Markov      Chain      Monte      Carlo      (MCMC)</h10><p>CMC)  \\n \\n  In      MCMC,   \\n \\na      Markov      chain      is      constructed      to      approximate   \\n \\na      target      distribution      such      that      the      equilib-  \\n \\n  rium      distribution      of      the      Markov      chain      converges      to      the      target      distribution.\\nHowever,      dealing      with  \\n \\n  high-dimensional      domain      spaces      is      challenging      for      MCMC.\\n \\n \\nA      common      strategy      involves      leveraging  \\n \\n  gradient      information,      such      as      the      Metropolis-adjusted      Langevin      algorithm      (MALA)      (Besag,      1994)  \\n \\n  or      Hamiltonian      Monte      Carlo      (HMC)      (Neal      et      al.,      2011;      Girolami      and      Calderhead,      2011).\\nMALA  \\n \\n  shares      similarities      with      classifier      guidance      and      value-weighted      sampling,      as      both      methods      depend  \\n \\n  on      the      first-order      information      from      reward      (or      value)      functions.</p><p> \\n \\n  Can      we      use      MCMC      for      fine-tuning      diffusion      models?\\nIn      the      context      of      fine-tuning      dif-  \\n \\n  fusion      models,      while      it      may      seem      intuitive      to      sample      from      the      target      distribution      (10)      (i.e.,  \\n \\n  Dy   \\n \\n&      exp(r(x))pP\"*(x))      using      MCMC,      it’s      not      straightforward      for      the      following      reasons:</p><p>¢\\n \\n   We      lack      of      an      analytical      form      of      p?\\n\"*(-).<ul><li>*\\n \\n   Additionally,      even      if      we      can      estimate      p?\\n’°(-)      in      an      unbiased      manner,      as      in      Chen      et      al.\\n(2018),  \\n \\n  the      mixing      time      for      obtaining   \\n \\na      single      sample      in      MCMC      might      be      lengthy.</li></ul></p><p> \\n \\n  Therefore,      we      explore      creating   \\n \\na      generative      model      (i.e.,      diffusion      model)      for      this      task      using      RL-  \\n \\n  based      fine-tuning      so      that      we      can      easily      sample      from      p,(-)      during      inference      (i.e.,      simulating      the  \\n \\n  time-reversal      SDE      (2)      with      policies      from   \\n \\n¢   \\n \\n=      T\\\\\\\\\\\\\\'+   \\n \\n1      to   \\n \\nt   \\n \\n=      1),      without      relying      on      MCMC.\\nThis  \\n \\n  addresses      the      concern      of      MCMC      by      avoiding      the      need      to      estimate      p?\\n\"*(-)      and      minimizing      inference  \\n \\n  time,      albeit      at      the      expense      of      increased      training      time.</p><p>10.2}      RL-Based      Approaches  \\n \\n  While      MCMC      has      historically      been      widely      used      for      this      purpose,      recent      works      have      proposed      an  \\n \\n  RL-based      approach      or      its      variant      (e.g.,      Bernton      et      al.      (2019);      Heng      et      al.\\n(2020);      Huang      et      al.\\n(2023);</p><p> \\n \\n  Vargas      et      al.\\n(2023)).\\nFor      example,   \\n \\na      seminal      study      (Zhang      and      Chen,      2021)      introduced   \\n \\na      method  \\n \\n  very      similar      to      reward      backpropagation.\\nIn      their      work,      they      initially      define      Brownian      motion      as   \\n \\na       reference      SDE      (from      7’      to      0),      similar      to      the      pre-trained      diffusion      models      in      our      context.\\nThen,      by  \\n \\n  appropriately      setting      rewards,      Zhang      and      Chen      (2021)      aims      to      learn   \\n \\na      new      SDE      (from      T\\\\\\\\\\\\\\'      to      0)      such  \\n \\n  that      we      can      sample      from      the      target      distribution      at      the      endpoint      0.</p><p> \\n \\n  These      approaches      offer      advantages      over      MCMC      due      to      their      ability      to      minimize      inference      time  \\n \\n  (amortization)      by      constructing   \\n \\na      generative      model      comprising      policies      in      the      training      time      and      solely  \\n \\n  executing      learned      policies      during      inference.\\nUnlike      MCMC,      these      methods      significantly      reduce      the  \\n \\n  inference      cost      as      we      don’t      need      to      be      concerned      about      the      mixing      time.\\nAdditionally,      RL-based  \\n \\n  approaches      have      the      potential      to      effectively      handle      high-dimensional,      complex      (i.e.,      multi-modal)  \\n \\n  distributions      by      harnessing      the      considerable      expressive      power      arising      from      latent      states.</p><p> \\n \\n  Despite      these      advantages,      it      is      often      unclear      how      to      select   \\n \\na      reference      SDE      in      these      works.\\nIn  \\n \\n  contrast,      in      RL-based      fine-tuning      of      diffusion      models,      we      directly      leverage      pre-trained      diffusion  \\n \\n  models      as      the      reference      SDEs,      i.e.,      as      our      prior      knowledge.</p><p> \\n \\n  Key      message.\\nFinally,      it      is      worth      noting      that      we      can      technically      utilize      any      off-the-shelf      RL      algo-  \\n \\n  rithms      for      training.\\nFor      example,      Zhang      and      Chen      (2021)      employs      reward      backpropagation,      while  \\n \\n  works      in      Gflownets      typically      leverage      PCL,      as      we      mention      in      Section      6.3.\\nHowever,      additionally,      it  \\n \\n  is      possible      to      use      PPO      or      reward-weighted      MLE      mentioned      in      Section      4.2.</p><p>11      Closely      Related      Directions</p><p>Lastly,      we      highlight      several      closely      related      directions      we      have      not      yet      discussed.</p><p> \\n \\n  Aligning      text-to-image      models.\\nWe      recognize      that      current      efforts      in      fine-tuning      diffusion      models  \\n \\n  primarily      revolve      around      aligning      text-to-image      models      using      human      feedback      (Lee      et      al.,      2023;  \\n \\n  Wallace      et      al.,      2023;      Wu      et      al.,      2023;      Yang      et      al.,      2023).\\nIn      this      paper,      our      goal      is      to      consider   \\n \\na      more  \\n \\n  general      and      fundamental      formulation      that      is      not      tailored      to      any      particular      task.</p><p> \\n \\n  Diffusion      models      for      RL.\\nDiffusion      models      are      well-suited      for      specific      reinforcement      learning  \\n \\n  applications      due      to      their      ability      to      model      complex      and      multimodal      distributions      as      polices      (Janner  \\n \\n  et      al.,      2022;      Ajay      et      al.,      2023;      Wang      et      al.,      2022;      Hansen-Estruch      et      al.,      2023;      Du      et      al.,      2024;      Zhu  \\n \\n  et      al.,      2023).\\nFor      example,      Wang      et      al.\\n(2022)      use      conditional      diffusion      models      as      policies      and  \\n \\n  demonstrate      their      effectiveness      in      typical      offline      RL      benchmarks.\\nWhile      RL-based      fine-tuning      also  \\n \\n  aims      to      maximize      certain      reward      functions,      the      primary      focus      of      this      tutorial      covers      methods      that  \\n \\n  leverage      pre-trained      diffusion      models      for      this      purpose.</p><p> \\n \\n  RL      from      human      feedback      (RLHF)      for      language      models.\\nRLHF      is      widely      discussed      in      the  \\n \\n  context      of      language      models,      where      many      algorithms      share      similarities      with      those      used      in      diffusion  \\n \\n  models      (Ouyang      et      al.,      2022;      Bai      et      al.,      2022;      Casper      et      al.,      2023).\\nFor      instance,      both      domains  \\n \\n  commonly      employ      PPO      as   \\n \\na      standard      approach,      and      reward-weighted      training      (or      reward-weighted  \\n \\n  MLE)      often      serves      as   \\n \\na      basic      baseline      approach.\\nDespite      these      similarities,      nuanced      differences  \\n \\n  exist      mainly      because      diffusion      models      typically      operate      in   \\n \\na      continuous      input      space      rather      than  \\n \\n  a      discrete      one.\\nMoreover,      techniques      like      value-weighted      sampling      emerge      uniquely      within      the  \\n \\n  context      of      diffusion      models.</p><p>12      Summary</p><p> \\n \\n  In      this      article,      we      comprehensively      explain      how      fine-tuning      diffusion      models      to      maximize      down-  \\n \\n  stream      reward      functions      can      be      formalized      as   \\n \\na      reinforcement      learning      (RL)      problem      in      entropy-  \\n \\n  regularized      Markov      decision      processes      (MDPs).\\nBased      on      this      viewpoint,      we      elaborate      on      the  \\n \\n  application      of      various      RL      algorithms,      such      as      PPO      and      reward-weighted      MLE,      specifically      tailored  \\n \\n  for      fine-tuning      diffusion      models.\\nAdditionally,      we      categorize      different      scenarios      based      on      how  \\n \\n  reward      feedback      is      obtained.\\nWhile      this      categorization      is      not      always      explicitly      mentioned      in      many  \\n \\n  existing      works,      it      is      crucial      when      selecting      appropriate      algorithms.\\nFinally,      we      discuss      the      rela-  \\n \\n  tionships      with      several      related      topics,      including      classifier      guidance,      Gflownets,      path      integral      control  \\n \\n  theory,      and      sampling      from      unnormalized      distributions.</p><h4>References</h4><p>Agarwal,      A.,      N.      Jiang,      S.      M.      Kakade,      and      W.      Sun      (2019).\\nReinforcement      learning:      Theory      and  \\n \\n  algorithms.\\nCS      Dept.,      UW      Seattle,      Seattle,      WA,      USA,      Tech.\\nRep,      10-4.</p><p>Agarwal,      V.      and      D.      R.      Kelley      (2022).\\nThe      genetic      and      biochemical      determinants      of      mrna      degradation  \\n \\n  rates      in      mammals.\\nGenome      biology      23(1),      245.</p><p> \\n \\n  Ajay,      A.,      Y.      Du,      A.      Gupta,      J.      B.      Tenenbaum,      T.      S.      Jaakkola,      and      P.      Agrawal      (2023).\\nIs      conditional  \\n \\n  generative      modeling      all      you      need      for      decision      making?\\nIn      The      Eleventh      International      Conference  \\n \\n  on      Learning      Representations.</p><p>Albergo,      M.      S.      and      E.      Vanden-Eijnden      (2022).\\nBuilding      normalizing      flows      with      stochastic      inter-  \\n \\n  polants.\\narXiv      preprint      arXiv:2209.15571.</p><p>Anderson,      B.      D.      (1982).\\nReverse-time      diffusion      equation      models.\\nStochastic      Processes      and      their  \\n \\n  Applications      12(3),      313-326.</p><p> \\n \\n  Austin,      J.,      D.      D.      Johnson,      J.      Ho,      D.      Tarlow,      and      R.      Van      Den      Berg      (2021).\\nStructured      denoising  \\n \\n  diffusion      models      in      discrete      state-spaces.\\nAdvances      in      Neural      Information      Processing      Systems      34,  \\n \\n  17981-17993.</p><p>Avdeyey,      P.,      C.      Shi,      Y.      Tan,      K.      Dudnyk,      and      J.      Zhou      (2023).\\nDirichlet      diffusion      score      model      for  \\n \\n  biological      sequence      generation.\\narXiv      preprint      arXiv:2305.\\n10699.</p><p>Bai,      Y.,      S.      Kadavath,      S.      Kundu,      A.      Askell,      J.      Kernion,      A.      Jones,      A.      Chen,      A.      Goldie,      A.      Mirhoseini, C.      McKinnon,      et      al.\\n(2022).\\nConstitutional      ai:      Harmlessness      from      ai      feedback.\\narXiv      preprint  \\n \\n  arXiv:2212.08073.</p><p> \\n \\n  Bansal,      A.,      H.-M.\\nChu,      A.      Schwarzschild,      S.      Sengupta,      M.      Goldblum,      J.      Geiping,      and      T.      Goldstein  \\n \\n  (2023).\\nUniversal      guidance      for      diffusion      models.\\nIn      Proceedings      of      the      IEEE/CVF      Conference  \\n \\n  on      Computer      Vision      and      Pattern      Recognition,      pp.\\n843-852.</p><p>Bengio,      Y.,      S.      Lahlou,      T.      Deleu,      E.      J.      Hu,      M.      Tiwari,      and      E.      Bengio      (2023).\\nGflownet      foundations.\\n \\n \\n  Journal      of      Machine      Learning      Research      24(210),      1-55.</p><p> \\n \\n  Benton,      J.,      Y.      Shi,      V.      De      Bortoli,      G.      Deligiannidis,      and      A.      Doucet      (2024).\\nFrom      denoising      diffusions  \\n \\n  to      denoising      markov      models.\\nJournal      of      the      Royal      Statistical      Society      Series      B:      Statistical  \\n \\n  Methodology      86(2),      286-301.</p><p>Bernton,      E.,      J.      Heng,      A.      Doucet,      and      P.      E.      Jacob      (2019).\\nSchr\\\\\\\\\\\\\\\\””      odinger      bridge      samplers.\\narXiv  \\n \\n  preprint      arXiv:      1912.13170.</p><p>Besag,      J.\\n(1994).\\nComments      on      “representations      of      knowledge      in      complex      systems”      by      u.      grenander  \\n \\n  and      mi      miller.\\nJ.      Roy.\\nStatist.\\nSoc.      Ser.\\n \\n \\nB      56(591-592),      4.</p><p>Black,      K.,      M.      Janner,      Y.      Du,      I.      Kostrikov,      and      S.      Levine      (2023).\\nTraining      diffusion      models      with  \\n \\n  reinforcement      learning.\\narXiv      preprint      arXiv:2305.13301.</p><p> \\n \\n  Campbell,      A.,      J.      Benton,      V.      De      Bortoli,      T.      Rainforth,      G.      Deligiannidis,      and      A.      Doucet      (2022).\\n \\n \\nA       continuous      time      framework      for      discrete      denoising      models.\\nAdvances      in      Neural      Information  \\n \\n  Processing      Systems      35,      28266-28279.</p><p> \\n \\n  Campbell,      A.,      J.      Yim,      R.      Barzilay,      T.      Rainforth,      and      T.      Jaakkola      (2024).\\nGenerative      flows      on  \\n \\n  discrete      state-spaces:      Enabling      multimodal      flows      with      applications      to      protein      co-design.\\narXiv  \\n \\n  preprint      arXiv:2402.04997.</p><p>Cao,      H.,      C.      Tan,      Z.      Gao,      Y.      Xu,      G.      Chen,      P.-A.\\nHeng,      and      S.      Z.      Li      (2024).\\n \\n \\nA      survey      on      generative  \\n \\n  diffusion      models.\\n[EEE      Transactions      on      Knowledge      and      Data      Engineering.</p><p>Casper,      S.,      X.      Davies,      C.      Shi,      T.      K.      Gilbert,      J.      Scheurer,      J.      Rando,      R.      Freedman,      T.      Korbak, D.      Lindner,   \\n \\nP P.      Freire,      et      al.\\n(2023).\\nOpen      problems      and      fundamental      limitations      of      reinforcement  \\n \\n  learning      from      human      feedback.\\narXiv      preprint      arXiv:2307.15217.</p><p>Castillo-Hair,      S.      M.      and      G.      Seelig      (2021).\\nMachine      learning      for      designing      next-generation      mrna  \\n \\n  therapeutics.\\nAccounts      of      Chemical      Research      55(1),      24-34.</p><p>Chang,      J.      D.,      M.      Uehara,      D.      Sreenivas,      R.      Kidambi,      and      W.      Sun      (2021).\\nMitigating      covariate      shift  \\n \\n  in      imitation      learning      via      offline      data      without      great      coverage.\\narXiv      preprint      arXiv:2106.03207.</p><p>Chen,      M.,      S.      Mei,      J.      Fan,      and      M.      Wang      (2024).\\nAn      overview      of      diffusion      models:      Applications,  \\n \\n  guided      generation,      statistical      rates      and      optimization.\\narXiv      preprint      arXiv:2404.07771.</p><p>Chen,      R.      T.,      Y.      Rubanova,      J.      Bettencourt,      and      D.      K.      Duvenaud      (2018).\\nNeural      ordinary      differential  \\n \\n  equations.\\nAdvances      in      neural      information      processing      systems      31.</p><p>Chung,      H.,      J.      Kim,      M.      T.      Mccann,      M.      L.      Klasky,      and      J.      C.      Ye      (2022).\\nDiffusion      posterior      sampling  \\n \\n  for      general      noisy      inverse      problems.\\narXiv      preprint      arXiv:2209.\\n14687.</p><p> \\n \\n  Chung,      H.,      B.      Sim,      D.      Ryu,      and      J.      C.      Ye      (2022).\\nImproving      diffusion      models      for      inverse      problems  \\n \\n  using      manifold      constraints.\\nAdvances      in      Neural      Information      Processing      Systems      35,      25683-—  \\n \\n  25696.</p><p>Clark,      K.,      P.      Vicol,      K.      Swersky,      and      D.      J.      Fleet      (2023).\\nDirectly      fine-tuning      diffusion      models      on  \\n \\n  differentiable      rewards.\\narXiv      preprint      arXiv:2309.\\n17400.</p><p> \\n \\n  De      Bortoli,      V.,      E.      Mathieu,      M.      Hutchinson,      J.      Thornton,      Y.      W.      Teh,      and      A.      Doucet      (2022).\\nRieman-  \\n \\n  nian      score-based      generative      modelling.\\nAdvances      in      Neural      Information      Processing      Systems      35,  \\n \\n  2406-2422.</p><p>Deleu,      T.,      P.      Nouri,      N.      Malkin,      D.      Precup,      and      Y.      Bengio      (2024).\\nDiscrete      probabilistic      inference      as  \\n \\n  control      in      multi-path      environments.\\narXiv      preprint      arXiv:2402.10309.</p><p>Dhariwal,      P.      and      A.      Nichol      (2021).\\nDiffusion      models      beat      gans      on      image      synthesis.\\nAdvances      in  \\n \\n  neural      information      processing      systems      34,      8780-8794.</p><p> \\n \\n  Du,      Y.,      S.      Yang,      B.      Dai,      H.      Dai,      O.      Nachum,      J.      Tenenbaum,      D.      Schuurmans,      and      P.      Abbeel      (2024).\\n \\n \\n  Learning      universal      policies      via      text-guided      video      generation.\\nAdvances      in      Neural      Information  \\n \\n  Processing      Systems      36.</p><p>Fan,      Y.,      O.      Watkins,      Y.      Du,      H.      Liu,      M.      Ryu,      C.      Boutilier,      P.      Abbeel,      M.      Ghavamzadeh,      K.      Lee,      and K.      Lee      (2023) 023).\\nDPOK:      Reinforcement      learning      for      fine-tuning      text-to-image      diffusion      models.\\n \\n \\n  arXiv      preprint      arXiv:2305.16381.</p><p>Finn,      C.,      S.      Levine,      and      P.      Abbeel      (2016).\\nGuided      cost      learning:      Deep      inverse      optimal      control      via  \\n \\n  policy      optimization.\\nIn      International      conference      on      machine      learning,      pp.\\n49-58.\\nPMLR.</p><p>Fox,      R.,      A.      Pakman,      and      N.      Tishby      (2015).\\nTaming      the      noise      in      reinforcement      learning      via      soft  \\n \\n  updates.\\narXiv      preprint      arXiv:      1512.08562.</p><p>Frey,      N.      C.,      D.      Berenberg,      K.      Zadorozhny,      J.      Kleinhenz,      J.      Lafrance-Vanasse,      I.      Hotzel,      Y.      Wu, S.      Ra,   \\n \\nR R.      Bonneau,      K.      Cho,      et      al.\\n(2023).\\nProtein      discovery      with      discrete      walk-jump      sampling.\\n \\n \\n  arXiv      preprint      arXiv:2306.\\n12360.</p><p>Geist,      M.,      B.      Scherrer,      and      O.      Pietquin      (2019).\\n \\n \\nA      theory      of      regularized      markov      decision      processes.\\n \\n \\n  In      International      Conference      on      Machine      Learning,      pp.\\n2160-2169.\\nPMLR.</p><p> \\n \\n  Girolami,      M.      and      B.      Calderhead      (2011).\\nRiemann      manifold      langevin      and      hamiltonian      monte  \\n \\n  carlo      methods.\\nJournal      of      the      Royal      Statistical      Society      Series      B:      Statistical      Methodology      73(2),  \\n \\n  123-214.</p><p>Gosai,      S.      J.,      R.      I.      Castro,      N.      Fuentes,      J.      C.      Butts,      S.      Kales,      R.      R.      Noche,      K.      Mouri,      P.      C.      Sabeti, S.   \\n \\nK K.      Reilly,      and      R.      Tewhey      (2023).\\nMachine-guided      design      of      synthetic      cell      type-specific  \\n \\n  cis-regulatory      elements.\\nbioRxiv.</p><p>Haarnoja,      T.,      H.      Tang,      P.      Abbeel,      and      S.      Levine      (2017).\\nReinforcement      learning      with      deep      energy-  \\n \\n  based      policies.\\nIn      International      conference      on      machine      learning,      pp.\\n1352-1361.\\nPMLR.</p><p>Hansen-Estruch,      P.,      I.      Kostrikov,      M.      Janner,      J.      G.      Kuba,      and      S.      Levine      (2023).\\nIdql:      Implicit  \\n \\n  q-learning      as      an      actor-critic      method      with      diffusion      policies.\\narXiv      preprint      arXiv:2304.10573.</p><p>Heng,      J.,      A.      N.      Bishop,      G.      Deligiannidis,      and      A.      Doucet      (2020).\\nControlled      sequential      monte      carlo.</p><p>Ho,      J.,      A.      Jain,      and      P.      Abbeel      (2020).\\nDenoising      diffusion      probabilistic      models.\\nAdvances      in      neural  \\n \\n  information      processing      systems      33,      6840-6851.</p><p>Ho,      J.      and      T.      Salimans      (2022).\\nClassifier-free      diffusion      guidance.\\narXiv      preprint      arXiv:2207.12598.</p><p>Ho,      J.,      T.      Salimans,      A.      Gritsenko,      W.      Chan,      M.      Norouzi,      and      D.      J.      Fleet      (2022).\\nVideo      diffusion  \\n \\n  models.\\nAdvances      in      Neural      Information      Processing      Systems      35,      8633-8646.</p><p> \\n \\n  Hoogeboom,      E.,      V.      G.      Satorras,      C.      Vignac,      and      M.      Welling      (2022).\\nEquivariant      diffusion      for  \\n \\n  molecule      generation      in      3d.\\nIn      International      conference      on      machine      learning,      pp.\\n8867-8887.</p><h4>      PMLR.</h4><p>Huang,      X.,      H.      Dong,      H.      Yifan,      Y.      Ma,      and      T.      Zhang      (2023).\\nReverse      diffusion      monte      carlo.\\nIn      The  \\n \\n  Twelfth      International      Conference      on      Learning      Representations.</p><p>Janner,      M.,      Y.      Du,      J.      B.      Tenenbaum,      and      S.      Levine      (2022).\\nPlanning      with      diffusion      for      flexible  \\n \\n  behavior      synthesis.\\narXiv      preprint      arXiv:2205.09991.</p><p> \\n \\n  Jo,      J.,      S.      Lee,      and      S.      J.      Hwang      (2022).\\nScore-based      generative      modeling      of      graphs      via      the      system  \\n \\n  of      stochastic      differential      equations.\\nIn      International      Conference      on      Machine      Learning,      pp.\\n \\n \\n  10362-10383.\\nPMLR.</p><p> \\n \\n  Kazim,      M.,      J.      Hong,      M.-G.\\nKim,      and      K.-K.\\nK.      Kim      (2024).\\nRecent      advances      in      path      integral      control  \\n \\n  for      trajectory      optimization:      An      overview      in      theoretical      and      algorithmic      perspectives.\\nAnnual  \\n \\n  Reviews      in      Control      57,      100931.</p><p>Krishnamoorthy,      S.,      S.      M.      Mashkaria,      and      A.      Grover      (2023).\\nDiffusion      models      for      black-box  \\n \\n  optimization.\\narXiv      preprint      arXiv:2306.07180.</p><p>Kumar,      A.,      A.      Zhou,      G.      Tucker,      and      S.      Levine      (2020).\\nConservative      q-learning      for      offline      reinforce-  \\n \\n  ment      learning.\\nAdvances      in      Neural      Information      Processing      Systems      33,      1179-1191.</p><p>Lal,      A.,      D.      Garfield,      T.      Biancalani,      and      G.      Eraslan      (2024).\\nreglm:      Designing      realistic      regulatory  \\n \\n  dna      with      autoregressive      language      models.\\nbioRxiv,      2024-02.</p><p>Lee,      K.,      H.      Liu,      M.      Ryu,      O.      Watkins,      Y.      Du,      C.      Boutilier,      P.      Abbeel,      M.      Ghavamzadeh,      and      S.      S.      Gu  \\n \\n  (2023).\\nAligning      text-to-image      models      using      human      feedback.\\narXiv      preprint      arXiv:2302.12192.</p><p>Levine,      S.      (2018).\\nReinforcement      learning      and      control      as      probabilistic      inference:      Tutorial      and  \\n \\n  review.\\narXiv      preprint      arXiv:      1805.00909.</p><p>Li,      Z.,      Y.      Ni,      T.      A.      B.      Huygelen,      A.      Das,      G.      Xia,      G.-B.\\nStan,      and      Y.      Zhao      (2023).\\nLatent      diffusion  \\n \\n  model      for      dna      sequence      generation.\\narXiv      preprint      arXiv:2310.06150.</p><p>Lipman,      Y.,      R.      T.      Chen,      H.      Ben-Hamu,      M.      Nickel,      and      M.      Le      (2023).\\nFlow      matching      for      generative  \\n \\n  modeling.\\nJCLR      2023.</p><p>Liu,      X.,      C.      Gong,      and      Q.      Liu      (2022).\\nFlow      straight      and      fast:      Learning      to      generate      and      transfer      data  \\n \\n  with      rectified      flow.\\narXiv      preprint      arXiv:2209.03003.</p><p>Liu,      X.,      L.      Wu,      M.      Ye,      and      Q.      Liu      (2022).\\nLet      us      build      bridges:      Understanding      and      extending  \\n \\n  diffusion      generative      models.\\narXiv      preprint      arXiv:2208.14699.</p><p>Lou,      A.,      C.      Meng,      and      S.      Ermon      (2023).\\nDiscrete      diffusion      language      modeling      by      estimating      the  \\n \\n  ratios      of      the      data      distribution.\\narXiv      preprint      arXiv:2310.16834.</p><p>Malkin,      N.,      M.      Jain,      E.      Bengio,      C.      Sun,      and      Y.      Bengio      (2022).\\nTrajectory      balance:      Improved      credit  \\n \\n  assignment      in      gflownets.\\nAdvances      in      Neural      Information      Processing      Systems      35,      5955-5967.</p><p>Mohammadpour,      S.,      E.      Bengio,      E.      Frejinger,      and      P.-L.\\nBacon      (2023).\\nMaximum      entropy      gflownets  \\n \\n  with      soft      q-learning.\\narXiv      preprint      arXiv:2312.14331.</p><p>Nachum,      O.,      M.      Norouzi,      K.      Xu,      and      D.      Schuurmans      (2017).\\nBridging      the      gap      between      value      and  \\n \\n  policy      based      reinforcement      learning.\\nAdvances      in      neural      information      processing      systems      30.</p><p>Neal,      R.      M.      et      al.\\n(2011).\\nMcmce      using      hamiltonian      dynamics.\\nHandbook      of      markov      chain      monte  \\n \\n  carlo      2(11),      2.</p><p>Neu,      G.,      A.      Jonsson,      and      V.      Gémez      (2017).\\n \\n \\nA      unified      view      of      entropy-regularized      markov      decision  \\n \\n  processes.\\narXiv      preprint      arXiv:      1705.07798.</p><p>Ouyang,      L.,      J.      Wu,      X.      Jiang,      D.      Almeida,      C.      Wainwright,      P.      Mishkin,      C.      Zhang,      S.      Agarwal, K.      Slama,      A A.      Ray,      et      al.\\n(2022).\\nTraining      language      models      to      follow      instructions      with      human  \\n \\n  feedback.\\nAdvances      in      Neural      Information      Processing      Systems      35,      27730-27744.</p><p>Pattanaik,      L.      and      C.      W.      Coley      (2020).\\nMolecular      representation:      going      long      on      fingerprints.\\n \\n \\n  Chem      6(6),      1204-1207.</p><p>Peng,      X.      B.,      A.      Kumar,      G.      Zhang,      and      S.      Levine      (2019).\\nAdvantage-weighted      regression:      Simple  \\n \\n  and      scalable      off-policy      reinforcement      learning.\\narXiv      preprint      arXiv:1910.00177.</p><p>Peters,      J.,      K.      Mulling,      and      Y.      Altun      (2010).\\nRelative      entropy      policy      search.\\nIn      Proceedings      of      the  \\n \\n  AAAI      Conference      on      Artificial      Intelligence,      Volume      24,      pp.\\n1607-1612.</p><p> \\n \\n  Podell,      D.,      Z.      English,      K.      Lacey,      A.      Blattmann,      T.      Dockhorn,      J.      Miiller,      J.      Penna,      and      R.      Rombach  \\n \\n  (2023).\\nSdxl:      Improving      latent      diffusion      models      for      high-resolution      image      synthesis.\\narXiv  \\n \\n  preprint      arXiv:2307.01952.</p><p>Prabhudesai,      M.,      A.      Goyal,      D.      Pathak,      and      K.      Fragkiadaki      (2023).\\nAligning      text-to-image      diffusion  \\n \\n  models      with      reward      backpropagation.\\narXiv      preprint      arXiv:2310.03739.</p><p>Ramesh,      A.,      P.      Dhariwal,      A.      Nichol,      C.      Chu,      and      M.      Chen      (2022).\\nHierarchical      text-conditional  \\n \\n  image      generation      with      clip      latents.\\narXiv      preprint      arXiv:2204.06125      1(2),      3.</p><p>Rigter,      M.,      B.      Lacerda,      and      N.      Hawes      (2022).\\nRambo-rl:      Robust      adversarial      model-based      offline  \\n \\n  reinforcement      learning.\\nAdvances      in      neural      information      processing      systems      35,      16082-16097.</p><p>Robert,      C.      (2014).\\nStatistical      modeling      and      computation.</p><p>Rogers,      L.      C.      G.      and      D.      Williams      (2000).\\nDiffusions,      Markov      processes      and      martingales:      Volume      2,  \\n \\n  It6é      calculus,      Volume      2.\\nCambridge      university      press.</p><p> \\n \\n  Rombach,      R.,      A.      Blattmann,      D.      Lorenz,      P.      Esser,      and      B.      Ommer      (2022,      June).\\nHigh-resolution  \\n \\n  image      synthesis      with      latent      diffusion      models.\\nIn      Proceedings      of      the      IEEE/CVF      Conference      on  \\n \\n  Computer      Vision      and      Pattern      Recognition      (CVPR),      pp.\\n10684-10695.</p><p>Sarkar,      A.,      Z.      Tang,      C.      Zhao,      and      P.      Koo      (2024).\\nDesigning      dna      with      tunable      regulatory      activity  \\n \\n  using      discrete      diffusion.\\nbioRxiv,      2024-05.</p><p>Schulman,      J.,      X.      Chen,      and      P.      Abbeel      (2017).\\nEquivalence      between      policy      gradients      and      soft  \\n \\n  q-learning.\\narXiv      preprint      arXiv:      1704.06440.</p><p>Schulman,      J.,      S.      Levine,      P.      Abbeel,      M.      Jordan,      and      P.      Moritz      (2015).\\nTrust      region      policy      optimization.\\n \\n \\n  In      International      conference      on      machine      learning,      pp.\\n1889-1897.\\nPMLR.</p><p>Schulman,      J.,      F.      Wolski,      P.      Dhariwal,      A.      Radford,      and      O.      Klimov      (2017).\\nProximal      policy      optimiza-  \\n \\n  tion      algorithms.\\narXiv      preprint      arXiv:      1707.06347.</p><p>Shi,      Y.,      V.      De      Bortoli,      A.      Campbell,      and      A.      Doucet      (2024).\\nDiffusion      schrédinger      bridge      matching.\\n \\n \\n  Advances      in      Neural      Information      Processing      Systems      36.</p><p> \\n \\n  Sohl-Dickstein,      J.,      E.      Weiss,      N.      Maheswaranathan,      and      S.      Ganguli      (2015).\\nDeep      unsupervised  \\n \\n  learning      using      nonequilibrium      thermodynamics.\\nIn      /nternational      conference      on      machine      learning,  \\n \\n  pp.\\n2256-2265.\\nPMLR.</p><p>Song,      J.,      C.      Meng,      and      S.      Ermon      (2020).\\nDenoising      diffusion      implicit      models.\\narXiv      preprint  \\n \\n  arXiv:2010.02502.</p><p>Song,      Y.,      C.      Durkan,      I.      Murray,      and      S.      Ermon      (2021).\\nMaximum      likelihood      training      of      score-based  \\n \\n  diffusion      models.\\nAdvances      in      neural      information      processing      systems      34,      1415-1428.</p><p>Stark,      H.,      B.      Jing,      C.      Wang,      G.      Corso,      B.      Berger,      R.      Barzilay,      and      T.      Jaakkola      (2024).\\nDirichlet  \\n \\n  flow      matching      with      applications      to      dna      sequence      design.\\narXiv      preprint      arXiv:2402.05841.</p><p>Sutton,      R.      S.      and      A.      G.      Barto      (2018).\\nReinforcement      learning:      An      introduction.\\nMIT      press.</p><p>Tang,      W.      (2024).\\nFine-tuning      of      diffusion      models      via      stochastic      control:      entropy      regularization      and  \\n \\n  beyond.\\narXiv      preprint      arXiv:2403.06279.</p><p>Tang,      W.      and      H.      Zhao      (2024).\\nScore-based      diffusion      models      via      stochastic      differential      equations—a  \\n \\n  technical      tutorial.\\narXiv      preprint      arXiv:2402.07487.</p><p>Theodorou,      E.,      J.      Buchli,      and      S.      Schaal      (2010).\\n \\n \\nA      generalized      path      integral      control      approach      to  \\n \\n  reinforcement      learning.\\nThe      Journal      of      Machine      Learning      Research      11,      3137-3181.</p><p>Tiapkin,      D.,      N.      Morozov,      A.      Naumov,      and      D.      Vetrov      (2023).\\nGenerative      flow      networks      as      entropy-  \\n \\n  regularized      rl.\\narXiv      preprint      arXiv:2310.12934.</p><p> \\n \\n  Tong,      A.,      N.      Malkin,      G.      Huguet,      Y.      Zhang,      J.      Rector-Brooks,      K.      Fatras,      G.      Wolf,      and      Y.      Bengio  \\n \\n  (2023).\\nConditional      flow      matching:      Simulation-free      dynamic      optimal      transport.\\narXiv      preprint  \\n \\n  arXiv:2302.00482.</p><p>Uehara,      M.      and      W.      Sun      (2021).\\nPessimistic      model-based      offline      reinforcement      learning      under  \\n \\n  partial      coverage.\\narXiv      preprint      arXiv:2107.06226.</p><p>Uehara,      M.,      Y.      Zhao,      K.      Black,      E.      Hajiramezanali,      G.      Scalia,      N.      L.      Diamant,      A.      M.      Tseng, T.      Biancalani,      and      S S.      Levine      (2024).\\nFine-tuning      of      continuous-time      diffusion      models      as      entropy-  \\n \\n  regularized      control.\\narXiv      preprint      arXiv:2402.15194.</p><p> \\n \\n  Uehara,      M.,      Y.      Zhao,      K.      Black,      E.      Hajiramezanali,      G.      Scalia,      N.      L.      Diamant,      A.      M.      Tseng,      S.      Levine,  \\n \\n  and      T.      Biancalani      (2024).\\nFeedback      efficient      online      fine-tuning      of      diffusion      models.\\narXiv      preprint  \\n \\n  arXiv:2402.\\n16359.</p><p> \\n \\n  Uehara,      M.,      Y.      Zhao,      E.      Hajiramezanali,      G.      Scalia,      G.      Eraslan,      A.      Lal,      S.      Levine,      and      T.      Biancalani  \\n \\n  (2024).\\nBridging      model-based      optimization      and      generative      modeling      via      conservative      fine-tuning  \\n \\n  of      diffusion      models.\\narXiv      preprint      arXiv:2405.19673.</p><p>Vargas,      F.,      W.      Grathwohl,      and      A.      Doucet      (2023).\\nDenoising      diffusion      samplers.\\narXiv      preprint  \\n \\n  arXiv:2302.\\n13834.</p><p> \\n \\n  Wallace,      B.,      M.      Dang,      R.      Rafailov,      L.      Zhou,      A.      Lou,      S.      Purushwalkam,      S.      Ermon,      C.      Xiong,      S.      Joty,  \\n \\n  and      N.      Naik      (2023).\\nDiffusion      model      alignment      using      direct      preference      optimization.\\narXiv  \\n \\n  preprint      arXiv:2311.12908.</p><p>Wang,      Z.,      J.      J.      Hunt,      and      M.      Zhou      (2022).\\nDiffusion      policies      as      an      expressive      policy      class      for      offline  \\n \\n  reinforcement      learning.\\narXiv      preprint      arXiv:2208.06193.</p><p>Widatalla,      T.,      R.      Rafailov,      and      B.      Hie      (2024).\\nAligning      protein      generative      models      with      experimental  \\n \\n  fitness      via      direct      preference      optimization.\\nbioRxiv,      2024-05.</p><p>Williams,      G.,      A.      Aldrich,      and      E.      A.      Theodorou      (2017).\\nModel      predictive      path      integral      control:      From  \\n \\n  theory      to      parallel      computation.\\nJournal      of      Guidance,      Control,      and      Dynamics      40(2),      344-357.</p><p>Wu,      X.,      K.      Sun,      F.      Zhu,      R.      Zhao,      and      H.      Li      (2023).\\nBetter      aligning      text-to-image      models      with  \\n \\n  human      preference.\\narXiv      preprint      arXiv:2303.14420.</p><p>Wu,      Y.,      G.      Tucker,      and      O.      Nachum      (2019).\\nBehavior      regularized      offline      reinforcement      learning.\\n \\n \\n  arXiv      preprint      arXiv:      1911.11361.</p><p>Wulfmeier,      M.,      P.      Ondruska,      and      I.      Posner      (2015).\\nMaximum      entropy      deep      inverse      reinforcement  \\n \\n  learning.\\narXiv      preprint      arXiv:      1507.04888.</p><p>Xie,      T.,      C.-A.\\nCheng,      N.      Jiang,      P.      Mineiro,      and      A.      Agarwal      (2021).\\nBellman-consistent      pessimism  \\n \\n  for      offline      reinforcement      learning.\\nAdvances      in      neural      information      processing      systems      34.</p><p>Xu,      M.,      L.      Yu,      Y.      Song,      C.      Shi,      S.      Ermon,      and      J.      Tang      (2022).\\nGeodiff:   \\n \\nA      geometric      diffusion  \\n \\n  model      for      molecular      conformation      generation.\\narXiv      preprint      arXiv:2203.02923.</p><p> \\n \\n  Yang,      L.,      Z.      Zhang,      Y.      Song,      S.      Hong,      R.      Xu,      Y.      Zhao,      W.      Zhang,      B.      Cui,      and      M.-H.\\nYang      (2023).\\n \\n \\n  Diffusion      models:   \\n \\nA      comprehensive      survey      of      methods      and      applications.\\nACM      Computing  \\n \\n  Surveys      56(4),      1-39.</p><p> \\n \\n  Yu,      T.,      G.      Thomas,      L.      Yu,      S.      Ermon,      J.      Y.      Zou,      S.      Levine,      C.      Finn,      and      T.      Ma      (2020).\\nMopo:  \\n \\n  Model-based      offline      policy      optimization.\\nAdvances      in      Neural      Information      Processing      Systems      33,  \\n \\n  14129-14142.</p><p> \\n \\n  Yuan,      H.,      K.      Huang,      C.      Ni,      M.      Chen,      and      M.      Wang      (2023).\\nReward-directed      conditional      diffusion:  \\n \\n  Provable      distribution      estimation      and      reward      improvement.\\nIn      Thirty-seventh      Conference      on  \\n \\n  Neural      Information      Processing      Systems.</p><p>Zhang,      H.      and      T.      Xu      (2023).\\nTowards      controllable      diffusion      models      via      reward-guided      exploration.\\n \\n \\n  arXiv      preprint      arXiv:2304.07132.</p><p>Zhang,      Q.      and      Y.      Chen      (2021).\\nPath      integral      sampler:   \\n \\na      stochastic      control      approach      for      sampling.\\n \\n \\n  arXiv      preprint      arXiv:2111,15141.</p><p> \\n \\n  Zhao,      Y.,      M.      Uehara,      G.      Scalia,      T.      Biancalani,      S.      Levine,      and      E.      Hajiramezanali      (2024).\\n \\n \\n  Adding      conditional      control      to      diffusion      models      with      reinforcement      learning.\\narXiv      preprint  \\n \\n  arXiv:2406.\\n12120.</p><p>Zhou,      Z.,      S.      Kearnes,      L.      Li,      R.      N.      Zare,      and      P.      Riley      (2019).\\nOptimization      of      molecules      via      deep  \\n \\n  reinforcement      learning.\\nScientific      reports      9(1),      10752.</p><p>Zhu,      Z.,      H.      Zhao,      H.      He,      Y.      Zhong,      S.      Zhang,      Y.      Yu,      and      W.      Zhang      (2023).\\nDiffusion      models      for  \\n \\n  reinforcement      learning:   \\n \\nA      survey.\\narXiv      preprint      arXiv:2311.01223.</p><p>Ziebart,      B.      D.,      A.      L.      Maas,      J.      A.      Bagnell,      A.      K.      Dey,      et      al.\\n(2008).\\nMaximum      entropy      inverse  \\n \\n  reinforcement      learning.\\nIn      Aaai,      Volume      8,      pp.\\n1433-1438.\\nChicago,      IL,      USA.</p><h2>6.1.1      Relation      with      Loss      Functions      for      the      Original      Training      Objective</h2><p>tive In      this      subsection,      we      explore      the      connection      with      the      original      loss      function      of      pre-trained      diffusion  \\n \\n  models.\\nTo      see      that,      as      we      see      in      Section      1.1.1,      recall ol)   \\n \\n=      al   \\n \\n4      (0.50,   \\n \\n—      1/o%eg,,,      (a1,   \\n \\nT   \\n \\n—      t)](5t)      +e?\\n02,      €)   \\n \\n~      N(0,1).</p><p> \\n \\n \\n/       \\\\\\\\\\\\\\\\-  \\n \\n  (al?\\nt;Opre) Then,      the      loss      function      (24)      in      reward-weighted      MLE      reduces      to</p><table><th><td colSpan=1>      =      r(x”)       A,      —      Vo      S>      Ss\"      exp      |      ~~      —       t=T+1      i=1</td><td colSpan=1>      .      _      ;      2       (i,t)      (ay,      Tt;      Opre)      _      eal,      T      -t;      0)       ey      _       {or}?</td><td colSpan=1>|o=0</td></th><tr><td colSpan=1></td><td colSpan=1>      2</td><td colSpan=1>      (26)</td></tr></table><p>This      objective      function      closely      resembles      the      reward-weighted      version      of      the      loss      function      (5)      used  \\n \\n  for      training      pre-trained      diffusion      models.</p><h3>6.2      Value-Weighted      Sampling</h3><p>ling  \\n \\n  Thus      far,      we      have      discussed      methods      for      fine-tuning      pre-trained      diffusion      models.\\nNow,      let’s      delve  \\n \\n  into      an      alternative      approach      during      inference      that      aims      to      sample      from      the      target      distribution      p,.\\n \\n \\n  without      explicitly      fine-tuning      the      diffusion      models.\\nIn      essence,      this      approach      involves      incorporating  \\n \\n  gradients      of      value      functions      during      inference      alongside      the      denoising      process      in      pre-trained      diffusion  \\n \\n  models.\\nHence,      we      refer      to      this      approach      as      value-weighted      sampling.\\nWhile      it      seems      that      this  \\n \\n  method      has      not      been      explicitly      formalized      in      previous      literature,      the      value-weighted      sampling  \\n \\n  closely      connects      with      classifier      guidance,      as      discussed      in      Section      8.1.  \\n \\n  Before      delving      into      the      algorithmic      details,      we      outline      the      motivation.\\nSubsequently,      we      present  \\n \\n  the      concrete      algorithm      and      discuss      its      advantages—specifically,      its      capability      to      operate      without      the  \\n \\n  necessity      of      fine-tuning      diffusion      models—and      its      disadvantage,      which      involves      the      need      to      obtain  \\n \\n  differentiable      value      functions.</p><p>Algorithm   \\n \\n4      Value-weighted      sampling 1:      Require:      Pre-trained      model      {p?\\n\"*(2,_1|r1)      }:   \\n \\n=      {N(0(      22,      t;      Opre),      07(t))      be.\\n \\n \\n  2:      Estimate      v   \\n \\n:   \\n \\n¥   \\n \\nx      [0,7]   \\n \\n—      R      and      denote      it      by      i(.,      -) ¢\\n \\n   (1)      Monte-Carlo      approach      in      (28) e      (2)      Value      iteration      approach      (Soft      Q-learning)      in      (29)      in      Section      6.2.1 ¢\\n \\n   (3)      Approximation      using      Tweedie’s      formula      in      Section      6.2.2 3:      fort   \\n \\n€      [7      +1,---   \\n \\n,      1}      do  \\n \\n  4:      Set.\\na?\\n \\n \\nt      Ve0   \\n \\nx      xL=Lt  \\n \\n  P(t,      t)   \\n \\n=   \\n \\n(   \\n \\n)      \\\\\\\\\\\\\\\\   \\n \\nu   \\n \\n+      p(xz,t      Are)</p><p>5:      end      for  \\n \\n  6:      Output:      {N(p      O(      Le,      t),o7(t))      ery</p><p> \\n \\n  Motivation.\\nConsidering   \\n \\na      Gaussian      policy      7:1   \\n \\n~      N(/(2:,t),07(t)),      we      aim      to      determine  \\n \\n  P(x4,t;      0)      such      that      N(A(2x;,      t;      0),      0?(t))      closely      approximates      p*(-|x,).\\nHere,      typically,      we      have  \\n \\n  p(xz,t;0)   \\n \\n=      a,   \\n \\n+      (t)g(a;,t)      and      o?(t)   \\n \\n=      g(t)(ot)      for      certain      function   \\n \\ng   \\n \\n:   \\n \\nY   \\n \\nx      [0,7]   \\n \\n>      R¢  \\n \\n  and   \\n \\ng   \\n \\n:      [0,7]   \\n \\n—      R,      as      we      have      explained      in      Section      1.1.1.\\nThen,      using   \\n \\nx      as      equality      up      to      the  \\n \\n  normalizing      constant, Piles      |)      exp(eralar-s)/a)      exp      (BEE      OI)</p><p>2\\n \\n   exp(vy(am)   \\n \\n+      Ver(      ee)»      {a1   \\n \\n—      3)      exp      (—      “2      Clr      uF)</p><p>_\\n \\n   lve1   \\n \\n—      ae   \\n \\n—      (58)      9(t)      Vora)      /o   \\n \\n—      (6t)9(ae,      t))      |?\\n \\n \\n  esp      (-      0.59(#)      (6t)      )</p><h4>Hence,      we      can      approximate:</h4><p> \\n \\n  o°(t)      Vu;      (a4)  \\n \\n  a   \\n \\n+      p(      xz,      ti      Ore):      (27)  \\n \\n  P(x,      t)      od</p><p>Remark      5.\\nNote      while      the      above      derivation      is      heuristic,      it      is      formalized      in      Uehara      et      al.\\n(2024,  \\n \\n  Lemma      2)      in      the      continuous-time      formulation.</p><p> \\n \\n  Algorithm.\\nUtilizing      (27),      the      entire      algorithm      of      value-weighted      sampling      is      outlined      in      Algo-  \\n \\n  rithm      4.\\nThis      method      doesn’t      involve      updating      parameters      in   \\n \\na      diffusion      model;      hence,      it’s      not   \\n \\na       fine-tuning      method.</p><p> \\n \\n  Note      that      in      this      algorithm,      we      require   \\n \\na      form      of      v;(-)      to      compute      gradients.\\nThis      value      function  \\n \\n  can      be      estimated      through      regression      using      the      characterization      described      in      Lemma      2.\\nHere,      inspired  \\n \\n  by      Lemma      2,      we      use      the      following      loss      function:</p><p>1\\n \\n   om      (i,t)      (i,t)   \\n \\n°       o(2)   \\n \\n=      argmin      S-      S-      {es      (me)   \\n \\n—      exp   \\n \\n|   \\n \\n,      (28)</p><p>h:[R4,{0,T]]-R      t=T+1      i=1</p><p>where      the      data      is      collected      in      an      off-policy      way.\\nLater,      we      explain      two      alternative      approaches      in  \\n \\n  Section      6.2.1      and      Section      6.2.2.</p><p> \\n \\n  Advantages      and      potential      disadvantages.\\nThe      value-weighted      sampling      is      straightforward      and  \\n \\n  less      memory-intensive      since      it      avoids      fine-tuning.\\nTherefore,      it      presents      an      appealing      simple      option  \\n \\n  if      it      performs      well.\\nIndeed,      in      various      inverse      problems,      such      as      inpainting,      super-resolution,      and  \\n \\n  colorization,      setting      r()      as   \\n \\na      likelihood      of      the      measurement      model   \\n \\ny   \\n \\n=      g(a)      +e      (where   \\n \\ny      represents  \\n \\n  actual      measurements,   \\n \\n€      denotes      noise,      and   \\n \\ng      defines      the      measurement      function)      has      proven      highly  \\n \\n  successful      (Chung      et      al.,      2022;      Bansal      et      al.,      2023;      Chung      et      al.,      2022).</p><p> \\n \\n  The      potential      drawback      compared      to      fine-tuning      algorithms      so      far      (i.e.,      PPO      and      reward-  \\n \\n  weighted      MLE)      is      the      necessity      to      learn      differetiable      soft      value      functions      like      direct      reward      back-  \\n \\n  propagation.\\nThis      learning      process      is      often      not      straightforward      as      explained      in      Section      5.2.\\nThe  \\n \\n  previously      discussed      fine-tuning      algorithms      (i.e.,      PPO,      reward-weighted      MLE)      circumvent      this  \\n \\n  requirement      by      obtaining      rewards      to      go      via      Monte      Carlo      approaches,      thereby      avoiding      the      need      for  \\n \\n  soft      value      functions.</p><h3>6.2      Value-Weighted      Sampling</h3><p>ling  \\n \\n  Thus      far,      we      have      discussed      methods      for      fine-tuning      pre-trained      diffusion      models.\\nNow,      let’s      delve  \\n \\n  into      an      alternative      approach      during      inference      that      aims      to      sample      from      the      target      distribution      p,.\\n \\n \\n  without      explicitly      fine-tuning      the      diffusion      models.\\nIn      essence,      this      approach      involves      incorporating  \\n \\n  gradients      of      value      functions      during      inference      alongside      the      denoising      process      in      pre-trained      diffusion  \\n \\n  models.\\nHence,      we      refer      to      this      approach      as      value-weighted      sampling.\\nWhile      it      seems      that      this  \\n \\n  method      has      not      been      explicitly      formalized      in      previous      literature,      the      value-weighted      sampling  \\n \\n  closely      connects      with      classifier      guidance,      as      discussed      in      Section      8.1.  \\n \\n  Before      delving      into      the      algorithmic      details,      we      outline      the      motivation.\\nSubsequently,      we      present  \\n \\n  the      concrete      algorithm      and      discuss      its      advantages—specifically,      its      capability      to      operate      without      the  \\n \\n  necessity      of      fine-tuning      diffusion      models—and      its      disadvantage,      which      involves      the      need      to      obtain  \\n \\n  differentiable      value      functions.</p><p>Algorithm   \\n \\n4      Value-weighted      sampling 1:      Require:      Pre-trained      model      {p?\\n\"*(2,_1|r1)      }:   \\n \\n=      {N(0(      22,      t;      Opre),      07(t))      be.\\n \\n \\n  2:      Estimate      v   \\n \\n:   \\n \\n¥   \\n \\nx      [0,7]   \\n \\n—      R      and      denote      it      by      i(.,      -) ¢\\n \\n   (1)      Monte-Carlo      approach      in      (28) e      (2)      Value      iteration      approach      (Soft      Q-learning)      in      (29)      in      Section      6.2.1 ¢\\n \\n   (3)      Approximation      using      Tweedie’s      formula      in      Section      6.2.2 3:      fort   \\n \\n€      [7      +1,---   \\n \\n,      1}      do  \\n \\n  4:      Set.\\na?\\n \\n \\nt      Ve0   \\n \\nx      xL=Lt  \\n \\n  P(t,      t)   \\n \\n=   \\n \\n(   \\n \\n)      \\\\\\\\\\\\\\\\   \\n \\nu   \\n \\n+      p(xz,t      Are)</p><p>5:      end      for  \\n \\n  6:      Output:      {N(p      O(      Le,      t),o7(t))      ery</p><p> \\n \\n  Motivation.\\nConsidering   \\n \\na      Gaussian      policy      7:1   \\n \\n~      N(/(2:,t),07(t)),      we      aim      to      determine  \\n \\n  P(x4,t;      0)      such      that      N(A(2x;,      t;      0),      0?(t))      closely      approximates      p*(-|x,).\\nHere,      typically,      we      have  \\n \\n  p(xz,t;0)   \\n \\n=      a,   \\n \\n+      (t)g(a;,t)      and      o?(t)   \\n \\n=      g(t)(ot)      for      certain      function   \\n \\ng   \\n \\n:   \\n \\nY   \\n \\nx      [0,7]   \\n \\n>      R¢  \\n \\n  and   \\n \\ng   \\n \\n:      [0,7]   \\n \\n—      R,      as      we      have      explained      in      Section      1.1.1.\\nThen,      using   \\n \\nx      as      equality      up      to      the  \\n \\n  normalizing      constant, Piles      |)      exp(eralar-s)/a)      exp      (BEE      OI)</p><p>2\\n \\n   exp(vy(am)   \\n \\n+      Ver(      ee)»      {a1   \\n \\n—      3)      exp      (—      “2      Clr      uF)</p><p>_\\n \\n   lve1   \\n \\n—      ae   \\n \\n—      (58)      9(t)      Vora)      /o   \\n \\n—      (6t)9(ae,      t))      |?\\n \\n \\n  esp      (-      0.59(#)      (6t)      )</p><h4>Hence,      we      can      approximate:</h4><p> \\n \\n  o°(t)      Vu;      (a4)  \\n \\n  a   \\n \\n+      p(      xz,      ti      Ore):      (27)  \\n \\n  P(x,      t)      od</p><p>Remark      5.\\nNote      while      the      above      derivation      is      heuristic,      it      is      formalized      in      Uehara      et      al.\\n(2024,  \\n \\n  Lemma      2)      in      the      continuous-time      formulation.</p><p> \\n \\n  Algorithm.\\nUtilizing      (27),      the      entire      algorithm      of      value-weighted      sampling      is      outlined      in      Algo-  \\n \\n  rithm      4.\\nThis      method      doesn’t      involve      updating      parameters      in   \\n \\na      diffusion      model;      hence,      it’s      not   \\n \\na       fine-tuning      method.</p><p> \\n \\n  Note      that      in      this      algorithm,      we      require   \\n \\na      form      of      v;(-)      to      compute      gradients.\\nThis      value      function  \\n \\n  can      be      estimated      through      regression      using      the      characterization      described      in      Lemma      2.\\nHere,      inspired  \\n \\n  by      Lemma      2,      we      use      the      following      loss      function:</p><p>1\\n \\n   om      (i,t)      (i,t)   \\n \\n°       o(2)   \\n \\n=      argmin      S-      S-      {es      (me)   \\n \\n—      exp   \\n \\n|   \\n \\n,      (28)</p><p>h:[R4,{0,T]]-R      t=T+1      i=1</p><p>where      the      data      is      collected      in      an      off-policy      way.\\nLater,      we      explain      two      alternative      approaches      in  \\n \\n  Section      6.2.1      and      Section      6.2.2.</p><p> \\n \\n  Advantages      and      potential      disadvantages.\\nThe      value-weighted      sampling      is      straightforward      and  \\n \\n  less      memory-intensive      since      it      avoids      fine-tuning.\\nTherefore,      it      presents      an      appealing      simple      option  \\n \\n  if      it      performs      well.\\nIndeed,      in      various      inverse      problems,      such      as      inpainting,      super-resolution,      and  \\n \\n  colorization,      setting      r()      as   \\n \\na      likelihood      of      the      measurement      model   \\n \\ny   \\n \\n=      g(a)      +e      (where   \\n \\ny      represents  \\n \\n  actual      measurements,   \\n \\n€      denotes      noise,      and   \\n \\ng      defines      the      measurement      function)      has      proven      highly  \\n \\n  successful      (Chung      et      al.,      2022;      Bansal      et      al.,      2023;      Chung      et      al.,      2022).</p><p> \\n \\n  The      potential      drawback      compared      to      fine-tuning      algorithms      so      far      (i.e.,      PPO      and      reward-  \\n \\n  weighted      MLE)      is      the      necessity      to      learn      differetiable      soft      value      functions      like      direct      reward      back-  \\n \\n  propagation.\\nThis      learning      process      is      often      not      straightforward      as      explained      in      Section      5.2.\\nThe  \\n \\n  previously      discussed      fine-tuning      algorithms      (i.e.,      PPO,      reward-weighted      MLE)      circumvent      this  \\n \\n  requirement      by      obtaining      rewards      to      go      via      Monte      Carlo      approaches,      thereby      avoiding      the      need      for  \\n \\n  soft      value      functions.</p><h4>Hence,      we      can      approximate:</h4><p> \\n \\n  o°(t)      Vu;      (a4)  \\n \\n  a   \\n \\n+      p(      xz,      ti      Ore):      (27)  \\n \\n  P(x,      t)      od</p><p>Remark      5.\\nNote      while      the      above      derivation      is      heuristic,      it      is      formalized      in      Uehara      et      al.\\n(2024,  \\n \\n  Lemma      2)      in      the      continuous-time      formulation.</p><p> \\n \\n  Algorithm.\\nUtilizing      (27),      the      entire      algorithm      of      value-weighted      sampling      is      outlined      in      Algo-  \\n \\n  rithm      4.\\nThis      method      doesn’t      involve      updating      parameters      in   \\n \\na      diffusion      model;      hence,      it’s      not   \\n \\na       fine-tuning      method.</p><p> \\n \\n  Note      that      in      this      algorithm,      we      require   \\n \\na      form      of      v;(-)      to      compute      gradients.\\nThis      value      function  \\n \\n  can      be      estimated      through      regression      using      the      characterization      described      in      Lemma      2.\\nHere,      inspired  \\n \\n  by      Lemma      2,      we      use      the      following      loss      function:</p><p>1\\n \\n   om      (i,t)      (i,t)   \\n \\n°       o(2)   \\n \\n=      argmin      S-      S-      {es      (me)   \\n \\n—      exp   \\n \\n|   \\n \\n,      (28)</p><p>h:[R4,{0,T]]-R      t=T+1      i=1</p><p>where      the      data      is      collected      in      an      off-policy      way.\\nLater,      we      explain      two      alternative      approaches      in  \\n \\n  Section      6.2.1      and      Section      6.2.2.</p><p> \\n \\n  Advantages      and      potential      disadvantages.\\nThe      value-weighted      sampling      is      straightforward      and  \\n \\n  less      memory-intensive      since      it      avoids      fine-tuning.\\nTherefore,      it      presents      an      appealing      simple      option  \\n \\n  if      it      performs      well.\\nIndeed,      in      various      inverse      problems,      such      as      inpainting,      super-resolution,      and  \\n \\n  colorization,      setting      r()      as   \\n \\na      likelihood      of      the      measurement      model   \\n \\ny   \\n \\n=      g(a)      +e      (where   \\n \\ny      represents  \\n \\n  actual      measurements,   \\n \\n€      denotes      noise,      and   \\n \\ng      defines      the      measurement      function)      has      proven      highly  \\n \\n  successful      (Chung      et      al.,      2022;      Bansal      et      al.,      2023;      Chung      et      al.,      2022).</p><p> \\n \\n  The      potential      drawback      compared      to      fine-tuning      algorithms      so      far      (i.e.,      PPO      and      reward-  \\n \\n  weighted      MLE)      is      the      necessity      to      learn      differetiable      soft      value      functions      like      direct      reward      back-  \\n \\n  propagation.\\nThis      learning      process      is      often      not      straightforward      as      explained      in      Section      5.2.\\nThe  \\n \\n  previously      discussed      fine-tuning      algorithms      (i.e.,      PPO,      reward-weighted      MLE)      circumvent      this  \\n \\n  requirement      by      obtaining      rewards      to      go      via      Monte      Carlo      approaches,      thereby      avoiding      the      need      for  \\n \\n  soft      value      functions.</p><h2>6.2.1      Soft      Q-learning</h2><p>ning  \\n \\n  We      have      elucidated      that      leveraging      Lemma      2,      we      can      estimate      soft      value      functions      v;(-)      based  \\n \\n  on      Equation      (28)      in   \\n \\na      Monte      Carlo      way.\\nSubsequently,      these      soft      value      functions      are      used      in  \\n \\n  value-weighted      sampling      to      sample      from      the      target      distribution      p,.\\nAlternatively,      there      is      another  \\n \\n  method      that      involves      using      soft      Bellman      equations      to      estimate      soft      value      functions      v;(-).\\nThis  \\n \\n  technique      is      commonly      called      soft      Q-learning      in      the      context      of      standard      RL.</p><p> \\n \\n  First,      recalling      the      soft      Bellman      equations      in      (16),      we      have en      (22)   \\n \\n=      fey      (=)      pa      nd</p><p>Taking      the      logarithm,      we      obtain u(ay)   \\n \\n=      alog      f      exp      (eaen)      De      (@4-1   \\n \\n|      @4)dxy-1.</p><p>a</p><h3>Hence,</h3><p> \\n \\n \\n_   \\n \\n:      (xt)      Up-1(Lt-1)   \\n \\n°       vu   \\n \\n=      argmin      Ey,.u,   \\n \\n|   \\n \\n4      ——   \\n \\n—      log   \\n \\n|      exp   \\n \\n|      ————   \\n \\n]      Dpre(@e-1]      21)      da4_1  \\n \\n  a   \\n \\na h:X->R  \\n \\n  where      u,   \\n \\n€      A(%)      is      any      roll-in      distribution      that      covers      the      entire      space      V.      Using      this      relation      and  \\n \\n  replacing      the      expectation      E,,,.,,,      with      empirical      approximation,      we      are      able      to      estimate      soft      value  \\n \\n  functions      v;      in   \\n \\na      recursive      manner:</p><p> \\n \\n \\n1   \\n \\nm      ~(j—1)   \\n \\n°       aj   \\n \\n;   \\n \\ni      Ur      Ut   \\n \\ny       {6      (x)},   \\n \\n—       argmin   \\n \\ny      S      {ht   \\n \\n)   \\n \\n—      log   \\n \\n/      exp      (eee)      Ppre(@1—1|}      yan] h:      [R¢,[0,7]]|-R      t=T+1      i=1</p><p>where      the      data      is      collected      in      an      off-policy      way.</p><p> \\n \\n  Remark      6.\\nAlthough      soft      Q-learning      is      widely      used      in      standard      RL      (Schulman      et      al.,      2017),      it  \\n \\n  cannot      be      directly      applied      to      our      fine-tuning      context      without      resorting      to      value-weighted      sampling  \\n \\n  or      value-weighted      MLE.\\nThis      is      because,      even      if      we      estimate      soft-value      functions      as      %;,      substituting  \\n \\n  Uz      with      0,      in      the      soft-optimal      policy      results      in      an      unnormalized      policy.<ul><li>6.2.2.\\nApproximation      using      Tweedie’s      formula</li></ul></p><p> \\n \\n  So      far,      we      have      explained      two      approaches:   \\n \\na      Monte      Carlo      approach      and   \\n \\na      value      iteration      approach      to  \\n \\n  estimate      soft      value      functions.\\nHowever,      learning      value      functions      in      (28)      can      still      be      often      challenging  \\n \\n  in      practice.\\nTherefore,      we      can      employ      approximation      strategies      inspired      by      recent      literature      on  \\n \\n  classifier      guidance      (e.g.,      reconstruction      guidance      (Ho      et      al.,      2022),      manifold      constrained      gradients  \\n \\n  (Chung      et      al.,      2022),      universal      guidance      (Bansal      et      al.,      2023),      and      diffusion      posterior      sampling  \\n \\n  (Chung      et      al.,      2022)).</p><p> \\n \\n  Specifically,      we      adopt      the      following      approximation:</p><p>ur(xz)   \\n \\n=      alog      Egpry      lex      (“)      oy   \\n \\n=      alog      (/      exp      (om)      p      (colar      (30)</p><p>~\\n \\n   alog      (exw      (ceo)   \\n \\n)   \\n \\n»       £o(t1)   \\n \\n=      Egprey|xo   \\n \\n|      xe],      (31) a\\n \\n    =\\n \\n   1r(Xo(2t)).</p><p> \\n \\n  Here,      we      replace      the      integration      in      (30)      with   \\n \\na      Dirac      delta      distribution      with      the      posterior      mean.\\n \\n \\n  Importantly,      we      can      calculate      Zo      (x;)   \\n \\n=      E,»*-[xo   \\n \\n|      x4]      using      the      pre-trained      (score-based)      diffusion  \\n \\n  model      based      on      Tweedie’s      formula:</p><table><tr><td colSpan=1>      O12</td><td colSpan=1><p>]\\n \\n \\n  E,pre      [xo   \\n \\n|      x1   \\n \\n—      Ut   \\n \\n+      {or}      V      0g      Ge(&e)  \\n \\n  Le</p></td></tr></table><p>Recall      that      the      notation      p/?,      0?,      q,      are      defined      in      (3).\\nFinally,      by      recalling      V      log   \\n \\n@   \\n \\n=      Si      ore      (x;,t)      in  \\n \\n  score-based      diffusion      models,      we      can      approximate      Vv;(a)      with</p><h4>Le</h4><p> \\n \\n  and      plug      it      into      Algorithm   \\n \\n4      (i.e.,      value-weighted      sampling).\\n \\n \\n  As      mentioned      earlier,      this      approach      has      been      practically      widely      used      in      the      context      of      classifier  \\n \\n  guidance.\\nDespite      its      simplicity,      the      approximation      error      can      be      significant,      as      it      can      not      be  \\n \\n  diminished      even      with   \\n \\na      large      sample      size      because      the      discrepancy      from      (30)      to      (31)      is      not      merely   \\n \\na       statistical      error      that      diminishes      with      increasing      sample      size.</p><li>6.2.3.\\nZeroth-Order      Guidance      using      Path      Integral      Control</li><p> \\n \\n  In      the      previous      subsection      (Section      6.2.2),      we      discussed      an      approximation      technique      to      bypass      the  \\n \\n  necessity      of      learning      explicit      value      functions.\\nAnother      approach      to      bypass      this      requirement      is      to      use  \\n \\n  control-based      techniques      for      obtaining      soft-optimal      policies,      a.k.a.,      path      integral      control      (Theodorou  \\n \\n  et      al.,      2010;      Williams      et      al.,      2017;      Kazim      et      al.,      2024).</p><p> \\n \\n  Recall      that      we      have      p(x;,t;      6)   \\n \\n=      2;   \\n \\n+      (dt)g(x+,      t)      and      o°(t)   \\n \\n=      g(t)(dt),      the      motivation      behind this      approach      is      as      follows.\\nInitially,      we      have:</p><h5>Vo,      exp(v(#1)/@)</h5><p>=\\n \\n   Egppreyexp(vr_1      (22-1)      /@)   \\n \\n|      24]      (Soft      Bellman      equation) _\\n \\n   (1   \\n \\n—   \\n \\nx   \\n \\n—      (Ot)      G(a2,      t))?\\n=\\n \\n   Vz»,   \\n \\n{   \\n \\n[      explatena/ay)      exp      (-      0-5a(t)      (ot)   \\n \\n)      in,-}</p><p>=\\n \\n   Byes      [ese      a(era/a))      {ea      ~~      (61)g(nn,)}      EE      Cah</p><p>~\\n \\n   arp      Buty      lexr(era(ea/a))      Gera   \\n \\n—      21   \\n \\n~      (8t)ale      plod</p><p>_t   \\n \\n1       Ht)      ot)      Eg,prey      [E      ppp}      [exp(r(2o)/a)   \\n \\n|      T-1]      e|24|   \\n \\n=      Woon)      [exp(r(xo/a))e|x¢]      ;</p><h5>Now,      we      obtain:</h5><p>o7(t)Vnv(ae)   \\n \\n1      Eqppry      lexp(r(wo)/a)      eel    \\n \\nQ   \\n \\n~   \\n \\na   \\n \\n*      Egyprey      lexp(r(2o)/a))      |x]      (32)</p><p> \\n \\n  Importantly,      this      approach      does      not      require      any      differentiation.\\nTherefore,      by      running      policies  \\n \\n  from      pre-trained      models      and      simply      using      Monte      Carlo      approximation      in      both      the      denominator  \\n \\n  and      numerator,      we      can      estimate      the      above      quantity      (the      right-hand      side      of      (32))      without      making  \\n \\n  any      models      (classifiers),      unlike      classifier      guidance.\\nNote      while      this      approach      is      widely      used      in      the  \\n \\n  control      community,      it      may      not      be      feasible      for      diffusion      models      due      to      the      high      dimensionality      of  \\n \\n  the      input      space.</p><h3>6.3      Path      Consistency      Learning      (Losses      Often      Used      in      Gflownets)</h3><p>ets)  \\n \\n  Now,      we      explain      how      to      apply      path      consistency      learning      (PCL)      (Nachum      et      al.,      2017)      to      fine-  \\n \\n  tune      diffusion      models.\\nIn      the      Gflownets      literature      (Bengio      et      al.,      2023),      it      seems      that      this      variant      is  \\n \\n  utilized      as      either   \\n \\na      detailed      balance      or   \\n \\na      trajectory      balance      loss,      as      discussed      in      Mohammadpour  \\n \\n  et      al.\\n(2023);      Tiapkin      et      al.\\n(2023);      Deleu      et      al.\\n(2024).\\nHowever,      to      the      best      of      our      knowledge,  \\n \\n  the      precise      formulation      of      path      consistency      learning      in      the      context      of      fine-tuning      diffusion      models  \\n \\n  has      not      been      established.\\nTherefore,      we      start      by      elucidating      the      rationale      of      PCL.\\nSubsequently,  \\n \\n  we      provide   \\n \\na      comprehensive      explanation      of      the      PCL.\\nFinally,      we      discuss      its      connection      with      the  \\n \\n  literature      on      Gflownets.</p><p>Motivation.\\nHere,      we      present      the      fundamental      principles      of      the      PCL.\\nTo      start      with,      we      prove      the  \\n \\n  following      lemma,      which      characterizes      soft-value      functions      and      soft-optimal      policies      recursively.</p><p>Algorithm   \\n \\n5      Path      Consistency      Learning      (Training      with      detailed      balance      loss)  \\n \\n  1:      Require:      Diffusion-model      {N(p(2,t;),07(t))}i-74,,  \\n \\n  -pre-trained   \\n \\n=      model  \\n \\n  {N      (p(t,      t;      Opre),      07      (t))      }i_744,      batch      size      m,      parameter   \\n \\na   \\n \\n€      Rt,      learning      rate   \\n \\n7       2:      Set   \\n \\na      model      {v;(-;@)}      to      learn      optimal      soft      value      function,      and   \\n \\na      model      {p;(-|-;@)}      to      learn  \\n \\n  optimal      polices.\\n \\n \\n  3:      Initialize:      0;   \\n \\n=      Opre  \\n \\n  4:      for      s   \\n \\n=      {1,---      ,S}do  \\n \\n  5:      Collect   \\n \\nm      samples      {a      (0)      }?_7,,      from   \\n \\na      current      diffusion      model      (i.e.,      generating      by      sequen- tially      running      polices      {N(p(2z,      t;      8),      07(t))      }i_-74,      from   \\n \\nt   \\n \\n=   \\n \\nT      to   \\n \\nt   \\n \\n=      0)  \\n \\n  6:      Setuo=r</p><li>(i)   \\n \\n?       a!\\n \\n \\n3   \\n \\na   \\n \\n4      Up—-1(@      13      ds)      re  \\n \\n  bs41   \\n \\n—      bs   \\n \\n—      Vo   \\n \\n3   \\n \\na      9)   \\n \\n+      log      p:(a\\\\\\\\\\\\\\\\,      |a\\\\\\\\\\\\\\\\;0,)   \\n \\n-      EP   \\n \\n—      Jog      rl      >}      Ibe:</li><p>a\\n \\n \\n  t=T+1      i=1  \\n \\n  (c}      ite)      ()      via)      43s)   \\n \\n;       Os41   \\n \\n<      Os   \\n \\n—      Vo   \\n \\n3      TE      velee      ids)      —S<   \\n \\n+      log      pe(ay      2s      |p      0)      —§      log      pee      (ar?\\nifort?\\nJe      lass</p><p>t=T+1      i=1 7:      end      for  \\n \\n  8:      Output:      {p,(x1-1      [215      Os)      fe</p><h3>Lemma      4      (1-step      Consistency      Equation).</h3><li>(2)   \\n \\n+      log      pi      (aa      |a1)   \\n \\n=      (eaeen)   \\n \\n+      log      pp      (x+-1|2+)      (33)</li><p> \\n \\n  Proof.\\nWe      consider      the      marginal      distribution      with      respect      to      x,      and      2;_,      induced      by      the      soft-  \\n \\n  optimal      policy,      and      denote      it      [(z;,      2:1)   \\n \\n€      A(X   \\n \\nx      X).\\nThen,      it      is      clearly      I(x,)l(a,_1   \\n \\n|      x)   \\n \\n=       I(x4-1)l      (xz   \\n \\n|      a1).\\nNow,      from      Theorem   \\n \\n2      that      characterizes      marginal      distributions,      and      Theorem   \\n \\n3       that      characterizes      posterior      distributions,      this      results      in      :</p><p>1\\n \\n   UE      L      re   \\n \\nx   \\n \\n1      Otte      me  \\n \\n  exp   \\n \\n(   \\n \\ni      2)      Pe      (xt)   \\n \\nx      pe      (®e-1|@e)   \\n \\n=   \\n \\nB      exp      (faeev)      Pra      (@1-1)   \\n \\nX      Pe      (1/01) a\\n \\n   —_-_——”’   \\n \\nC   \\n \\na      ~~  \\n \\n  C  \\n \\n  we  \\n \\n  Optimal      policy  \\n \\n  “~   \\n \\n“      Posterior      distribution  \\n \\n  Marginal      distribution      at   \\n \\nt      Marginal      distribution      at      t-1 Rearranging      yields:</p><p>1\\n \\n   UL(&   \\n \\n1      Up—1      (Le      re  \\n \\n  G      exp   \\n \\n(      ))   \\n \\nx      Di      (@-1|%4)   \\n \\n=   \\n \\nG      exp      (eaeen)   \\n \\nx      pe      (@4-1|24)</p><p>Taking      the      logarithm,      the      statement      is      concluded.\\nO</p><p>Algorithm.\\nBeing      motivated      by      the      relation      in      (33),      after      initializing      v9   \\n \\n=      r,      we      obtain      the  \\n \\n  recursive      equation:</p><p>2 Olen      U_-1      (Lt  \\n \\n  (pp)   \\n \\n=      argmin   \\n \\n—      Egany   \\n \\n|      oO   \\n \\n4      tog   \\n \\ng      (arp      ng)   \\n \\n—      PEED)   \\n \\n—      dog      pP\"      (ya      er) g):%3R,g2):¥      A(X)   \\n \\na   \\n \\no U   \\n \\n_   \\n \\nx   \\n \\n_      re      re  \\n \\n  =      (MEY)      tog      peal      altiansa)      +++      Flog      pe      (esl),  \\n \\n  which      is      an      extension      of      (33).\\nThe      loss      function      based      on      the      above      k-step      consistency      equation  \\n \\n  could      make      training      faster      without      learning      value      functions      at      every      time      point,      as      noted      in      the  \\n \\n  literature      in      PCL.\\nIn      the      extreme      case      (i.e.,      when      we      recursively      apply      it      with   \\n \\nt   \\n \\n=      7’),      we      obtain      the  \\n \\n  following.</p><li>(34)  \\n \\n  where      u,   \\n \\n€      A(X)      is      any      exploratory      roll-in      distribution.\\nBased      on      this      algorithm,      we      outline      the  \\n \\n  entire      algorithm      in      Algorithm      5.  \\n \\n  We      make      several      important      remarks      regarding      Algorithm      5.\\nFirstly,      while      we      use      on-policy      data  \\n \\n  collection,      technically,      any      policy      can      be      used      in      this      off-policy      algorithm,      like      reward-weighted  \\n \\n  MLE.\\nSecondly,      in      practice,      it      might      be      preferable      to      utilize   \\n \\na      sub-trajectory      from      x;      to      x;_,      based  \\n \\n  on      the      following      expression:</li><p>UFZ  \\n \\n  log      pi_      pa      (Gee      |Vt—ng1)   \\n \\n+      +++   \\n \\n+      log      pi      (4-1      |a4)   \\n \\n+      (2)</p><h3>Corollary      1      (7-step      consistency).</h3><p>log      pf      (xo|21)   \\n \\n+      +++   \\n \\n+      log      pp(wr-a|er)   \\n \\n+      log      pp      (x7)      (35) r(x      re      re      re  \\n \\n  —      (=)   \\n \\n+      log      p?\\n*(xo|v1)      +--+   \\n \\n+      log      pr\"      (ar-i|er)   \\n \\n+      log      pe      (xr).</p><p>Proof.\\nWe      consider      the      marginal      distribution      with      respect      to      7,      +--+   \\n \\n,   \\n \\n%      induced      by      the      soft-optimal  \\n \\n  policy,      and      denote      it      I(vp,---      ,--+      ,%o)   \\n \\n€   \\n \\n¥      X--+   \\n \\nx      X.      We      have Uar)l(ar—1   \\n \\n|      Lr)      tee      I(xo|21)   \\n \\n=      U(x      )l(a4   \\n \\n|      Xo)      te      (ar   \\n \\n|      LT-1)      (36) From      Theorem   \\n \\n2      that      characterized      marginal      distributions,      and      Theorem   \\n \\n3      that      characterize      posterior  \\n \\n  distributions,      the      left      hand      side      of      (36)      is      equal      to *\\n \\n   *\\n \\n   *</p><p>Pr      (xr)   \\n \\nx      pp      (r|er-1)      X-++   \\n \\nx      pi      (Xo|@1) Marginal      distribution      atT      Optimal      policy      at   \\n \\nT   \\n \\n—   \\n \\n1      Optimal      policy      at   \\n \\n1 and      the      right-hand      side      in      (36)      is      equal      to exp(r      (xo)      /a)      pre      pre      pre  \\n \\n  aq      Po      (xo)   \\n \\nX      po      (a1   \\n \\n|      Zo)      X+++   \\n \\nX      prey      (@r   \\n \\n|      er-1)-.\\n \\n \\n|      ~~  \\n \\n  Marginal      distribution      at   \\n \\n0      Posterior      distribution      Posterior      distribution By      rearranging      the      term,      we      obtain      (35).\\nO</p><p> \\n \\n  Comparison      with      Gflownets.\\nIn      the      Gflownets      literature,      similar      losses      are      used.\\nFor      instance,  \\n \\n  the      loss      derived      from      (33)      or      (35)      is      commonly      known      as   \\n \\na      detailed      balance      loss      (Bengio      et      al.,  \\n \\n  2023)      or   \\n \\na      trajectory      loss      (Malkin      et      al.,      2022),      respectively.</p><p> \\n \\n  Note      in      general,      the      literature      in      Gflownets      primarily      focuses      on      sampling      from      unnormalized  \\n \\n  models      (distributions      proportional      to      exp(r(a))).\\nHence,      reference      policies      (i.e,      {p?\"\"}\\n \\n \\n)      or      latent  \\n \\n  states      (1.e.,      %7.;      before      x9)      are      introduced      without      relying      on      pre-trained      diffusion      models.\\nIn  \\n \\n  contrast,      in      our      context,      we      use      policies      derived      from      pre-trained      diffusion      models      as      reference  \\n \\n  policies,      leveraging      them      as      our      prior      knowledge.</p><p>7\\n \\n   Fine-Tuning      Settings      Taxonomy</p><p> \\n \\n  So      far,      we      implicitly      assume      we      have      access      to      reward      functions.\\nHowever,      these      functions      are      often  \\n \\n  unknown      and      need      to      be      learned      from      data.\\nWe      classify      several      settings      in      terms      of      whether      reward  \\n \\n  functions      are      available      or,      if      not,      how      they      could      be      learned.\\nThis      section      is      summarized      in      Figure      3.</p><h4>Direct      backpropagation      (Sec.      5.2)       _       Differentiable</h4><p>_\\n \\n   ——_      Non-differentiable PP      5.1</p><h4>Accurate      rewards      O      (Sec.      5.1)</h4><p> \\n \\n  feedback   \\n \\n;    \\n \\n—      Reward-weighted      MLE      (Sec.      6.1  \\n \\n  Xo      Unknown      reward      Memory      effi-      oe      eward-welgnte      (Sec.      6.1) unctions      —_\\\\\\\\\\\\\\\\_—      ciency</p><h4>No      fine-tuning      Computational</h4><p>efficiency —\\n \\n    NN      Direct      backpropagation      (Sec.      5.2)</p><h5>Value-weighted      sampling      (Sec.      6.2)</h5><p>Figure      3:      Practical      recommendation      of      RL-based      algorithms      to      optimize      downstream      reward  \\n \\n  functions.</p><h6>7.1      Fine-Tuning      with      Known,      Differentiable      Reward      Functions</h6><p>ions  \\n \\n  When      accurate      differentiable      reward      functions      are      available      (e.g.,      standard      in      computer      vision      tasks),  \\n \\n  our      primary      focus      is      typically      on      computational      and      memory      efficiency.\\nIn      general,      we      advocate      for  \\n \\n  employing      reward      backpropagation      (c.f.      Algorithm      2)      for      its      computational      efficiency.\\nAlternatively,  \\n \\n  if      memory      efficiency      is   \\n \\na      significant      concern,      we      suggest      using      PPO      due      to      its      stability.</p><h6>7.2      Fine-Tuning      with      Black-Box      Reward      Feedback</h6><p>back  \\n \\n  Here,      we      explore      scenarios      where      we      have      access      to      accurate      but      non-differentiable      (black-box)  \\n \\n  reward      feedback,      often      found      in      scientific      simulations.\\nThe      emphasis      still      remains      primarily      on  \\n \\n  computational      and      memory      efficiency.\\nIn      such      scenarios,      due      to      the      non-differentiability      of      reward  \\n \\n  feedback,      we      recommend      using      PPO      or      reward-weighted      MLE      for      the      following      reasons.</p><p> \\n \\n  In      the      preceding      section,      we      advocated      for      the      use      of      reward      backpropagation      due      to      its      com-  \\n \\n  putational      advantages.\\nHowever,      when      dealing      with      non-differentiable      feedback,      the      advantage  \\n \\n  of      reward      backpropagation      may      diminish.\\nThis      is      because      learning      from      such      feedback      requires  \\n \\n  learning      differentiable      reward      functions      to      make      the      algorithm      work      as      we      discuss      in      Section      5.2.  \\n \\n  However,      obtaining   \\n \\na      differentiable      reward      function      can      be      challenging      in      many      domains.\\nFor  \\n \\n  example,      in      molecular      property      prediction,      molecular      fingerprints      are      informative      features,      and  \\n \\n  accurate      reward      functions      can      be      derived      by      training      simple      neural      networks      on      these      features  \\n \\n  (Pattanaik      and      Coley,      2020).\\nYet,      these      mappings,      grounded      in      prior      scientific      understanding,      are  \\n \\n  non-differentiable,      thereby      restricting      their      applicability.</p><p> \\n \\n  In      contrast,      PPO      and      reward-weighted      MLE      allow      for      policy      updates      without      the      explicit  \\n \\n  requirement      to      learn   \\n \\na      differentiable      reward      function,      offering   \\n \\na      significant      advantage      over      reward  \\n \\n  backpropagation.</p><h6>7.3      Fine-Tuning      with      Unknown      Rewards      Functions</h6><p>ions  \\n \\n  When      reward      functions      (or      computational      proxies      in      Section      7.2)      are      unavailable,      we      must      learn  \\n \\n  from      data      with      reward      feedback.\\nIn      such      cases,      compared      to      the      previous      scenarios,      two      important  \\n \\n  considerations      arise:</p><p>¢\\n \\n   Not      only      computational      or      memory      efficiency      but      also      feedback      efficiency      (i.e.,      sample  \\n \\n  efficiency      regarding      reward      feedback)      is      crucial.</p><p> \\n \\n \\n¢      Given      that      learned      reward      feedback      may      not      generalize      well      outside      the      training      data      distri-  \\n \\n  butions,      it      is      essential      to      constrain      fine-tuned      models      to      prevent      significant      divergence      from  \\n \\n  diffusion      models.\\nIn      these      situations,      not      only      soft      PPO      and      direct      backpropagation      but      also  \\n \\n  methods      discussed      in      Section      6,      such      as      reward-weighted      MLE,      can      be      particularly      effective.</p><p>Further,      the      scenario      involving      unknown      reward      functions      can      be      broadly      categorized      into      two      cases,  \\n \\n  which      we      will      discuss      in      more      detail.</p><p>Offline      scenario      (Figure      4).\\nIn      many      scenarios,      even  \\n \\n  in      cases      where      reward      functions      are      unknown,      we      of-  \\n \\n.     .</p><p>Offline      data      with ten      have      access      to      offline      data      with      reward      feedback      reward      feedback</p><li>(i)      (i)   \\n \\nO   \\n \\n;   \\n \\ni       a”      ra  \\n \\n.      ne      straightforward      approach      is      to</li><p> \\n \\n \\n{   \\n \\n,   \\n \\n(   \\n \\n)    \\n.\\n    .\\n \\n   g\\n \\n   PP  \\n \\n.      <=>      Learning      reward  \\n \\n  perform      regression      with      neural      networks      and      build   \\n \\na      models  \\n \\n  learned      regressor      *.\\nHowever,      this      learned      reward      func-      SS  \\n \\n  tion   \\n \\n*      might      not      generalize      well      beyond      the      distribution      x)  \\n \\n  of      the      offline      data.\\nIn      regions      outside      this      distribution,   \\n \\n7       could      assign   \\n \\na      high      value      even      when      the      actual      reward  \\n \\n  r      is      low      due      to      high      uncertainty.\\nConsequently,      we  \\n \\n  could      be      easily      misled      by      out-of-distribution      samples  \\n \\n  if      we      solely      optimize      7.\\n|Fine-tuning      diffusion      models   \\n \\n|       One      approach      to      alleviate      this      issue      is      to      adopt   \\n \\na       pessimistic      (i.e.,      conservative)      strategy,      as      proposed      in      Figure      4:      Offline      scenario  \\n \\n  Uehara      et      al.\\n(2024).\\nThis      technique      has      been      widely      Collecting      data]   \\n \\nY      Learning      reward  \\n \\n  applied      in      the      field      of      offline      RL      (Levine,      2018).\\nThe      aa  \\n \\n  main      idea      is      to      penalize   \\n \\n7      outside      the      distributions      us-  \\n \\n  ing      techniques      such      as      adding      an      explicit      penalty      term  \\n \\n  (Yu      et      al.,      2020;      Chang      et      al.,      2021),      bootstrapping,  \\n \\n  or      more      sophisticated      methods      (Kumar      et      al.,      2020;  \\n \\n  Xie      et      al.,      2021;      Rigter      et      al.,      2022;      Uehara      and      Sun,  \\n \\n  2021).\\nBy      doing      so,      we      can      avoid      being      fooled      by  \\n \\n  out-to-distribution      regions      while      still      benefiting      from  \\n \\n  the      extrapolation      capabilities      of      reward      models.</p><p>S—      diffusion      models      |</p><p>Figure      5:      Online      scenario</p><p> \\n \\n  Online      scenario      (Figure      5).\\nConsider      situations  \\n \\n  where      reward      functions      are      unknown,      but      we      can      gather      data      online.\\nThese      scenarios      are      often  \\n \\n  referred      to      as   \\n \\na      lab-in-the-loop      setting.</p><p> \\n \\n  In      such      scenarios,      Uehara      et      al.\\n(2024)      proposes      an      iterative      procedure      comprising      three      steps:</p><li>(1)      collecting      feedback      data      by      exploring      new      areas      (i.e.,      obtaining   \\n \\nx      from      current      diffusion      models  \\n \\n  and      corresponding      r(x)),      (2)      learning      the      reward      function      from      the      collected      feedback      data,      and</li><li>(3)      fine-tuning      current      diffusion      models      by      maximizing      the      learned      reward      function      enhanced      by  \\n \\n  an      optimistic      bonus      term,      using      algorithms      discussed      in      Section      4.2.\\nIn      contrast      to      standard      online  \\n \\n  PPO,      where      feedback      data      is      directly      used      in      the      fine-tuning      process,      actual      reward      feedback      is  \\n \\n  only      used      in      step      (2)      and      not      in      step      (3).\\nAn      additional      key      aspect      is      the      use      of      an      optimistic  \\n \\n  reward      function      that      encourages      exploration      beyond      the      distributions      of      the      current      diffusion      model.\\n \\n \\n  This      deliberate      separation      between      reward      learning      and      fine-tuning      steps,      coupled      with      the      use      of  \\n \\n  optimism,      significantly      enhances      feedback      efficiency.</li><p>8\\n \\n   Connection      with      Classifier      Guidance</p><p> \\n \\n  Classifier      guidance      (Dhariwal      and      Nichol,      2021)      is      commonly      used      for      conditional      generation      in  \\n \\n  diffusion      models.\\nInterestingly,      RL-based      methods      share   \\n \\na      close      connection      with      classifier      guidance.\\n \\n \\n  More      specifically,      in      this      section,      following      Zhao      et      al.\\n(2024),      we      elucidate      how      RL-based      methods  \\n \\n  can      be      applied      to      conditional      generation.\\nFurthermore,      from      this      unified      viewpoint,      we      highlight  \\n \\n  that      classifier      guidance      is      seen      as   \\n \\na      value-weighted      sampling      in      Section      4.2.</p><h6>8.1      Classfier      Guidance</h6><p>ance  \\n \\n  In      this      section,      we      first      introduce      classifier      guidance.\\nClassifier      guidance      utilizes      an      unconditional  \\n \\n  pre-trained      model      for      conditional      generation.\\nMore      specifically,      we      consider   \\n \\na      scenario      where      we  \\n \\n  have   \\n \\na      pre-trained      diffusion      model,      which      enables      us      to      sample      from      p?\\n\"*(x),      and      data      with      feedback  \\n \\n  {x,y},      where   \\n \\ny      represents      the      new      label      we      want      to      condition      on.\\nOur      objective      here      is      to      sample  \\n \\n  from      p(-|y)   \\n \\n€      [Y   \\n \\n>      ACR), P(-   \\n \\n|      y)   \\n \\nx      py(yl-)pP*(-).\\n(37)</p><p>where      p,   \\n \\n:   \\n \\nX      —>      A(Y)      denotes      the      conditional      distribution      of   \\n \\ny      given      x.</p><p>Motivation.\\nIn      classifier      guidance,      we      consider      the      following      policy:</p><p>pe      (-lare,      y)   \\n \\n=   \\n \\nN      (p(22,      t;      Opre)   \\n \\n+      a(t)      Ve      log      Vy      (Xt;      t),      a*(t))   \\n \\n’ dy(x,t)   \\n \\n=      Egy.\\nyap,      (-leo)      L(Y   \\n \\n=      y)|a,   \\n \\n=      2].</p><p>Under      the      continuous-time      formulation      of      diffusion      models,      applying      Doob’s      h-transform      (Rogers  \\n \\n  and      Williams,      2000),      it      is      shown      that      we      can      sample      from      p(x|y)      by      sequentially      running      {p7\"\\\\\\\\\\\\\\'(-|7+,      y)      }i-r41-</p><p>Algorithm.\\nSpecifically,      the      algorithm      comprises      two      steps.\\nIt’s      important      to      note      that      this      is      not  \\n \\n  a      fine-tuning      method      but      rather      an      inference-time      technique      as      we      freeze      the      pre-trained      model.</p><li>1. Learning      q,(x,t)      through      regression      by      gathering      data      containing      {z;,      y}.\\nHere,      for      each  \\n \\n  (Xo,      y),      using   \\n \\na      forward      policy      for      the      pre-training      (from   \\n \\n0      to      T),      we      obtain      2;      starting      from</li><p>Lo.</p><li>2. During      inference,      sequentially      execute      {p2\"(-|a,,      y)      }1_>   \\n \\n1      by      leverage      the      fact      that      compared</li><p>gui to      the      policy      in      pre-trained      diffusion      models,      the      mean      of      {p?\\n\"\\\\\\\\\\\\\\'(-|71,      y)      }i-74,      is      just      shifted  \\n \\n  with      an      additional      term      V,,      log      q,(2,      t).</p><p> \\n \\n  Notably,      the      method      described      above      can      be      reduced      to      value-weighted      sampling      (cf.      Sec-  \\n \\n  tion      6.2),      when      r(x,      y)   \\n \\n=      log      p(y|x).\\nInspired      by      such      an      observation,      we      apply      an      RL-based  \\n \\n  fine-tuning      method      for      conditional      generation      as      below.</p><h7>8.2.      RL-Based      Fine-Tuning      for      Conditional      Generation</h7><p>tion  \\n \\n  In      this      section,      following      Zhao      et      al.\\n(2024),      we      explain      an      RL-based      fine-tuning      approach      for  \\n \\n  conditional      generation.\\nThe      core      idea      is      that      setting      r(x,      y)   \\n \\n=      log      p(y|x)      allows      us      to      sample      from  \\n \\n  the      target      conditional      distribution      (37),      drawing      on      the      insights      discussed      in      Section      4.2.  \\n \\n  More      precisely,      introducing   \\n \\na      distribution      over   \\n \\ny      as      q(-),      we      formulate      the      following      RL      problem:</p><p>{Di      ft   \\n \\n=      argmax      Egy,      (-\\\\\\\\\\\\\\\\-y)}.yott      log      p(y|xo)   \\n \\n—      NeopyaKL(pe(-|e,      y)      ||P      (late).\\n \\n \\n  {prE[(V,.X)PA(A)]      Heyy (38)</p><p> \\n \\n  Compared      to      the      objective      function      (18)      in      Section      4.2,      the      main      differences      are:      (1)      policy      space  \\n \\n  is      expanded      to      accommodate      additional      control      (i.e.,      y):      pz(-|-,-)   \\n \\nC      [R¢   \\n \\nx   \\n \\nY   \\n \\n+      A(R®)],      (2)      the  \\n \\n  fine-tuning      objectives      are      set      as      log-likelihoods:      log      p(y|z).\\nThen,      following      Theorem      1,      we      can  \\n \\n  establish      the      following      result.</p><p>Corollary      2.\\nThe      distribution      induced      by      {p*}1_74      (ue.   \\n \\n[      {Teri      Ph      (ae-1   \\n \\n|      te,      y)}deur      is      the target      conditional      distribution      (37).</p><p> \\n \\n  The      above      corollary      suggests      that      for      conditional      generation,      we      can      utilize      various      off-the-shelf  \\n \\n  algorithms      discussed      in      Section      4.2      and      6,      such      as      PPO,      reward      backpropagation,      and      reward-  \\n \\n  weighted      MLE,      as      well      as      value-weighted      sampling.</p><p>9\\n \\n   Connection      with      Flow-Based      Diffusion      Models</p><p> \\n \\n  We      elucidate      the      close      relationship      between      bridge      matching      and      fine-tuning,      as      summarized      in  \\n \\n  Table      2.\\nBridge      (flow)      matching      (Liu      et      al.,      2022;      Tong      et      al.,      2023;      Lipman      et      al.,      2023;      Liu      et      al.,  \\n \\n  2022;      Shi      et      al.,      2024;      Albergo      and      Vanden-Eijnden,      2022),      has      recently      gained      popularity      as   \\n \\na       distinct      type      of      diffusion      model      from      score-matching-based      ones.\\nThese      works      suggest      that      training  \\n \\n  and      inference      speed      will      be      accelerated      due      to      the      ability      to      define   \\n \\na      more      direct      trajectory      from  \\n \\n  noise      to      data      distribution      than      score-matching-based      diffusion      models.</p><p> \\n \\n  The      connection      between      bridge      matching      and      fine-tuning      becomes      evident      in      the      continuous-  \\n \\n  time      formulation.\\nTo      begin,      we      first      provide   \\n \\na      brief      overview      of      the      continuous-time      formulation      of  \\n \\n  score-matching-based      diffusion      models.</p><p> \\n \\n  Table      2:      Comparison      between      training      score/flow-based      diffusion      models      and      fine-tuning.\\nWe  \\n \\n  optimize      the      parameter   \\n \\n6      in      P’,      which      represents      the      induced      distribution      over      trajectories      associated  \\n \\n  with      6.</p><h8>Loss      Note      of      Q</h8><p>Score-based      KL(Q\\\\\\\\\\\\\\'™¢|P?)      Qtime:      Time-reversal      of      reference      SDE      (reference training      SDE      is      defined      from   \\n \\n0      to      T’      where      the      initial      dis-  \\n \\n  tribute      at   \\n \\n0      is      data      distribution)</p><p> \\n \\n  Flow-based      KL(Q*°>      ||P?)      @Q*°°h:      Coupling      between      the      reference      SDE      from  \\n \\n  training   \\n \\nT      to   \\n \\n0      conditioning      at   \\n \\n0      (dist.      over      x.;|7o)      and  \\n \\n  a      data      distribution      at      time   \\n \\n0      (dis.      over      x).\\nNote  \\n \\n  that      reference      SDE      is      defined      from      7’      to      0.</p><p> \\n \\n  Fine-tuning      KL(P?||Q*°)      @*:      Coupling      between      the      pre-trained      SDE  \\n \\n  from   \\n \\nT      to   \\n \\n0      conditioned      on   \\n \\n0      (dist.      over      x7.1|X0)  \\n \\n  and   \\n \\na      data      distribution      at      time   \\n \\n0      (dis.      over      Xo) 9.1      Continuous      Time      Formulation      of      Score-Based      diffusion      models</p><p> \\n \\n  We      provide   \\n \\na      brief      overview      of      the      continuous-time      formulation      of      diffusion      models      (Song      et      al.,  \\n \\n  2021),      further      expanding      Section      1.1.1.\\nIn      this      formulation,      we      initially      define      the      forward      reference  \\n \\n  SDE,      spanning      from      time   \\n \\n0      to      7’      (e.g.,      variance      exploding      (VE)      process      or      variance      preserving      (VP)  \\n \\n  process),      by      setting      an      initial      distribution      at   \\n \\n0      as      the      data      distribution.\\nAdditionally,      the      reference  \\n \\n  SDE      is      tailored      to      converge      to      an      easy      distribution      at      time      7’,      such      as      the      normal      distribution.</p><p> \\n \\n  Subsequently,      we      consider      the      time-reversal      SDE      (Anderson,      1982),      progressing      from      T\\\\\\\\\\\\\\'      to      0.\\nIf  \\n \\n  we      could      learn      this      time-reversal      SDE,      it      would      enable      us      to      sample      from      the      data      distribution      by  \\n \\n  starting      with      the      easy      initial      distribution      (at      7’)      and      following      the      time-reversal      SDE      from   \\n \\nT      to   \\n \\n0      at  \\n \\n  inference      time.</p><p> \\n \\n  Training.\\nNow,      the      remaining      question      is      how      to      learn      this      time-reversal      SDE.\\nHere,      by      repre-  \\n \\n  senting      the      induced      distribution      over      trajectories      from   \\n \\nT      to   \\n \\n0      with      this      time-reversal      SDE      as      Q\\\\\\\\\\\\\\'™®,  \\n \\n  our      goal      is      to      train   \\n \\na      new      SDE,      parameterized      by   \\n \\n@      in      the      neural      network,      such      that      the      induced  \\n \\n  distribution      P®      over      trajectories      associated      with   \\n \\n6      aligns      with      Q\"™*.\\nSpecifically,      the      objective      is      to minimize      the      following      losses:</p><p> \\n \\n  argmin      KL(Q\"™||P*).\\n(39)  \\n \\n  0  \\n \\n  With      some      algebra,      leveraging      the      observation      that      the      time-reversal      SDE      comprises      the      original  \\n \\n  forward      SDE      and      the      scoring      term      of      the      marginal      distribution      (Anderson,      1982)      as      in      (2),      Song  \\n \\n  et      al.\\n(2021)      demonstrates      that      this      KL      loss      (39)      is      approximated      as Dore   \\n \\n=      argmin      Eeuni((0,7))01~qyo0(weleo)co~pP\"e      Fe      Fl      Vey      log      jo      (Xt   \\n \\n|      Xo)   \\n \\n_      so(Xt,      t)|I?\\n],      (40)  \\n \\n  which      is      equal      to      (4)      where      the      weight      A(t)      is      {o?\\n}?\\n(for      notation,      refer      to      Section      1.1.1).</p><h9>9.2      Flow-Based      (Bridge-Based)      Diffusion      Models</h9><p>dels We      adopt      the      explanation      outlined      in      Liu      et      al.\\n(2022).\\nIn      this      framework,      we      begin      by      considering  \\n \\n  a      reference      SDE      (from      T\\\\\\\\\\\\\\'      to      0),      x;   \\n \\n=      zr_;      where      z;      is      defined      as      Brownian      motion:</p><p>dz,   \\n \\n=      dwt,      te      (0,      7).\\n(41)</p><p> \\n \\n  Subsequently,      we      consider      the      coupling      of      the      reference      SDE      conditioned      on      state      at      time   \\n \\n0      (i.e.,  \\n \\n  distribution      of      xo.7      conditional      on      x)      and      the      data      distribution      at      time      0,      denoting      the      resulting  \\n \\n  distribution      as      Q*°?,      Informally,      this      Q*°°?\\nis      characterized      as      follows.</p><p>Informal      characterization      of      Q*”.\\nWhen      discretizing      the      time      step,      by      denoting      the      distribution  \\n \\n  induced      by      the      reference      SDE      as      p\"(x7,--+   \\n \\n,      29),      we      define QF      (a7,      tae   \\n \\n,      20)   \\n \\n=      p(x,      ay   \\n \\n|      r9)pP\"°      (x9).</p><p>The      distribution      Q*”      above      is   \\n \\na      limiting      distribution      obtained      by      making      the      discretization      step small.</p><p>Training.\\nIn      bridge-based      diffusion      models,      our      objective      is      to      train   \\n \\na      new      SDE,      parametrized      by  \\n \\n  6      (e.g.,   \\n \\na      neural      network),      to      align      the      induced      distribution      P®      with      Q*°?.\\nSpecifically,      bridge-based training      aims      to      minimize aremin      KL(Q*>||P’).\\n(42)  \\n \\n  0  \\n \\n  While      the      method      to      minimize      the      above      is      currently      unclear,      we      can      derive   \\n \\na      trainable      objective  \\n \\n  through      algebraic      manipulation.\\nInitially,      we      observe      that      the      reference      SDE      (41),      conditioned      on  \\n \\n  state      xo      at      time   \\n \\n0      (zr   \\n \\n=      D),      is      expressed      as:</p><p>b\\n \\n \\n  b—      2;</p><p>t\\n \\n   €\\n \\n   [0,T];dz?\\n \\n \\n=      g?(z?,t)dt      +du,,  \\n \\n  g(z?,t)   \\n \\n=   \\n \\na       (43)</p><p>using      the      seminal      “Doob’s      h-transform”      (Rogers      and      Williams,      2000).\\nThis      SDE      is      commonly  \\n \\n  known      as      the      Brownian      bridge.\\nNow,      we      consider   \\n \\na      new      SDE      parameterized      by      0:</p><p>t\\n \\n   €\\n \\n   [0,7];      dz   \\n \\n=      s(z,t;      @)dt   \\n \\n+      dw.\\n(44)</p><p>Then,      we      can      demonstrate      that      minimizing      the      KL      loss      in      (42)      is      equivalent      to argmin      E,W      unil0,7],1~4\"°      cop?\\n[Ilg”      (2,      t)   \\n \\n_      s(21,      t;      0)      M3]      (45)</p><p>where      q;°      denotes      the      induced      distribution      at      time   \\n \\nt      following      SDE      (43)      conditioned      on      xo,      and  \\n \\n  s:      Xx      |0,T]   \\n \\n>   \\n \\n&      is      the      model      we      aim      to      optimize.</p><p> \\n \\n  Flow-based      diffusion      models.\\nThe      aforementioned      formulation      has      been      originally      proposed      as   \\n \\na       bridge-based      SDE      (Liu      et      al.,      2022)      because      the      conditional      SDE      is      often      referred      to      as   \\n \\na      Brownian  \\n \\n  bridge.\\nFlow-based      diffusion      models      proposed      in      Lipman      et      al.\\n(2023);      Tong      et      al.\\n(2023)      share   \\n \\na       close      relationship      with      bridge-based      diffusion      models.\\nThese      flow-based      models      use   \\n \\na      target      SDE (43)      and      an      SDE      with   \\n \\na      parameter      (44)      without      stochastic      terms      (1.e.,      no      dw,)      and      aim      to      minimize  \\n \\n  the      loss      function      (45).</p><p> \\n \\n  Comparison      with      score-based      diffusion      models.\\nBoth      score-based      diffusion      models      and      bridge-  \\n \\n  based      diffusion      models      aim      to      minimize      KL      divergences.\\nDespite      its      similarity,      in      contrast      to  \\n \\n  score-based      diffusion      models,      bridge-based      diffusion      models      target      Q*°>      rather      than      the      time-  \\n \\n  reversal      SDE      Q\"™®.\\nThe      distribution      Q*”      is      often      considered      preferable      because      it      can      impose      an  \\n \\n  SDE      that      navigates      efficiently      from   \\n \\n0      to      7’      (e.g.,   \\n \\na      Brownian      bridge      with      minimal      noise).\\nThis      can  \\n \\n  expedite      the      learning      process      since      we      lack      direct      control      over      the      time-reversal      SDE      in      score-based  \\n \\n  diffusion      models.</p><h10>9.3.      Connection      with      RL-Based      Fine-Tuning</h10><p>ning  \\n \\n  RL-based      fine-tuning      shares   \\n \\na      close      relationship      with      flow-based      training.\\nTo      illustrate      this,      consider  \\n \\n  the      reference      SDE      (from      7’      to      0)      following      the      pre-trained      diffusion      model.\\nThen,      akin      to      flow-  \\n \\n  based      training,      we      consider      the      coupling      of      the      reference      SDE      conditioned      on      state      at      time   \\n \\n0       (i.e.,      distribution      over      xo-r      conditioned      on      x9)      and      the      target      distribution      at      time   \\n \\n0      (i.e.,      p,(-)   \\n \\n«       exp(r(-)/a)p?\\n\"°(-)),      denoting      the      induced      distribution      as      Q*™.</p><p>Informal      characterization      of      Q™°*.\\nInformally,      Q*\"°      is      introduced      as      follows.\\nWhen      discretizing  \\n \\n  the      time      step,      by      denoting      the      distribution      induced      by      the      reference      SDE      as      p™\\\\\\\\\\\\\\'(xp,-+-   \\n \\n,      9),      we define Qi\"      (xr,   \\n \\na   \\n \\n,      0)   \\n \\n=      p(x,      nr      Ly   \\n \\n|      L0)Pr      (Xo).</p><p>The      distribution      Q*°      above      is   \\n \\na      limiting      distribution      obtained      by      making      the      discretization      step small.</p><p>Training.\\nNow,      we      introduce   \\n \\na      new      SDE      parameterized      by   \\n \\n@      such      as      an      neural      network      to      align  \\n \\n  the      induced      distribution      P’      with      Q®\"*.\\nActually,      in      the      continuous      formulation,      the      RL-problem      (18) in      Section      4.2      is      equal      to      solving argmin      KL(P?||Q\"™).\\n \\n \\n  0 This      formalization      has      been      presented      in      Uehara      et      al.\\n(2024).\\nIntuitively,      as      we      see      in      Theorem      3,  \\n \\n  this      is      expected      because      we      see      that      the      posterior      distribution      induced      by      pre-trained      models:</p><p>ref   \\n \\n(      PD      (ar,-++      21   \\n \\n|      to)      (=      vey      (er|er_-1)p      Pp      (ar-1   \\n \\n|      er_2)      +++      pp\"      (x1   \\n \\n|      20), remains      preserved      after      fine-tuning      based      on      the      RL-formulation      (18).</p><p> \\n \\n  Comparison      with      flow-based      training.\\nIn      contrast      to      flow-based      training,      RL-based      fine-tuning  \\n \\n  minimizes      the      inverse      KL      divergence      rather      than      the      KL      divergence.\\nIntuitively,      this      is      because,  \\n \\n  unlike      Qt™*      or      Q*>   \\n \\n,      we      cannot      sample      from      Q*”      (recall      that   \\n \\na      marginal      distribution      at      time   \\n \\n0      in  \\n \\n  Qi,      i.e.,      p,,      is      unnormalized);      on      the      other      hand,      the      marginal      distributions      at      time   \\n \\n0      in      both      Q\\\\\\\\\\\\\\'™*  \\n \\n  and      Q*°°>      are      data      distributions.</p><p> \\n \\n  Remark   \\n \\n7      (Extension      to      f-divergence).\\nEach      of      these      methods      can      be      technically      extended      by  \\n \\n  incorporating      more      general      divergences      beyond      KL      divergence.\\nFor      instance,      in      the      context      of  \\n \\n  fine-tuning,      please      see      Tang      (2024).</p><p>10      Connection      with      Sampling      from      Unnormalized      Distributions</p><p> \\n \\n  Numerous      studies      delve      into      sampling      from      an      unnormalized      distribution      proportional      to      exp(r()),  \\n \\n  commonly      known      as      the      Gibbs      distribution,      extensively      discussed      in      the      computational      statistics  \\n \\n  and      statistical      physics      community      (Robert,      2014).\\nThis      issue      is      pertinent      to      our      work,      as      during  \\n \\n  fine-tuning,      the      target      distribution      is      also      formulated      in      the      Gibbs      distribution      form      in      (10),      which      is  \\n \\n  proportional      to      exp(r(a))p?\\n\"*      (x).</p><p> \\n \\n  The      literature      employs      two      main      approaches      to      tackle      this      challenge:      Markov      Chain      Monte  \\n \\n  Carlo      (MCMC)      and      RL-based      methods.\\nIn      particular,      the      RL-based      diffusion      model      fine-tuning  \\n \\n  we      have      discussed      so      far      is      closely      related      to      the      latter      approach.\\nIn      this      section,      we      explain      and  \\n \\n  compare      these      two      approaches.</p><h10>10.1      Markov      Chain      Monte      Carlo      (MCMC)</h10><p>CMC)  \\n \\n  In      MCMC,   \\n \\na      Markov      chain      is      constructed      to      approximate   \\n \\na      target      distribution      such      that      the      equilib-  \\n \\n  rium      distribution      of      the      Markov      chain      converges      to      the      target      distribution.\\nHowever,      dealing      with  \\n \\n  high-dimensional      domain      spaces      is      challenging      for      MCMC.\\n \\n \\nA      common      strategy      involves      leveraging  \\n \\n  gradient      information,      such      as      the      Metropolis-adjusted      Langevin      algorithm      (MALA)      (Besag,      1994)  \\n \\n  or      Hamiltonian      Monte      Carlo      (HMC)      (Neal      et      al.,      2011;      Girolami      and      Calderhead,      2011).\\nMALA  \\n \\n  shares      similarities      with      classifier      guidance      and      value-weighted      sampling,      as      both      methods      depend  \\n \\n  on      the      first-order      information      from      reward      (or      value)      functions.</p><p> \\n \\n  Can      we      use      MCMC      for      fine-tuning      diffusion      models?\\nIn      the      context      of      fine-tuning      dif-  \\n \\n  fusion      models,      while      it      may      seem      intuitive      to      sample      from      the      target      distribution      (10)      (i.e.,  \\n \\n  Dy   \\n \\n&      exp(r(x))pP\"*(x))      using      MCMC,      it’s      not      straightforward      for      the      following      reasons:</p><p>¢\\n \\n   We      lack      of      an      analytical      form      of      p?\\n\"*(-).<ul><li>*\\n \\n   Additionally,      even      if      we      can      estimate      p?\\n’°(-)      in      an      unbiased      manner,      as      in      Chen      et      al.\\n(2018),  \\n \\n  the      mixing      time      for      obtaining   \\n \\na      single      sample      in      MCMC      might      be      lengthy.</li></ul></p><p> \\n \\n  Therefore,      we      explore      creating   \\n \\na      generative      model      (i.e.,      diffusion      model)      for      this      task      using      RL-  \\n \\n  based      fine-tuning      so      that      we      can      easily      sample      from      p,(-)      during      inference      (i.e.,      simulating      the  \\n \\n  time-reversal      SDE      (2)      with      policies      from   \\n \\n¢   \\n \\n=      T\\\\\\\\\\\\\\'+   \\n \\n1      to   \\n \\nt   \\n \\n=      1),      without      relying      on      MCMC.\\nThis  \\n \\n  addresses      the      concern      of      MCMC      by      avoiding      the      need      to      estimate      p?\\n\"*(-)      and      minimizing      inference  \\n \\n  time,      albeit      at      the      expense      of      increased      training      time.</p><p>10.2}      RL-Based      Approaches  \\n \\n  While      MCMC      has      historically      been      widely      used      for      this      purpose,      recent      works      have      proposed      an  \\n \\n  RL-based      approach      or      its      variant      (e.g.,      Bernton      et      al.      (2019);      Heng      et      al.\\n(2020);      Huang      et      al.\\n(2023);</p><p> \\n \\n  Vargas      et      al.\\n(2023)).\\nFor      example,   \\n \\na      seminal      study      (Zhang      and      Chen,      2021)      introduced   \\n \\na      method  \\n \\n  very      similar      to      reward      backpropagation.\\nIn      their      work,      they      initially      define      Brownian      motion      as   \\n \\na       reference      SDE      (from      7’      to      0),      similar      to      the      pre-trained      diffusion      models      in      our      context.\\nThen,      by  \\n \\n  appropriately      setting      rewards,      Zhang      and      Chen      (2021)      aims      to      learn   \\n \\na      new      SDE      (from      T\\\\\\\\\\\\\\'      to      0)      such  \\n \\n  that      we      can      sample      from      the      target      distribution      at      the      endpoint      0.</p><p> \\n \\n  These      approaches      offer      advantages      over      MCMC      due      to      their      ability      to      minimize      inference      time  \\n \\n  (amortization)      by      constructing   \\n \\na      generative      model      comprising      policies      in      the      training      time      and      solely  \\n \\n  executing      learned      policies      during      inference.\\nUnlike      MCMC,      these      methods      significantly      reduce      the  \\n \\n  inference      cost      as      we      don’t      need      to      be      concerned      about      the      mixing      time.\\nAdditionally,      RL-based  \\n \\n  approaches      have      the      potential      to      effectively      handle      high-dimensional,      complex      (i.e.,      multi-modal)  \\n \\n  distributions      by      harnessing      the      considerable      expressive      power      arising      from      latent      states.</p><p> \\n \\n  Despite      these      advantages,      it      is      often      unclear      how      to      select   \\n \\na      reference      SDE      in      these      works.\\nIn  \\n \\n  contrast,      in      RL-based      fine-tuning      of      diffusion      models,      we      directly      leverage      pre-trained      diffusion  \\n \\n  models      as      the      reference      SDEs,      i.e.,      as      our      prior      knowledge.</p><p> \\n \\n  Key      message.\\nFinally,      it      is      worth      noting      that      we      can      technically      utilize      any      off-the-shelf      RL      algo-  \\n \\n  rithms      for      training.\\nFor      example,      Zhang      and      Chen      (2021)      employs      reward      backpropagation,      while  \\n \\n  works      in      Gflownets      typically      leverage      PCL,      as      we      mention      in      Section      6.3.\\nHowever,      additionally,      it  \\n \\n  is      possible      to      use      PPO      or      reward-weighted      MLE      mentioned      in      Section      4.2.</p><p>11      Closely      Related      Directions</p><p>Lastly,      we      highlight      several      closely      related      directions      we      have      not      yet      discussed.</p><p> \\n \\n  Aligning      text-to-image      models.\\nWe      recognize      that      current      efforts      in      fine-tuning      diffusion      models  \\n \\n  primarily      revolve      around      aligning      text-to-image      models      using      human      feedback      (Lee      et      al.,      2023;  \\n \\n  Wallace      et      al.,      2023;      Wu      et      al.,      2023;      Yang      et      al.,      2023).\\nIn      this      paper,      our      goal      is      to      consider   \\n \\na      more  \\n \\n  general      and      fundamental      formulation      that      is      not      tailored      to      any      particular      task.</p><p> \\n \\n  Diffusion      models      for      RL.\\nDiffusion      models      are      well-suited      for      specific      reinforcement      learning  \\n \\n  applications      due      to      their      ability      to      model      complex      and      multimodal      distributions      as      polices      (Janner  \\n \\n  et      al.,      2022;      Ajay      et      al.,      2023;      Wang      et      al.,      2022;      Hansen-Estruch      et      al.,      2023;      Du      et      al.,      2024;      Zhu  \\n \\n  et      al.,      2023).\\nFor      example,      Wang      et      al.\\n(2022)      use      conditional      diffusion      models      as      policies      and  \\n \\n  demonstrate      their      effectiveness      in      typical      offline      RL      benchmarks.\\nWhile      RL-based      fine-tuning      also  \\n \\n  aims      to      maximize      certain      reward      functions,      the      primary      focus      of      this      tutorial      covers      methods      that  \\n \\n  leverage      pre-trained      diffusion      models      for      this      purpose.</p><p> \\n \\n  RL      from      human      feedback      (RLHF)      for      language      models.\\nRLHF      is      widely      discussed      in      the  \\n \\n  context      of      language      models,      where      many      algorithms      share      similarities      with      those      used      in      diffusion  \\n \\n  models      (Ouyang      et      al.,      2022;      Bai      et      al.,      2022;      Casper      et      al.,      2023).\\nFor      instance,      both      domains  \\n \\n  commonly      employ      PPO      as   \\n \\na      standard      approach,      and      reward-weighted      training      (or      reward-weighted  \\n \\n  MLE)      often      serves      as   \\n \\na      basic      baseline      approach.\\nDespite      these      similarities,      nuanced      differences  \\n \\n  exist      mainly      because      diffusion      models      typically      operate      in   \\n \\na      continuous      input      space      rather      than  \\n \\n  a      discrete      one.\\nMoreover,      techniques      like      value-weighted      sampling      emerge      uniquely      within      the  \\n \\n  context      of      diffusion      models.</p><p>12      Summary</p><p> \\n \\n  In      this      article,      we      comprehensively      explain      how      fine-tuning      diffusion      models      to      maximize      down-  \\n \\n  stream      reward      functions      can      be      formalized      as   \\n \\na      reinforcement      learning      (RL)      problem      in      entropy-  \\n \\n  regularized      Markov      decision      processes      (MDPs).\\nBased      on      this      viewpoint,      we      elaborate      on      the  \\n \\n  application      of      various      RL      algorithms,      such      as      PPO      and      reward-weighted      MLE,      specifically      tailored  \\n \\n  for      fine-tuning      diffusion      models.\\nAdditionally,      we      categorize      different      scenarios      based      on      how  \\n \\n  reward      feedback      is      obtained.\\nWhile      this      categorization      is      not      always      explicitly      mentioned      in      many  \\n \\n  existing      works,      it      is      crucial      when      selecting      appropriate      algorithms.\\nFinally,      we      discuss      the      rela-  \\n \\n  tionships      with      several      related      topics,      including      classifier      guidance,      Gflownets,      path      integral      control  \\n \\n  theory,      and      sampling      from      unnormalized      distributions.</p><h4>References</h4><p>Agarwal,      A.,      N.      Jiang,      S.      M.      Kakade,      and      W.      Sun      (2019).\\nReinforcement      learning:      Theory      and  \\n \\n  algorithms.\\nCS      Dept.,      UW      Seattle,      Seattle,      WA,      USA,      Tech.\\nRep,      10-4.</p><p>Agarwal,      V.      and      D.      R.      Kelley      (2022).\\nThe      genetic      and      biochemical      determinants      of      mrna      degradation  \\n \\n  rates      in      mammals.\\nGenome      biology      23(1),      245.</p><p> \\n \\n  Ajay,      A.,      Y.      Du,      A.      Gupta,      J.      B.      Tenenbaum,      T.      S.      Jaakkola,      and      P.      Agrawal      (2023).\\nIs      conditional  \\n \\n  generative      modeling      all      you      need      for      decision      making?\\nIn      The      Eleventh      International      Conference  \\n \\n  on      Learning      Representations.</p><p>Albergo,      M.      S.      and      E.      Vanden-Eijnden      (2022).\\nBuilding      normalizing      flows      with      stochastic      inter-  \\n \\n  polants.\\narXiv      preprint      arXiv:2209.15571.</p><p>Anderson,      B.      D.      (1982).\\nReverse-time      diffusion      equation      models.\\nStochastic      Processes      and      their  \\n \\n  Applications      12(3),      313-326.</p><p> \\n \\n  Austin,      J.,      D.      D.      Johnson,      J.      Ho,      D.      Tarlow,      and      R.      Van      Den      Berg      (2021).\\nStructured      denoising  \\n \\n  diffusion      models      in      discrete      state-spaces.\\nAdvances      in      Neural      Information      Processing      Systems      34,  \\n \\n  17981-17993.</p><p>Avdeyey,      P.,      C.      Shi,      Y.      Tan,      K.      Dudnyk,      and      J.      Zhou      (2023).\\nDirichlet      diffusion      score      model      for  \\n \\n  biological      sequence      generation.\\narXiv      preprint      arXiv:2305.\\n10699.</p><p>Bai,      Y.,      S.      Kadavath,      S.      Kundu,      A.      Askell,      J.      Kernion,      A.      Jones,      A.      Chen,      A.      Goldie,      A.      Mirhoseini, C.      McKinnon,      et      al.\\n(2022).\\nConstitutional      ai:      Harmlessness      from      ai      feedback.\\narXiv      preprint  \\n \\n  arXiv:2212.08073.</p><p> \\n \\n  Bansal,      A.,      H.-M.\\nChu,      A.      Schwarzschild,      S.      Sengupta,      M.      Goldblum,      J.      Geiping,      and      T.      Goldstein  \\n \\n  (2023).\\nUniversal      guidance      for      diffusion      models.\\nIn      Proceedings      of      the      IEEE/CVF      Conference  \\n \\n  on      Computer      Vision      and      Pattern      Recognition,      pp.\\n843-852.</p><p>Bengio,      Y.,      S.      Lahlou,      T.      Deleu,      E.      J.      Hu,      M.      Tiwari,      and      E.      Bengio      (2023).\\nGflownet      foundations.\\n \\n \\n  Journal      of      Machine      Learning      Research      24(210),      1-55.</p><p> \\n \\n  Benton,      J.,      Y.      Shi,      V.      De      Bortoli,      G.      Deligiannidis,      and      A.      Doucet      (2024).\\nFrom      denoising      diffusions  \\n \\n  to      denoising      markov      models.\\nJournal      of      the      Royal      Statistical      Society      Series      B:      Statistical  \\n \\n  Methodology      86(2),      286-301.</p><p>Bernton,      E.,      J.      Heng,      A.      Doucet,      and      P.      E.      Jacob      (2019).\\nSchr\\\\\\\\\\\\\\\\””      odinger      bridge      samplers.\\narXiv  \\n \\n  preprint      arXiv:      1912.13170.</p><p>Besag,      J.\\n(1994).\\nComments      on      “representations      of      knowledge      in      complex      systems”      by      u.      grenander  \\n \\n  and      mi      miller.\\nJ.      Roy.\\nStatist.\\nSoc.      Ser.\\n \\n \\nB      56(591-592),      4.</p><p>Black,      K.,      M.      Janner,      Y.      Du,      I.      Kostrikov,      and      S.      Levine      (2023).\\nTraining      diffusion      models      with  \\n \\n  reinforcement      learning.\\narXiv      preprint      arXiv:2305.13301.</p><p> \\n \\n  Campbell,      A.,      J.      Benton,      V.      De      Bortoli,      T.      Rainforth,      G.      Deligiannidis,      and      A.      Doucet      (2022).\\n \\n \\nA       continuous      time      framework      for      discrete      denoising      models.\\nAdvances      in      Neural      Information  \\n \\n  Processing      Systems      35,      28266-28279.</p><p> \\n \\n  Campbell,      A.,      J.      Yim,      R.      Barzilay,      T.      Rainforth,      and      T.      Jaakkola      (2024).\\nGenerative      flows      on  \\n \\n  discrete      state-spaces:      Enabling      multimodal      flows      with      applications      to      protein      co-design.\\narXiv  \\n \\n  preprint      arXiv:2402.04997.</p><p>Cao,      H.,      C.      Tan,      Z.      Gao,      Y.      Xu,      G.      Chen,      P.-A.\\nHeng,      and      S.      Z.      Li      (2024).\\n \\n \\nA      survey      on      generative  \\n \\n  diffusion      models.\\n[EEE      Transactions      on      Knowledge      and      Data      Engineering.</p><p>Casper,      S.,      X.      Davies,      C.      Shi,      T.      K.      Gilbert,      J.      Scheurer,      J.      Rando,      R.      Freedman,      T.      Korbak, D.      Lindner,   \\n \\nP P.      Freire,      et      al.\\n(2023).\\nOpen      problems      and      fundamental      limitations      of      reinforcement  \\n \\n  learning      from      human      feedback.\\narXiv      preprint      arXiv:2307.15217.</p><p>Castillo-Hair,      S.      M.      and      G.      Seelig      (2021).\\nMachine      learning      for      designing      next-generation      mrna  \\n \\n  therapeutics.\\nAccounts      of      Chemical      Research      55(1),      24-34.</p><p>Chang,      J.      D.,      M.      Uehara,      D.      Sreenivas,      R.      Kidambi,      and      W.      Sun      (2021).\\nMitigating      covariate      shift  \\n \\n  in      imitation      learning      via      offline      data      without      great      coverage.\\narXiv      preprint      arXiv:2106.03207.</p><p>Chen,      M.,      S.      Mei,      J.      Fan,      and      M.      Wang      (2024).\\nAn      overview      of      diffusion      models:      Applications,  \\n \\n  guided      generation,      statistical      rates      and      optimization.\\narXiv      preprint      arXiv:2404.07771.</p><p>Chen,      R.      T.,      Y.      Rubanova,      J.      Bettencourt,      and      D.      K.      Duvenaud      (2018).\\nNeural      ordinary      differential  \\n \\n  equations.\\nAdvances      in      neural      information      processing      systems      31.</p><p>Chung,      H.,      J.      Kim,      M.      T.      Mccann,      M.      L.      Klasky,      and      J.      C.      Ye      (2022).\\nDiffusion      posterior      sampling  \\n \\n  for      general      noisy      inverse      problems.\\narXiv      preprint      arXiv:2209.\\n14687.</p><p> \\n \\n  Chung,      H.,      B.      Sim,      D.      Ryu,      and      J.      C.      Ye      (2022).\\nImproving      diffusion      models      for      inverse      problems  \\n \\n  using      manifold      constraints.\\nAdvances      in      Neural      Information      Processing      Systems      35,      25683-—  \\n \\n  25696.</p><p>Clark,      K.,      P.      Vicol,      K.      Swersky,      and      D.      J.      Fleet      (2023).\\nDirectly      fine-tuning      diffusion      models      on  \\n \\n  differentiable      rewards.\\narXiv      preprint      arXiv:2309.\\n17400.</p><p> \\n \\n  De      Bortoli,      V.,      E.      Mathieu,      M.      Hutchinson,      J.      Thornton,      Y.      W.      Teh,      and      A.      Doucet      (2022).\\nRieman-  \\n \\n  nian      score-based      generative      modelling.\\nAdvances      in      Neural      Information      Processing      Systems      35,  \\n \\n  2406-2422.</p><p>Deleu,      T.,      P.      Nouri,      N.      Malkin,      D.      Precup,      and      Y.      Bengio      (2024).\\nDiscrete      probabilistic      inference      as  \\n \\n  control      in      multi-path      environments.\\narXiv      preprint      arXiv:2402.10309.</p><p>Dhariwal,      P.      and      A.      Nichol      (2021).\\nDiffusion      models      beat      gans      on      image      synthesis.\\nAdvances      in  \\n \\n  neural      information      processing      systems      34,      8780-8794.</p><p> \\n \\n  Du,      Y.,      S.      Yang,      B.      Dai,      H.      Dai,      O.      Nachum,      J.      Tenenbaum,      D.      Schuurmans,      and      P.      Abbeel      (2024).\\n \\n \\n  Learning      universal      policies      via      text-guided      video      generation.\\nAdvances      in      Neural      Information  \\n \\n  Processing      Systems      36.</p><p>Fan,      Y.,      O.      Watkins,      Y.      Du,      H.      Liu,      M.      Ryu,      C.      Boutilier,      P.      Abbeel,      M.      Ghavamzadeh,      K.      Lee,      and K.      Lee      (2023) 023).\\nDPOK:      Reinforcement      learning      for      fine-tuning      text-to-image      diffusion      models.\\n \\n \\n  arXiv      preprint      arXiv:2305.16381.</p><p>Finn,      C.,      S.      Levine,      and      P.      Abbeel      (2016).\\nGuided      cost      learning:      Deep      inverse      optimal      control      via  \\n \\n  policy      optimization.\\nIn      International      conference      on      machine      learning,      pp.\\n49-58.\\nPMLR.</p><p>Fox,      R.,      A.      Pakman,      and      N.      Tishby      (2015).\\nTaming      the      noise      in      reinforcement      learning      via      soft  \\n \\n  updates.\\narXiv      preprint      arXiv:      1512.08562.</p><p>Frey,      N.      C.,      D.      Berenberg,      K.      Zadorozhny,      J.      Kleinhenz,      J.      Lafrance-Vanasse,      I.      Hotzel,      Y.      Wu, S.      Ra,   \\n \\nR R.      Bonneau,      K.      Cho,      et      al.\\n(2023).\\nProtein      discovery      with      discrete      walk-jump      sampling.\\n \\n \\n  arXiv      preprint      arXiv:2306.\\n12360.</p><p>Geist,      M.,      B.      Scherrer,      and      O.      Pietquin      (2019).\\n \\n \\nA      theory      of      regularized      markov      decision      processes.\\n \\n \\n  In      International      Conference      on      Machine      Learning,      pp.\\n2160-2169.\\nPMLR.</p><p> \\n \\n  Girolami,      M.      and      B.      Calderhead      (2011).\\nRiemann      manifold      langevin      and      hamiltonian      monte  \\n \\n  carlo      methods.\\nJournal      of      the      Royal      Statistical      Society      Series      B:      Statistical      Methodology      73(2),  \\n \\n  123-214.</p><p>Gosai,      S.      J.,      R.      I.      Castro,      N.      Fuentes,      J.      C.      Butts,      S.      Kales,      R.      R.      Noche,      K.      Mouri,      P.      C.      Sabeti, S.   \\n \\nK K.      Reilly,      and      R.      Tewhey      (2023).\\nMachine-guided      design      of      synthetic      cell      type-specific  \\n \\n  cis-regulatory      elements.\\nbioRxiv.</p><p>Haarnoja,      T.,      H.      Tang,      P.      Abbeel,      and      S.      Levine      (2017).\\nReinforcement      learning      with      deep      energy-  \\n \\n  based      policies.\\nIn      International      conference      on      machine      learning,      pp.\\n1352-1361.\\nPMLR.</p><p>Hansen-Estruch,      P.,      I.      Kostrikov,      M.      Janner,      J.      G.      Kuba,      and      S.      Levine      (2023).\\nIdql:      Implicit  \\n \\n  q-learning      as      an      actor-critic      method      with      diffusion      policies.\\narXiv      preprint      arXiv:2304.10573.</p><p>Heng,      J.,      A.      N.      Bishop,      G.      Deligiannidis,      and      A.      Doucet      (2020).\\nControlled      sequential      monte      carlo.</p><p>Ho,      J.,      A.      Jain,      and      P.      Abbeel      (2020).\\nDenoising      diffusion      probabilistic      models.\\nAdvances      in      neural  \\n \\n  information      processing      systems      33,      6840-6851.</p><p>Ho,      J.      and      T.      Salimans      (2022).\\nClassifier-free      diffusion      guidance.\\narXiv      preprint      arXiv:2207.12598.</p><p>Ho,      J.,      T.      Salimans,      A.      Gritsenko,      W.      Chan,      M.      Norouzi,      and      D.      J.      Fleet      (2022).\\nVideo      diffusion  \\n \\n  models.\\nAdvances      in      Neural      Information      Processing      Systems      35,      8633-8646.</p><p> \\n \\n  Hoogeboom,      E.,      V.      G.      Satorras,      C.      Vignac,      and      M.      Welling      (2022).\\nEquivariant      diffusion      for  \\n \\n  molecule      generation      in      3d.\\nIn      International      conference      on      machine      learning,      pp.\\n8867-8887.</p><h4>      PMLR.</h4><p>Huang,      X.,      H.      Dong,      H.      Yifan,      Y.      Ma,      and      T.      Zhang      (2023).\\nReverse      diffusion      monte      carlo.\\nIn      The  \\n \\n  Twelfth      International      Conference      on      Learning      Representations.</p><p>Janner,      M.,      Y.      Du,      J.      B.      Tenenbaum,      and      S.      Levine      (2022).\\nPlanning      with      diffusion      for      flexible  \\n \\n  behavior      synthesis.\\narXiv      preprint      arXiv:2205.09991.</p><p> \\n \\n  Jo,      J.,      S.      Lee,      and      S.      J.      Hwang      (2022).\\nScore-based      generative      modeling      of      graphs      via      the      system  \\n \\n  of      stochastic      differential      equations.\\nIn      International      Conference      on      Machine      Learning,      pp.\\n \\n \\n  10362-10383.\\nPMLR.</p><p> \\n \\n  Kazim,      M.,      J.      Hong,      M.-G.\\nKim,      and      K.-K.\\nK.      Kim      (2024).\\nRecent      advances      in      path      integral      control  \\n \\n  for      trajectory      optimization:      An      overview      in      theoretical      and      algorithmic      perspectives.\\nAnnual  \\n \\n  Reviews      in      Control      57,      100931.</p><p>Krishnamoorthy,      S.,      S.      M.      Mashkaria,      and      A.      Grover      (2023).\\nDiffusion      models      for      black-box  \\n \\n  optimization.\\narXiv      preprint      arXiv:2306.07180.</p><p>Kumar,      A.,      A.      Zhou,      G.      Tucker,      and      S.      Levine      (2020).\\nConservative      q-learning      for      offline      reinforce-  \\n \\n  ment      learning.\\nAdvances      in      Neural      Information      Processing      Systems      33,      1179-1191.</p><p>Lal,      A.,      D.      Garfield,      T.      Biancalani,      and      G.      Eraslan      (2024).\\nreglm:      Designing      realistic      regulatory  \\n \\n  dna      with      autoregressive      language      models.\\nbioRxiv,      2024-02.</p><p>Lee,      K.,      H.      Liu,      M.      Ryu,      O.      Watkins,      Y.      Du,      C.      Boutilier,      P.      Abbeel,      M.      Ghavamzadeh,      and      S.      S.      Gu  \\n \\n  (2023).\\nAligning      text-to-image      models      using      human      feedback.\\narXiv      preprint      arXiv:2302.12192.</p><p>Levine,      S.      (2018).\\nReinforcement      learning      and      control      as      probabilistic      inference:      Tutorial      and  \\n \\n  review.\\narXiv      preprint      arXiv:      1805.00909.</p><p>Li,      Z.,      Y.      Ni,      T.      A.      B.      Huygelen,      A.      Das,      G.      Xia,      G.-B.\\nStan,      and      Y.      Zhao      (2023).\\nLatent      diffusion  \\n \\n  model      for      dna      sequence      generation.\\narXiv      preprint      arXiv:2310.06150.</p><p>Lipman,      Y.,      R.      T.      Chen,      H.      Ben-Hamu,      M.      Nickel,      and      M.      Le      (2023).\\nFlow      matching      for      generative  \\n \\n  modeling.\\nJCLR      2023.</p><p>Liu,      X.,      C.      Gong,      and      Q.      Liu      (2022).\\nFlow      straight      and      fast:      Learning      to      generate      and      transfer      data  \\n \\n  with      rectified      flow.\\narXiv      preprint      arXiv:2209.03003.</p><p>Liu,      X.,      L.      Wu,      M.      Ye,      and      Q.      Liu      (2022).\\nLet      us      build      bridges:      Understanding      and      extending  \\n \\n  diffusion      generative      models.\\narXiv      preprint      arXiv:2208.14699.</p><p>Lou,      A.,      C.      Meng,      and      S.      Ermon      (2023).\\nDiscrete      diffusion      language      modeling      by      estimating      the  \\n \\n  ratios      of      the      data      distribution.\\narXiv      preprint      arXiv:2310.16834.</p><p>Malkin,      N.,      M.      Jain,      E.      Bengio,      C.      Sun,      and      Y.      Bengio      (2022).\\nTrajectory      balance:      Improved      credit  \\n \\n  assignment      in      gflownets.\\nAdvances      in      Neural      Information      Processing      Systems      35,      5955-5967.</p><p>Mohammadpour,      S.,      E.      Bengio,      E.      Frejinger,      and      P.-L.\\nBacon      (2023).\\nMaximum      entropy      gflownets  \\n \\n  with      soft      q-learning.\\narXiv      preprint      arXiv:2312.14331.</p><p>Nachum,      O.,      M.      Norouzi,      K.      Xu,      and      D.      Schuurmans      (2017).\\nBridging      the      gap      between      value      and  \\n \\n  policy      based      reinforcement      learning.\\nAdvances      in      neural      information      processing      systems      30.</p><p>Neal,      R.      M.      et      al.\\n(2011).\\nMcmce      using      hamiltonian      dynamics.\\nHandbook      of      markov      chain      monte  \\n \\n  carlo      2(11),      2.</p><p>Neu,      G.,      A.      Jonsson,      and      V.      Gémez      (2017).\\n \\n \\nA      unified      view      of      entropy-regularized      markov      decision  \\n \\n  processes.\\narXiv      preprint      arXiv:      1705.07798.</p><p>Ouyang,      L.,      J.      Wu,      X.      Jiang,      D.      Almeida,      C.      Wainwright,      P.      Mishkin,      C.      Zhang,      S.      Agarwal, K.      Slama,      A A.      Ray,      et      al.\\n(2022).\\nTraining      language      models      to      follow      instructions      with      human  \\n \\n  feedback.\\nAdvances      in      Neural      Information      Processing      Systems      35,      27730-27744.</p><p>Pattanaik,      L.      and      C.      W.      Coley      (2020).\\nMolecular      representation:      going      long      on      fingerprints.\\n \\n \\n  Chem      6(6),      1204-1207.</p><p>Peng,      X.      B.,      A.      Kumar,      G.      Zhang,      and      S.      Levine      (2019).\\nAdvantage-weighted      regression:      Simple  \\n \\n  and      scalable      off-policy      reinforcement      learning.\\narXiv      preprint      arXiv:1910.00177.</p><p>Peters,      J.,      K.      Mulling,      and      Y.      Altun      (2010).\\nRelative      entropy      policy      search.\\nIn      Proceedings      of      the  \\n \\n  AAAI      Conference      on      Artificial      Intelligence,      Volume      24,      pp.\\n1607-1612.</p><p> \\n \\n  Podell,      D.,      Z.      English,      K.      Lacey,      A.      Blattmann,      T.      Dockhorn,      J.      Miiller,      J.      Penna,      and      R.      Rombach  \\n \\n  (2023).\\nSdxl:      Improving      latent      diffusion      models      for      high-resolution      image      synthesis.\\narXiv  \\n \\n  preprint      arXiv:2307.01952.</p><p>Prabhudesai,      M.,      A.      Goyal,      D.      Pathak,      and      K.      Fragkiadaki      (2023).\\nAligning      text-to-image      diffusion  \\n \\n  models      with      reward      backpropagation.\\narXiv      preprint      arXiv:2310.03739.</p><p>Ramesh,      A.,      P.      Dhariwal,      A.      Nichol,      C.      Chu,      and      M.      Chen      (2022).\\nHierarchical      text-conditional  \\n \\n  image      generation      with      clip      latents.\\narXiv      preprint      arXiv:2204.06125      1(2),      3.</p><p>Rigter,      M.,      B.      Lacerda,      and      N.      Hawes      (2022).\\nRambo-rl:      Robust      adversarial      model-based      offline  \\n \\n  reinforcement      learning.\\nAdvances      in      neural      information      processing      systems      35,      16082-16097.</p><p>Robert,      C.      (2014).\\nStatistical      modeling      and      computation.</p><p>Rogers,      L.      C.      G.      and      D.      Williams      (2000).\\nDiffusions,      Markov      processes      and      martingales:      Volume      2,  \\n \\n  It6é      calculus,      Volume      2.\\nCambridge      university      press.</p><p> \\n \\n  Rombach,      R.,      A.      Blattmann,      D.      Lorenz,      P.      Esser,      and      B.      Ommer      (2022,      June).\\nHigh-resolution  \\n \\n  image      synthesis      with      latent      diffusion      models.\\nIn      Proceedings      of      the      IEEE/CVF      Conference      on  \\n \\n  Computer      Vision      and      Pattern      Recognition      (CVPR),      pp.\\n10684-10695.</p><p>Sarkar,      A.,      Z.      Tang,      C.      Zhao,      and      P.      Koo      (2024).\\nDesigning      dna      with      tunable      regulatory      activity  \\n \\n  using      discrete      diffusion.\\nbioRxiv,      2024-05.</p><p>Schulman,      J.,      X.      Chen,      and      P.      Abbeel      (2017).\\nEquivalence      between      policy      gradients      and      soft  \\n \\n  q-learning.\\narXiv      preprint      arXiv:      1704.06440.</p><p>Schulman,      J.,      S.      Levine,      P.      Abbeel,      M.      Jordan,      and      P.      Moritz      (2015).\\nTrust      region      policy      optimization.\\n \\n \\n  In      International      conference      on      machine      learning,      pp.\\n1889-1897.\\nPMLR.</p><p>Schulman,      J.,      F.      Wolski,      P.      Dhariwal,      A.      Radford,      and      O.      Klimov      (2017).\\nProximal      policy      optimiza-  \\n \\n  tion      algorithms.\\narXiv      preprint      arXiv:      1707.06347.</p><p>Shi,      Y.,      V.      De      Bortoli,      A.      Campbell,      and      A.      Doucet      (2024).\\nDiffusion      schrédinger      bridge      matching.\\n \\n \\n  Advances      in      Neural      Information      Processing      Systems      36.</p><p> \\n \\n  Sohl-Dickstein,      J.,      E.      Weiss,      N.      Maheswaranathan,      and      S.      Ganguli      (2015).\\nDeep      unsupervised  \\n \\n  learning      using      nonequilibrium      thermodynamics.\\nIn      /nternational      conference      on      machine      learning,  \\n \\n  pp.\\n2256-2265.\\nPMLR.</p><p>Song,      J.,      C.      Meng,      and      S.      Ermon      (2020).\\nDenoising      diffusion      implicit      models.\\narXiv      preprint  \\n \\n  arXiv:2010.02502.</p><p>Song,      Y.,      C.      Durkan,      I.      Murray,      and      S.      Ermon      (2021).\\nMaximum      likelihood      training      of      score-based  \\n \\n  diffusion      models.\\nAdvances      in      neural      information      processing      systems      34,      1415-1428.</p><p>Stark,      H.,      B.      Jing,      C.      Wang,      G.      Corso,      B.      Berger,      R.      Barzilay,      and      T.      Jaakkola      (2024).\\nDirichlet  \\n \\n  flow      matching      with      applications      to      dna      sequence      design.\\narXiv      preprint      arXiv:2402.05841.</p><p>Sutton,      R.      S.      and      A.      G.      Barto      (2018).\\nReinforcement      learning:      An      introduction.\\nMIT      press.</p><p>Tang,      W.      (2024).\\nFine-tuning      of      diffusion      models      via      stochastic      control:      entropy      regularization      and  \\n \\n  beyond.\\narXiv      preprint      arXiv:2403.06279.</p><p>Tang,      W.      and      H.      Zhao      (2024).\\nScore-based      diffusion      models      via      stochastic      differential      equations—a  \\n \\n  technical      tutorial.\\narXiv      preprint      arXiv:2402.07487.</p><p>Theodorou,      E.,      J.      Buchli,      and      S.      Schaal      (2010).\\n \\n \\nA      generalized      path      integral      control      approach      to  \\n \\n  reinforcement      learning.\\nThe      Journal      of      Machine      Learning      Research      11,      3137-3181.</p><p>Tiapkin,      D.,      N.      Morozov,      A.      Naumov,      and      D.      Vetrov      (2023).\\nGenerative      flow      networks      as      entropy-  \\n \\n  regularized      rl.\\narXiv      preprint      arXiv:2310.12934.</p><p> \\n \\n  Tong,      A.,      N.      Malkin,      G.      Huguet,      Y.      Zhang,      J.      Rector-Brooks,      K.      Fatras,      G.      Wolf,      and      Y.      Bengio  \\n \\n  (2023).\\nConditional      flow      matching:      Simulation-free      dynamic      optimal      transport.\\narXiv      preprint  \\n \\n  arXiv:2302.00482.</p><p>Uehara,      M.      and      W.      Sun      (2021).\\nPessimistic      model-based      offline      reinforcement      learning      under  \\n \\n  partial      coverage.\\narXiv      preprint      arXiv:2107.06226.</p><p>Uehara,      M.,      Y.      Zhao,      K.      Black,      E.      Hajiramezanali,      G.      Scalia,      N.      L.      Diamant,      A.      M.      Tseng, T.      Biancalani,      and      S S.      Levine      (2024).\\nFine-tuning      of      continuous-time      diffusion      models      as      entropy-  \\n \\n  regularized      control.\\narXiv      preprint      arXiv:2402.15194.</p><p> \\n \\n  Uehara,      M.,      Y.      Zhao,      K.      Black,      E.      Hajiramezanali,      G.      Scalia,      N.      L.      Diamant,      A.      M.      Tseng,      S.      Levine,  \\n \\n  and      T.      Biancalani      (2024).\\nFeedback      efficient      online      fine-tuning      of      diffusion      models.\\narXiv      preprint  \\n \\n  arXiv:2402.\\n16359.</p><p> \\n \\n  Uehara,      M.,      Y.      Zhao,      E.      Hajiramezanali,      G.      Scalia,      G.      Eraslan,      A.      Lal,      S.      Levine,      and      T.      Biancalani  \\n \\n  (2024).\\nBridging      model-based      optimization      and      generative      modeling      via      conservative      fine-tuning  \\n \\n  of      diffusion      models.\\narXiv      preprint      arXiv:2405.19673.</p><p>Vargas,      F.,      W.      Grathwohl,      and      A.      Doucet      (2023).\\nDenoising      diffusion      samplers.\\narXiv      preprint  \\n \\n  arXiv:2302.\\n13834.</p><p> \\n \\n  Wallace,      B.,      M.      Dang,      R.      Rafailov,      L.      Zhou,      A.      Lou,      S.      Purushwalkam,      S.      Ermon,      C.      Xiong,      S.      Joty,  \\n \\n  and      N.      Naik      (2023).\\nDiffusion      model      alignment      using      direct      preference      optimization.\\narXiv  \\n \\n  preprint      arXiv:2311.12908.</p><p>Wang,      Z.,      J.      J.      Hunt,      and      M.      Zhou      (2022).\\nDiffusion      policies      as      an      expressive      policy      class      for      offline  \\n \\n  reinforcement      learning.\\narXiv      preprint      arXiv:2208.06193.</p><p>Widatalla,      T.,      R.      Rafailov,      and      B.      Hie      (2024).\\nAligning      protein      generative      models      with      experimental  \\n \\n  fitness      via      direct      preference      optimization.\\nbioRxiv,      2024-05.</p><p>Williams,      G.,      A.      Aldrich,      and      E.      A.      Theodorou      (2017).\\nModel      predictive      path      integral      control:      From  \\n \\n  theory      to      parallel      computation.\\nJournal      of      Guidance,      Control,      and      Dynamics      40(2),      344-357.</p><p>Wu,      X.,      K.      Sun,      F.      Zhu,      R.      Zhao,      and      H.      Li      (2023).\\nBetter      aligning      text-to-image      models      with  \\n \\n  human      preference.\\narXiv      preprint      arXiv:2303.14420.</p><p>Wu,      Y.,      G.      Tucker,      and      O.      Nachum      (2019).\\nBehavior      regularized      offline      reinforcement      learning.\\n \\n \\n  arXiv      preprint      arXiv:      1911.11361.</p><p>Wulfmeier,      M.,      P.      Ondruska,      and      I.      Posner      (2015).\\nMaximum      entropy      deep      inverse      reinforcement  \\n \\n  learning.\\narXiv      preprint      arXiv:      1507.04888.</p><p>Xie,      T.,      C.-A.\\nCheng,      N.      Jiang,      P.      Mineiro,      and      A.      Agarwal      (2021).\\nBellman-consistent      pessimism  \\n \\n  for      offline      reinforcement      learning.\\nAdvances      in      neural      information      processing      systems      34.</p><p>Xu,      M.,      L.      Yu,      Y.      Song,      C.      Shi,      S.      Ermon,      and      J.      Tang      (2022).\\nGeodiff:   \\n \\nA      geometric      diffusion  \\n \\n  model      for      molecular      conformation      generation.\\narXiv      preprint      arXiv:2203.02923.</p><p> \\n \\n  Yang,      L.,      Z.      Zhang,      Y.      Song,      S.      Hong,      R.      Xu,      Y.      Zhao,      W.      Zhang,      B.      Cui,      and      M.-H.\\nYang      (2023).\\n \\n \\n  Diffusion      models:   \\n \\nA      comprehensive      survey      of      methods      and      applications.\\nACM      Computing  \\n \\n  Surveys      56(4),      1-39.</p><p> \\n \\n  Yu,      T.,      G.      Thomas,      L.      Yu,      S.      Ermon,      J.      Y.      Zou,      S.      Levine,      C.      Finn,      and      T.      Ma      (2020).\\nMopo:  \\n \\n  Model-based      offline      policy      optimization.\\nAdvances      in      Neural      Information      Processing      Systems      33,  \\n \\n  14129-14142.</p><p> \\n \\n  Yuan,      H.,      K.      Huang,      C.      Ni,      M.      Chen,      and      M.      Wang      (2023).\\nReward-directed      conditional      diffusion:  \\n \\n  Provable      distribution      estimation      and      reward      improvement.\\nIn      Thirty-seventh      Conference      on  \\n \\n  Neural      Information      Processing      Systems.</p><p>Zhang,      H.      and      T.      Xu      (2023).\\nTowards      controllable      diffusion      models      via      reward-guided      exploration.\\n \\n \\n  arXiv      preprint      arXiv:2304.07132.</p><p>Zhang,      Q.      and      Y.      Chen      (2021).\\nPath      integral      sampler:   \\n \\na      stochastic      control      approach      for      sampling.\\n \\n \\n  arXiv      preprint      arXiv:2111,15141.</p><p> \\n \\n  Zhao,      Y.,      M.      Uehara,      G.      Scalia,      T.      Biancalani,      S.      Levine,      and      E.      Hajiramezanali      (2024).\\n \\n \\n  Adding      conditional      control      to      diffusion      models      with      reinforcement      learning.\\narXiv      preprint  \\n \\n  arXiv:2406.\\n12120.</p><p>Zhou,      Z.,      S.      Kearnes,      L.      Li,      R.      N.      Zare,      and      P.      Riley      (2019).\\nOptimization      of      molecules      via      deep  \\n \\n  reinforcement      learning.\\nScientific      reports      9(1),      10752.</p><p>Zhu,      Z.,      H.      Zhao,      H.      He,      Y.      Zhong,      S.      Zhang,      Y.      Yu,      and      W.      Zhang      (2023).\\nDiffusion      models      for  \\n \\n  reinforcement      learning:   \\n \\nA      survey.\\narXiv      preprint      arXiv:2311.01223.</p><p>Ziebart,      B.      D.,      A.      L.      Maas,      J.      A.      Bagnell,      A.      K.      Dey,      et      al.\\n(2008).\\nMaximum      entropy      inverse  \\n \\n  reinforcement      learning.\\nIn      Aaai,      Volume      8,      pp.\\n1433-1438.\\nChicago,      IL,      USA.</p><h3>Hence,</h3><p> \\n \\n \\n_   \\n \\n:      (xt)      Up-1(Lt-1)   \\n \\n°       vu   \\n \\n=      argmin      Ey,.u,   \\n \\n|   \\n \\n4      ——   \\n \\n—      log   \\n \\n|      exp   \\n \\n|      ————   \\n \\n]      Dpre(@e-1]      21)      da4_1  \\n \\n  a   \\n \\na h:X->R  \\n \\n  where      u,   \\n \\n€      A(%)      is      any      roll-in      distribution      that      covers      the      entire      space      V.      Using      this      relation      and  \\n \\n  replacing      the      expectation      E,,,.,,,      with      empirical      approximation,      we      are      able      to      estimate      soft      value  \\n \\n  functions      v;      in   \\n \\na      recursive      manner:</p><p> \\n \\n \\n1   \\n \\nm      ~(j—1)   \\n \\n°       aj   \\n \\n;   \\n \\ni      Ur      Ut   \\n \\ny       {6      (x)},   \\n \\n—       argmin   \\n \\ny      S      {ht   \\n \\n)   \\n \\n—      log   \\n \\n/      exp      (eee)      Ppre(@1—1|}      yan] h:      [R¢,[0,7]]|-R      t=T+1      i=1</p><p>where      the      data      is      collected      in      an      off-policy      way.</p><p> \\n \\n  Remark      6.\\nAlthough      soft      Q-learning      is      widely      used      in      standard      RL      (Schulman      et      al.,      2017),      it  \\n \\n  cannot      be      directly      applied      to      our      fine-tuning      context      without      resorting      to      value-weighted      sampling  \\n \\n  or      value-weighted      MLE.\\nThis      is      because,      even      if      we      estimate      soft-value      functions      as      %;,      substituting  \\n \\n  Uz      with      0,      in      the      soft-optimal      policy      results      in      an      unnormalized      policy.<ul><li>6.2.2.\\nApproximation      using      Tweedie’s      formula</li></ul></p><p> \\n \\n  So      far,      we      have      explained      two      approaches:   \\n \\na      Monte      Carlo      approach      and   \\n \\na      value      iteration      approach      to  \\n \\n  estimate      soft      value      functions.\\nHowever,      learning      value      functions      in      (28)      can      still      be      often      challenging  \\n \\n  in      practice.\\nTherefore,      we      can      employ      approximation      strategies      inspired      by      recent      literature      on  \\n \\n  classifier      guidance      (e.g.,      reconstruction      guidance      (Ho      et      al.,      2022),      manifold      constrained      gradients  \\n \\n  (Chung      et      al.,      2022),      universal      guidance      (Bansal      et      al.,      2023),      and      diffusion      posterior      sampling  \\n \\n  (Chung      et      al.,      2022)).</p><p> \\n \\n  Specifically,      we      adopt      the      following      approximation:</p><p>ur(xz)   \\n \\n=      alog      Egpry      lex      (“)      oy   \\n \\n=      alog      (/      exp      (om)      p      (colar      (30)</p><p>~\\n \\n   alog      (exw      (ceo)   \\n \\n)   \\n \\n»       £o(t1)   \\n \\n=      Egprey|xo   \\n \\n|      xe],      (31) a\\n \\n    =\\n \\n   1r(Xo(2t)).</p><p> \\n \\n  Here,      we      replace      the      integration      in      (30)      with   \\n \\na      Dirac      delta      distribution      with      the      posterior      mean.\\n \\n \\n  Importantly,      we      can      calculate      Zo      (x;)   \\n \\n=      E,»*-[xo   \\n \\n|      x4]      using      the      pre-trained      (score-based)      diffusion  \\n \\n  model      based      on      Tweedie’s      formula:</p><table><tr><td colSpan=1>      O12</td><td colSpan=1><p>]\\n \\n \\n  E,pre      [xo   \\n \\n|      x1   \\n \\n—      Ut   \\n \\n+      {or}      V      0g      Ge(&e)  \\n \\n  Le</p></td></tr></table><p>Recall      that      the      notation      p/?,      0?,      q,      are      defined      in      (3).\\nFinally,      by      recalling      V      log   \\n \\n@   \\n \\n=      Si      ore      (x;,t)      in  \\n \\n  score-based      diffusion      models,      we      can      approximate      Vv;(a)      with</p><h4>Le</h4><p> \\n \\n  and      plug      it      into      Algorithm   \\n \\n4      (i.e.,      value-weighted      sampling).\\n \\n \\n  As      mentioned      earlier,      this      approach      has      been      practically      widely      used      in      the      context      of      classifier  \\n \\n  guidance.\\nDespite      its      simplicity,      the      approximation      error      can      be      significant,      as      it      can      not      be  \\n \\n  diminished      even      with   \\n \\na      large      sample      size      because      the      discrepancy      from      (30)      to      (31)      is      not      merely   \\n \\na       statistical      error      that      diminishes      with      increasing      sample      size.</p><li>6.2.3.\\nZeroth-Order      Guidance      using      Path      Integral      Control</li><p> \\n \\n  In      the      previous      subsection      (Section      6.2.2),      we      discussed      an      approximation      technique      to      bypass      the  \\n \\n  necessity      of      learning      explicit      value      functions.\\nAnother      approach      to      bypass      this      requirement      is      to      use  \\n \\n  control-based      techniques      for      obtaining      soft-optimal      policies,      a.k.a.,      path      integral      control      (Theodorou  \\n \\n  et      al.,      2010;      Williams      et      al.,      2017;      Kazim      et      al.,      2024).</p><p> \\n \\n  Recall      that      we      have      p(x;,t;      6)   \\n \\n=      2;   \\n \\n+      (dt)g(x+,      t)      and      o°(t)   \\n \\n=      g(t)(dt),      the      motivation      behind this      approach      is      as      follows.\\nInitially,      we      have:</p><h5>Vo,      exp(v(#1)/@)</h5><p>=\\n \\n   Egppreyexp(vr_1      (22-1)      /@)   \\n \\n|      24]      (Soft      Bellman      equation) _\\n \\n   (1   \\n \\n—   \\n \\nx   \\n \\n—      (Ot)      G(a2,      t))?\\n=\\n \\n   Vz»,   \\n \\n{   \\n \\n[      explatena/ay)      exp      (-      0-5a(t)      (ot)   \\n \\n)      in,-}</p><p>=\\n \\n   Byes      [ese      a(era/a))      {ea      ~~      (61)g(nn,)}      EE      Cah</p><p>~\\n \\n   arp      Buty      lexr(era(ea/a))      Gera   \\n \\n—      21   \\n \\n~      (8t)ale      plod</p><p>_t   \\n \\n1       Ht)      ot)      Eg,prey      [E      ppp}      [exp(r(2o)/a)   \\n \\n|      T-1]      e|24|   \\n \\n=      Woon)      [exp(r(xo/a))e|x¢]      ;</p><h5>Now,      we      obtain:</h5><p>o7(t)Vnv(ae)   \\n \\n1      Eqppry      lexp(r(wo)/a)      eel    \\n \\nQ   \\n \\n~   \\n \\na   \\n \\n*      Egyprey      lexp(r(2o)/a))      |x]      (32)</p><p> \\n \\n  Importantly,      this      approach      does      not      require      any      differentiation.\\nTherefore,      by      running      policies  \\n \\n  from      pre-trained      models      and      simply      using      Monte      Carlo      approximation      in      both      the      denominator  \\n \\n  and      numerator,      we      can      estimate      the      above      quantity      (the      right-hand      side      of      (32))      without      making  \\n \\n  any      models      (classifiers),      unlike      classifier      guidance.\\nNote      while      this      approach      is      widely      used      in      the  \\n \\n  control      community,      it      may      not      be      feasible      for      diffusion      models      due      to      the      high      dimensionality      of  \\n \\n  the      input      space.</p><h4>Le</h4><p> \\n \\n  and      plug      it      into      Algorithm   \\n \\n4      (i.e.,      value-weighted      sampling).\\n \\n \\n  As      mentioned      earlier,      this      approach      has      been      practically      widely      used      in      the      context      of      classifier  \\n \\n  guidance.\\nDespite      its      simplicity,      the      approximation      error      can      be      significant,      as      it      can      not      be  \\n \\n  diminished      even      with   \\n \\na      large      sample      size      because      the      discrepancy      from      (30)      to      (31)      is      not      merely   \\n \\na       statistical      error      that      diminishes      with      increasing      sample      size.</p><li>6.2.3.\\nZeroth-Order      Guidance      using      Path      Integral      Control</li><p> \\n \\n  In      the      previous      subsection      (Section      6.2.2),      we      discussed      an      approximation      technique      to      bypass      the  \\n \\n  necessity      of      learning      explicit      value      functions.\\nAnother      approach      to      bypass      this      requirement      is      to      use  \\n \\n  control-based      techniques      for      obtaining      soft-optimal      policies,      a.k.a.,      path      integral      control      (Theodorou  \\n \\n  et      al.,      2010;      Williams      et      al.,      2017;      Kazim      et      al.,      2024).</p><p> \\n \\n  Recall      that      we      have      p(x;,t;      6)   \\n \\n=      2;   \\n \\n+      (dt)g(x+,      t)      and      o°(t)   \\n \\n=      g(t)(dt),      the      motivation      behind this      approach      is      as      follows.\\nInitially,      we      have:</p><h5>Vo,      exp(v(#1)/@)</h5><p>=\\n \\n   Egppreyexp(vr_1      (22-1)      /@)   \\n \\n|      24]      (Soft      Bellman      equation) _\\n \\n   (1   \\n \\n—   \\n \\nx   \\n \\n—      (Ot)      G(a2,      t))?\\n=\\n \\n   Vz»,   \\n \\n{   \\n \\n[      explatena/ay)      exp      (-      0-5a(t)      (ot)   \\n \\n)      in,-}</p><p>=\\n \\n   Byes      [ese      a(era/a))      {ea      ~~      (61)g(nn,)}      EE      Cah</p><p>~\\n \\n   arp      Buty      lexr(era(ea/a))      Gera   \\n \\n—      21   \\n \\n~      (8t)ale      plod</p><p>_t   \\n \\n1       Ht)      ot)      Eg,prey      [E      ppp}      [exp(r(2o)/a)   \\n \\n|      T-1]      e|24|   \\n \\n=      Woon)      [exp(r(xo/a))e|x¢]      ;</p><h5>Now,      we      obtain:</h5><p>o7(t)Vnv(ae)   \\n \\n1      Eqppry      lexp(r(wo)/a)      eel    \\n \\nQ   \\n \\n~   \\n \\na   \\n \\n*      Egyprey      lexp(r(2o)/a))      |x]      (32)</p><p> \\n \\n  Importantly,      this      approach      does      not      require      any      differentiation.\\nTherefore,      by      running      policies  \\n \\n  from      pre-trained      models      and      simply      using      Monte      Carlo      approximation      in      both      the      denominator  \\n \\n  and      numerator,      we      can      estimate      the      above      quantity      (the      right-hand      side      of      (32))      without      making  \\n \\n  any      models      (classifiers),      unlike      classifier      guidance.\\nNote      while      this      approach      is      widely      used      in      the  \\n \\n  control      community,      it      may      not      be      feasible      for      diffusion      models      due      to      the      high      dimensionality      of  \\n \\n  the      input      space.</p><h5>Vo,      exp(v(#1)/@)</h5><p>=\\n \\n   Egppreyexp(vr_1      (22-1)      /@)   \\n \\n|      24]      (Soft      Bellman      equation) _\\n \\n   (1   \\n \\n—   \\n \\nx   \\n \\n—      (Ot)      G(a2,      t))?\\n=\\n \\n   Vz»,   \\n \\n{   \\n \\n[      explatena/ay)      exp      (-      0-5a(t)      (ot)   \\n \\n)      in,-}</p><p>=\\n \\n   Byes      [ese      a(era/a))      {ea      ~~      (61)g(nn,)}      EE      Cah</p><p>~\\n \\n   arp      Buty      lexr(era(ea/a))      Gera   \\n \\n—      21   \\n \\n~      (8t)ale      plod</p><p>_t   \\n \\n1       Ht)      ot)      Eg,prey      [E      ppp}      [exp(r(2o)/a)   \\n \\n|      T-1]      e|24|   \\n \\n=      Woon)      [exp(r(xo/a))e|x¢]      ;</p><h5>Now,      we      obtain:</h5><p>o7(t)Vnv(ae)   \\n \\n1      Eqppry      lexp(r(wo)/a)      eel    \\n \\nQ   \\n \\n~   \\n \\na   \\n \\n*      Egyprey      lexp(r(2o)/a))      |x]      (32)</p><p> \\n \\n  Importantly,      this      approach      does      not      require      any      differentiation.\\nTherefore,      by      running      policies  \\n \\n  from      pre-trained      models      and      simply      using      Monte      Carlo      approximation      in      both      the      denominator  \\n \\n  and      numerator,      we      can      estimate      the      above      quantity      (the      right-hand      side      of      (32))      without      making  \\n \\n  any      models      (classifiers),      unlike      classifier      guidance.\\nNote      while      this      approach      is      widely      used      in      the  \\n \\n  control      community,      it      may      not      be      feasible      for      diffusion      models      due      to      the      high      dimensionality      of  \\n \\n  the      input      space.</p><h3>6.3      Path      Consistency      Learning      (Losses      Often      Used      in      Gflownets)</h3><p>ets)  \\n \\n  Now,      we      explain      how      to      apply      path      consistency      learning      (PCL)      (Nachum      et      al.,      2017)      to      fine-  \\n \\n  tune      diffusion      models.\\nIn      the      Gflownets      literature      (Bengio      et      al.,      2023),      it      seems      that      this      variant      is  \\n \\n  utilized      as      either   \\n \\na      detailed      balance      or   \\n \\na      trajectory      balance      loss,      as      discussed      in      Mohammadpour  \\n \\n  et      al.\\n(2023);      Tiapkin      et      al.\\n(2023);      Deleu      et      al.\\n(2024).\\nHowever,      to      the      best      of      our      knowledge,  \\n \\n  the      precise      formulation      of      path      consistency      learning      in      the      context      of      fine-tuning      diffusion      models  \\n \\n  has      not      been      established.\\nTherefore,      we      start      by      elucidating      the      rationale      of      PCL.\\nSubsequently,  \\n \\n  we      provide   \\n \\na      comprehensive      explanation      of      the      PCL.\\nFinally,      we      discuss      its      connection      with      the  \\n \\n  literature      on      Gflownets.</p><p>Motivation.\\nHere,      we      present      the      fundamental      principles      of      the      PCL.\\nTo      start      with,      we      prove      the  \\n \\n  following      lemma,      which      characterizes      soft-value      functions      and      soft-optimal      policies      recursively.</p><p>Algorithm   \\n \\n5      Path      Consistency      Learning      (Training      with      detailed      balance      loss)  \\n \\n  1:      Require:      Diffusion-model      {N(p(2,t;),07(t))}i-74,,  \\n \\n  -pre-trained   \\n \\n=      model  \\n \\n  {N      (p(t,      t;      Opre),      07      (t))      }i_744,      batch      size      m,      parameter   \\n \\na   \\n \\n€      Rt,      learning      rate   \\n \\n7       2:      Set   \\n \\na      model      {v;(-;@)}      to      learn      optimal      soft      value      function,      and   \\n \\na      model      {p;(-|-;@)}      to      learn  \\n \\n  optimal      polices.\\n \\n \\n  3:      Initialize:      0;   \\n \\n=      Opre  \\n \\n  4:      for      s   \\n \\n=      {1,---      ,S}do  \\n \\n  5:      Collect   \\n \\nm      samples      {a      (0)      }?_7,,      from   \\n \\na      current      diffusion      model      (i.e.,      generating      by      sequen- tially      running      polices      {N(p(2z,      t;      8),      07(t))      }i_-74,      from   \\n \\nt   \\n \\n=   \\n \\nT      to   \\n \\nt   \\n \\n=      0)  \\n \\n  6:      Setuo=r</p><li>(i)   \\n \\n?       a!\\n \\n \\n3   \\n \\na   \\n \\n4      Up—-1(@      13      ds)      re  \\n \\n  bs41   \\n \\n—      bs   \\n \\n—      Vo   \\n \\n3   \\n \\na      9)   \\n \\n+      log      p:(a\\\\\\\\\\\\\\\\,      |a\\\\\\\\\\\\\\\\;0,)   \\n \\n-      EP   \\n \\n—      Jog      rl      >}      Ibe:</li><p>a\\n \\n \\n  t=T+1      i=1  \\n \\n  (c}      ite)      ()      via)      43s)   \\n \\n;       Os41   \\n \\n<      Os   \\n \\n—      Vo   \\n \\n3      TE      velee      ids)      —S<   \\n \\n+      log      pe(ay      2s      |p      0)      —§      log      pee      (ar?\\nifort?\\nJe      lass</p><p>t=T+1      i=1 7:      end      for  \\n \\n  8:      Output:      {p,(x1-1      [215      Os)      fe</p><h3>Lemma      4      (1-step      Consistency      Equation).</h3><li>(2)   \\n \\n+      log      pi      (aa      |a1)   \\n \\n=      (eaeen)   \\n \\n+      log      pp      (x+-1|2+)      (33)</li><p> \\n \\n  Proof.\\nWe      consider      the      marginal      distribution      with      respect      to      x,      and      2;_,      induced      by      the      soft-  \\n \\n  optimal      policy,      and      denote      it      [(z;,      2:1)   \\n \\n€      A(X   \\n \\nx      X).\\nThen,      it      is      clearly      I(x,)l(a,_1   \\n \\n|      x)   \\n \\n=       I(x4-1)l      (xz   \\n \\n|      a1).\\nNow,      from      Theorem   \\n \\n2      that      characterizes      marginal      distributions,      and      Theorem   \\n \\n3       that      characterizes      posterior      distributions,      this      results      in      :</p><p>1\\n \\n   UE      L      re   \\n \\nx   \\n \\n1      Otte      me  \\n \\n  exp   \\n \\n(   \\n \\ni      2)      Pe      (xt)   \\n \\nx      pe      (®e-1|@e)   \\n \\n=   \\n \\nB      exp      (faeev)      Pra      (@1-1)   \\n \\nX      Pe      (1/01) a\\n \\n   —_-_——”’   \\n \\nC   \\n \\na      ~~  \\n \\n  C  \\n \\n  we  \\n \\n  Optimal      policy  \\n \\n  “~   \\n \\n“      Posterior      distribution  \\n \\n  Marginal      distribution      at   \\n \\nt      Marginal      distribution      at      t-1 Rearranging      yields:</p><p>1\\n \\n   UL(&   \\n \\n1      Up—1      (Le      re  \\n \\n  G      exp   \\n \\n(      ))   \\n \\nx      Di      (@-1|%4)   \\n \\n=   \\n \\nG      exp      (eaeen)   \\n \\nx      pe      (@4-1|24)</p><p>Taking      the      logarithm,      the      statement      is      concluded.\\nO</p><p>Algorithm.\\nBeing      motivated      by      the      relation      in      (33),      after      initializing      v9   \\n \\n=      r,      we      obtain      the  \\n \\n  recursive      equation:</p><p>2 Olen      U_-1      (Lt  \\n \\n  (pp)   \\n \\n=      argmin   \\n \\n—      Egany   \\n \\n|      oO   \\n \\n4      tog   \\n \\ng      (arp      ng)   \\n \\n—      PEED)   \\n \\n—      dog      pP\"      (ya      er) g):%3R,g2):¥      A(X)   \\n \\na   \\n \\no U   \\n \\n_   \\n \\nx   \\n \\n_      re      re  \\n \\n  =      (MEY)      tog      peal      altiansa)      +++      Flog      pe      (esl),  \\n \\n  which      is      an      extension      of      (33).\\nThe      loss      function      based      on      the      above      k-step      consistency      equation  \\n \\n  could      make      training      faster      without      learning      value      functions      at      every      time      point,      as      noted      in      the  \\n \\n  literature      in      PCL.\\nIn      the      extreme      case      (i.e.,      when      we      recursively      apply      it      with   \\n \\nt   \\n \\n=      7’),      we      obtain      the  \\n \\n  following.</p><li>(34)  \\n \\n  where      u,   \\n \\n€      A(X)      is      any      exploratory      roll-in      distribution.\\nBased      on      this      algorithm,      we      outline      the  \\n \\n  entire      algorithm      in      Algorithm      5.  \\n \\n  We      make      several      important      remarks      regarding      Algorithm      5.\\nFirstly,      while      we      use      on-policy      data  \\n \\n  collection,      technically,      any      policy      can      be      used      in      this      off-policy      algorithm,      like      reward-weighted  \\n \\n  MLE.\\nSecondly,      in      practice,      it      might      be      preferable      to      utilize   \\n \\na      sub-trajectory      from      x;      to      x;_,      based  \\n \\n  on      the      following      expression:</li><p>UFZ  \\n \\n  log      pi_      pa      (Gee      |Vt—ng1)   \\n \\n+      +++   \\n \\n+      log      pi      (4-1      |a4)   \\n \\n+      (2)</p><h3>Corollary      1      (7-step      consistency).</h3><p>log      pf      (xo|21)   \\n \\n+      +++   \\n \\n+      log      pp(wr-a|er)   \\n \\n+      log      pp      (x7)      (35) r(x      re      re      re  \\n \\n  —      (=)   \\n \\n+      log      p?\\n*(xo|v1)      +--+   \\n \\n+      log      pr\"      (ar-i|er)   \\n \\n+      log      pe      (xr).</p><p>Proof.\\nWe      consider      the      marginal      distribution      with      respect      to      7,      +--+   \\n \\n,   \\n \\n%      induced      by      the      soft-optimal  \\n \\n  policy,      and      denote      it      I(vp,---      ,--+      ,%o)   \\n \\n€   \\n \\n¥      X--+   \\n \\nx      X.      We      have Uar)l(ar—1   \\n \\n|      Lr)      tee      I(xo|21)   \\n \\n=      U(x      )l(a4   \\n \\n|      Xo)      te      (ar   \\n \\n|      LT-1)      (36) From      Theorem   \\n \\n2      that      characterized      marginal      distributions,      and      Theorem   \\n \\n3      that      characterize      posterior  \\n \\n  distributions,      the      left      hand      side      of      (36)      is      equal      to *\\n \\n   *\\n \\n   *</p><p>Pr      (xr)   \\n \\nx      pp      (r|er-1)      X-++   \\n \\nx      pi      (Xo|@1) Marginal      distribution      atT      Optimal      policy      at   \\n \\nT   \\n \\n—   \\n \\n1      Optimal      policy      at   \\n \\n1 and      the      right-hand      side      in      (36)      is      equal      to exp(r      (xo)      /a)      pre      pre      pre  \\n \\n  aq      Po      (xo)   \\n \\nX      po      (a1   \\n \\n|      Zo)      X+++   \\n \\nX      prey      (@r   \\n \\n|      er-1)-.\\n \\n \\n|      ~~  \\n \\n  Marginal      distribution      at   \\n \\n0      Posterior      distribution      Posterior      distribution By      rearranging      the      term,      we      obtain      (35).\\nO</p><p> \\n \\n  Comparison      with      Gflownets.\\nIn      the      Gflownets      literature,      similar      losses      are      used.\\nFor      instance,  \\n \\n  the      loss      derived      from      (33)      or      (35)      is      commonly      known      as   \\n \\na      detailed      balance      loss      (Bengio      et      al.,  \\n \\n  2023)      or   \\n \\na      trajectory      loss      (Malkin      et      al.,      2022),      respectively.</p><p> \\n \\n  Note      in      general,      the      literature      in      Gflownets      primarily      focuses      on      sampling      from      unnormalized  \\n \\n  models      (distributions      proportional      to      exp(r(a))).\\nHence,      reference      policies      (i.e,      {p?\"\"}\\n \\n \\n)      or      latent  \\n \\n  states      (1.e.,      %7.;      before      x9)      are      introduced      without      relying      on      pre-trained      diffusion      models.\\nIn  \\n \\n  contrast,      in      our      context,      we      use      policies      derived      from      pre-trained      diffusion      models      as      reference  \\n \\n  policies,      leveraging      them      as      our      prior      knowledge.</p><p>7\\n \\n   Fine-Tuning      Settings      Taxonomy</p><p> \\n \\n  So      far,      we      implicitly      assume      we      have      access      to      reward      functions.\\nHowever,      these      functions      are      often  \\n \\n  unknown      and      need      to      be      learned      from      data.\\nWe      classify      several      settings      in      terms      of      whether      reward  \\n \\n  functions      are      available      or,      if      not,      how      they      could      be      learned.\\nThis      section      is      summarized      in      Figure      3.</p><h4>Direct      backpropagation      (Sec.      5.2)       _       Differentiable</h4><p>_\\n \\n   ——_      Non-differentiable PP      5.1</p><h4>Accurate      rewards      O      (Sec.      5.1)</h4><p> \\n \\n  feedback   \\n \\n;    \\n \\n—      Reward-weighted      MLE      (Sec.      6.1  \\n \\n  Xo      Unknown      reward      Memory      effi-      oe      eward-welgnte      (Sec.      6.1) unctions      —_\\\\\\\\\\\\\\\\_—      ciency</p><h4>No      fine-tuning      Computational</h4><p>efficiency —\\n \\n    NN      Direct      backpropagation      (Sec.      5.2)</p><h5>Value-weighted      sampling      (Sec.      6.2)</h5><p>Figure      3:      Practical      recommendation      of      RL-based      algorithms      to      optimize      downstream      reward  \\n \\n  functions.</p><h6>7.1      Fine-Tuning      with      Known,      Differentiable      Reward      Functions</h6><p>ions  \\n \\n  When      accurate      differentiable      reward      functions      are      available      (e.g.,      standard      in      computer      vision      tasks),  \\n \\n  our      primary      focus      is      typically      on      computational      and      memory      efficiency.\\nIn      general,      we      advocate      for  \\n \\n  employing      reward      backpropagation      (c.f.      Algorithm      2)      for      its      computational      efficiency.\\nAlternatively,  \\n \\n  if      memory      efficiency      is   \\n \\na      significant      concern,      we      suggest      using      PPO      due      to      its      stability.</p><h6>7.2      Fine-Tuning      with      Black-Box      Reward      Feedback</h6><p>back  \\n \\n  Here,      we      explore      scenarios      where      we      have      access      to      accurate      but      non-differentiable      (black-box)  \\n \\n  reward      feedback,      often      found      in      scientific      simulations.\\nThe      emphasis      still      remains      primarily      on  \\n \\n  computational      and      memory      efficiency.\\nIn      such      scenarios,      due      to      the      non-differentiability      of      reward  \\n \\n  feedback,      we      recommend      using      PPO      or      reward-weighted      MLE      for      the      following      reasons.</p><p> \\n \\n  In      the      preceding      section,      we      advocated      for      the      use      of      reward      backpropagation      due      to      its      com-  \\n \\n  putational      advantages.\\nHowever,      when      dealing      with      non-differentiable      feedback,      the      advantage  \\n \\n  of      reward      backpropagation      may      diminish.\\nThis      is      because      learning      from      such      feedback      requires  \\n \\n  learning      differentiable      reward      functions      to      make      the      algorithm      work      as      we      discuss      in      Section      5.2.  \\n \\n  However,      obtaining   \\n \\na      differentiable      reward      function      can      be      challenging      in      many      domains.\\nFor  \\n \\n  example,      in      molecular      property      prediction,      molecular      fingerprints      are      informative      features,      and  \\n \\n  accurate      reward      functions      can      be      derived      by      training      simple      neural      networks      on      these      features  \\n \\n  (Pattanaik      and      Coley,      2020).\\nYet,      these      mappings,      grounded      in      prior      scientific      understanding,      are  \\n \\n  non-differentiable,      thereby      restricting      their      applicability.</p><p> \\n \\n  In      contrast,      PPO      and      reward-weighted      MLE      allow      for      policy      updates      without      the      explicit  \\n \\n  requirement      to      learn   \\n \\na      differentiable      reward      function,      offering   \\n \\na      significant      advantage      over      reward  \\n \\n  backpropagation.</p><h6>7.3      Fine-Tuning      with      Unknown      Rewards      Functions</h6><p>ions  \\n \\n  When      reward      functions      (or      computational      proxies      in      Section      7.2)      are      unavailable,      we      must      learn  \\n \\n  from      data      with      reward      feedback.\\nIn      such      cases,      compared      to      the      previous      scenarios,      two      important  \\n \\n  considerations      arise:</p><p>¢\\n \\n   Not      only      computational      or      memory      efficiency      but      also      feedback      efficiency      (i.e.,      sample  \\n \\n  efficiency      regarding      reward      feedback)      is      crucial.</p><p> \\n \\n \\n¢      Given      that      learned      reward      feedback      may      not      generalize      well      outside      the      training      data      distri-  \\n \\n  butions,      it      is      essential      to      constrain      fine-tuned      models      to      prevent      significant      divergence      from  \\n \\n  diffusion      models.\\nIn      these      situations,      not      only      soft      PPO      and      direct      backpropagation      but      also  \\n \\n  methods      discussed      in      Section      6,      such      as      reward-weighted      MLE,      can      be      particularly      effective.</p><p>Further,      the      scenario      involving      unknown      reward      functions      can      be      broadly      categorized      into      two      cases,  \\n \\n  which      we      will      discuss      in      more      detail.</p><p>Offline      scenario      (Figure      4).\\nIn      many      scenarios,      even  \\n \\n  in      cases      where      reward      functions      are      unknown,      we      of-  \\n \\n.     .</p><p>Offline      data      with ten      have      access      to      offline      data      with      reward      feedback      reward      feedback</p><li>(i)      (i)   \\n \\nO   \\n \\n;   \\n \\ni       a”      ra  \\n \\n.      ne      straightforward      approach      is      to</li><p> \\n \\n \\n{   \\n \\n,   \\n \\n(   \\n \\n)    \\n.\\n    .\\n \\n   g\\n \\n   PP  \\n \\n.      <=>      Learning      reward  \\n \\n  perform      regression      with      neural      networks      and      build   \\n \\na      models  \\n \\n  learned      regressor      *.\\nHowever,      this      learned      reward      func-      SS  \\n \\n  tion   \\n \\n*      might      not      generalize      well      beyond      the      distribution      x)  \\n \\n  of      the      offline      data.\\nIn      regions      outside      this      distribution,   \\n \\n7       could      assign   \\n \\na      high      value      even      when      the      actual      reward  \\n \\n  r      is      low      due      to      high      uncertainty.\\nConsequently,      we  \\n \\n  could      be      easily      misled      by      out-of-distribution      samples  \\n \\n  if      we      solely      optimize      7.\\n|Fine-tuning      diffusion      models   \\n \\n|       One      approach      to      alleviate      this      issue      is      to      adopt   \\n \\na       pessimistic      (i.e.,      conservative)      strategy,      as      proposed      in      Figure      4:      Offline      scenario  \\n \\n  Uehara      et      al.\\n(2024).\\nThis      technique      has      been      widely      Collecting      data]   \\n \\nY      Learning      reward  \\n \\n  applied      in      the      field      of      offline      RL      (Levine,      2018).\\nThe      aa  \\n \\n  main      idea      is      to      penalize   \\n \\n7      outside      the      distributions      us-  \\n \\n  ing      techniques      such      as      adding      an      explicit      penalty      term  \\n \\n  (Yu      et      al.,      2020;      Chang      et      al.,      2021),      bootstrapping,  \\n \\n  or      more      sophisticated      methods      (Kumar      et      al.,      2020;  \\n \\n  Xie      et      al.,      2021;      Rigter      et      al.,      2022;      Uehara      and      Sun,  \\n \\n  2021).\\nBy      doing      so,      we      can      avoid      being      fooled      by  \\n \\n  out-to-distribution      regions      while      still      benefiting      from  \\n \\n  the      extrapolation      capabilities      of      reward      models.</p><p>S—      diffusion      models      |</p><p>Figure      5:      Online      scenario</p><p> \\n \\n  Online      scenario      (Figure      5).\\nConsider      situations  \\n \\n  where      reward      functions      are      unknown,      but      we      can      gather      data      online.\\nThese      scenarios      are      often  \\n \\n  referred      to      as   \\n \\na      lab-in-the-loop      setting.</p><p> \\n \\n  In      such      scenarios,      Uehara      et      al.\\n(2024)      proposes      an      iterative      procedure      comprising      three      steps:</p><li>(1)      collecting      feedback      data      by      exploring      new      areas      (i.e.,      obtaining   \\n \\nx      from      current      diffusion      models  \\n \\n  and      corresponding      r(x)),      (2)      learning      the      reward      function      from      the      collected      feedback      data,      and</li><li>(3)      fine-tuning      current      diffusion      models      by      maximizing      the      learned      reward      function      enhanced      by  \\n \\n  an      optimistic      bonus      term,      using      algorithms      discussed      in      Section      4.2.\\nIn      contrast      to      standard      online  \\n \\n  PPO,      where      feedback      data      is      directly      used      in      the      fine-tuning      process,      actual      reward      feedback      is  \\n \\n  only      used      in      step      (2)      and      not      in      step      (3).\\nAn      additional      key      aspect      is      the      use      of      an      optimistic  \\n \\n  reward      function      that      encourages      exploration      beyond      the      distributions      of      the      current      diffusion      model.\\n \\n \\n  This      deliberate      separation      between      reward      learning      and      fine-tuning      steps,      coupled      with      the      use      of  \\n \\n  optimism,      significantly      enhances      feedback      efficiency.</li><p>8\\n \\n   Connection      with      Classifier      Guidance</p><p> \\n \\n  Classifier      guidance      (Dhariwal      and      Nichol,      2021)      is      commonly      used      for      conditional      generation      in  \\n \\n  diffusion      models.\\nInterestingly,      RL-based      methods      share   \\n \\na      close      connection      with      classifier      guidance.\\n \\n \\n  More      specifically,      in      this      section,      following      Zhao      et      al.\\n(2024),      we      elucidate      how      RL-based      methods  \\n \\n  can      be      applied      to      conditional      generation.\\nFurthermore,      from      this      unified      viewpoint,      we      highlight  \\n \\n  that      classifier      guidance      is      seen      as   \\n \\na      value-weighted      sampling      in      Section      4.2.</p><h6>8.1      Classfier      Guidance</h6><p>ance  \\n \\n  In      this      section,      we      first      introduce      classifier      guidance.\\nClassifier      guidance      utilizes      an      unconditional  \\n \\n  pre-trained      model      for      conditional      generation.\\nMore      specifically,      we      consider   \\n \\na      scenario      where      we  \\n \\n  have   \\n \\na      pre-trained      diffusion      model,      which      enables      us      to      sample      from      p?\\n\"*(x),      and      data      with      feedback  \\n \\n  {x,y},      where   \\n \\ny      represents      the      new      label      we      want      to      condition      on.\\nOur      objective      here      is      to      sample  \\n \\n  from      p(-|y)   \\n \\n€      [Y   \\n \\n>      ACR), P(-   \\n \\n|      y)   \\n \\nx      py(yl-)pP*(-).\\n(37)</p><p>where      p,   \\n \\n:   \\n \\nX      —>      A(Y)      denotes      the      conditional      distribution      of   \\n \\ny      given      x.</p><p>Motivation.\\nIn      classifier      guidance,      we      consider      the      following      policy:</p><p>pe      (-lare,      y)   \\n \\n=   \\n \\nN      (p(22,      t;      Opre)   \\n \\n+      a(t)      Ve      log      Vy      (Xt;      t),      a*(t))   \\n \\n’ dy(x,t)   \\n \\n=      Egy.\\nyap,      (-leo)      L(Y   \\n \\n=      y)|a,   \\n \\n=      2].</p><p>Under      the      continuous-time      formulation      of      diffusion      models,      applying      Doob’s      h-transform      (Rogers  \\n \\n  and      Williams,      2000),      it      is      shown      that      we      can      sample      from      p(x|y)      by      sequentially      running      {p7\"\\\\\\\\\\\\\\'(-|7+,      y)      }i-r41-</p><p>Algorithm.\\nSpecifically,      the      algorithm      comprises      two      steps.\\nIt’s      important      to      note      that      this      is      not  \\n \\n  a      fine-tuning      method      but      rather      an      inference-time      technique      as      we      freeze      the      pre-trained      model.</p><li>1. Learning      q,(x,t)      through      regression      by      gathering      data      containing      {z;,      y}.\\nHere,      for      each  \\n \\n  (Xo,      y),      using   \\n \\na      forward      policy      for      the      pre-training      (from   \\n \\n0      to      T),      we      obtain      2;      starting      from</li><p>Lo.</p><li>2. During      inference,      sequentially      execute      {p2\"(-|a,,      y)      }1_>   \\n \\n1      by      leverage      the      fact      that      compared</li><p>gui to      the      policy      in      pre-trained      diffusion      models,      the      mean      of      {p?\\n\"\\\\\\\\\\\\\\'(-|71,      y)      }i-74,      is      just      shifted  \\n \\n  with      an      additional      term      V,,      log      q,(2,      t).</p><p> \\n \\n  Notably,      the      method      described      above      can      be      reduced      to      value-weighted      sampling      (cf.      Sec-  \\n \\n  tion      6.2),      when      r(x,      y)   \\n \\n=      log      p(y|x).\\nInspired      by      such      an      observation,      we      apply      an      RL-based  \\n \\n  fine-tuning      method      for      conditional      generation      as      below.</p><h7>8.2.      RL-Based      Fine-Tuning      for      Conditional      Generation</h7><p>tion  \\n \\n  In      this      section,      following      Zhao      et      al.\\n(2024),      we      explain      an      RL-based      fine-tuning      approach      for  \\n \\n  conditional      generation.\\nThe      core      idea      is      that      setting      r(x,      y)   \\n \\n=      log      p(y|x)      allows      us      to      sample      from  \\n \\n  the      target      conditional      distribution      (37),      drawing      on      the      insights      discussed      in      Section      4.2.  \\n \\n  More      precisely,      introducing   \\n \\na      distribution      over   \\n \\ny      as      q(-),      we      formulate      the      following      RL      problem:</p><p>{Di      ft   \\n \\n=      argmax      Egy,      (-\\\\\\\\\\\\\\\\-y)}.yott      log      p(y|xo)   \\n \\n—      NeopyaKL(pe(-|e,      y)      ||P      (late).\\n \\n \\n  {prE[(V,.X)PA(A)]      Heyy (38)</p><p> \\n \\n  Compared      to      the      objective      function      (18)      in      Section      4.2,      the      main      differences      are:      (1)      policy      space  \\n \\n  is      expanded      to      accommodate      additional      control      (i.e.,      y):      pz(-|-,-)   \\n \\nC      [R¢   \\n \\nx   \\n \\nY   \\n \\n+      A(R®)],      (2)      the  \\n \\n  fine-tuning      objectives      are      set      as      log-likelihoods:      log      p(y|z).\\nThen,      following      Theorem      1,      we      can  \\n \\n  establish      the      following      result.</p><p>Corollary      2.\\nThe      distribution      induced      by      {p*}1_74      (ue.   \\n \\n[      {Teri      Ph      (ae-1   \\n \\n|      te,      y)}deur      is      the target      conditional      distribution      (37).</p><p> \\n \\n  The      above      corollary      suggests      that      for      conditional      generation,      we      can      utilize      various      off-the-shelf  \\n \\n  algorithms      discussed      in      Section      4.2      and      6,      such      as      PPO,      reward      backpropagation,      and      reward-  \\n \\n  weighted      MLE,      as      well      as      value-weighted      sampling.</p><p>9\\n \\n   Connection      with      Flow-Based      Diffusion      Models</p><p> \\n \\n  We      elucidate      the      close      relationship      between      bridge      matching      and      fine-tuning,      as      summarized      in  \\n \\n  Table      2.\\nBridge      (flow)      matching      (Liu      et      al.,      2022;      Tong      et      al.,      2023;      Lipman      et      al.,      2023;      Liu      et      al.,  \\n \\n  2022;      Shi      et      al.,      2024;      Albergo      and      Vanden-Eijnden,      2022),      has      recently      gained      popularity      as   \\n \\na       distinct      type      of      diffusion      model      from      score-matching-based      ones.\\nThese      works      suggest      that      training  \\n \\n  and      inference      speed      will      be      accelerated      due      to      the      ability      to      define   \\n \\na      more      direct      trajectory      from  \\n \\n  noise      to      data      distribution      than      score-matching-based      diffusion      models.</p><p> \\n \\n  The      connection      between      bridge      matching      and      fine-tuning      becomes      evident      in      the      continuous-  \\n \\n  time      formulation.\\nTo      begin,      we      first      provide   \\n \\na      brief      overview      of      the      continuous-time      formulation      of  \\n \\n  score-matching-based      diffusion      models.</p><p> \\n \\n  Table      2:      Comparison      between      training      score/flow-based      diffusion      models      and      fine-tuning.\\nWe  \\n \\n  optimize      the      parameter   \\n \\n6      in      P’,      which      represents      the      induced      distribution      over      trajectories      associated  \\n \\n  with      6.</p><h8>Loss      Note      of      Q</h8><p>Score-based      KL(Q\\\\\\\\\\\\\\'™¢|P?)      Qtime:      Time-reversal      of      reference      SDE      (reference training      SDE      is      defined      from   \\n \\n0      to      T’      where      the      initial      dis-  \\n \\n  tribute      at   \\n \\n0      is      data      distribution)</p><p> \\n \\n  Flow-based      KL(Q*°>      ||P?)      @Q*°°h:      Coupling      between      the      reference      SDE      from  \\n \\n  training   \\n \\nT      to   \\n \\n0      conditioning      at   \\n \\n0      (dist.      over      x.;|7o)      and  \\n \\n  a      data      distribution      at      time   \\n \\n0      (dis.      over      x).\\nNote  \\n \\n  that      reference      SDE      is      defined      from      7’      to      0.</p><p> \\n \\n  Fine-tuning      KL(P?||Q*°)      @*:      Coupling      between      the      pre-trained      SDE  \\n \\n  from   \\n \\nT      to   \\n \\n0      conditioned      on   \\n \\n0      (dist.      over      x7.1|X0)  \\n \\n  and   \\n \\na      data      distribution      at      time   \\n \\n0      (dis.      over      Xo) 9.1      Continuous      Time      Formulation      of      Score-Based      diffusion      models</p><p> \\n \\n  We      provide   \\n \\na      brief      overview      of      the      continuous-time      formulation      of      diffusion      models      (Song      et      al.,  \\n \\n  2021),      further      expanding      Section      1.1.1.\\nIn      this      formulation,      we      initially      define      the      forward      reference  \\n \\n  SDE,      spanning      from      time   \\n \\n0      to      7’      (e.g.,      variance      exploding      (VE)      process      or      variance      preserving      (VP)  \\n \\n  process),      by      setting      an      initial      distribution      at   \\n \\n0      as      the      data      distribution.\\nAdditionally,      the      reference  \\n \\n  SDE      is      tailored      to      converge      to      an      easy      distribution      at      time      7’,      such      as      the      normal      distribution.</p><p> \\n \\n  Subsequently,      we      consider      the      time-reversal      SDE      (Anderson,      1982),      progressing      from      T\\\\\\\\\\\\\\'      to      0.\\nIf  \\n \\n  we      could      learn      this      time-reversal      SDE,      it      would      enable      us      to      sample      from      the      data      distribution      by  \\n \\n  starting      with      the      easy      initial      distribution      (at      7’)      and      following      the      time-reversal      SDE      from   \\n \\nT      to   \\n \\n0      at  \\n \\n  inference      time.</p><p> \\n \\n  Training.\\nNow,      the      remaining      question      is      how      to      learn      this      time-reversal      SDE.\\nHere,      by      repre-  \\n \\n  senting      the      induced      distribution      over      trajectories      from   \\n \\nT      to   \\n \\n0      with      this      time-reversal      SDE      as      Q\\\\\\\\\\\\\\'™®,  \\n \\n  our      goal      is      to      train   \\n \\na      new      SDE,      parameterized      by   \\n \\n@      in      the      neural      network,      such      that      the      induced  \\n \\n  distribution      P®      over      trajectories      associated      with   \\n \\n6      aligns      with      Q\"™*.\\nSpecifically,      the      objective      is      to minimize      the      following      losses:</p><p> \\n \\n  argmin      KL(Q\"™||P*).\\n(39)  \\n \\n  0  \\n \\n  With      some      algebra,      leveraging      the      observation      that      the      time-reversal      SDE      comprises      the      original  \\n \\n  forward      SDE      and      the      scoring      term      of      the      marginal      distribution      (Anderson,      1982)      as      in      (2),      Song  \\n \\n  et      al.\\n(2021)      demonstrates      that      this      KL      loss      (39)      is      approximated      as Dore   \\n \\n=      argmin      Eeuni((0,7))01~qyo0(weleo)co~pP\"e      Fe      Fl      Vey      log      jo      (Xt   \\n \\n|      Xo)   \\n \\n_      so(Xt,      t)|I?\\n],      (40)  \\n \\n  which      is      equal      to      (4)      where      the      weight      A(t)      is      {o?\\n}?\\n(for      notation,      refer      to      Section      1.1.1).</p><h9>9.2      Flow-Based      (Bridge-Based)      Diffusion      Models</h9><p>dels We      adopt      the      explanation      outlined      in      Liu      et      al.\\n(2022).\\nIn      this      framework,      we      begin      by      considering  \\n \\n  a      reference      SDE      (from      T\\\\\\\\\\\\\\'      to      0),      x;   \\n \\n=      zr_;      where      z;      is      defined      as      Brownian      motion:</p><p>dz,   \\n \\n=      dwt,      te      (0,      7).\\n(41)</p><p> \\n \\n  Subsequently,      we      consider      the      coupling      of      the      reference      SDE      conditioned      on      state      at      time   \\n \\n0      (i.e.,  \\n \\n  distribution      of      xo.7      conditional      on      x)      and      the      data      distribution      at      time      0,      denoting      the      resulting  \\n \\n  distribution      as      Q*°?,      Informally,      this      Q*°°?\\nis      characterized      as      follows.</p><p>Informal      characterization      of      Q*”.\\nWhen      discretizing      the      time      step,      by      denoting      the      distribution  \\n \\n  induced      by      the      reference      SDE      as      p\"(x7,--+   \\n \\n,      29),      we      define QF      (a7,      tae   \\n \\n,      20)   \\n \\n=      p(x,      ay   \\n \\n|      r9)pP\"°      (x9).</p><p>The      distribution      Q*”      above      is   \\n \\na      limiting      distribution      obtained      by      making      the      discretization      step small.</p><p>Training.\\nIn      bridge-based      diffusion      models,      our      objective      is      to      train   \\n \\na      new      SDE,      parametrized      by  \\n \\n  6      (e.g.,   \\n \\na      neural      network),      to      align      the      induced      distribution      P®      with      Q*°?.\\nSpecifically,      bridge-based training      aims      to      minimize aremin      KL(Q*>||P’).\\n(42)  \\n \\n  0  \\n \\n  While      the      method      to      minimize      the      above      is      currently      unclear,      we      can      derive   \\n \\na      trainable      objective  \\n \\n  through      algebraic      manipulation.\\nInitially,      we      observe      that      the      reference      SDE      (41),      conditioned      on  \\n \\n  state      xo      at      time   \\n \\n0      (zr   \\n \\n=      D),      is      expressed      as:</p><p>b\\n \\n \\n  b—      2;</p><p>t\\n \\n   €\\n \\n   [0,T];dz?\\n \\n \\n=      g?(z?,t)dt      +du,,  \\n \\n  g(z?,t)   \\n \\n=   \\n \\na       (43)</p><p>using      the      seminal      “Doob’s      h-transform”      (Rogers      and      Williams,      2000).\\nThis      SDE      is      commonly  \\n \\n  known      as      the      Brownian      bridge.\\nNow,      we      consider   \\n \\na      new      SDE      parameterized      by      0:</p><p>t\\n \\n   €\\n \\n   [0,7];      dz   \\n \\n=      s(z,t;      @)dt   \\n \\n+      dw.\\n(44)</p><p>Then,      we      can      demonstrate      that      minimizing      the      KL      loss      in      (42)      is      equivalent      to argmin      E,W      unil0,7],1~4\"°      cop?\\n[Ilg”      (2,      t)   \\n \\n_      s(21,      t;      0)      M3]      (45)</p><p>where      q;°      denotes      the      induced      distribution      at      time   \\n \\nt      following      SDE      (43)      conditioned      on      xo,      and  \\n \\n  s:      Xx      |0,T]   \\n \\n>   \\n \\n&      is      the      model      we      aim      to      optimize.</p><p> \\n \\n  Flow-based      diffusion      models.\\nThe      aforementioned      formulation      has      been      originally      proposed      as   \\n \\na       bridge-based      SDE      (Liu      et      al.,      2022)      because      the      conditional      SDE      is      often      referred      to      as   \\n \\na      Brownian  \\n \\n  bridge.\\nFlow-based      diffusion      models      proposed      in      Lipman      et      al.\\n(2023);      Tong      et      al.\\n(2023)      share   \\n \\na       close      relationship      with      bridge-based      diffusion      models.\\nThese      flow-based      models      use   \\n \\na      target      SDE (43)      and      an      SDE      with   \\n \\na      parameter      (44)      without      stochastic      terms      (1.e.,      no      dw,)      and      aim      to      minimize  \\n \\n  the      loss      function      (45).</p><p> \\n \\n  Comparison      with      score-based      diffusion      models.\\nBoth      score-based      diffusion      models      and      bridge-  \\n \\n  based      diffusion      models      aim      to      minimize      KL      divergences.\\nDespite      its      similarity,      in      contrast      to  \\n \\n  score-based      diffusion      models,      bridge-based      diffusion      models      target      Q*°>      rather      than      the      time-  \\n \\n  reversal      SDE      Q\"™®.\\nThe      distribution      Q*”      is      often      considered      preferable      because      it      can      impose      an  \\n \\n  SDE      that      navigates      efficiently      from   \\n \\n0      to      7’      (e.g.,   \\n \\na      Brownian      bridge      with      minimal      noise).\\nThis      can  \\n \\n  expedite      the      learning      process      since      we      lack      direct      control      over      the      time-reversal      SDE      in      score-based  \\n \\n  diffusion      models.</p><h10>9.3.      Connection      with      RL-Based      Fine-Tuning</h10><p>ning  \\n \\n  RL-based      fine-tuning      shares   \\n \\na      close      relationship      with      flow-based      training.\\nTo      illustrate      this,      consider  \\n \\n  the      reference      SDE      (from      7’      to      0)      following      the      pre-trained      diffusion      model.\\nThen,      akin      to      flow-  \\n \\n  based      training,      we      consider      the      coupling      of      the      reference      SDE      conditioned      on      state      at      time   \\n \\n0       (i.e.,      distribution      over      xo-r      conditioned      on      x9)      and      the      target      distribution      at      time   \\n \\n0      (i.e.,      p,(-)   \\n \\n«       exp(r(-)/a)p?\\n\"°(-)),      denoting      the      induced      distribution      as      Q*™.</p><p>Informal      characterization      of      Q™°*.\\nInformally,      Q*\"°      is      introduced      as      follows.\\nWhen      discretizing  \\n \\n  the      time      step,      by      denoting      the      distribution      induced      by      the      reference      SDE      as      p™\\\\\\\\\\\\\\'(xp,-+-   \\n \\n,      9),      we define Qi\"      (xr,   \\n \\na   \\n \\n,      0)   \\n \\n=      p(x,      nr      Ly   \\n \\n|      L0)Pr      (Xo).</p><p>The      distribution      Q*°      above      is   \\n \\na      limiting      distribution      obtained      by      making      the      discretization      step small.</p><p>Training.\\nNow,      we      introduce   \\n \\na      new      SDE      parameterized      by   \\n \\n@      such      as      an      neural      network      to      align  \\n \\n  the      induced      distribution      P’      with      Q®\"*.\\nActually,      in      the      continuous      formulation,      the      RL-problem      (18) in      Section      4.2      is      equal      to      solving argmin      KL(P?||Q\"™).\\n \\n \\n  0 This      formalization      has      been      presented      in      Uehara      et      al.\\n(2024).\\nIntuitively,      as      we      see      in      Theorem      3,  \\n \\n  this      is      expected      because      we      see      that      the      posterior      distribution      induced      by      pre-trained      models:</p><p>ref   \\n \\n(      PD      (ar,-++      21   \\n \\n|      to)      (=      vey      (er|er_-1)p      Pp      (ar-1   \\n \\n|      er_2)      +++      pp\"      (x1   \\n \\n|      20), remains      preserved      after      fine-tuning      based      on      the      RL-formulation      (18).</p><p> \\n \\n  Comparison      with      flow-based      training.\\nIn      contrast      to      flow-based      training,      RL-based      fine-tuning  \\n \\n  minimizes      the      inverse      KL      divergence      rather      than      the      KL      divergence.\\nIntuitively,      this      is      because,  \\n \\n  unlike      Qt™*      or      Q*>   \\n \\n,      we      cannot      sample      from      Q*”      (recall      that   \\n \\na      marginal      distribution      at      time   \\n \\n0      in  \\n \\n  Qi,      i.e.,      p,,      is      unnormalized);      on      the      other      hand,      the      marginal      distributions      at      time   \\n \\n0      in      both      Q\\\\\\\\\\\\\\'™*  \\n \\n  and      Q*°°>      are      data      distributions.</p><p> \\n \\n  Remark   \\n \\n7      (Extension      to      f-divergence).\\nEach      of      these      methods      can      be      technically      extended      by  \\n \\n  incorporating      more      general      divergences      beyond      KL      divergence.\\nFor      instance,      in      the      context      of  \\n \\n  fine-tuning,      please      see      Tang      (2024).</p><p>10      Connection      with      Sampling      from      Unnormalized      Distributions</p><p> \\n \\n  Numerous      studies      delve      into      sampling      from      an      unnormalized      distribution      proportional      to      exp(r()),  \\n \\n  commonly      known      as      the      Gibbs      distribution,      extensively      discussed      in      the      computational      statistics  \\n \\n  and      statistical      physics      community      (Robert,      2014).\\nThis      issue      is      pertinent      to      our      work,      as      during  \\n \\n  fine-tuning,      the      target      distribution      is      also      formulated      in      the      Gibbs      distribution      form      in      (10),      which      is  \\n \\n  proportional      to      exp(r(a))p?\\n\"*      (x).</p><p> \\n \\n  The      literature      employs      two      main      approaches      to      tackle      this      challenge:      Markov      Chain      Monte  \\n \\n  Carlo      (MCMC)      and      RL-based      methods.\\nIn      particular,      the      RL-based      diffusion      model      fine-tuning  \\n \\n  we      have      discussed      so      far      is      closely      related      to      the      latter      approach.\\nIn      this      section,      we      explain      and  \\n \\n  compare      these      two      approaches.</p><h10>10.1      Markov      Chain      Monte      Carlo      (MCMC)</h10><p>CMC)  \\n \\n  In      MCMC,   \\n \\na      Markov      chain      is      constructed      to      approximate   \\n \\na      target      distribution      such      that      the      equilib-  \\n \\n  rium      distribution      of      the      Markov      chain      converges      to      the      target      distribution.\\nHowever,      dealing      with  \\n \\n  high-dimensional      domain      spaces      is      challenging      for      MCMC.\\n \\n \\nA      common      strategy      involves      leveraging  \\n \\n  gradient      information,      such      as      the      Metropolis-adjusted      Langevin      algorithm      (MALA)      (Besag,      1994)  \\n \\n  or      Hamiltonian      Monte      Carlo      (HMC)      (Neal      et      al.,      2011;      Girolami      and      Calderhead,      2011).\\nMALA  \\n \\n  shares      similarities      with      classifier      guidance      and      value-weighted      sampling,      as      both      methods      depend  \\n \\n  on      the      first-order      information      from      reward      (or      value)      functions.</p><p> \\n \\n  Can      we      use      MCMC      for      fine-tuning      diffusion      models?\\nIn      the      context      of      fine-tuning      dif-  \\n \\n  fusion      models,      while      it      may      seem      intuitive      to      sample      from      the      target      distribution      (10)      (i.e.,  \\n \\n  Dy   \\n \\n&      exp(r(x))pP\"*(x))      using      MCMC,      it’s      not      straightforward      for      the      following      reasons:</p><p>¢\\n \\n   We      lack      of      an      analytical      form      of      p?\\n\"*(-).<ul><li>*\\n \\n   Additionally,      even      if      we      can      estimate      p?\\n’°(-)      in      an      unbiased      manner,      as      in      Chen      et      al.\\n(2018),  \\n \\n  the      mixing      time      for      obtaining   \\n \\na      single      sample      in      MCMC      might      be      lengthy.</li></ul></p><p> \\n \\n  Therefore,      we      explore      creating   \\n \\na      generative      model      (i.e.,      diffusion      model)      for      this      task      using      RL-  \\n \\n  based      fine-tuning      so      that      we      can      easily      sample      from      p,(-)      during      inference      (i.e.,      simulating      the  \\n \\n  time-reversal      SDE      (2)      with      policies      from   \\n \\n¢   \\n \\n=      T\\\\\\\\\\\\\\'+   \\n \\n1      to   \\n \\nt   \\n \\n=      1),      without      relying      on      MCMC.\\nThis  \\n \\n  addresses      the      concern      of      MCMC      by      avoiding      the      need      to      estimate      p?\\n\"*(-)      and      minimizing      inference  \\n \\n  time,      albeit      at      the      expense      of      increased      training      time.</p><p>10.2}      RL-Based      Approaches  \\n \\n  While      MCMC      has      historically      been      widely      used      for      this      purpose,      recent      works      have      proposed      an  \\n \\n  RL-based      approach      or      its      variant      (e.g.,      Bernton      et      al.      (2019);      Heng      et      al.\\n(2020);      Huang      et      al.\\n(2023);</p><p> \\n \\n  Vargas      et      al.\\n(2023)).\\nFor      example,   \\n \\na      seminal      study      (Zhang      and      Chen,      2021)      introduced   \\n \\na      method  \\n \\n  very      similar      to      reward      backpropagation.\\nIn      their      work,      they      initially      define      Brownian      motion      as   \\n \\na       reference      SDE      (from      7’      to      0),      similar      to      the      pre-trained      diffusion      models      in      our      context.\\nThen,      by  \\n \\n  appropriately      setting      rewards,      Zhang      and      Chen      (2021)      aims      to      learn   \\n \\na      new      SDE      (from      T\\\\\\\\\\\\\\'      to      0)      such  \\n \\n  that      we      can      sample      from      the      target      distribution      at      the      endpoint      0.</p><p> \\n \\n  These      approaches      offer      advantages      over      MCMC      due      to      their      ability      to      minimize      inference      time  \\n \\n  (amortization)      by      constructing   \\n \\na      generative      model      comprising      policies      in      the      training      time      and      solely  \\n \\n  executing      learned      policies      during      inference.\\nUnlike      MCMC,      these      methods      significantly      reduce      the  \\n \\n  inference      cost      as      we      don’t      need      to      be      concerned      about      the      mixing      time.\\nAdditionally,      RL-based  \\n \\n  approaches      have      the      potential      to      effectively      handle      high-dimensional,      complex      (i.e.,      multi-modal)  \\n \\n  distributions      by      harnessing      the      considerable      expressive      power      arising      from      latent      states.</p><p> \\n \\n  Despite      these      advantages,      it      is      often      unclear      how      to      select   \\n \\na      reference      SDE      in      these      works.\\nIn  \\n \\n  contrast,      in      RL-based      fine-tuning      of      diffusion      models,      we      directly      leverage      pre-trained      diffusion  \\n \\n  models      as      the      reference      SDEs,      i.e.,      as      our      prior      knowledge.</p><p> \\n \\n  Key      message.\\nFinally,      it      is      worth      noting      that      we      can      technically      utilize      any      off-the-shelf      RL      algo-  \\n \\n  rithms      for      training.\\nFor      example,      Zhang      and      Chen      (2021)      employs      reward      backpropagation,      while  \\n \\n  works      in      Gflownets      typically      leverage      PCL,      as      we      mention      in      Section      6.3.\\nHowever,      additionally,      it  \\n \\n  is      possible      to      use      PPO      or      reward-weighted      MLE      mentioned      in      Section      4.2.</p><p>11      Closely      Related      Directions</p><p>Lastly,      we      highlight      several      closely      related      directions      we      have      not      yet      discussed.</p><p> \\n \\n  Aligning      text-to-image      models.\\nWe      recognize      that      current      efforts      in      fine-tuning      diffusion      models  \\n \\n  primarily      revolve      around      aligning      text-to-image      models      using      human      feedback      (Lee      et      al.,      2023;  \\n \\n  Wallace      et      al.,      2023;      Wu      et      al.,      2023;      Yang      et      al.,      2023).\\nIn      this      paper,      our      goal      is      to      consider   \\n \\na      more  \\n \\n  general      and      fundamental      formulation      that      is      not      tailored      to      any      particular      task.</p><p> \\n \\n  Diffusion      models      for      RL.\\nDiffusion      models      are      well-suited      for      specific      reinforcement      learning  \\n \\n  applications      due      to      their      ability      to      model      complex      and      multimodal      distributions      as      polices      (Janner  \\n \\n  et      al.,      2022;      Ajay      et      al.,      2023;      Wang      et      al.,      2022;      Hansen-Estruch      et      al.,      2023;      Du      et      al.,      2024;      Zhu  \\n \\n  et      al.,      2023).\\nFor      example,      Wang      et      al.\\n(2022)      use      conditional      diffusion      models      as      policies      and  \\n \\n  demonstrate      their      effectiveness      in      typical      offline      RL      benchmarks.\\nWhile      RL-based      fine-tuning      also  \\n \\n  aims      to      maximize      certain      reward      functions,      the      primary      focus      of      this      tutorial      covers      methods      that  \\n \\n  leverage      pre-trained      diffusion      models      for      this      purpose.</p><p> \\n \\n  RL      from      human      feedback      (RLHF)      for      language      models.\\nRLHF      is      widely      discussed      in      the  \\n \\n  context      of      language      models,      where      many      algorithms      share      similarities      with      those      used      in      diffusion  \\n \\n  models      (Ouyang      et      al.,      2022;      Bai      et      al.,      2022;      Casper      et      al.,      2023).\\nFor      instance,      both      domains  \\n \\n  commonly      employ      PPO      as   \\n \\na      standard      approach,      and      reward-weighted      training      (or      reward-weighted  \\n \\n  MLE)      often      serves      as   \\n \\na      basic      baseline      approach.\\nDespite      these      similarities,      nuanced      differences  \\n \\n  exist      mainly      because      diffusion      models      typically      operate      in   \\n \\na      continuous      input      space      rather      than  \\n \\n  a      discrete      one.\\nMoreover,      techniques      like      value-weighted      sampling      emerge      uniquely      within      the  \\n \\n  context      of      diffusion      models.</p><p>12      Summary</p><p> \\n \\n  In      this      article,      we      comprehensively      explain      how      fine-tuning      diffusion      models      to      maximize      down-  \\n \\n  stream      reward      functions      can      be      formalized      as   \\n \\na      reinforcement      learning      (RL)      problem      in      entropy-  \\n \\n  regularized      Markov      decision      processes      (MDPs).\\nBased      on      this      viewpoint,      we      elaborate      on      the  \\n \\n  application      of      various      RL      algorithms,      such      as      PPO      and      reward-weighted      MLE,      specifically      tailored  \\n \\n  for      fine-tuning      diffusion      models.\\nAdditionally,      we      categorize      different      scenarios      based      on      how  \\n \\n  reward      feedback      is      obtained.\\nWhile      this      categorization      is      not      always      explicitly      mentioned      in      many  \\n \\n  existing      works,      it      is      crucial      when      selecting      appropriate      algorithms.\\nFinally,      we      discuss      the      rela-  \\n \\n  tionships      with      several      related      topics,      including      classifier      guidance,      Gflownets,      path      integral      control  \\n \\n  theory,      and      sampling      from      unnormalized      distributions.</p><h4>References</h4><p>Agarwal,      A.,      N.      Jiang,      S.      M.      Kakade,      and      W.      Sun      (2019).\\nReinforcement      learning:      Theory      and  \\n \\n  algorithms.\\nCS      Dept.,      UW      Seattle,      Seattle,      WA,      USA,      Tech.\\nRep,      10-4.</p><p>Agarwal,      V.      and      D.      R.      Kelley      (2022).\\nThe      genetic      and      biochemical      determinants      of      mrna      degradation  \\n \\n  rates      in      mammals.\\nGenome      biology      23(1),      245.</p><p> \\n \\n  Ajay,      A.,      Y.      Du,      A.      Gupta,      J.      B.      Tenenbaum,      T.      S.      Jaakkola,      and      P.      Agrawal      (2023).\\nIs      conditional  \\n \\n  generative      modeling      all      you      need      for      decision      making?\\nIn      The      Eleventh      International      Conference  \\n \\n  on      Learning      Representations.</p><p>Albergo,      M.      S.      and      E.      Vanden-Eijnden      (2022).\\nBuilding      normalizing      flows      with      stochastic      inter-  \\n \\n  polants.\\narXiv      preprint      arXiv:2209.15571.</p><p>Anderson,      B.      D.      (1982).\\nReverse-time      diffusion      equation      models.\\nStochastic      Processes      and      their  \\n \\n  Applications      12(3),      313-326.</p><p> \\n \\n  Austin,      J.,      D.      D.      Johnson,      J.      Ho,      D.      Tarlow,      and      R.      Van      Den      Berg      (2021).\\nStructured      denoising  \\n \\n  diffusion      models      in      discrete      state-spaces.\\nAdvances      in      Neural      Information      Processing      Systems      34,  \\n \\n  17981-17993.</p><p>Avdeyey,      P.,      C.      Shi,      Y.      Tan,      K.      Dudnyk,      and      J.      Zhou      (2023).\\nDirichlet      diffusion      score      model      for  \\n \\n  biological      sequence      generation.\\narXiv      preprint      arXiv:2305.\\n10699.</p><p>Bai,      Y.,      S.      Kadavath,      S.      Kundu,      A.      Askell,      J.      Kernion,      A.      Jones,      A.      Chen,      A.      Goldie,      A.      Mirhoseini, C.      McKinnon,      et      al.\\n(2022).\\nConstitutional      ai:      Harmlessness      from      ai      feedback.\\narXiv      preprint  \\n \\n  arXiv:2212.08073.</p><p> \\n \\n  Bansal,      A.,      H.-M.\\nChu,      A.      Schwarzschild,      S.      Sengupta,      M.      Goldblum,      J.      Geiping,      and      T.      Goldstein  \\n \\n  (2023).\\nUniversal      guidance      for      diffusion      models.\\nIn      Proceedings      of      the      IEEE/CVF      Conference  \\n \\n  on      Computer      Vision      and      Pattern      Recognition,      pp.\\n843-852.</p><p>Bengio,      Y.,      S.      Lahlou,      T.      Deleu,      E.      J.      Hu,      M.      Tiwari,      and      E.      Bengio      (2023).\\nGflownet      foundations.\\n \\n \\n  Journal      of      Machine      Learning      Research      24(210),      1-55.</p><p> \\n \\n  Benton,      J.,      Y.      Shi,      V.      De      Bortoli,      G.      Deligiannidis,      and      A.      Doucet      (2024).\\nFrom      denoising      diffusions  \\n \\n  to      denoising      markov      models.\\nJournal      of      the      Royal      Statistical      Society      Series      B:      Statistical  \\n \\n  Methodology      86(2),      286-301.</p><p>Bernton,      E.,      J.      Heng,      A.      Doucet,      and      P.      E.      Jacob      (2019).\\nSchr\\\\\\\\\\\\\\\\””      odinger      bridge      samplers.\\narXiv  \\n \\n  preprint      arXiv:      1912.13170.</p><p>Besag,      J.\\n(1994).\\nComments      on      “representations      of      knowledge      in      complex      systems”      by      u.      grenander  \\n \\n  and      mi      miller.\\nJ.      Roy.\\nStatist.\\nSoc.      Ser.\\n \\n \\nB      56(591-592),      4.</p><p>Black,      K.,      M.      Janner,      Y.      Du,      I.      Kostrikov,      and      S.      Levine      (2023).\\nTraining      diffusion      models      with  \\n \\n  reinforcement      learning.\\narXiv      preprint      arXiv:2305.13301.</p><p> \\n \\n  Campbell,      A.,      J.      Benton,      V.      De      Bortoli,      T.      Rainforth,      G.      Deligiannidis,      and      A.      Doucet      (2022).\\n \\n \\nA       continuous      time      framework      for      discrete      denoising      models.\\nAdvances      in      Neural      Information  \\n \\n  Processing      Systems      35,      28266-28279.</p><p> \\n \\n  Campbell,      A.,      J.      Yim,      R.      Barzilay,      T.      Rainforth,      and      T.      Jaakkola      (2024).\\nGenerative      flows      on  \\n \\n  discrete      state-spaces:      Enabling      multimodal      flows      with      applications      to      protein      co-design.\\narXiv  \\n \\n  preprint      arXiv:2402.04997.</p><p>Cao,      H.,      C.      Tan,      Z.      Gao,      Y.      Xu,      G.      Chen,      P.-A.\\nHeng,      and      S.      Z.      Li      (2024).\\n \\n \\nA      survey      on      generative  \\n \\n  diffusion      models.\\n[EEE      Transactions      on      Knowledge      and      Data      Engineering.</p><p>Casper,      S.,      X.      Davies,      C.      Shi,      T.      K.      Gilbert,      J.      Scheurer,      J.      Rando,      R.      Freedman,      T.      Korbak, D.      Lindner,   \\n \\nP P.      Freire,      et      al.\\n(2023).\\nOpen      problems      and      fundamental      limitations      of      reinforcement  \\n \\n  learning      from      human      feedback.\\narXiv      preprint      arXiv:2307.15217.</p><p>Castillo-Hair,      S.      M.      and      G.      Seelig      (2021).\\nMachine      learning      for      designing      next-generation      mrna  \\n \\n  therapeutics.\\nAccounts      of      Chemical      Research      55(1),      24-34.</p><p>Chang,      J.      D.,      M.      Uehara,      D.      Sreenivas,      R.      Kidambi,      and      W.      Sun      (2021).\\nMitigating      covariate      shift  \\n \\n  in      imitation      learning      via      offline      data      without      great      coverage.\\narXiv      preprint      arXiv:2106.03207.</p><p>Chen,      M.,      S.      Mei,      J.      Fan,      and      M.      Wang      (2024).\\nAn      overview      of      diffusion      models:      Applications,  \\n \\n  guided      generation,      statistical      rates      and      optimization.\\narXiv      preprint      arXiv:2404.07771.</p><p>Chen,      R.      T.,      Y.      Rubanova,      J.      Bettencourt,      and      D.      K.      Duvenaud      (2018).\\nNeural      ordinary      differential  \\n \\n  equations.\\nAdvances      in      neural      information      processing      systems      31.</p><p>Chung,      H.,      J.      Kim,      M.      T.      Mccann,      M.      L.      Klasky,      and      J.      C.      Ye      (2022).\\nDiffusion      posterior      sampling  \\n \\n  for      general      noisy      inverse      problems.\\narXiv      preprint      arXiv:2209.\\n14687.</p><p> \\n \\n  Chung,      H.,      B.      Sim,      D.      Ryu,      and      J.      C.      Ye      (2022).\\nImproving      diffusion      models      for      inverse      problems  \\n \\n  using      manifold      constraints.\\nAdvances      in      Neural      Information      Processing      Systems      35,      25683-—  \\n \\n  25696.</p><p>Clark,      K.,      P.      Vicol,      K.      Swersky,      and      D.      J.      Fleet      (2023).\\nDirectly      fine-tuning      diffusion      models      on  \\n \\n  differentiable      rewards.\\narXiv      preprint      arXiv:2309.\\n17400.</p><p> \\n \\n  De      Bortoli,      V.,      E.      Mathieu,      M.      Hutchinson,      J.      Thornton,      Y.      W.      Teh,      and      A.      Doucet      (2022).\\nRieman-  \\n \\n  nian      score-based      generative      modelling.\\nAdvances      in      Neural      Information      Processing      Systems      35,  \\n \\n  2406-2422.</p><p>Deleu,      T.,      P.      Nouri,      N.      Malkin,      D.      Precup,      and      Y.      Bengio      (2024).\\nDiscrete      probabilistic      inference      as  \\n \\n  control      in      multi-path      environments.\\narXiv      preprint      arXiv:2402.10309.</p><p>Dhariwal,      P.      and      A.      Nichol      (2021).\\nDiffusion      models      beat      gans      on      image      synthesis.\\nAdvances      in  \\n \\n  neural      information      processing      systems      34,      8780-8794.</p><p> \\n \\n  Du,      Y.,      S.      Yang,      B.      Dai,      H.      Dai,      O.      Nachum,      J.      Tenenbaum,      D.      Schuurmans,      and      P.      Abbeel      (2024).\\n \\n \\n  Learning      universal      policies      via      text-guided      video      generation.\\nAdvances      in      Neural      Information  \\n \\n  Processing      Systems      36.</p><p>Fan,      Y.,      O.      Watkins,      Y.      Du,      H.      Liu,      M.      Ryu,      C.      Boutilier,      P.      Abbeel,      M.      Ghavamzadeh,      K.      Lee,      and K.      Lee      (2023) 023).\\nDPOK:      Reinforcement      learning      for      fine-tuning      text-to-image      diffusion      models.\\n \\n \\n  arXiv      preprint      arXiv:2305.16381.</p><p>Finn,      C.,      S.      Levine,      and      P.      Abbeel      (2016).\\nGuided      cost      learning:      Deep      inverse      optimal      control      via  \\n \\n  policy      optimization.\\nIn      International      conference      on      machine      learning,      pp.\\n49-58.\\nPMLR.</p><p>Fox,      R.,      A.      Pakman,      and      N.      Tishby      (2015).\\nTaming      the      noise      in      reinforcement      learning      via      soft  \\n \\n  updates.\\narXiv      preprint      arXiv:      1512.08562.</p><p>Frey,      N.      C.,      D.      Berenberg,      K.      Zadorozhny,      J.      Kleinhenz,      J.      Lafrance-Vanasse,      I.      Hotzel,      Y.      Wu, S.      Ra,   \\n \\nR R.      Bonneau,      K.      Cho,      et      al.\\n(2023).\\nProtein      discovery      with      discrete      walk-jump      sampling.\\n \\n \\n  arXiv      preprint      arXiv:2306.\\n12360.</p><p>Geist,      M.,      B.      Scherrer,      and      O.      Pietquin      (2019).\\n \\n \\nA      theory      of      regularized      markov      decision      processes.\\n \\n \\n  In      International      Conference      on      Machine      Learning,      pp.\\n2160-2169.\\nPMLR.</p><p> \\n \\n  Girolami,      M.      and      B.      Calderhead      (2011).\\nRiemann      manifold      langevin      and      hamiltonian      monte  \\n \\n  carlo      methods.\\nJournal      of      the      Royal      Statistical      Society      Series      B:      Statistical      Methodology      73(2),  \\n \\n  123-214.</p><p>Gosai,      S.      J.,      R.      I.      Castro,      N.      Fuentes,      J.      C.      Butts,      S.      Kales,      R.      R.      Noche,      K.      Mouri,      P.      C.      Sabeti, S.   \\n \\nK K.      Reilly,      and      R.      Tewhey      (2023).\\nMachine-guided      design      of      synthetic      cell      type-specific  \\n \\n  cis-regulatory      elements.\\nbioRxiv.</p><p>Haarnoja,      T.,      H.      Tang,      P.      Abbeel,      and      S.      Levine      (2017).\\nReinforcement      learning      with      deep      energy-  \\n \\n  based      policies.\\nIn      International      conference      on      machine      learning,      pp.\\n1352-1361.\\nPMLR.</p><p>Hansen-Estruch,      P.,      I.      Kostrikov,      M.      Janner,      J.      G.      Kuba,      and      S.      Levine      (2023).\\nIdql:      Implicit  \\n \\n  q-learning      as      an      actor-critic      method      with      diffusion      policies.\\narXiv      preprint      arXiv:2304.10573.</p><p>Heng,      J.,      A.      N.      Bishop,      G.      Deligiannidis,      and      A.      Doucet      (2020).\\nControlled      sequential      monte      carlo.</p><p>Ho,      J.,      A.      Jain,      and      P.      Abbeel      (2020).\\nDenoising      diffusion      probabilistic      models.\\nAdvances      in      neural  \\n \\n  information      processing      systems      33,      6840-6851.</p><p>Ho,      J.      and      T.      Salimans      (2022).\\nClassifier-free      diffusion      guidance.\\narXiv      preprint      arXiv:2207.12598.</p><p>Ho,      J.,      T.      Salimans,      A.      Gritsenko,      W.      Chan,      M.      Norouzi,      and      D.      J.      Fleet      (2022).\\nVideo      diffusion  \\n \\n  models.\\nAdvances      in      Neural      Information      Processing      Systems      35,      8633-8646.</p><p> \\n \\n  Hoogeboom,      E.,      V.      G.      Satorras,      C.      Vignac,      and      M.      Welling      (2022).\\nEquivariant      diffusion      for  \\n \\n  molecule      generation      in      3d.\\nIn      International      conference      on      machine      learning,      pp.\\n8867-8887.</p><h4>      PMLR.</h4><p>Huang,      X.,      H.      Dong,      H.      Yifan,      Y.      Ma,      and      T.      Zhang      (2023).\\nReverse      diffusion      monte      carlo.\\nIn      The  \\n \\n  Twelfth      International      Conference      on      Learning      Representations.</p><p>Janner,      M.,      Y.      Du,      J.      B.      Tenenbaum,      and      S.      Levine      (2022).\\nPlanning      with      diffusion      for      flexible  \\n \\n  behavior      synthesis.\\narXiv      preprint      arXiv:2205.09991.</p><p> \\n \\n  Jo,      J.,      S.      Lee,      and      S.      J.      Hwang      (2022).\\nScore-based      generative      modeling      of      graphs      via      the      system  \\n \\n  of      stochastic      differential      equations.\\nIn      International      Conference      on      Machine      Learning,      pp.\\n \\n \\n  10362-10383.\\nPMLR.</p><p> \\n \\n  Kazim,      M.,      J.      Hong,      M.-G.\\nKim,      and      K.-K.\\nK.      Kim      (2024).\\nRecent      advances      in      path      integral      control  \\n \\n  for      trajectory      optimization:      An      overview      in      theoretical      and      algorithmic      perspectives.\\nAnnual  \\n \\n  Reviews      in      Control      57,      100931.</p><p>Krishnamoorthy,      S.,      S.      M.      Mashkaria,      and      A.      Grover      (2023).\\nDiffusion      models      for      black-box  \\n \\n  optimization.\\narXiv      preprint      arXiv:2306.07180.</p><p>Kumar,      A.,      A.      Zhou,      G.      Tucker,      and      S.      Levine      (2020).\\nConservative      q-learning      for      offline      reinforce-  \\n \\n  ment      learning.\\nAdvances      in      Neural      Information      Processing      Systems      33,      1179-1191.</p><p>Lal,      A.,      D.      Garfield,      T.      Biancalani,      and      G.      Eraslan      (2024).\\nreglm:      Designing      realistic      regulatory  \\n \\n  dna      with      autoregressive      language      models.\\nbioRxiv,      2024-02.</p><p>Lee,      K.,      H.      Liu,      M.      Ryu,      O.      Watkins,      Y.      Du,      C.      Boutilier,      P.      Abbeel,      M.      Ghavamzadeh,      and      S.      S.      Gu  \\n \\n  (2023).\\nAligning      text-to-image      models      using      human      feedback.\\narXiv      preprint      arXiv:2302.12192.</p><p>Levine,      S.      (2018).\\nReinforcement      learning      and      control      as      probabilistic      inference:      Tutorial      and  \\n \\n  review.\\narXiv      preprint      arXiv:      1805.00909.</p><p>Li,      Z.,      Y.      Ni,      T.      A.      B.      Huygelen,      A.      Das,      G.      Xia,      G.-B.\\nStan,      and      Y.      Zhao      (2023).\\nLatent      diffusion  \\n \\n  model      for      dna      sequence      generation.\\narXiv      preprint      arXiv:2310.06150.</p><p>Lipman,      Y.,      R.      T.      Chen,      H.      Ben-Hamu,      M.      Nickel,      and      M.      Le      (2023).\\nFlow      matching      for      generative  \\n \\n  modeling.\\nJCLR      2023.</p><p>Liu,      X.,      C.      Gong,      and      Q.      Liu      (2022).\\nFlow      straight      and      fast:      Learning      to      generate      and      transfer      data  \\n \\n  with      rectified      flow.\\narXiv      preprint      arXiv:2209.03003.</p><p>Liu,      X.,      L.      Wu,      M.      Ye,      and      Q.      Liu      (2022).\\nLet      us      build      bridges:      Understanding      and      extending  \\n \\n  diffusion      generative      models.\\narXiv      preprint      arXiv:2208.14699.</p><p>Lou,      A.,      C.      Meng,      and      S.      Ermon      (2023).\\nDiscrete      diffusion      language      modeling      by      estimating      the  \\n \\n  ratios      of      the      data      distribution.\\narXiv      preprint      arXiv:2310.16834.</p><p>Malkin,      N.,      M.      Jain,      E.      Bengio,      C.      Sun,      and      Y.      Bengio      (2022).\\nTrajectory      balance:      Improved      credit  \\n \\n  assignment      in      gflownets.\\nAdvances      in      Neural      Information      Processing      Systems      35,      5955-5967.</p><p>Mohammadpour,      S.,      E.      Bengio,      E.      Frejinger,      and      P.-L.\\nBacon      (2023).\\nMaximum      entropy      gflownets  \\n \\n  with      soft      q-learning.\\narXiv      preprint      arXiv:2312.14331.</p><p>Nachum,      O.,      M.      Norouzi,      K.      Xu,      and      D.      Schuurmans      (2017).\\nBridging      the      gap      between      value      and  \\n \\n  policy      based      reinforcement      learning.\\nAdvances      in      neural      information      processing      systems      30.</p><p>Neal,      R.      M.      et      al.\\n(2011).\\nMcmce      using      hamiltonian      dynamics.\\nHandbook      of      markov      chain      monte  \\n \\n  carlo      2(11),      2.</p><p>Neu,      G.,      A.      Jonsson,      and      V.      Gémez      (2017).\\n \\n \\nA      unified      view      of      entropy-regularized      markov      decision  \\n \\n  processes.\\narXiv      preprint      arXiv:      1705.07798.</p><p>Ouyang,      L.,      J.      Wu,      X.      Jiang,      D.      Almeida,      C.      Wainwright,      P.      Mishkin,      C.      Zhang,      S.      Agarwal, K.      Slama,      A A.      Ray,      et      al.\\n(2022).\\nTraining      language      models      to      follow      instructions      with      human  \\n \\n  feedback.\\nAdvances      in      Neural      Information      Processing      Systems      35,      27730-27744.</p><p>Pattanaik,      L.      and      C.      W.      Coley      (2020).\\nMolecular      representation:      going      long      on      fingerprints.\\n \\n \\n  Chem      6(6),      1204-1207.</p><p>Peng,      X.      B.,      A.      Kumar,      G.      Zhang,      and      S.      Levine      (2019).\\nAdvantage-weighted      regression:      Simple  \\n \\n  and      scalable      off-policy      reinforcement      learning.\\narXiv      preprint      arXiv:1910.00177.</p><p>Peters,      J.,      K.      Mulling,      and      Y.      Altun      (2010).\\nRelative      entropy      policy      search.\\nIn      Proceedings      of      the  \\n \\n  AAAI      Conference      on      Artificial      Intelligence,      Volume      24,      pp.\\n1607-1612.</p><p> \\n \\n  Podell,      D.,      Z.      English,      K.      Lacey,      A.      Blattmann,      T.      Dockhorn,      J.      Miiller,      J.      Penna,      and      R.      Rombach  \\n \\n  (2023).\\nSdxl:      Improving      latent      diffusion      models      for      high-resolution      image      synthesis.\\narXiv  \\n \\n  preprint      arXiv:2307.01952.</p><p>Prabhudesai,      M.,      A.      Goyal,      D.      Pathak,      and      K.      Fragkiadaki      (2023).\\nAligning      text-to-image      diffusion  \\n \\n  models      with      reward      backpropagation.\\narXiv      preprint      arXiv:2310.03739.</p><p>Ramesh,      A.,      P.      Dhariwal,      A.      Nichol,      C.      Chu,      and      M.      Chen      (2022).\\nHierarchical      text-conditional  \\n \\n  image      generation      with      clip      latents.\\narXiv      preprint      arXiv:2204.06125      1(2),      3.</p><p>Rigter,      M.,      B.      Lacerda,      and      N.      Hawes      (2022).\\nRambo-rl:      Robust      adversarial      model-based      offline  \\n \\n  reinforcement      learning.\\nAdvances      in      neural      information      processing      systems      35,      16082-16097.</p><p>Robert,      C.      (2014).\\nStatistical      modeling      and      computation.</p><p>Rogers,      L.      C.      G.      and      D.      Williams      (2000).\\nDiffusions,      Markov      processes      and      martingales:      Volume      2,  \\n \\n  It6é      calculus,      Volume      2.\\nCambridge      university      press.</p><p> \\n \\n  Rombach,      R.,      A.      Blattmann,      D.      Lorenz,      P.      Esser,      and      B.      Ommer      (2022,      June).\\nHigh-resolution  \\n \\n  image      synthesis      with      latent      diffusion      models.\\nIn      Proceedings      of      the      IEEE/CVF      Conference      on  \\n \\n  Computer      Vision      and      Pattern      Recognition      (CVPR),      pp.\\n10684-10695.</p><p>Sarkar,      A.,      Z.      Tang,      C.      Zhao,      and      P.      Koo      (2024).\\nDesigning      dna      with      tunable      regulatory      activity  \\n \\n  using      discrete      diffusion.\\nbioRxiv,      2024-05.</p><p>Schulman,      J.,      X.      Chen,      and      P.      Abbeel      (2017).\\nEquivalence      between      policy      gradients      and      soft  \\n \\n  q-learning.\\narXiv      preprint      arXiv:      1704.06440.</p><p>Schulman,      J.,      S.      Levine,      P.      Abbeel,      M.      Jordan,      and      P.      Moritz      (2015).\\nTrust      region      policy      optimization.\\n \\n \\n  In      International      conference      on      machine      learning,      pp.\\n1889-1897.\\nPMLR.</p><p>Schulman,      J.,      F.      Wolski,      P.      Dhariwal,      A.      Radford,      and      O.      Klimov      (2017).\\nProximal      policy      optimiza-  \\n \\n  tion      algorithms.\\narXiv      preprint      arXiv:      1707.06347.</p><p>Shi,      Y.,      V.      De      Bortoli,      A.      Campbell,      and      A.      Doucet      (2024).\\nDiffusion      schrédinger      bridge      matching.\\n \\n \\n  Advances      in      Neural      Information      Processing      Systems      36.</p><p> \\n \\n  Sohl-Dickstein,      J.,      E.      Weiss,      N.      Maheswaranathan,      and      S.      Ganguli      (2015).\\nDeep      unsupervised  \\n \\n  learning      using      nonequilibrium      thermodynamics.\\nIn      /nternational      conference      on      machine      learning,  \\n \\n  pp.\\n2256-2265.\\nPMLR.</p><p>Song,      J.,      C.      Meng,      and      S.      Ermon      (2020).\\nDenoising      diffusion      implicit      models.\\narXiv      preprint  \\n \\n  arXiv:2010.02502.</p><p>Song,      Y.,      C.      Durkan,      I.      Murray,      and      S.      Ermon      (2021).\\nMaximum      likelihood      training      of      score-based  \\n \\n  diffusion      models.\\nAdvances      in      neural      information      processing      systems      34,      1415-1428.</p><p>Stark,      H.,      B.      Jing,      C.      Wang,      G.      Corso,      B.      Berger,      R.      Barzilay,      and      T.      Jaakkola      (2024).\\nDirichlet  \\n \\n  flow      matching      with      applications      to      dna      sequence      design.\\narXiv      preprint      arXiv:2402.05841.</p><p>Sutton,      R.      S.      and      A.      G.      Barto      (2018).\\nReinforcement      learning:      An      introduction.\\nMIT      press.</p><p>Tang,      W.      (2024).\\nFine-tuning      of      diffusion      models      via      stochastic      control:      entropy      regularization      and  \\n \\n  beyond.\\narXiv      preprint      arXiv:2403.06279.</p><p>Tang,      W.      and      H.      Zhao      (2024).\\nScore-based      diffusion      models      via      stochastic      differential      equations—a  \\n \\n  technical      tutorial.\\narXiv      preprint      arXiv:2402.07487.</p><p>Theodorou,      E.,      J.      Buchli,      and      S.      Schaal      (2010).\\n \\n \\nA      generalized      path      integral      control      approach      to  \\n \\n  reinforcement      learning.\\nThe      Journal      of      Machine      Learning      Research      11,      3137-3181.</p><p>Tiapkin,      D.,      N.      Morozov,      A.      Naumov,      and      D.      Vetrov      (2023).\\nGenerative      flow      networks      as      entropy-  \\n \\n  regularized      rl.\\narXiv      preprint      arXiv:2310.12934.</p><p> \\n \\n  Tong,      A.,      N.      Malkin,      G.      Huguet,      Y.      Zhang,      J.      Rector-Brooks,      K.      Fatras,      G.      Wolf,      and      Y.      Bengio  \\n \\n  (2023).\\nConditional      flow      matching:      Simulation-free      dynamic      optimal      transport.\\narXiv      preprint  \\n \\n  arXiv:2302.00482.</p><p>Uehara,      M.      and      W.      Sun      (2021).\\nPessimistic      model-based      offline      reinforcement      learning      under  \\n \\n  partial      coverage.\\narXiv      preprint      arXiv:2107.06226.</p><p>Uehara,      M.,      Y.      Zhao,      K.      Black,      E.      Hajiramezanali,      G.      Scalia,      N.      L.      Diamant,      A.      M.      Tseng, T.      Biancalani,      and      S S.      Levine      (2024).\\nFine-tuning      of      continuous-time      diffusion      models      as      entropy-  \\n \\n  regularized      control.\\narXiv      preprint      arXiv:2402.15194.</p><p> \\n \\n  Uehara,      M.,      Y.      Zhao,      K.      Black,      E.      Hajiramezanali,      G.      Scalia,      N.      L.      Diamant,      A.      M.      Tseng,      S.      Levine,  \\n \\n  and      T.      Biancalani      (2024).\\nFeedback      efficient      online      fine-tuning      of      diffusion      models.\\narXiv      preprint  \\n \\n  arXiv:2402.\\n16359.</p><p> \\n \\n  Uehara,      M.,      Y.      Zhao,      E.      Hajiramezanali,      G.      Scalia,      G.      Eraslan,      A.      Lal,      S.      Levine,      and      T.      Biancalani  \\n \\n  (2024).\\nBridging      model-based      optimization      and      generative      modeling      via      conservative      fine-tuning  \\n \\n  of      diffusion      models.\\narXiv      preprint      arXiv:2405.19673.</p><p>Vargas,      F.,      W.      Grathwohl,      and      A.      Doucet      (2023).\\nDenoising      diffusion      samplers.\\narXiv      preprint  \\n \\n  arXiv:2302.\\n13834.</p><p> \\n \\n  Wallace,      B.,      M.      Dang,      R.      Rafailov,      L.      Zhou,      A.      Lou,      S.      Purushwalkam,      S.      Ermon,      C.      Xiong,      S.      Joty,  \\n \\n  and      N.      Naik      (2023).\\nDiffusion      model      alignment      using      direct      preference      optimization.\\narXiv  \\n \\n  preprint      arXiv:2311.12908.</p><p>Wang,      Z.,      J.      J.      Hunt,      and      M.      Zhou      (2022).\\nDiffusion      policies      as      an      expressive      policy      class      for      offline  \\n \\n  reinforcement      learning.\\narXiv      preprint      arXiv:2208.06193.</p><p>Widatalla,      T.,      R.      Rafailov,      and      B.      Hie      (2024).\\nAligning      protein      generative      models      with      experimental  \\n \\n  fitness      via      direct      preference      optimization.\\nbioRxiv,      2024-05.</p><p>Williams,      G.,      A.      Aldrich,      and      E.      A.      Theodorou      (2017).\\nModel      predictive      path      integral      control:      From  \\n \\n  theory      to      parallel      computation.\\nJournal      of      Guidance,      Control,      and      Dynamics      40(2),      344-357.</p><p>Wu,      X.,      K.      Sun,      F.      Zhu,      R.      Zhao,      and      H.      Li      (2023).\\nBetter      aligning      text-to-image      models      with  \\n \\n  human      preference.\\narXiv      preprint      arXiv:2303.14420.</p><p>Wu,      Y.,      G.      Tucker,      and      O.      Nachum      (2019).\\nBehavior      regularized      offline      reinforcement      learning.\\n \\n \\n  arXiv      preprint      arXiv:      1911.11361.</p><p>Wulfmeier,      M.,      P.      Ondruska,      and      I.      Posner      (2015).\\nMaximum      entropy      deep      inverse      reinforcement  \\n \\n  learning.\\narXiv      preprint      arXiv:      1507.04888.</p><p>Xie,      T.,      C.-A.\\nCheng,      N.      Jiang,      P.      Mineiro,      and      A.      Agarwal      (2021).\\nBellman-consistent      pessimism  \\n \\n  for      offline      reinforcement      learning.\\nAdvances      in      neural      information      processing      systems      34.</p><p>Xu,      M.,      L.      Yu,      Y.      Song,      C.      Shi,      S.      Ermon,      and      J.      Tang      (2022).\\nGeodiff:   \\n \\nA      geometric      diffusion  \\n \\n  model      for      molecular      conformation      generation.\\narXiv      preprint      arXiv:2203.02923.</p><p> \\n \\n  Yang,      L.,      Z.      Zhang,      Y.      Song,      S.      Hong,      R.      Xu,      Y.      Zhao,      W.      Zhang,      B.      Cui,      and      M.-H.\\nYang      (2023).\\n \\n \\n  Diffusion      models:   \\n \\nA      comprehensive      survey      of      methods      and      applications.\\nACM      Computing  \\n \\n  Surveys      56(4),      1-39.</p><p> \\n \\n  Yu,      T.,      G.      Thomas,      L.      Yu,      S.      Ermon,      J.      Y.      Zou,      S.      Levine,      C.      Finn,      and      T.      Ma      (2020).\\nMopo:  \\n \\n  Model-based      offline      policy      optimization.\\nAdvances      in      Neural      Information      Processing      Systems      33,  \\n \\n  14129-14142.</p><p> \\n \\n  Yuan,      H.,      K.      Huang,      C.      Ni,      M.      Chen,      and      M.      Wang      (2023).\\nReward-directed      conditional      diffusion:  \\n \\n  Provable      distribution      estimation      and      reward      improvement.\\nIn      Thirty-seventh      Conference      on  \\n \\n  Neural      Information      Processing      Systems.</p><p>Zhang,      H.      and      T.      Xu      (2023).\\nTowards      controllable      diffusion      models      via      reward-guided      exploration.\\n \\n \\n  arXiv      preprint      arXiv:2304.07132.</p><p>Zhang,      Q.      and      Y.      Chen      (2021).\\nPath      integral      sampler:   \\n \\na      stochastic      control      approach      for      sampling.\\n \\n \\n  arXiv      preprint      arXiv:2111,15141.</p><p> \\n \\n  Zhao,      Y.,      M.      Uehara,      G.      Scalia,      T.      Biancalani,      S.      Levine,      and      E.      Hajiramezanali      (2024).\\n \\n \\n  Adding      conditional      control      to      diffusion      models      with      reinforcement      learning.\\narXiv      preprint  \\n \\n  arXiv:2406.\\n12120.</p><p>Zhou,      Z.,      S.      Kearnes,      L.      Li,      R.      N.      Zare,      and      P.      Riley      (2019).\\nOptimization      of      molecules      via      deep  \\n \\n  reinforcement      learning.\\nScientific      reports      9(1),      10752.</p><p>Zhu,      Z.,      H.      Zhao,      H.      He,      Y.      Zhong,      S.      Zhang,      Y.      Yu,      and      W.      Zhang      (2023).\\nDiffusion      models      for  \\n \\n  reinforcement      learning:   \\n \\nA      survey.\\narXiv      preprint      arXiv:2311.01223.</p><p>Ziebart,      B.      D.,      A.      L.      Maas,      J.      A.      Bagnell,      A.      K.      Dey,      et      al.\\n(2008).\\nMaximum      entropy      inverse  \\n \\n  reinforcement      learning.\\nIn      Aaai,      Volume      8,      pp.\\n1433-1438.\\nChicago,      IL,      USA.</p><h4>Direct      backpropagation      (Sec.      5.2)       _       Differentiable</h4><p>_\\n \\n   ——_      Non-differentiable PP      5.1</p><h4>Accurate      rewards      O      (Sec.      5.1)</h4><p> \\n \\n  feedback   \\n \\n;    \\n \\n—      Reward-weighted      MLE      (Sec.      6.1  \\n \\n  Xo      Unknown      reward      Memory      effi-      oe      eward-welgnte      (Sec.      6.1) unctions      —_\\\\\\\\\\\\\\\\_—      ciency</p><h4>No      fine-tuning      Computational</h4><p>efficiency —\\n \\n    NN      Direct      backpropagation      (Sec.      5.2)</p><h5>Value-weighted      sampling      (Sec.      6.2)</h5><p>Figure      3:      Practical      recommendation      of      RL-based      algorithms      to      optimize      downstream      reward  \\n \\n  functions.</p><h6>7.1      Fine-Tuning      with      Known,      Differentiable      Reward      Functions</h6><p>ions  \\n \\n  When      accurate      differentiable      reward      functions      are      available      (e.g.,      standard      in      computer      vision      tasks),  \\n \\n  our      primary      focus      is      typically      on      computational      and      memory      efficiency.\\nIn      general,      we      advocate      for  \\n \\n  employing      reward      backpropagation      (c.f.      Algorithm      2)      for      its      computational      efficiency.\\nAlternatively,  \\n \\n  if      memory      efficiency      is   \\n \\na      significant      concern,      we      suggest      using      PPO      due      to      its      stability.</p><h6>7.2      Fine-Tuning      with      Black-Box      Reward      Feedback</h6><p>back  \\n \\n  Here,      we      explore      scenarios      where      we      have      access      to      accurate      but      non-differentiable      (black-box)  \\n \\n  reward      feedback,      often      found      in      scientific      simulations.\\nThe      emphasis      still      remains      primarily      on  \\n \\n  computational      and      memory      efficiency.\\nIn      such      scenarios,      due      to      the      non-differentiability      of      reward  \\n \\n  feedback,      we      recommend      using      PPO      or      reward-weighted      MLE      for      the      following      reasons.</p><p> \\n \\n  In      the      preceding      section,      we      advocated      for      the      use      of      reward      backpropagation      due      to      its      com-  \\n \\n  putational      advantages.\\nHowever,      when      dealing      with      non-differentiable      feedback,      the      advantage  \\n \\n  of      reward      backpropagation      may      diminish.\\nThis      is      because      learning      from      such      feedback      requires  \\n \\n  learning      differentiable      reward      functions      to      make      the      algorithm      work      as      we      discuss      in      Section      5.2.  \\n \\n  However,      obtaining   \\n \\na      differentiable      reward      function      can      be      challenging      in      many      domains.\\nFor  \\n \\n  example,      in      molecular      property      prediction,      molecular      fingerprints      are      informative      features,      and  \\n \\n  accurate      reward      functions      can      be      derived      by      training      simple      neural      networks      on      these      features  \\n \\n  (Pattanaik      and      Coley,      2020).\\nYet,      these      mappings,      grounded      in      prior      scientific      understanding,      are  \\n \\n  non-differentiable,      thereby      restricting      their      applicability.</p><p> \\n \\n  In      contrast,      PPO      and      reward-weighted      MLE      allow      for      policy      updates      without      the      explicit  \\n \\n  requirement      to      learn   \\n \\na      differentiable      reward      function,      offering   \\n \\na      significant      advantage      over      reward  \\n \\n  backpropagation.</p><h6>7.3      Fine-Tuning      with      Unknown      Rewards      Functions</h6><p>ions  \\n \\n  When      reward      functions      (or      computational      proxies      in      Section      7.2)      are      unavailable,      we      must      learn  \\n \\n  from      data      with      reward      feedback.\\nIn      such      cases,      compared      to      the      previous      scenarios,      two      important  \\n \\n  considerations      arise:</p><p>¢\\n \\n   Not      only      computational      or      memory      efficiency      but      also      feedback      efficiency      (i.e.,      sample  \\n \\n  efficiency      regarding      reward      feedback)      is      crucial.</p><p> \\n \\n \\n¢      Given      that      learned      reward      feedback      may      not      generalize      well      outside      the      training      data      distri-  \\n \\n  butions,      it      is      essential      to      constrain      fine-tuned      models      to      prevent      significant      divergence      from  \\n \\n  diffusion      models.\\nIn      these      situations,      not      only      soft      PPO      and      direct      backpropagation      but      also  \\n \\n  methods      discussed      in      Section      6,      such      as      reward-weighted      MLE,      can      be      particularly      effective.</p><p>Further,      the      scenario      involving      unknown      reward      functions      can      be      broadly      categorized      into      two      cases,  \\n \\n  which      we      will      discuss      in      more      detail.</p><p>Offline      scenario      (Figure      4).\\nIn      many      scenarios,      even  \\n \\n  in      cases      where      reward      functions      are      unknown,      we      of-  \\n \\n.     .</p><p>Offline      data      with ten      have      access      to      offline      data      with      reward      feedback      reward      feedback</p><li>(i)      (i)   \\n \\nO   \\n \\n;   \\n \\ni       a”      ra  \\n \\n.      ne      straightforward      approach      is      to</li><p> \\n \\n \\n{   \\n \\n,   \\n \\n(   \\n \\n)    \\n.\\n    .\\n \\n   g\\n \\n   PP  \\n \\n.      <=>      Learning      reward  \\n \\n  perform      regression      with      neural      networks      and      build   \\n \\na      models  \\n \\n  learned      regressor      *.\\nHowever,      this      learned      reward      func-      SS  \\n \\n  tion   \\n \\n*      might      not      generalize      well      beyond      the      distribution      x)  \\n \\n  of      the      offline      data.\\nIn      regions      outside      this      distribution,   \\n \\n7       could      assign   \\n \\na      high      value      even      when      the      actual      reward  \\n \\n  r      is      low      due      to      high      uncertainty.\\nConsequently,      we  \\n \\n  could      be      easily      misled      by      out-of-distribution      samples  \\n \\n  if      we      solely      optimize      7.\\n|Fine-tuning      diffusion      models   \\n \\n|       One      approach      to      alleviate      this      issue      is      to      adopt   \\n \\na       pessimistic      (i.e.,      conservative)      strategy,      as      proposed      in      Figure      4:      Offline      scenario  \\n \\n  Uehara      et      al.\\n(2024).\\nThis      technique      has      been      widely      Collecting      data]   \\n \\nY      Learning      reward  \\n \\n  applied      in      the      field      of      offline      RL      (Levine,      2018).\\nThe      aa  \\n \\n  main      idea      is      to      penalize   \\n \\n7      outside      the      distributions      us-  \\n \\n  ing      techniques      such      as      adding      an      explicit      penalty      term  \\n \\n  (Yu      et      al.,      2020;      Chang      et      al.,      2021),      bootstrapping,  \\n \\n  or      more      sophisticated      methods      (Kumar      et      al.,      2020;  \\n \\n  Xie      et      al.,      2021;      Rigter      et      al.,      2022;      Uehara      and      Sun,  \\n \\n  2021).\\nBy      doing      so,      we      can      avoid      being      fooled      by  \\n \\n  out-to-distribution      regions      while      still      benefiting      from  \\n \\n  the      extrapolation      capabilities      of      reward      models.</p><p>S—      diffusion      models      |</p><p>Figure      5:      Online      scenario</p><p> \\n \\n  Online      scenario      (Figure      5).\\nConsider      situations  \\n \\n  where      reward      functions      are      unknown,      but      we      can      gather      data      online.\\nThese      scenarios      are      often  \\n \\n  referred      to      as   \\n \\na      lab-in-the-loop      setting.</p><p> \\n \\n  In      such      scenarios,      Uehara      et      al.\\n(2024)      proposes      an      iterative      procedure      comprising      three      steps:</p><li>(1)      collecting      feedback      data      by      exploring      new      areas      (i.e.,      obtaining   \\n \\nx      from      current      diffusion      models  \\n \\n  and      corresponding      r(x)),      (2)      learning      the      reward      function      from      the      collected      feedback      data,      and</li><li>(3)      fine-tuning      current      diffusion      models      by      maximizing      the      learned      reward      function      enhanced      by  \\n \\n  an      optimistic      bonus      term,      using      algorithms      discussed      in      Section      4.2.\\nIn      contrast      to      standard      online  \\n \\n  PPO,      where      feedback      data      is      directly      used      in      the      fine-tuning      process,      actual      reward      feedback      is  \\n \\n  only      used      in      step      (2)      and      not      in      step      (3).\\nAn      additional      key      aspect      is      the      use      of      an      optimistic  \\n \\n  reward      function      that      encourages      exploration      beyond      the      distributions      of      the      current      diffusion      model.\\n \\n \\n  This      deliberate      separation      between      reward      learning      and      fine-tuning      steps,      coupled      with      the      use      of  \\n \\n  optimism,      significantly      enhances      feedback      efficiency.</li><p>8\\n \\n   Connection      with      Classifier      Guidance</p><p> \\n \\n  Classifier      guidance      (Dhariwal      and      Nichol,      2021)      is      commonly      used      for      conditional      generation      in  \\n \\n  diffusion      models.\\nInterestingly,      RL-based      methods      share   \\n \\na      close      connection      with      classifier      guidance.\\n \\n \\n  More      specifically,      in      this      section,      following      Zhao      et      al.\\n(2024),      we      elucidate      how      RL-based      methods  \\n \\n  can      be      applied      to      conditional      generation.\\nFurthermore,      from      this      unified      viewpoint,      we      highlight  \\n \\n  that      classifier      guidance      is      seen      as   \\n \\na      value-weighted      sampling      in      Section      4.2.</p><h6>8.1      Classfier      Guidance</h6><p>ance  \\n \\n  In      this      section,      we      first      introduce      classifier      guidance.\\nClassifier      guidance      utilizes      an      unconditional  \\n \\n  pre-trained      model      for      conditional      generation.\\nMore      specifically,      we      consider   \\n \\na      scenario      where      we  \\n \\n  have   \\n \\na      pre-trained      diffusion      model,      which      enables      us      to      sample      from      p?\\n\"*(x),      and      data      with      feedback  \\n \\n  {x,y},      where   \\n \\ny      represents      the      new      label      we      want      to      condition      on.\\nOur      objective      here      is      to      sample  \\n \\n  from      p(-|y)   \\n \\n€      [Y   \\n \\n>      ACR), P(-   \\n \\n|      y)   \\n \\nx      py(yl-)pP*(-).\\n(37)</p><p>where      p,   \\n \\n:   \\n \\nX      —>      A(Y)      denotes      the      conditional      distribution      of   \\n \\ny      given      x.</p><p>Motivation.\\nIn      classifier      guidance,      we      consider      the      following      policy:</p><p>pe      (-lare,      y)   \\n \\n=   \\n \\nN      (p(22,      t;      Opre)   \\n \\n+      a(t)      Ve      log      Vy      (Xt;      t),      a*(t))   \\n \\n’ dy(x,t)   \\n \\n=      Egy.\\nyap,      (-leo)      L(Y   \\n \\n=      y)|a,   \\n \\n=      2].</p><p>Under      the      continuous-time      formulation      of      diffusion      models,      applying      Doob’s      h-transform      (Rogers  \\n \\n  and      Williams,      2000),      it      is      shown      that      we      can      sample      from      p(x|y)      by      sequentially      running      {p7\"\\\\\\\\\\\\\\'(-|7+,      y)      }i-r41-</p><p>Algorithm.\\nSpecifically,      the      algorithm      comprises      two      steps.\\nIt’s      important      to      note      that      this      is      not  \\n \\n  a      fine-tuning      method      but      rather      an      inference-time      technique      as      we      freeze      the      pre-trained      model.</p><li>1. Learning      q,(x,t)      through      regression      by      gathering      data      containing      {z;,      y}.\\nHere,      for      each  \\n \\n  (Xo,      y),      using   \\n \\na      forward      policy      for      the      pre-training      (from   \\n \\n0      to      T),      we      obtain      2;      starting      from</li><p>Lo.</p><li>2. During      inference,      sequentially      execute      {p2\"(-|a,,      y)      }1_>   \\n \\n1      by      leverage      the      fact      that      compared</li><p>gui to      the      policy      in      pre-trained      diffusion      models,      the      mean      of      {p?\\n\"\\\\\\\\\\\\\\'(-|71,      y)      }i-74,      is      just      shifted  \\n \\n  with      an      additional      term      V,,      log      q,(2,      t).</p><p> \\n \\n  Notably,      the      method      described      above      can      be      reduced      to      value-weighted      sampling      (cf.      Sec-  \\n \\n  tion      6.2),      when      r(x,      y)   \\n \\n=      log      p(y|x).\\nInspired      by      such      an      observation,      we      apply      an      RL-based  \\n \\n  fine-tuning      method      for      conditional      generation      as      below.</p><h7>8.2.      RL-Based      Fine-Tuning      for      Conditional      Generation</h7><p>tion  \\n \\n  In      this      section,      following      Zhao      et      al.\\n(2024),      we      explain      an      RL-based      fine-tuning      approach      for  \\n \\n  conditional      generation.\\nThe      core      idea      is      that      setting      r(x,      y)   \\n \\n=      log      p(y|x)      allows      us      to      sample      from  \\n \\n  the      target      conditional      distribution      (37),      drawing      on      the      insights      discussed      in      Section      4.2.  \\n \\n  More      precisely,      introducing   \\n \\na      distribution      over   \\n \\ny      as      q(-),      we      formulate      the      following      RL      problem:</p><p>{Di      ft   \\n \\n=      argmax      Egy,      (-\\\\\\\\\\\\\\\\-y)}.yott      log      p(y|xo)   \\n \\n—      NeopyaKL(pe(-|e,      y)      ||P      (late).\\n \\n \\n  {prE[(V,.X)PA(A)]      Heyy (38)</p><p> \\n \\n  Compared      to      the      objective      function      (18)      in      Section      4.2,      the      main      differences      are:      (1)      policy      space  \\n \\n  is      expanded      to      accommodate      additional      control      (i.e.,      y):      pz(-|-,-)   \\n \\nC      [R¢   \\n \\nx   \\n \\nY   \\n \\n+      A(R®)],      (2)      the  \\n \\n  fine-tuning      objectives      are      set      as      log-likelihoods:      log      p(y|z).\\nThen,      following      Theorem      1,      we      can  \\n \\n  establish      the      following      result.</p><p>Corollary      2.\\nThe      distribution      induced      by      {p*}1_74      (ue.   \\n \\n[      {Teri      Ph      (ae-1   \\n \\n|      te,      y)}deur      is      the target      conditional      distribution      (37).</p><p> \\n \\n  The      above      corollary      suggests      that      for      conditional      generation,      we      can      utilize      various      off-the-shelf  \\n \\n  algorithms      discussed      in      Section      4.2      and      6,      such      as      PPO,      reward      backpropagation,      and      reward-  \\n \\n  weighted      MLE,      as      well      as      value-weighted      sampling.</p><p>9\\n \\n   Connection      with      Flow-Based      Diffusion      Models</p><p> \\n \\n  We      elucidate      the      close      relationship      between      bridge      matching      and      fine-tuning,      as      summarized      in  \\n \\n  Table      2.\\nBridge      (flow)      matching      (Liu      et      al.,      2022;      Tong      et      al.,      2023;      Lipman      et      al.,      2023;      Liu      et      al.,  \\n \\n  2022;      Shi      et      al.,      2024;      Albergo      and      Vanden-Eijnden,      2022),      has      recently      gained      popularity      as   \\n \\na       distinct      type      of      diffusion      model      from      score-matching-based      ones.\\nThese      works      suggest      that      training  \\n \\n  and      inference      speed      will      be      accelerated      due      to      the      ability      to      define   \\n \\na      more      direct      trajectory      from  \\n \\n  noise      to      data      distribution      than      score-matching-based      diffusion      models.</p><p> \\n \\n  The      connection      between      bridge      matching      and      fine-tuning      becomes      evident      in      the      continuous-  \\n \\n  time      formulation.\\nTo      begin,      we      first      provide   \\n \\na      brief      overview      of      the      continuous-time      formulation      of  \\n \\n  score-matching-based      diffusion      models.</p><p> \\n \\n  Table      2:      Comparison      between      training      score/flow-based      diffusion      models      and      fine-tuning.\\nWe  \\n \\n  optimize      the      parameter   \\n \\n6      in      P’,      which      represents      the      induced      distribution      over      trajectories      associated  \\n \\n  with      6.</p><h8>Loss      Note      of      Q</h8><p>Score-based      KL(Q\\\\\\\\\\\\\\'™¢|P?)      Qtime:      Time-reversal      of      reference      SDE      (reference training      SDE      is      defined      from   \\n \\n0      to      T’      where      the      initial      dis-  \\n \\n  tribute      at   \\n \\n0      is      data      distribution)</p><p> \\n \\n  Flow-based      KL(Q*°>      ||P?)      @Q*°°h:      Coupling      between      the      reference      SDE      from  \\n \\n  training   \\n \\nT      to   \\n \\n0      conditioning      at   \\n \\n0      (dist.      over      x.;|7o)      and  \\n \\n  a      data      distribution      at      time   \\n \\n0      (dis.      over      x).\\nNote  \\n \\n  that      reference      SDE      is      defined      from      7’      to      0.</p><p> \\n \\n  Fine-tuning      KL(P?||Q*°)      @*:      Coupling      between      the      pre-trained      SDE  \\n \\n  from   \\n \\nT      to   \\n \\n0      conditioned      on   \\n \\n0      (dist.      over      x7.1|X0)  \\n \\n  and   \\n \\na      data      distribution      at      time   \\n \\n0      (dis.      over      Xo) 9.1      Continuous      Time      Formulation      of      Score-Based      diffusion      models</p><p> \\n \\n  We      provide   \\n \\na      brief      overview      of      the      continuous-time      formulation      of      diffusion      models      (Song      et      al.,  \\n \\n  2021),      further      expanding      Section      1.1.1.\\nIn      this      formulation,      we      initially      define      the      forward      reference  \\n \\n  SDE,      spanning      from      time   \\n \\n0      to      7’      (e.g.,      variance      exploding      (VE)      process      or      variance      preserving      (VP)  \\n \\n  process),      by      setting      an      initial      distribution      at   \\n \\n0      as      the      data      distribution.\\nAdditionally,      the      reference  \\n \\n  SDE      is      tailored      to      converge      to      an      easy      distribution      at      time      7’,      such      as      the      normal      distribution.</p><p> \\n \\n  Subsequently,      we      consider      the      time-reversal      SDE      (Anderson,      1982),      progressing      from      T\\\\\\\\\\\\\\'      to      0.\\nIf  \\n \\n  we      could      learn      this      time-reversal      SDE,      it      would      enable      us      to      sample      from      the      data      distribution      by  \\n \\n  starting      with      the      easy      initial      distribution      (at      7’)      and      following      the      time-reversal      SDE      from   \\n \\nT      to   \\n \\n0      at  \\n \\n  inference      time.</p><p> \\n \\n  Training.\\nNow,      the      remaining      question      is      how      to      learn      this      time-reversal      SDE.\\nHere,      by      repre-  \\n \\n  senting      the      induced      distribution      over      trajectories      from   \\n \\nT      to   \\n \\n0      with      this      time-reversal      SDE      as      Q\\\\\\\\\\\\\\'™®,  \\n \\n  our      goal      is      to      train   \\n \\na      new      SDE,      parameterized      by   \\n \\n@      in      the      neural      network,      such      that      the      induced  \\n \\n  distribution      P®      over      trajectories      associated      with   \\n \\n6      aligns      with      Q\"™*.\\nSpecifically,      the      objective      is      to minimize      the      following      losses:</p><p> \\n \\n  argmin      KL(Q\"™||P*).\\n(39)  \\n \\n  0  \\n \\n  With      some      algebra,      leveraging      the      observation      that      the      time-reversal      SDE      comprises      the      original  \\n \\n  forward      SDE      and      the      scoring      term      of      the      marginal      distribution      (Anderson,      1982)      as      in      (2),      Song  \\n \\n  et      al.\\n(2021)      demonstrates      that      this      KL      loss      (39)      is      approximated      as Dore   \\n \\n=      argmin      Eeuni((0,7))01~qyo0(weleo)co~pP\"e      Fe      Fl      Vey      log      jo      (Xt   \\n \\n|      Xo)   \\n \\n_      so(Xt,      t)|I?\\n],      (40)  \\n \\n  which      is      equal      to      (4)      where      the      weight      A(t)      is      {o?\\n}?\\n(for      notation,      refer      to      Section      1.1.1).</p><h9>9.2      Flow-Based      (Bridge-Based)      Diffusion      Models</h9><p>dels We      adopt      the      explanation      outlined      in      Liu      et      al.\\n(2022).\\nIn      this      framework,      we      begin      by      considering  \\n \\n  a      reference      SDE      (from      T\\\\\\\\\\\\\\'      to      0),      x;   \\n \\n=      zr_;      where      z;      is      defined      as      Brownian      motion:</p><p>dz,   \\n \\n=      dwt,      te      (0,      7).\\n(41)</p><p> \\n \\n  Subsequently,      we      consider      the      coupling      of      the      reference      SDE      conditioned      on      state      at      time   \\n \\n0      (i.e.,  \\n \\n  distribution      of      xo.7      conditional      on      x)      and      the      data      distribution      at      time      0,      denoting      the      resulting  \\n \\n  distribution      as      Q*°?,      Informally,      this      Q*°°?\\nis      characterized      as      follows.</p><p>Informal      characterization      of      Q*”.\\nWhen      discretizing      the      time      step,      by      denoting      the      distribution  \\n \\n  induced      by      the      reference      SDE      as      p\"(x7,--+   \\n \\n,      29),      we      define QF      (a7,      tae   \\n \\n,      20)   \\n \\n=      p(x,      ay   \\n \\n|      r9)pP\"°      (x9).</p><p>The      distribution      Q*”      above      is   \\n \\na      limiting      distribution      obtained      by      making      the      discretization      step small.</p><p>Training.\\nIn      bridge-based      diffusion      models,      our      objective      is      to      train   \\n \\na      new      SDE,      parametrized      by  \\n \\n  6      (e.g.,   \\n \\na      neural      network),      to      align      the      induced      distribution      P®      with      Q*°?.\\nSpecifically,      bridge-based training      aims      to      minimize aremin      KL(Q*>||P’).\\n(42)  \\n \\n  0  \\n \\n  While      the      method      to      minimize      the      above      is      currently      unclear,      we      can      derive   \\n \\na      trainable      objective  \\n \\n  through      algebraic      manipulation.\\nInitially,      we      observe      that      the      reference      SDE      (41),      conditioned      on  \\n \\n  state      xo      at      time   \\n \\n0      (zr   \\n \\n=      D),      is      expressed      as:</p><p>b\\n \\n \\n  b—      2;</p><p>t\\n \\n   €\\n \\n   [0,T];dz?\\n \\n \\n=      g?(z?,t)dt      +du,,  \\n \\n  g(z?,t)   \\n \\n=   \\n \\na       (43)</p><p>using      the      seminal      “Doob’s      h-transform”      (Rogers      and      Williams,      2000).\\nThis      SDE      is      commonly  \\n \\n  known      as      the      Brownian      bridge.\\nNow,      we      consider   \\n \\na      new      SDE      parameterized      by      0:</p><p>t\\n \\n   €\\n \\n   [0,7];      dz   \\n \\n=      s(z,t;      @)dt   \\n \\n+      dw.\\n(44)</p><p>Then,      we      can      demonstrate      that      minimizing      the      KL      loss      in      (42)      is      equivalent      to argmin      E,W      unil0,7],1~4\"°      cop?\\n[Ilg”      (2,      t)   \\n \\n_      s(21,      t;      0)      M3]      (45)</p><p>where      q;°      denotes      the      induced      distribution      at      time   \\n \\nt      following      SDE      (43)      conditioned      on      xo,      and  \\n \\n  s:      Xx      |0,T]   \\n \\n>   \\n \\n&      is      the      model      we      aim      to      optimize.</p><p> \\n \\n  Flow-based      diffusion      models.\\nThe      aforementioned      formulation      has      been      originally      proposed      as   \\n \\na       bridge-based      SDE      (Liu      et      al.,      2022)      because      the      conditional      SDE      is      often      referred      to      as   \\n \\na      Brownian  \\n \\n  bridge.\\nFlow-based      diffusion      models      proposed      in      Lipman      et      al.\\n(2023);      Tong      et      al.\\n(2023)      share   \\n \\na       close      relationship      with      bridge-based      diffusion      models.\\nThese      flow-based      models      use   \\n \\na      target      SDE (43)      and      an      SDE      with   \\n \\na      parameter      (44)      without      stochastic      terms      (1.e.,      no      dw,)      and      aim      to      minimize  \\n \\n  the      loss      function      (45).</p><p> \\n \\n  Comparison      with      score-based      diffusion      models.\\nBoth      score-based      diffusion      models      and      bridge-  \\n \\n  based      diffusion      models      aim      to      minimize      KL      divergences.\\nDespite      its      similarity,      in      contrast      to  \\n \\n  score-based      diffusion      models,      bridge-based      diffusion      models      target      Q*°>      rather      than      the      time-  \\n \\n  reversal      SDE      Q\"™®.\\nThe      distribution      Q*”      is      often      considered      preferable      because      it      can      impose      an  \\n \\n  SDE      that      navigates      efficiently      from   \\n \\n0      to      7’      (e.g.,   \\n \\na      Brownian      bridge      with      minimal      noise).\\nThis      can  \\n \\n  expedite      the      learning      process      since      we      lack      direct      control      over      the      time-reversal      SDE      in      score-based  \\n \\n  diffusion      models.</p><h10>9.3.      Connection      with      RL-Based      Fine-Tuning</h10><p>ning  \\n \\n  RL-based      fine-tuning      shares   \\n \\na      close      relationship      with      flow-based      training.\\nTo      illustrate      this,      consider  \\n \\n  the      reference      SDE      (from      7’      to      0)      following      the      pre-trained      diffusion      model.\\nThen,      akin      to      flow-  \\n \\n  based      training,      we      consider      the      coupling      of      the      reference      SDE      conditioned      on      state      at      time   \\n \\n0       (i.e.,      distribution      over      xo-r      conditioned      on      x9)      and      the      target      distribution      at      time   \\n \\n0      (i.e.,      p,(-)   \\n \\n«       exp(r(-)/a)p?\\n\"°(-)),      denoting      the      induced      distribution      as      Q*™.</p><p>Informal      characterization      of      Q™°*.\\nInformally,      Q*\"°      is      introduced      as      follows.\\nWhen      discretizing  \\n \\n  the      time      step,      by      denoting      the      distribution      induced      by      the      reference      SDE      as      p™\\\\\\\\\\\\\\'(xp,-+-   \\n \\n,      9),      we define Qi\"      (xr,   \\n \\na   \\n \\n,      0)   \\n \\n=      p(x,      nr      Ly   \\n \\n|      L0)Pr      (Xo).</p><p>The      distribution      Q*°      above      is   \\n \\na      limiting      distribution      obtained      by      making      the      discretization      step small.</p><p>Training.\\nNow,      we      introduce   \\n \\na      new      SDE      parameterized      by   \\n \\n@      such      as      an      neural      network      to      align  \\n \\n  the      induced      distribution      P’      with      Q®\"*.\\nActually,      in      the      continuous      formulation,      the      RL-problem      (18) in      Section      4.2      is      equal      to      solving argmin      KL(P?||Q\"™).\\n \\n \\n  0 This      formalization      has      been      presented      in      Uehara      et      al.\\n(2024).\\nIntuitively,      as      we      see      in      Theorem      3,  \\n \\n  this      is      expected      because      we      see      that      the      posterior      distribution      induced      by      pre-trained      models:</p><p>ref   \\n \\n(      PD      (ar,-++      21   \\n \\n|      to)      (=      vey      (er|er_-1)p      Pp      (ar-1   \\n \\n|      er_2)      +++      pp\"      (x1   \\n \\n|      20), remains      preserved      after      fine-tuning      based      on      the      RL-formulation      (18).</p><p> \\n \\n  Comparison      with      flow-based      training.\\nIn      contrast      to      flow-based      training,      RL-based      fine-tuning  \\n \\n  minimizes      the      inverse      KL      divergence      rather      than      the      KL      divergence.\\nIntuitively,      this      is      because,  \\n \\n  unlike      Qt™*      or      Q*>   \\n \\n,      we      cannot      sample      from      Q*”      (recall      that   \\n \\na      marginal      distribution      at      time   \\n \\n0      in  \\n \\n  Qi,      i.e.,      p,,      is      unnormalized);      on      the      other      hand,      the      marginal      distributions      at      time   \\n \\n0      in      both      Q\\\\\\\\\\\\\\'™*  \\n \\n  and      Q*°°>      are      data      distributions.</p><p> \\n \\n  Remark   \\n \\n7      (Extension      to      f-divergence).\\nEach      of      these      methods      can      be      technically      extended      by  \\n \\n  incorporating      more      general      divergences      beyond      KL      divergence.\\nFor      instance,      in      the      context      of  \\n \\n  fine-tuning,      please      see      Tang      (2024).</p><p>10      Connection      with      Sampling      from      Unnormalized      Distributions</p><p> \\n \\n  Numerous      studies      delve      into      sampling      from      an      unnormalized      distribution      proportional      to      exp(r()),  \\n \\n  commonly      known      as      the      Gibbs      distribution,      extensively      discussed      in      the      computational      statistics  \\n \\n  and      statistical      physics      community      (Robert,      2014).\\nThis      issue      is      pertinent      to      our      work,      as      during  \\n \\n  fine-tuning,      the      target      distribution      is      also      formulated      in      the      Gibbs      distribution      form      in      (10),      which      is  \\n \\n  proportional      to      exp(r(a))p?\\n\"*      (x).</p><p> \\n \\n  The      literature      employs      two      main      approaches      to      tackle      this      challenge:      Markov      Chain      Monte  \\n \\n  Carlo      (MCMC)      and      RL-based      methods.\\nIn      particular,      the      RL-based      diffusion      model      fine-tuning  \\n \\n  we      have      discussed      so      far      is      closely      related      to      the      latter      approach.\\nIn      this      section,      we      explain      and  \\n \\n  compare      these      two      approaches.</p><h10>10.1      Markov      Chain      Monte      Carlo      (MCMC)</h10><p>CMC)  \\n \\n  In      MCMC,   \\n \\na      Markov      chain      is      constructed      to      approximate   \\n \\na      target      distribution      such      that      the      equilib-  \\n \\n  rium      distribution      of      the      Markov      chain      converges      to      the      target      distribution.\\nHowever,      dealing      with  \\n \\n  high-dimensional      domain      spaces      is      challenging      for      MCMC.\\n \\n \\nA      common      strategy      involves      leveraging  \\n \\n  gradient      information,      such      as      the      Metropolis-adjusted      Langevin      algorithm      (MALA)      (Besag,      1994)  \\n \\n  or      Hamiltonian      Monte      Carlo      (HMC)      (Neal      et      al.,      2011;      Girolami      and      Calderhead,      2011).\\nMALA  \\n \\n  shares      similarities      with      classifier      guidance      and      value-weighted      sampling,      as      both      methods      depend  \\n \\n  on      the      first-order      information      from      reward      (or      value)      functions.</p><p> \\n \\n  Can      we      use      MCMC      for      fine-tuning      diffusion      models?\\nIn      the      context      of      fine-tuning      dif-  \\n \\n  fusion      models,      while      it      may      seem      intuitive      to      sample      from      the      target      distribution      (10)      (i.e.,  \\n \\n  Dy   \\n \\n&      exp(r(x))pP\"*(x))      using      MCMC,      it’s      not      straightforward      for      the      following      reasons:</p><p>¢\\n \\n   We      lack      of      an      analytical      form      of      p?\\n\"*(-).<ul><li>*\\n \\n   Additionally,      even      if      we      can      estimate      p?\\n’°(-)      in      an      unbiased      manner,      as      in      Chen      et      al.\\n(2018),  \\n \\n  the      mixing      time      for      obtaining   \\n \\na      single      sample      in      MCMC      might      be      lengthy.</li></ul></p><p> \\n \\n  Therefore,      we      explore      creating   \\n \\na      generative      model      (i.e.,      diffusion      model)      for      this      task      using      RL-  \\n \\n  based      fine-tuning      so      that      we      can      easily      sample      from      p,(-)      during      inference      (i.e.,      simulating      the  \\n \\n  time-reversal      SDE      (2)      with      policies      from   \\n \\n¢   \\n \\n=      T\\\\\\\\\\\\\\'+   \\n \\n1      to   \\n \\nt   \\n \\n=      1),      without      relying      on      MCMC.\\nThis  \\n \\n  addresses      the      concern      of      MCMC      by      avoiding      the      need      to      estimate      p?\\n\"*(-)      and      minimizing      inference  \\n \\n  time,      albeit      at      the      expense      of      increased      training      time.</p><p>10.2}      RL-Based      Approaches  \\n \\n  While      MCMC      has      historically      been      widely      used      for      this      purpose,      recent      works      have      proposed      an  \\n \\n  RL-based      approach      or      its      variant      (e.g.,      Bernton      et      al.      (2019);      Heng      et      al.\\n(2020);      Huang      et      al.\\n(2023);</p><p> \\n \\n  Vargas      et      al.\\n(2023)).\\nFor      example,   \\n \\na      seminal      study      (Zhang      and      Chen,      2021)      introduced   \\n \\na      method  \\n \\n  very      similar      to      reward      backpropagation.\\nIn      their      work,      they      initially      define      Brownian      motion      as   \\n \\na       reference      SDE      (from      7’      to      0),      similar      to      the      pre-trained      diffusion      models      in      our      context.\\nThen,      by  \\n \\n  appropriately      setting      rewards,      Zhang      and      Chen      (2021)      aims      to      learn   \\n \\na      new      SDE      (from      T\\\\\\\\\\\\\\'      to      0)      such  \\n \\n  that      we      can      sample      from      the      target      distribution      at      the      endpoint      0.</p><p> \\n \\n  These      approaches      offer      advantages      over      MCMC      due      to      their      ability      to      minimize      inference      time  \\n \\n  (amortization)      by      constructing   \\n \\na      generative      model      comprising      policies      in      the      training      time      and      solely  \\n \\n  executing      learned      policies      during      inference.\\nUnlike      MCMC,      these      methods      significantly      reduce      the  \\n \\n  inference      cost      as      we      don’t      need      to      be      concerned      about      the      mixing      time.\\nAdditionally,      RL-based  \\n \\n  approaches      have      the      potential      to      effectively      handle      high-dimensional,      complex      (i.e.,      multi-modal)  \\n \\n  distributions      by      harnessing      the      considerable      expressive      power      arising      from      latent      states.</p><p> \\n \\n  Despite      these      advantages,      it      is      often      unclear      how      to      select   \\n \\na      reference      SDE      in      these      works.\\nIn  \\n \\n  contrast,      in      RL-based      fine-tuning      of      diffusion      models,      we      directly      leverage      pre-trained      diffusion  \\n \\n  models      as      the      reference      SDEs,      i.e.,      as      our      prior      knowledge.</p><p> \\n \\n  Key      message.\\nFinally,      it      is      worth      noting      that      we      can      technically      utilize      any      off-the-shelf      RL      algo-  \\n \\n  rithms      for      training.\\nFor      example,      Zhang      and      Chen      (2021)      employs      reward      backpropagation,      while  \\n \\n  works      in      Gflownets      typically      leverage      PCL,      as      we      mention      in      Section      6.3.\\nHowever,      additionally,      it  \\n \\n  is      possible      to      use      PPO      or      reward-weighted      MLE      mentioned      in      Section      4.2.</p><p>11      Closely      Related      Directions</p><p>Lastly,      we      highlight      several      closely      related      directions      we      have      not      yet      discussed.</p><p> \\n \\n  Aligning      text-to-image      models.\\nWe      recognize      that      current      efforts      in      fine-tuning      diffusion      models  \\n \\n  primarily      revolve      around      aligning      text-to-image      models      using      human      feedback      (Lee      et      al.,      2023;  \\n \\n  Wallace      et      al.,      2023;      Wu      et      al.,      2023;      Yang      et      al.,      2023).\\nIn      this      paper,      our      goal      is      to      consider   \\n \\na      more  \\n \\n  general      and      fundamental      formulation      that      is      not      tailored      to      any      particular      task.</p><p> \\n \\n  Diffusion      models      for      RL.\\nDiffusion      models      are      well-suited      for      specific      reinforcement      learning  \\n \\n  applications      due      to      their      ability      to      model      complex      and      multimodal      distributions      as      polices      (Janner  \\n \\n  et      al.,      2022;      Ajay      et      al.,      2023;      Wang      et      al.,      2022;      Hansen-Estruch      et      al.,      2023;      Du      et      al.,      2024;      Zhu  \\n \\n  et      al.,      2023).\\nFor      example,      Wang      et      al.\\n(2022)      use      conditional      diffusion      models      as      policies      and  \\n \\n  demonstrate      their      effectiveness      in      typical      offline      RL      benchmarks.\\nWhile      RL-based      fine-tuning      also  \\n \\n  aims      to      maximize      certain      reward      functions,      the      primary      focus      of      this      tutorial      covers      methods      that  \\n \\n  leverage      pre-trained      diffusion      models      for      this      purpose.</p><p> \\n \\n  RL      from      human      feedback      (RLHF)      for      language      models.\\nRLHF      is      widely      discussed      in      the  \\n \\n  context      of      language      models,      where      many      algorithms      share      similarities      with      those      used      in      diffusion  \\n \\n  models      (Ouyang      et      al.,      2022;      Bai      et      al.,      2022;      Casper      et      al.,      2023).\\nFor      instance,      both      domains  \\n \\n  commonly      employ      PPO      as   \\n \\na      standard      approach,      and      reward-weighted      training      (or      reward-weighted  \\n \\n  MLE)      often      serves      as   \\n \\na      basic      baseline      approach.\\nDespite      these      similarities,      nuanced      differences  \\n \\n  exist      mainly      because      diffusion      models      typically      operate      in   \\n \\na      continuous      input      space      rather      than  \\n \\n  a      discrete      one.\\nMoreover,      techniques      like      value-weighted      sampling      emerge      uniquely      within      the  \\n \\n  context      of      diffusion      models.</p><p>12      Summary</p><p> \\n \\n  In      this      article,      we      comprehensively      explain      how      fine-tuning      diffusion      models      to      maximize      down-  \\n \\n  stream      reward      functions      can      be      formalized      as   \\n \\na      reinforcement      learning      (RL)      problem      in      entropy-  \\n \\n  regularized      Markov      decision      processes      (MDPs).\\nBased      on      this      viewpoint,      we      elaborate      on      the  \\n \\n  application      of      various      RL      algorithms,      such      as      PPO      and      reward-weighted      MLE,      specifically      tailored  \\n \\n  for      fine-tuning      diffusion      models.\\nAdditionally,      we      categorize      different      scenarios      based      on      how  \\n \\n  reward      feedback      is      obtained.\\nWhile      this      categorization      is      not      always      explicitly      mentioned      in      many  \\n \\n  existing      works,      it      is      crucial      when      selecting      appropriate      algorithms.\\nFinally,      we      discuss      the      rela-  \\n \\n  tionships      with      several      related      topics,      including      classifier      guidance,      Gflownets,      path      integral      control  \\n \\n  theory,      and      sampling      from      unnormalized      distributions.</p><h5>Value-weighted      sampling      (Sec.      6.2)</h5><p>Figure      3:      Practical      recommendation      of      RL-based      algorithms      to      optimize      downstream      reward  \\n \\n  functions.</p><h6>7.1      Fine-Tuning      with      Known,      Differentiable      Reward      Functions</h6><p>ions  \\n \\n  When      accurate      differentiable      reward      functions      are      available      (e.g.,      standard      in      computer      vision      tasks),  \\n \\n  our      primary      focus      is      typically      on      computational      and      memory      efficiency.\\nIn      general,      we      advocate      for  \\n \\n  employing      reward      backpropagation      (c.f.      Algorithm      2)      for      its      computational      efficiency.\\nAlternatively,  \\n \\n  if      memory      efficiency      is   \\n \\na      significant      concern,      we      suggest      using      PPO      due      to      its      stability.</p><h6>7.2      Fine-Tuning      with      Black-Box      Reward      Feedback</h6><p>back  \\n \\n  Here,      we      explore      scenarios      where      we      have      access      to      accurate      but      non-differentiable      (black-box)  \\n \\n  reward      feedback,      often      found      in      scientific      simulations.\\nThe      emphasis      still      remains      primarily      on  \\n \\n  computational      and      memory      efficiency.\\nIn      such      scenarios,      due      to      the      non-differentiability      of      reward  \\n \\n  feedback,      we      recommend      using      PPO      or      reward-weighted      MLE      for      the      following      reasons.</p><p> \\n \\n  In      the      preceding      section,      we      advocated      for      the      use      of      reward      backpropagation      due      to      its      com-  \\n \\n  putational      advantages.\\nHowever,      when      dealing      with      non-differentiable      feedback,      the      advantage  \\n \\n  of      reward      backpropagation      may      diminish.\\nThis      is      because      learning      from      such      feedback      requires  \\n \\n  learning      differentiable      reward      functions      to      make      the      algorithm      work      as      we      discuss      in      Section      5.2.  \\n \\n  However,      obtaining   \\n \\na      differentiable      reward      function      can      be      challenging      in      many      domains.\\nFor  \\n \\n  example,      in      molecular      property      prediction,      molecular      fingerprints      are      informative      features,      and  \\n \\n  accurate      reward      functions      can      be      derived      by      training      simple      neural      networks      on      these      features  \\n \\n  (Pattanaik      and      Coley,      2020).\\nYet,      these      mappings,      grounded      in      prior      scientific      understanding,      are  \\n \\n  non-differentiable,      thereby      restricting      their      applicability.</p><p> \\n \\n  In      contrast,      PPO      and      reward-weighted      MLE      allow      for      policy      updates      without      the      explicit  \\n \\n  requirement      to      learn   \\n \\na      differentiable      reward      function,      offering   \\n \\na      significant      advantage      over      reward  \\n \\n  backpropagation.</p><h6>7.3      Fine-Tuning      with      Unknown      Rewards      Functions</h6><p>ions  \\n \\n  When      reward      functions      (or      computational      proxies      in      Section      7.2)      are      unavailable,      we      must      learn  \\n \\n  from      data      with      reward      feedback.\\nIn      such      cases,      compared      to      the      previous      scenarios,      two      important  \\n \\n  considerations      arise:</p><p>¢\\n \\n   Not      only      computational      or      memory      efficiency      but      also      feedback      efficiency      (i.e.,      sample  \\n \\n  efficiency      regarding      reward      feedback)      is      crucial.</p><p> \\n \\n \\n¢      Given      that      learned      reward      feedback      may      not      generalize      well      outside      the      training      data      distri-  \\n \\n  butions,      it      is      essential      to      constrain      fine-tuned      models      to      prevent      significant      divergence      from  \\n \\n  diffusion      models.\\nIn      these      situations,      not      only      soft      PPO      and      direct      backpropagation      but      also  \\n \\n  methods      discussed      in      Section      6,      such      as      reward-weighted      MLE,      can      be      particularly      effective.</p><p>Further,      the      scenario      involving      unknown      reward      functions      can      be      broadly      categorized      into      two      cases,  \\n \\n  which      we      will      discuss      in      more      detail.</p><p>Offline      scenario      (Figure      4).\\nIn      many      scenarios,      even  \\n \\n  in      cases      where      reward      functions      are      unknown,      we      of-  \\n \\n.     .</p><p>Offline      data      with ten      have      access      to      offline      data      with      reward      feedback      reward      feedback</p><li>(i)      (i)   \\n \\nO   \\n \\n;   \\n \\ni       a”      ra  \\n \\n.      ne      straightforward      approach      is      to</li><p> \\n \\n \\n{   \\n \\n,   \\n \\n(   \\n \\n)    \\n.\\n    .\\n \\n   g\\n \\n   PP  \\n \\n.      <=>      Learning      reward  \\n \\n  perform      regression      with      neural      networks      and      build   \\n \\na      models  \\n \\n  learned      regressor      *.\\nHowever,      this      learned      reward      func-      SS  \\n \\n  tion   \\n \\n*      might      not      generalize      well      beyond      the      distribution      x)  \\n \\n  of      the      offline      data.\\nIn      regions      outside      this      distribution,   \\n \\n7       could      assign   \\n \\na      high      value      even      when      the      actual      reward  \\n \\n  r      is      low      due      to      high      uncertainty.\\nConsequently,      we  \\n \\n  could      be      easily      misled      by      out-of-distribution      samples  \\n \\n  if      we      solely      optimize      7.\\n|Fine-tuning      diffusion      models   \\n \\n|       One      approach      to      alleviate      this      issue      is      to      adopt   \\n \\na       pessimistic      (i.e.,      conservative)      strategy,      as      proposed      in      Figure      4:      Offline      scenario  \\n \\n  Uehara      et      al.\\n(2024).\\nThis      technique      has      been      widely      Collecting      data]   \\n \\nY      Learning      reward  \\n \\n  applied      in      the      field      of      offline      RL      (Levine,      2018).\\nThe      aa  \\n \\n  main      idea      is      to      penalize   \\n \\n7      outside      the      distributions      us-  \\n \\n  ing      techniques      such      as      adding      an      explicit      penalty      term  \\n \\n  (Yu      et      al.,      2020;      Chang      et      al.,      2021),      bootstrapping,  \\n \\n  or      more      sophisticated      methods      (Kumar      et      al.,      2020;  \\n \\n  Xie      et      al.,      2021;      Rigter      et      al.,      2022;      Uehara      and      Sun,  \\n \\n  2021).\\nBy      doing      so,      we      can      avoid      being      fooled      by  \\n \\n  out-to-distribution      regions      while      still      benefiting      from  \\n \\n  the      extrapolation      capabilities      of      reward      models.</p><p>S—      diffusion      models      |</p><p>Figure      5:      Online      scenario</p><p> \\n \\n  Online      scenario      (Figure      5).\\nConsider      situations  \\n \\n  where      reward      functions      are      unknown,      but      we      can      gather      data      online.\\nThese      scenarios      are      often  \\n \\n  referred      to      as   \\n \\na      lab-in-the-loop      setting.</p><p> \\n \\n  In      such      scenarios,      Uehara      et      al.\\n(2024)      proposes      an      iterative      procedure      comprising      three      steps:</p><li>(1)      collecting      feedback      data      by      exploring      new      areas      (i.e.,      obtaining   \\n \\nx      from      current      diffusion      models  \\n \\n  and      corresponding      r(x)),      (2)      learning      the      reward      function      from      the      collected      feedback      data,      and</li><li>(3)      fine-tuning      current      diffusion      models      by      maximizing      the      learned      reward      function      enhanced      by  \\n \\n  an      optimistic      bonus      term,      using      algorithms      discussed      in      Section      4.2.\\nIn      contrast      to      standard      online  \\n \\n  PPO,      where      feedback      data      is      directly      used      in      the      fine-tuning      process,      actual      reward      feedback      is  \\n \\n  only      used      in      step      (2)      and      not      in      step      (3).\\nAn      additional      key      aspect      is      the      use      of      an      optimistic  \\n \\n  reward      function      that      encourages      exploration      beyond      the      distributions      of      the      current      diffusion      model.\\n \\n \\n  This      deliberate      separation      between      reward      learning      and      fine-tuning      steps,      coupled      with      the      use      of  \\n \\n  optimism,      significantly      enhances      feedback      efficiency.</li><p>8\\n \\n   Connection      with      Classifier      Guidance</p><p> \\n \\n  Classifier      guidance      (Dhariwal      and      Nichol,      2021)      is      commonly      used      for      conditional      generation      in  \\n \\n  diffusion      models.\\nInterestingly,      RL-based      methods      share   \\n \\na      close      connection      with      classifier      guidance.\\n \\n \\n  More      specifically,      in      this      section,      following      Zhao      et      al.\\n(2024),      we      elucidate      how      RL-based      methods  \\n \\n  can      be      applied      to      conditional      generation.\\nFurthermore,      from      this      unified      viewpoint,      we      highlight  \\n \\n  that      classifier      guidance      is      seen      as   \\n \\na      value-weighted      sampling      in      Section      4.2.</p><h6>8.1      Classfier      Guidance</h6><p>ance  \\n \\n  In      this      section,      we      first      introduce      classifier      guidance.\\nClassifier      guidance      utilizes      an      unconditional  \\n \\n  pre-trained      model      for      conditional      generation.\\nMore      specifically,      we      consider   \\n \\na      scenario      where      we  \\n \\n  have   \\n \\na      pre-trained      diffusion      model,      which      enables      us      to      sample      from      p?\\n\"*(x),      and      data      with      feedback  \\n \\n  {x,y},      where   \\n \\ny      represents      the      new      label      we      want      to      condition      on.\\nOur      objective      here      is      to      sample  \\n \\n  from      p(-|y)   \\n \\n€      [Y   \\n \\n>      ACR), P(-   \\n \\n|      y)   \\n \\nx      py(yl-)pP*(-).\\n(37)</p><p>where      p,   \\n \\n:   \\n \\nX      —>      A(Y)      denotes      the      conditional      distribution      of   \\n \\ny      given      x.</p><p>Motivation.\\nIn      classifier      guidance,      we      consider      the      following      policy:</p><p>pe      (-lare,      y)   \\n \\n=   \\n \\nN      (p(22,      t;      Opre)   \\n \\n+      a(t)      Ve      log      Vy      (Xt;      t),      a*(t))   \\n \\n’ dy(x,t)   \\n \\n=      Egy.\\nyap,      (-leo)      L(Y   \\n \\n=      y)|a,   \\n \\n=      2].</p><p>Under      the      continuous-time      formulation      of      diffusion      models,      applying      Doob’s      h-transform      (Rogers  \\n \\n  and      Williams,      2000),      it      is      shown      that      we      can      sample      from      p(x|y)      by      sequentially      running      {p7\"\\\\\\\\\\\\\\'(-|7+,      y)      }i-r41-</p><p>Algorithm.\\nSpecifically,      the      algorithm      comprises      two      steps.\\nIt’s      important      to      note      that      this      is      not  \\n \\n  a      fine-tuning      method      but      rather      an      inference-time      technique      as      we      freeze      the      pre-trained      model.</p><li>1. Learning      q,(x,t)      through      regression      by      gathering      data      containing      {z;,      y}.\\nHere,      for      each  \\n \\n  (Xo,      y),      using   \\n \\na      forward      policy      for      the      pre-training      (from   \\n \\n0      to      T),      we      obtain      2;      starting      from</li><p>Lo.</p><li>2. During      inference,      sequentially      execute      {p2\"(-|a,,      y)      }1_>   \\n \\n1      by      leverage      the      fact      that      compared</li><p>gui to      the      policy      in      pre-trained      diffusion      models,      the      mean      of      {p?\\n\"\\\\\\\\\\\\\\'(-|71,      y)      }i-74,      is      just      shifted  \\n \\n  with      an      additional      term      V,,      log      q,(2,      t).</p><p> \\n \\n  Notably,      the      method      described      above      can      be      reduced      to      value-weighted      sampling      (cf.      Sec-  \\n \\n  tion      6.2),      when      r(x,      y)   \\n \\n=      log      p(y|x).\\nInspired      by      such      an      observation,      we      apply      an      RL-based  \\n \\n  fine-tuning      method      for      conditional      generation      as      below.</p><h7>8.2.      RL-Based      Fine-Tuning      for      Conditional      Generation</h7><p>tion  \\n \\n  In      this      section,      following      Zhao      et      al.\\n(2024),      we      explain      an      RL-based      fine-tuning      approach      for  \\n \\n  conditional      generation.\\nThe      core      idea      is      that      setting      r(x,      y)   \\n \\n=      log      p(y|x)      allows      us      to      sample      from  \\n \\n  the      target      conditional      distribution      (37),      drawing      on      the      insights      discussed      in      Section      4.2.  \\n \\n  More      precisely,      introducing   \\n \\na      distribution      over   \\n \\ny      as      q(-),      we      formulate      the      following      RL      problem:</p><p>{Di      ft   \\n \\n=      argmax      Egy,      (-\\\\\\\\\\\\\\\\-y)}.yott      log      p(y|xo)   \\n \\n—      NeopyaKL(pe(-|e,      y)      ||P      (late).\\n \\n \\n  {prE[(V,.X)PA(A)]      Heyy (38)</p><p> \\n \\n  Compared      to      the      objective      function      (18)      in      Section      4.2,      the      main      differences      are:      (1)      policy      space  \\n \\n  is      expanded      to      accommodate      additional      control      (i.e.,      y):      pz(-|-,-)   \\n \\nC      [R¢   \\n \\nx   \\n \\nY   \\n \\n+      A(R®)],      (2)      the  \\n \\n  fine-tuning      objectives      are      set      as      log-likelihoods:      log      p(y|z).\\nThen,      following      Theorem      1,      we      can  \\n \\n  establish      the      following      result.</p><p>Corollary      2.\\nThe      distribution      induced      by      {p*}1_74      (ue.   \\n \\n[      {Teri      Ph      (ae-1   \\n \\n|      te,      y)}deur      is      the target      conditional      distribution      (37).</p><p> \\n \\n  The      above      corollary      suggests      that      for      conditional      generation,      we      can      utilize      various      off-the-shelf  \\n \\n  algorithms      discussed      in      Section      4.2      and      6,      such      as      PPO,      reward      backpropagation,      and      reward-  \\n \\n  weighted      MLE,      as      well      as      value-weighted      sampling.</p><p>9\\n \\n   Connection      with      Flow-Based      Diffusion      Models</p><p> \\n \\n  We      elucidate      the      close      relationship      between      bridge      matching      and      fine-tuning,      as      summarized      in  \\n \\n  Table      2.\\nBridge      (flow)      matching      (Liu      et      al.,      2022;      Tong      et      al.,      2023;      Lipman      et      al.,      2023;      Liu      et      al.,  \\n \\n  2022;      Shi      et      al.,      2024;      Albergo      and      Vanden-Eijnden,      2022),      has      recently      gained      popularity      as   \\n \\na       distinct      type      of      diffusion      model      from      score-matching-based      ones.\\nThese      works      suggest      that      training  \\n \\n  and      inference      speed      will      be      accelerated      due      to      the      ability      to      define   \\n \\na      more      direct      trajectory      from  \\n \\n  noise      to      data      distribution      than      score-matching-based      diffusion      models.</p><p> \\n \\n  The      connection      between      bridge      matching      and      fine-tuning      becomes      evident      in      the      continuous-  \\n \\n  time      formulation.\\nTo      begin,      we      first      provide   \\n \\na      brief      overview      of      the      continuous-time      formulation      of  \\n \\n  score-matching-based      diffusion      models.</p><p> \\n \\n  Table      2:      Comparison      between      training      score/flow-based      diffusion      models      and      fine-tuning.\\nWe  \\n \\n  optimize      the      parameter   \\n \\n6      in      P’,      which      represents      the      induced      distribution      over      trajectories      associated  \\n \\n  with      6.</p><h8>Loss      Note      of      Q</h8><p>Score-based      KL(Q\\\\\\\\\\\\\\'™¢|P?)      Qtime:      Time-reversal      of      reference      SDE      (reference training      SDE      is      defined      from   \\n \\n0      to      T’      where      the      initial      dis-  \\n \\n  tribute      at   \\n \\n0      is      data      distribution)</p><p> \\n \\n  Flow-based      KL(Q*°>      ||P?)      @Q*°°h:      Coupling      between      the      reference      SDE      from  \\n \\n  training   \\n \\nT      to   \\n \\n0      conditioning      at   \\n \\n0      (dist.      over      x.;|7o)      and  \\n \\n  a      data      distribution      at      time   \\n \\n0      (dis.      over      x).\\nNote  \\n \\n  that      reference      SDE      is      defined      from      7’      to      0.</p><p> \\n \\n  Fine-tuning      KL(P?||Q*°)      @*:      Coupling      between      the      pre-trained      SDE  \\n \\n  from   \\n \\nT      to   \\n \\n0      conditioned      on   \\n \\n0      (dist.      over      x7.1|X0)  \\n \\n  and   \\n \\na      data      distribution      at      time   \\n \\n0      (dis.      over      Xo) 9.1      Continuous      Time      Formulation      of      Score-Based      diffusion      models</p><p> \\n \\n  We      provide   \\n \\na      brief      overview      of      the      continuous-time      formulation      of      diffusion      models      (Song      et      al.,  \\n \\n  2021),      further      expanding      Section      1.1.1.\\nIn      this      formulation,      we      initially      define      the      forward      reference  \\n \\n  SDE,      spanning      from      time   \\n \\n0      to      7’      (e.g.,      variance      exploding      (VE)      process      or      variance      preserving      (VP)  \\n \\n  process),      by      setting      an      initial      distribution      at   \\n \\n0      as      the      data      distribution.\\nAdditionally,      the      reference  \\n \\n  SDE      is      tailored      to      converge      to      an      easy      distribution      at      time      7’,      such      as      the      normal      distribution.</p><p> \\n \\n  Subsequently,      we      consider      the      time-reversal      SDE      (Anderson,      1982),      progressing      from      T\\\\\\\\\\\\\\'      to      0.\\nIf  \\n \\n  we      could      learn      this      time-reversal      SDE,      it      would      enable      us      to      sample      from      the      data      distribution      by  \\n \\n  starting      with      the      easy      initial      distribution      (at      7’)      and      following      the      time-reversal      SDE      from   \\n \\nT      to   \\n \\n0      at  \\n \\n  inference      time.</p><p> \\n \\n  Training.\\nNow,      the      remaining      question      is      how      to      learn      this      time-reversal      SDE.\\nHere,      by      repre-  \\n \\n  senting      the      induced      distribution      over      trajectories      from   \\n \\nT      to   \\n \\n0      with      this      time-reversal      SDE      as      Q\\\\\\\\\\\\\\'™®,  \\n \\n  our      goal      is      to      train   \\n \\na      new      SDE,      parameterized      by   \\n \\n@      in      the      neural      network,      such      that      the      induced  \\n \\n  distribution      P®      over      trajectories      associated      with   \\n \\n6      aligns      with      Q\"™*.\\nSpecifically,      the      objective      is      to minimize      the      following      losses:</p><p> \\n \\n  argmin      KL(Q\"™||P*).\\n(39)  \\n \\n  0  \\n \\n  With      some      algebra,      leveraging      the      observation      that      the      time-reversal      SDE      comprises      the      original  \\n \\n  forward      SDE      and      the      scoring      term      of      the      marginal      distribution      (Anderson,      1982)      as      in      (2),      Song  \\n \\n  et      al.\\n(2021)      demonstrates      that      this      KL      loss      (39)      is      approximated      as Dore   \\n \\n=      argmin      Eeuni((0,7))01~qyo0(weleo)co~pP\"e      Fe      Fl      Vey      log      jo      (Xt   \\n \\n|      Xo)   \\n \\n_      so(Xt,      t)|I?\\n],      (40)  \\n \\n  which      is      equal      to      (4)      where      the      weight      A(t)      is      {o?\\n}?\\n(for      notation,      refer      to      Section      1.1.1).</p><h9>9.2      Flow-Based      (Bridge-Based)      Diffusion      Models</h9><p>dels We      adopt      the      explanation      outlined      in      Liu      et      al.\\n(2022).\\nIn      this      framework,      we      begin      by      considering  \\n \\n  a      reference      SDE      (from      T\\\\\\\\\\\\\\'      to      0),      x;   \\n \\n=      zr_;      where      z;      is      defined      as      Brownian      motion:</p><p>dz,   \\n \\n=      dwt,      te      (0,      7).\\n(41)</p><p> \\n \\n  Subsequently,      we      consider      the      coupling      of      the      reference      SDE      conditioned      on      state      at      time   \\n \\n0      (i.e.,  \\n \\n  distribution      of      xo.7      conditional      on      x)      and      the      data      distribution      at      time      0,      denoting      the      resulting  \\n \\n  distribution      as      Q*°?,      Informally,      this      Q*°°?\\nis      characterized      as      follows.</p><p>Informal      characterization      of      Q*”.\\nWhen      discretizing      the      time      step,      by      denoting      the      distribution  \\n \\n  induced      by      the      reference      SDE      as      p\"(x7,--+   \\n \\n,      29),      we      define QF      (a7,      tae   \\n \\n,      20)   \\n \\n=      p(x,      ay   \\n \\n|      r9)pP\"°      (x9).</p><p>The      distribution      Q*”      above      is   \\n \\na      limiting      distribution      obtained      by      making      the      discretization      step small.</p><p>Training.\\nIn      bridge-based      diffusion      models,      our      objective      is      to      train   \\n \\na      new      SDE,      parametrized      by  \\n \\n  6      (e.g.,   \\n \\na      neural      network),      to      align      the      induced      distribution      P®      with      Q*°?.\\nSpecifically,      bridge-based training      aims      to      minimize aremin      KL(Q*>||P’).\\n(42)  \\n \\n  0  \\n \\n  While      the      method      to      minimize      the      above      is      currently      unclear,      we      can      derive   \\n \\na      trainable      objective  \\n \\n  through      algebraic      manipulation.\\nInitially,      we      observe      that      the      reference      SDE      (41),      conditioned      on  \\n \\n  state      xo      at      time   \\n \\n0      (zr   \\n \\n=      D),      is      expressed      as:</p><p>b\\n \\n \\n  b—      2;</p><p>t\\n \\n   €\\n \\n   [0,T];dz?\\n \\n \\n=      g?(z?,t)dt      +du,,  \\n \\n  g(z?,t)   \\n \\n=   \\n \\na       (43)</p><p>using      the      seminal      “Doob’s      h-transform”      (Rogers      and      Williams,      2000).\\nThis      SDE      is      commonly  \\n \\n  known      as      the      Brownian      bridge.\\nNow,      we      consider   \\n \\na      new      SDE      parameterized      by      0:</p><p>t\\n \\n   €\\n \\n   [0,7];      dz   \\n \\n=      s(z,t;      @)dt   \\n \\n+      dw.\\n(44)</p><p>Then,      we      can      demonstrate      that      minimizing      the      KL      loss      in      (42)      is      equivalent      to argmin      E,W      unil0,7],1~4\"°      cop?\\n[Ilg”      (2,      t)   \\n \\n_      s(21,      t;      0)      M3]      (45)</p><p>where      q;°      denotes      the      induced      distribution      at      time   \\n \\nt      following      SDE      (43)      conditioned      on      xo,      and  \\n \\n  s:      Xx      |0,T]   \\n \\n>   \\n \\n&      is      the      model      we      aim      to      optimize.</p><p> \\n \\n  Flow-based      diffusion      models.\\nThe      aforementioned      formulation      has      been      originally      proposed      as   \\n \\na       bridge-based      SDE      (Liu      et      al.,      2022)      because      the      conditional      SDE      is      often      referred      to      as   \\n \\na      Brownian  \\n \\n  bridge.\\nFlow-based      diffusion      models      proposed      in      Lipman      et      al.\\n(2023);      Tong      et      al.\\n(2023)      share   \\n \\na       close      relationship      with      bridge-based      diffusion      models.\\nThese      flow-based      models      use   \\n \\na      target      SDE (43)      and      an      SDE      with   \\n \\na      parameter      (44)      without      stochastic      terms      (1.e.,      no      dw,)      and      aim      to      minimize  \\n \\n  the      loss      function      (45).</p><p> \\n \\n  Comparison      with      score-based      diffusion      models.\\nBoth      score-based      diffusion      models      and      bridge-  \\n \\n  based      diffusion      models      aim      to      minimize      KL      divergences.\\nDespite      its      similarity,      in      contrast      to  \\n \\n  score-based      diffusion      models,      bridge-based      diffusion      models      target      Q*°>      rather      than      the      time-  \\n \\n  reversal      SDE      Q\"™®.\\nThe      distribution      Q*”      is      often      considered      preferable      because      it      can      impose      an  \\n \\n  SDE      that      navigates      efficiently      from   \\n \\n0      to      7’      (e.g.,   \\n \\na      Brownian      bridge      with      minimal      noise).\\nThis      can  \\n \\n  expedite      the      learning      process      since      we      lack      direct      control      over      the      time-reversal      SDE      in      score-based  \\n \\n  diffusion      models.</p><h10>9.3.      Connection      with      RL-Based      Fine-Tuning</h10><p>ning  \\n \\n  RL-based      fine-tuning      shares   \\n \\na      close      relationship      with      flow-based      training.\\nTo      illustrate      this,      consider  \\n \\n  the      reference      SDE      (from      7’      to      0)      following      the      pre-trained      diffusion      model.\\nThen,      akin      to      flow-  \\n \\n  based      training,      we      consider      the      coupling      of      the      reference      SDE      conditioned      on      state      at      time   \\n \\n0       (i.e.,      distribution      over      xo-r      conditioned      on      x9)      and      the      target      distribution      at      time   \\n \\n0      (i.e.,      p,(-)   \\n \\n«       exp(r(-)/a)p?\\n\"°(-)),      denoting      the      induced      distribution      as      Q*™.</p><p>Informal      characterization      of      Q™°*.\\nInformally,      Q*\"°      is      introduced      as      follows.\\nWhen      discretizing  \\n \\n  the      time      step,      by      denoting      the      distribution      induced      by      the      reference      SDE      as      p™\\\\\\\\\\\\\\'(xp,-+-   \\n \\n,      9),      we define Qi\"      (xr,   \\n \\na   \\n \\n,      0)   \\n \\n=      p(x,      nr      Ly   \\n \\n|      L0)Pr      (Xo).</p><p>The      distribution      Q*°      above      is   \\n \\na      limiting      distribution      obtained      by      making      the      discretization      step small.</p><p>Training.\\nNow,      we      introduce   \\n \\na      new      SDE      parameterized      by   \\n \\n@      such      as      an      neural      network      to      align  \\n \\n  the      induced      distribution      P’      with      Q®\"*.\\nActually,      in      the      continuous      formulation,      the      RL-problem      (18) in      Section      4.2      is      equal      to      solving argmin      KL(P?||Q\"™).\\n \\n \\n  0 This      formalization      has      been      presented      in      Uehara      et      al.\\n(2024).\\nIntuitively,      as      we      see      in      Theorem      3,  \\n \\n  this      is      expected      because      we      see      that      the      posterior      distribution      induced      by      pre-trained      models:</p><p>ref   \\n \\n(      PD      (ar,-++      21   \\n \\n|      to)      (=      vey      (er|er_-1)p      Pp      (ar-1   \\n \\n|      er_2)      +++      pp\"      (x1   \\n \\n|      20), remains      preserved      after      fine-tuning      based      on      the      RL-formulation      (18).</p><p> \\n \\n  Comparison      with      flow-based      training.\\nIn      contrast      to      flow-based      training,      RL-based      fine-tuning  \\n \\n  minimizes      the      inverse      KL      divergence      rather      than      the      KL      divergence.\\nIntuitively,      this      is      because,  \\n \\n  unlike      Qt™*      or      Q*>   \\n \\n,      we      cannot      sample      from      Q*”      (recall      that   \\n \\na      marginal      distribution      at      time   \\n \\n0      in  \\n \\n  Qi,      i.e.,      p,,      is      unnormalized);      on      the      other      hand,      the      marginal      distributions      at      time   \\n \\n0      in      both      Q\\\\\\\\\\\\\\'™*  \\n \\n  and      Q*°°>      are      data      distributions.</p><p> \\n \\n  Remark   \\n \\n7      (Extension      to      f-divergence).\\nEach      of      these      methods      can      be      technically      extended      by  \\n \\n  incorporating      more      general      divergences      beyond      KL      divergence.\\nFor      instance,      in      the      context      of  \\n \\n  fine-tuning,      please      see      Tang      (2024).</p><p>10      Connection      with      Sampling      from      Unnormalized      Distributions</p><p> \\n \\n  Numerous      studies      delve      into      sampling      from      an      unnormalized      distribution      proportional      to      exp(r()),  \\n \\n  commonly      known      as      the      Gibbs      distribution,      extensively      discussed      in      the      computational      statistics  \\n \\n  and      statistical      physics      community      (Robert,      2014).\\nThis      issue      is      pertinent      to      our      work,      as      during  \\n \\n  fine-tuning,      the      target      distribution      is      also      formulated      in      the      Gibbs      distribution      form      in      (10),      which      is  \\n \\n  proportional      to      exp(r(a))p?\\n\"*      (x).</p><p> \\n \\n  The      literature      employs      two      main      approaches      to      tackle      this      challenge:      Markov      Chain      Monte  \\n \\n  Carlo      (MCMC)      and      RL-based      methods.\\nIn      particular,      the      RL-based      diffusion      model      fine-tuning  \\n \\n  we      have      discussed      so      far      is      closely      related      to      the      latter      approach.\\nIn      this      section,      we      explain      and  \\n \\n  compare      these      two      approaches.</p><h10>10.1      Markov      Chain      Monte      Carlo      (MCMC)</h10><p>CMC)  \\n \\n  In      MCMC,   \\n \\na      Markov      chain      is      constructed      to      approximate   \\n \\na      target      distribution      such      that      the      equilib-  \\n \\n  rium      distribution      of      the      Markov      chain      converges      to      the      target      distribution.\\nHowever,      dealing      with  \\n \\n  high-dimensional      domain      spaces      is      challenging      for      MCMC.\\n \\n \\nA      common      strategy      involves      leveraging  \\n \\n  gradient      information,      such      as      the      Metropolis-adjusted      Langevin      algorithm      (MALA)      (Besag,      1994)  \\n \\n  or      Hamiltonian      Monte      Carlo      (HMC)      (Neal      et      al.,      2011;      Girolami      and      Calderhead,      2011).\\nMALA  \\n \\n  shares      similarities      with      classifier      guidance      and      value-weighted      sampling,      as      both      methods      depend  \\n \\n  on      the      first-order      information      from      reward      (or      value)      functions.</p><p> \\n \\n  Can      we      use      MCMC      for      fine-tuning      diffusion      models?\\nIn      the      context      of      fine-tuning      dif-  \\n \\n  fusion      models,      while      it      may      seem      intuitive      to      sample      from      the      target      distribution      (10)      (i.e.,  \\n \\n  Dy   \\n \\n&      exp(r(x))pP\"*(x))      using      MCMC,      it’s      not      straightforward      for      the      following      reasons:</p><p>¢\\n \\n   We      lack      of      an      analytical      form      of      p?\\n\"*(-).<ul><li>*\\n \\n   Additionally,      even      if      we      can      estimate      p?\\n’°(-)      in      an      unbiased      manner,      as      in      Chen      et      al.\\n(2018),  \\n \\n  the      mixing      time      for      obtaining   \\n \\na      single      sample      in      MCMC      might      be      lengthy.</li></ul></p><p> \\n \\n  Therefore,      we      explore      creating   \\n \\na      generative      model      (i.e.,      diffusion      model)      for      this      task      using      RL-  \\n \\n  based      fine-tuning      so      that      we      can      easily      sample      from      p,(-)      during      inference      (i.e.,      simulating      the  \\n \\n  time-reversal      SDE      (2)      with      policies      from   \\n \\n¢   \\n \\n=      T\\\\\\\\\\\\\\'+   \\n \\n1      to   \\n \\nt   \\n \\n=      1),      without      relying      on      MCMC.\\nThis  \\n \\n  addresses      the      concern      of      MCMC      by      avoiding      the      need      to      estimate      p?\\n\"*(-)      and      minimizing      inference  \\n \\n  time,      albeit      at      the      expense      of      increased      training      time.</p><p>10.2}      RL-Based      Approaches  \\n \\n  While      MCMC      has      historically      been      widely      used      for      this      purpose,      recent      works      have      proposed      an  \\n \\n  RL-based      approach      or      its      variant      (e.g.,      Bernton      et      al.      (2019);      Heng      et      al.\\n(2020);      Huang      et      al.\\n(2023);</p><p> \\n \\n  Vargas      et      al.\\n(2023)).\\nFor      example,   \\n \\na      seminal      study      (Zhang      and      Chen,      2021)      introduced   \\n \\na      method  \\n \\n  very      similar      to      reward      backpropagation.\\nIn      their      work,      they      initially      define      Brownian      motion      as   \\n \\na       reference      SDE      (from      7’      to      0),      similar      to      the      pre-trained      diffusion      models      in      our      context.\\nThen,      by  \\n \\n  appropriately      setting      rewards,      Zhang      and      Chen      (2021)      aims      to      learn   \\n \\na      new      SDE      (from      T\\\\\\\\\\\\\\'      to      0)      such  \\n \\n  that      we      can      sample      from      the      target      distribution      at      the      endpoint      0.</p><p> \\n \\n  These      approaches      offer      advantages      over      MCMC      due      to      their      ability      to      minimize      inference      time  \\n \\n  (amortization)      by      constructing   \\n \\na      generative      model      comprising      policies      in      the      training      time      and      solely  \\n \\n  executing      learned      policies      during      inference.\\nUnlike      MCMC,      these      methods      significantly      reduce      the  \\n \\n  inference      cost      as      we      don’t      need      to      be      concerned      about      the      mixing      time.\\nAdditionally,      RL-based  \\n \\n  approaches      have      the      potential      to      effectively      handle      high-dimensional,      complex      (i.e.,      multi-modal)  \\n \\n  distributions      by      harnessing      the      considerable      expressive      power      arising      from      latent      states.</p><p> \\n \\n  Despite      these      advantages,      it      is      often      unclear      how      to      select   \\n \\na      reference      SDE      in      these      works.\\nIn  \\n \\n  contrast,      in      RL-based      fine-tuning      of      diffusion      models,      we      directly      leverage      pre-trained      diffusion  \\n \\n  models      as      the      reference      SDEs,      i.e.,      as      our      prior      knowledge.</p><p> \\n \\n  Key      message.\\nFinally,      it      is      worth      noting      that      we      can      technically      utilize      any      off-the-shelf      RL      algo-  \\n \\n  rithms      for      training.\\nFor      example,      Zhang      and      Chen      (2021)      employs      reward      backpropagation,      while  \\n \\n  works      in      Gflownets      typically      leverage      PCL,      as      we      mention      in      Section      6.3.\\nHowever,      additionally,      it  \\n \\n  is      possible      to      use      PPO      or      reward-weighted      MLE      mentioned      in      Section      4.2.</p><p>11      Closely      Related      Directions</p><p>Lastly,      we      highlight      several      closely      related      directions      we      have      not      yet      discussed.</p><p> \\n \\n  Aligning      text-to-image      models.\\nWe      recognize      that      current      efforts      in      fine-tuning      diffusion      models  \\n \\n  primarily      revolve      around      aligning      text-to-image      models      using      human      feedback      (Lee      et      al.,      2023;  \\n \\n  Wallace      et      al.,      2023;      Wu      et      al.,      2023;      Yang      et      al.,      2023).\\nIn      this      paper,      our      goal      is      to      consider   \\n \\na      more  \\n \\n  general      and      fundamental      formulation      that      is      not      tailored      to      any      particular      task.</p><p> \\n \\n  Diffusion      models      for      RL.\\nDiffusion      models      are      well-suited      for      specific      reinforcement      learning  \\n \\n  applications      due      to      their      ability      to      model      complex      and      multimodal      distributions      as      polices      (Janner  \\n \\n  et      al.,      2022;      Ajay      et      al.,      2023;      Wang      et      al.,      2022;      Hansen-Estruch      et      al.,      2023;      Du      et      al.,      2024;      Zhu  \\n \\n  et      al.,      2023).\\nFor      example,      Wang      et      al.\\n(2022)      use      conditional      diffusion      models      as      policies      and  \\n \\n  demonstrate      their      effectiveness      in      typical      offline      RL      benchmarks.\\nWhile      RL-based      fine-tuning      also  \\n \\n  aims      to      maximize      certain      reward      functions,      the      primary      focus      of      this      tutorial      covers      methods      that  \\n \\n  leverage      pre-trained      diffusion      models      for      this      purpose.</p><p> \\n \\n  RL      from      human      feedback      (RLHF)      for      language      models.\\nRLHF      is      widely      discussed      in      the  \\n \\n  context      of      language      models,      where      many      algorithms      share      similarities      with      those      used      in      diffusion  \\n \\n  models      (Ouyang      et      al.,      2022;      Bai      et      al.,      2022;      Casper      et      al.,      2023).\\nFor      instance,      both      domains  \\n \\n  commonly      employ      PPO      as   \\n \\na      standard      approach,      and      reward-weighted      training      (or      reward-weighted  \\n \\n  MLE)      often      serves      as   \\n \\na      basic      baseline      approach.\\nDespite      these      similarities,      nuanced      differences  \\n \\n  exist      mainly      because      diffusion      models      typically      operate      in   \\n \\na      continuous      input      space      rather      than  \\n \\n  a      discrete      one.\\nMoreover,      techniques      like      value-weighted      sampling      emerge      uniquely      within      the  \\n \\n  context      of      diffusion      models.</p><p>12      Summary</p><p> \\n \\n  In      this      article,      we      comprehensively      explain      how      fine-tuning      diffusion      models      to      maximize      down-  \\n \\n  stream      reward      functions      can      be      formalized      as   \\n \\na      reinforcement      learning      (RL)      problem      in      entropy-  \\n \\n  regularized      Markov      decision      processes      (MDPs).\\nBased      on      this      viewpoint,      we      elaborate      on      the  \\n \\n  application      of      various      RL      algorithms,      such      as      PPO      and      reward-weighted      MLE,      specifically      tailored  \\n \\n  for      fine-tuning      diffusion      models.\\nAdditionally,      we      categorize      different      scenarios      based      on      how  \\n \\n  reward      feedback      is      obtained.\\nWhile      this      categorization      is      not      always      explicitly      mentioned      in      many  \\n \\n  existing      works,      it      is      crucial      when      selecting      appropriate      algorithms.\\nFinally,      we      discuss      the      rela-  \\n \\n  tionships      with      several      related      topics,      including      classifier      guidance,      Gflownets,      path      integral      control  \\n \\n  theory,      and      sampling      from      unnormalized      distributions.</p><h6>7.1      Fine-Tuning      with      Known,      Differentiable      Reward      Functions</h6><p>ions  \\n \\n  When      accurate      differentiable      reward      functions      are      available      (e.g.,      standard      in      computer      vision      tasks),  \\n \\n  our      primary      focus      is      typically      on      computational      and      memory      efficiency.\\nIn      general,      we      advocate      for  \\n \\n  employing      reward      backpropagation      (c.f.      Algorithm      2)      for      its      computational      efficiency.\\nAlternatively,  \\n \\n  if      memory      efficiency      is   \\n \\na      significant      concern,      we      suggest      using      PPO      due      to      its      stability.</p><h6>7.2      Fine-Tuning      with      Black-Box      Reward      Feedback</h6><p>back  \\n \\n  Here,      we      explore      scenarios      where      we      have      access      to      accurate      but      non-differentiable      (black-box)  \\n \\n  reward      feedback,      often      found      in      scientific      simulations.\\nThe      emphasis      still      remains      primarily      on  \\n \\n  computational      and      memory      efficiency.\\nIn      such      scenarios,      due      to      the      non-differentiability      of      reward  \\n \\n  feedback,      we      recommend      using      PPO      or      reward-weighted      MLE      for      the      following      reasons.</p><p> \\n \\n  In      the      preceding      section,      we      advocated      for      the      use      of      reward      backpropagation      due      to      its      com-  \\n \\n  putational      advantages.\\nHowever,      when      dealing      with      non-differentiable      feedback,      the      advantage  \\n \\n  of      reward      backpropagation      may      diminish.\\nThis      is      because      learning      from      such      feedback      requires  \\n \\n  learning      differentiable      reward      functions      to      make      the      algorithm      work      as      we      discuss      in      Section      5.2.  \\n \\n  However,      obtaining   \\n \\na      differentiable      reward      function      can      be      challenging      in      many      domains.\\nFor  \\n \\n  example,      in      molecular      property      prediction,      molecular      fingerprints      are      informative      features,      and  \\n \\n  accurate      reward      functions      can      be      derived      by      training      simple      neural      networks      on      these      features  \\n \\n  (Pattanaik      and      Coley,      2020).\\nYet,      these      mappings,      grounded      in      prior      scientific      understanding,      are  \\n \\n  non-differentiable,      thereby      restricting      their      applicability.</p><p> \\n \\n  In      contrast,      PPO      and      reward-weighted      MLE      allow      for      policy      updates      without      the      explicit  \\n \\n  requirement      to      learn   \\n \\na      differentiable      reward      function,      offering   \\n \\na      significant      advantage      over      reward  \\n \\n  backpropagation.</p><h6>7.3      Fine-Tuning      with      Unknown      Rewards      Functions</h6><p>ions  \\n \\n  When      reward      functions      (or      computational      proxies      in      Section      7.2)      are      unavailable,      we      must      learn  \\n \\n  from      data      with      reward      feedback.\\nIn      such      cases,      compared      to      the      previous      scenarios,      two      important  \\n \\n  considerations      arise:</p><p>¢\\n \\n   Not      only      computational      or      memory      efficiency      but      also      feedback      efficiency      (i.e.,      sample  \\n \\n  efficiency      regarding      reward      feedback)      is      crucial.</p><p> \\n \\n \\n¢      Given      that      learned      reward      feedback      may      not      generalize      well      outside      the      training      data      distri-  \\n \\n  butions,      it      is      essential      to      constrain      fine-tuned      models      to      prevent      significant      divergence      from  \\n \\n  diffusion      models.\\nIn      these      situations,      not      only      soft      PPO      and      direct      backpropagation      but      also  \\n \\n  methods      discussed      in      Section      6,      such      as      reward-weighted      MLE,      can      be      particularly      effective.</p><p>Further,      the      scenario      involving      unknown      reward      functions      can      be      broadly      categorized      into      two      cases,  \\n \\n  which      we      will      discuss      in      more      detail.</p><p>Offline      scenario      (Figure      4).\\nIn      many      scenarios,      even  \\n \\n  in      cases      where      reward      functions      are      unknown,      we      of-  \\n \\n.     .</p><p>Offline      data      with ten      have      access      to      offline      data      with      reward      feedback      reward      feedback</p><li>(i)      (i)   \\n \\nO   \\n \\n;   \\n \\ni       a”      ra  \\n \\n.      ne      straightforward      approach      is      to</li><p> \\n \\n \\n{   \\n \\n,   \\n \\n(   \\n \\n)    \\n.\\n    .\\n \\n   g\\n \\n   PP  \\n \\n.      <=>      Learning      reward  \\n \\n  perform      regression      with      neural      networks      and      build   \\n \\na      models  \\n \\n  learned      regressor      *.\\nHowever,      this      learned      reward      func-      SS  \\n \\n  tion   \\n \\n*      might      not      generalize      well      beyond      the      distribution      x)  \\n \\n  of      the      offline      data.\\nIn      regions      outside      this      distribution,   \\n \\n7       could      assign   \\n \\na      high      value      even      when      the      actual      reward  \\n \\n  r      is      low      due      to      high      uncertainty.\\nConsequently,      we  \\n \\n  could      be      easily      misled      by      out-of-distribution      samples  \\n \\n  if      we      solely      optimize      7.\\n|Fine-tuning      diffusion      models   \\n \\n|       One      approach      to      alleviate      this      issue      is      to      adopt   \\n \\na       pessimistic      (i.e.,      conservative)      strategy,      as      proposed      in      Figure      4:      Offline      scenario  \\n \\n  Uehara      et      al.\\n(2024).\\nThis      technique      has      been      widely      Collecting      data]   \\n \\nY      Learning      reward  \\n \\n  applied      in      the      field      of      offline      RL      (Levine,      2018).\\nThe      aa  \\n \\n  main      idea      is      to      penalize   \\n \\n7      outside      the      distributions      us-  \\n \\n  ing      techniques      such      as      adding      an      explicit      penalty      term  \\n \\n  (Yu      et      al.,      2020;      Chang      et      al.,      2021),      bootstrapping,  \\n \\n  or      more      sophisticated      methods      (Kumar      et      al.,      2020;  \\n \\n  Xie      et      al.,      2021;      Rigter      et      al.,      2022;      Uehara      and      Sun,  \\n \\n  2021).\\nBy      doing      so,      we      can      avoid      being      fooled      by  \\n \\n  out-to-distribution      regions      while      still      benefiting      from  \\n \\n  the      extrapolation      capabilities      of      reward      models.</p><p>S—      diffusion      models      |</p><p>Figure      5:      Online      scenario</p><p> \\n \\n  Online      scenario      (Figure      5).\\nConsider      situations  \\n \\n  where      reward      functions      are      unknown,      but      we      can      gather      data      online.\\nThese      scenarios      are      often  \\n \\n  referred      to      as   \\n \\na      lab-in-the-loop      setting.</p><p> \\n \\n  In      such      scenarios,      Uehara      et      al.\\n(2024)      proposes      an      iterative      procedure      comprising      three      steps:</p><li>(1)      collecting      feedback      data      by      exploring      new      areas      (i.e.,      obtaining   \\n \\nx      from      current      diffusion      models  \\n \\n  and      corresponding      r(x)),      (2)      learning      the      reward      function      from      the      collected      feedback      data,      and</li><li>(3)      fine-tuning      current      diffusion      models      by      maximizing      the      learned      reward      function      enhanced      by  \\n \\n  an      optimistic      bonus      term,      using      algorithms      discussed      in      Section      4.2.\\nIn      contrast      to      standard      online  \\n \\n  PPO,      where      feedback      data      is      directly      used      in      the      fine-tuning      process,      actual      reward      feedback      is  \\n \\n  only      used      in      step      (2)      and      not      in      step      (3).\\nAn      additional      key      aspect      is      the      use      of      an      optimistic  \\n \\n  reward      function      that      encourages      exploration      beyond      the      distributions      of      the      current      diffusion      model.\\n \\n \\n  This      deliberate      separation      between      reward      learning      and      fine-tuning      steps,      coupled      with      the      use      of  \\n \\n  optimism,      significantly      enhances      feedback      efficiency.</li><p>8\\n \\n   Connection      with      Classifier      Guidance</p><p> \\n \\n  Classifier      guidance      (Dhariwal      and      Nichol,      2021)      is      commonly      used      for      conditional      generation      in  \\n \\n  diffusion      models.\\nInterestingly,      RL-based      methods      share   \\n \\na      close      connection      with      classifier      guidance.\\n \\n \\n  More      specifically,      in      this      section,      following      Zhao      et      al.\\n(2024),      we      elucidate      how      RL-based      methods  \\n \\n  can      be      applied      to      conditional      generation.\\nFurthermore,      from      this      unified      viewpoint,      we      highlight  \\n \\n  that      classifier      guidance      is      seen      as   \\n \\na      value-weighted      sampling      in      Section      4.2.</p><h6>8.1      Classfier      Guidance</h6><p>ance  \\n \\n  In      this      section,      we      first      introduce      classifier      guidance.\\nClassifier      guidance      utilizes      an      unconditional  \\n \\n  pre-trained      model      for      conditional      generation.\\nMore      specifically,      we      consider   \\n \\na      scenario      where      we  \\n \\n  have   \\n \\na      pre-trained      diffusion      model,      which      enables      us      to      sample      from      p?\\n\"*(x),      and      data      with      feedback  \\n \\n  {x,y},      where   \\n \\ny      represents      the      new      label      we      want      to      condition      on.\\nOur      objective      here      is      to      sample  \\n \\n  from      p(-|y)   \\n \\n€      [Y   \\n \\n>      ACR), P(-   \\n \\n|      y)   \\n \\nx      py(yl-)pP*(-).\\n(37)</p><p>where      p,   \\n \\n:   \\n \\nX      —>      A(Y)      denotes      the      conditional      distribution      of   \\n \\ny      given      x.</p><p>Motivation.\\nIn      classifier      guidance,      we      consider      the      following      policy:</p><p>pe      (-lare,      y)   \\n \\n=   \\n \\nN      (p(22,      t;      Opre)   \\n \\n+      a(t)      Ve      log      Vy      (Xt;      t),      a*(t))   \\n \\n’ dy(x,t)   \\n \\n=      Egy.\\nyap,      (-leo)      L(Y   \\n \\n=      y)|a,   \\n \\n=      2].</p><p>Under      the      continuous-time      formulation      of      diffusion      models,      applying      Doob’s      h-transform      (Rogers  \\n \\n  and      Williams,      2000),      it      is      shown      that      we      can      sample      from      p(x|y)      by      sequentially      running      {p7\"\\\\\\\\\\\\\\'(-|7+,      y)      }i-r41-</p><p>Algorithm.\\nSpecifically,      the      algorithm      comprises      two      steps.\\nIt’s      important      to      note      that      this      is      not  \\n \\n  a      fine-tuning      method      but      rather      an      inference-time      technique      as      we      freeze      the      pre-trained      model.</p><li>1. Learning      q,(x,t)      through      regression      by      gathering      data      containing      {z;,      y}.\\nHere,      for      each  \\n \\n  (Xo,      y),      using   \\n \\na      forward      policy      for      the      pre-training      (from   \\n \\n0      to      T),      we      obtain      2;      starting      from</li><p>Lo.</p><li>2. During      inference,      sequentially      execute      {p2\"(-|a,,      y)      }1_>   \\n \\n1      by      leverage      the      fact      that      compared</li><p>gui to      the      policy      in      pre-trained      diffusion      models,      the      mean      of      {p?\\n\"\\\\\\\\\\\\\\'(-|71,      y)      }i-74,      is      just      shifted  \\n \\n  with      an      additional      term      V,,      log      q,(2,      t).</p><p> \\n \\n  Notably,      the      method      described      above      can      be      reduced      to      value-weighted      sampling      (cf.      Sec-  \\n \\n  tion      6.2),      when      r(x,      y)   \\n \\n=      log      p(y|x).\\nInspired      by      such      an      observation,      we      apply      an      RL-based  \\n \\n  fine-tuning      method      for      conditional      generation      as      below.</p><h7>8.2.      RL-Based      Fine-Tuning      for      Conditional      Generation</h7><p>tion  \\n \\n  In      this      section,      following      Zhao      et      al.\\n(2024),      we      explain      an      RL-based      fine-tuning      approach      for  \\n \\n  conditional      generation.\\nThe      core      idea      is      that      setting      r(x,      y)   \\n \\n=      log      p(y|x)      allows      us      to      sample      from  \\n \\n  the      target      conditional      distribution      (37),      drawing      on      the      insights      discussed      in      Section      4.2.  \\n \\n  More      precisely,      introducing   \\n \\na      distribution      over   \\n \\ny      as      q(-),      we      formulate      the      following      RL      problem:</p><p>{Di      ft   \\n \\n=      argmax      Egy,      (-\\\\\\\\\\\\\\\\-y)}.yott      log      p(y|xo)   \\n \\n—      NeopyaKL(pe(-|e,      y)      ||P      (late).\\n \\n \\n  {prE[(V,.X)PA(A)]      Heyy (38)</p><p> \\n \\n  Compared      to      the      objective      function      (18)      in      Section      4.2,      the      main      differences      are:      (1)      policy      space  \\n \\n  is      expanded      to      accommodate      additional      control      (i.e.,      y):      pz(-|-,-)   \\n \\nC      [R¢   \\n \\nx   \\n \\nY   \\n \\n+      A(R®)],      (2)      the  \\n \\n  fine-tuning      objectives      are      set      as      log-likelihoods:      log      p(y|z).\\nThen,      following      Theorem      1,      we      can  \\n \\n  establish      the      following      result.</p><p>Corollary      2.\\nThe      distribution      induced      by      {p*}1_74      (ue.   \\n \\n[      {Teri      Ph      (ae-1   \\n \\n|      te,      y)}deur      is      the target      conditional      distribution      (37).</p><p> \\n \\n  The      above      corollary      suggests      that      for      conditional      generation,      we      can      utilize      various      off-the-shelf  \\n \\n  algorithms      discussed      in      Section      4.2      and      6,      such      as      PPO,      reward      backpropagation,      and      reward-  \\n \\n  weighted      MLE,      as      well      as      value-weighted      sampling.</p><p>9\\n \\n   Connection      with      Flow-Based      Diffusion      Models</p><p> \\n \\n  We      elucidate      the      close      relationship      between      bridge      matching      and      fine-tuning,      as      summarized      in  \\n \\n  Table      2.\\nBridge      (flow)      matching      (Liu      et      al.,      2022;      Tong      et      al.,      2023;      Lipman      et      al.,      2023;      Liu      et      al.,  \\n \\n  2022;      Shi      et      al.,      2024;      Albergo      and      Vanden-Eijnden,      2022),      has      recently      gained      popularity      as   \\n \\na       distinct      type      of      diffusion      model      from      score-matching-based      ones.\\nThese      works      suggest      that      training  \\n \\n  and      inference      speed      will      be      accelerated      due      to      the      ability      to      define   \\n \\na      more      direct      trajectory      from  \\n \\n  noise      to      data      distribution      than      score-matching-based      diffusion      models.</p><p> \\n \\n  The      connection      between      bridge      matching      and      fine-tuning      becomes      evident      in      the      continuous-  \\n \\n  time      formulation.\\nTo      begin,      we      first      provide   \\n \\na      brief      overview      of      the      continuous-time      formulation      of  \\n \\n  score-matching-based      diffusion      models.</p><p> \\n \\n  Table      2:      Comparison      between      training      score/flow-based      diffusion      models      and      fine-tuning.\\nWe  \\n \\n  optimize      the      parameter   \\n \\n6      in      P’,      which      represents      the      induced      distribution      over      trajectories      associated  \\n \\n  with      6.</p><h8>Loss      Note      of      Q</h8><p>Score-based      KL(Q\\\\\\\\\\\\\\'™¢|P?)      Qtime:      Time-reversal      of      reference      SDE      (reference training      SDE      is      defined      from   \\n \\n0      to      T’      where      the      initial      dis-  \\n \\n  tribute      at   \\n \\n0      is      data      distribution)</p><p> \\n \\n  Flow-based      KL(Q*°>      ||P?)      @Q*°°h:      Coupling      between      the      reference      SDE      from  \\n \\n  training   \\n \\nT      to   \\n \\n0      conditioning      at   \\n \\n0      (dist.      over      x.;|7o)      and  \\n \\n  a      data      distribution      at      time   \\n \\n0      (dis.      over      x).\\nNote  \\n \\n  that      reference      SDE      is      defined      from      7’      to      0.</p><p> \\n \\n  Fine-tuning      KL(P?||Q*°)      @*:      Coupling      between      the      pre-trained      SDE  \\n \\n  from   \\n \\nT      to   \\n \\n0      conditioned      on   \\n \\n0      (dist.      over      x7.1|X0)  \\n \\n  and   \\n \\na      data      distribution      at      time   \\n \\n0      (dis.      over      Xo) 9.1      Continuous      Time      Formulation      of      Score-Based      diffusion      models</p><p> \\n \\n  We      provide   \\n \\na      brief      overview      of      the      continuous-time      formulation      of      diffusion      models      (Song      et      al.,  \\n \\n  2021),      further      expanding      Section      1.1.1.\\nIn      this      formulation,      we      initially      define      the      forward      reference  \\n \\n  SDE,      spanning      from      time   \\n \\n0      to      7’      (e.g.,      variance      exploding      (VE)      process      or      variance      preserving      (VP)  \\n \\n  process),      by      setting      an      initial      distribution      at   \\n \\n0      as      the      data      distribution.\\nAdditionally,      the      reference  \\n \\n  SDE      is      tailored      to      converge      to      an      easy      distribution      at      time      7’,      such      as      the      normal      distribution.</p><p> \\n \\n  Subsequently,      we      consider      the      time-reversal      SDE      (Anderson,      1982),      progressing      from      T\\\\\\\\\\\\\\'      to      0.\\nIf  \\n \\n  we      could      learn      this      time-reversal      SDE,      it      would      enable      us      to      sample      from      the      data      distribution      by  \\n \\n  starting      with      the      easy      initial      distribution      (at      7’)      and      following      the      time-reversal      SDE      from   \\n \\nT      to   \\n \\n0      at  \\n \\n  inference      time.</p><p> \\n \\n  Training.\\nNow,      the      remaining      question      is      how      to      learn      this      time-reversal      SDE.\\nHere,      by      repre-  \\n \\n  senting      the      induced      distribution      over      trajectories      from   \\n \\nT      to   \\n \\n0      with      this      time-reversal      SDE      as      Q\\\\\\\\\\\\\\'™®,  \\n \\n  our      goal      is      to      train   \\n \\na      new      SDE,      parameterized      by   \\n \\n@      in      the      neural      network,      such      that      the      induced  \\n \\n  distribution      P®      over      trajectories      associated      with   \\n \\n6      aligns      with      Q\"™*.\\nSpecifically,      the      objective      is      to minimize      the      following      losses:</p><p> \\n \\n  argmin      KL(Q\"™||P*).\\n(39)  \\n \\n  0  \\n \\n  With      some      algebra,      leveraging      the      observation      that      the      time-reversal      SDE      comprises      the      original  \\n \\n  forward      SDE      and      the      scoring      term      of      the      marginal      distribution      (Anderson,      1982)      as      in      (2),      Song  \\n \\n  et      al.\\n(2021)      demonstrates      that      this      KL      loss      (39)      is      approximated      as Dore   \\n \\n=      argmin      Eeuni((0,7))01~qyo0(weleo)co~pP\"e      Fe      Fl      Vey      log      jo      (Xt   \\n \\n|      Xo)   \\n \\n_      so(Xt,      t)|I?\\n],      (40)  \\n \\n  which      is      equal      to      (4)      where      the      weight      A(t)      is      {o?\\n}?\\n(for      notation,      refer      to      Section      1.1.1).</p><h9>9.2      Flow-Based      (Bridge-Based)      Diffusion      Models</h9><p>dels We      adopt      the      explanation      outlined      in      Liu      et      al.\\n(2022).\\nIn      this      framework,      we      begin      by      considering  \\n \\n  a      reference      SDE      (from      T\\\\\\\\\\\\\\'      to      0),      x;   \\n \\n=      zr_;      where      z;      is      defined      as      Brownian      motion:</p><p>dz,   \\n \\n=      dwt,      te      (0,      7).\\n(41)</p><p> \\n \\n  Subsequently,      we      consider      the      coupling      of      the      reference      SDE      conditioned      on      state      at      time   \\n \\n0      (i.e.,  \\n \\n  distribution      of      xo.7      conditional      on      x)      and      the      data      distribution      at      time      0,      denoting      the      resulting  \\n \\n  distribution      as      Q*°?,      Informally,      this      Q*°°?\\nis      characterized      as      follows.</p><p>Informal      characterization      of      Q*”.\\nWhen      discretizing      the      time      step,      by      denoting      the      distribution  \\n \\n  induced      by      the      reference      SDE      as      p\"(x7,--+   \\n \\n,      29),      we      define QF      (a7,      tae   \\n \\n,      20)   \\n \\n=      p(x,      ay   \\n \\n|      r9)pP\"°      (x9).</p><p>The      distribution      Q*”      above      is   \\n \\na      limiting      distribution      obtained      by      making      the      discretization      step small.</p><p>Training.\\nIn      bridge-based      diffusion      models,      our      objective      is      to      train   \\n \\na      new      SDE,      parametrized      by  \\n \\n  6      (e.g.,   \\n \\na      neural      network),      to      align      the      induced      distribution      P®      with      Q*°?.\\nSpecifically,      bridge-based training      aims      to      minimize aremin      KL(Q*>||P’).\\n(42)  \\n \\n  0  \\n \\n  While      the      method      to      minimize      the      above      is      currently      unclear,      we      can      derive   \\n \\na      trainable      objective  \\n \\n  through      algebraic      manipulation.\\nInitially,      we      observe      that      the      reference      SDE      (41),      conditioned      on  \\n \\n  state      xo      at      time   \\n \\n0      (zr   \\n \\n=      D),      is      expressed      as:</p><p>b\\n \\n \\n  b—      2;</p><p>t\\n \\n   €\\n \\n   [0,T];dz?\\n \\n \\n=      g?(z?,t)dt      +du,,  \\n \\n  g(z?,t)   \\n \\n=   \\n \\na       (43)</p><p>using      the      seminal      “Doob’s      h-transform”      (Rogers      and      Williams,      2000).\\nThis      SDE      is      commonly  \\n \\n  known      as      the      Brownian      bridge.\\nNow,      we      consider   \\n \\na      new      SDE      parameterized      by      0:</p><p>t\\n \\n   €\\n \\n   [0,7];      dz   \\n \\n=      s(z,t;      @)dt   \\n \\n+      dw.\\n(44)</p><p>Then,      we      can      demonstrate      that      minimizing      the      KL      loss      in      (42)      is      equivalent      to argmin      E,W      unil0,7],1~4\"°      cop?\\n[Ilg”      (2,      t)   \\n \\n_      s(21,      t;      0)      M3]      (45)</p><p>where      q;°      denotes      the      induced      distribution      at      time   \\n \\nt      following      SDE      (43)      conditioned      on      xo,      and  \\n \\n  s:      Xx      |0,T]   \\n \\n>   \\n \\n&      is      the      model      we      aim      to      optimize.</p><p> \\n \\n  Flow-based      diffusion      models.\\nThe      aforementioned      formulation      has      been      originally      proposed      as   \\n \\na       bridge-based      SDE      (Liu      et      al.,      2022)      because      the      conditional      SDE      is      often      referred      to      as   \\n \\na      Brownian  \\n \\n  bridge.\\nFlow-based      diffusion      models      proposed      in      Lipman      et      al.\\n(2023);      Tong      et      al.\\n(2023)      share   \\n \\na       close      relationship      with      bridge-based      diffusion      models.\\nThese      flow-based      models      use   \\n \\na      target      SDE (43)      and      an      SDE      with   \\n \\na      parameter      (44)      without      stochastic      terms      (1.e.,      no      dw,)      and      aim      to      minimize  \\n \\n  the      loss      function      (45).</p><p> \\n \\n  Comparison      with      score-based      diffusion      models.\\nBoth      score-based      diffusion      models      and      bridge-  \\n \\n  based      diffusion      models      aim      to      minimize      KL      divergences.\\nDespite      its      similarity,      in      contrast      to  \\n \\n  score-based      diffusion      models,      bridge-based      diffusion      models      target      Q*°>      rather      than      the      time-  \\n \\n  reversal      SDE      Q\"™®.\\nThe      distribution      Q*”      is      often      considered      preferable      because      it      can      impose      an  \\n \\n  SDE      that      navigates      efficiently      from   \\n \\n0      to      7’      (e.g.,   \\n \\na      Brownian      bridge      with      minimal      noise).\\nThis      can  \\n \\n  expedite      the      learning      process      since      we      lack      direct      control      over      the      time-reversal      SDE      in      score-based  \\n \\n  diffusion      models.</p><h10>9.3.      Connection      with      RL-Based      Fine-Tuning</h10><p>ning  \\n \\n  RL-based      fine-tuning      shares   \\n \\na      close      relationship      with      flow-based      training.\\nTo      illustrate      this,      consider  \\n \\n  the      reference      SDE      (from      7’      to      0)      following      the      pre-trained      diffusion      model.\\nThen,      akin      to      flow-  \\n \\n  based      training,      we      consider      the      coupling      of      the      reference      SDE      conditioned      on      state      at      time   \\n \\n0       (i.e.,      distribution      over      xo-r      conditioned      on      x9)      and      the      target      distribution      at      time   \\n \\n0      (i.e.,      p,(-)   \\n \\n«       exp(r(-)/a)p?\\n\"°(-)),      denoting      the      induced      distribution      as      Q*™.</p><p>Informal      characterization      of      Q™°*.\\nInformally,      Q*\"°      is      introduced      as      follows.\\nWhen      discretizing  \\n \\n  the      time      step,      by      denoting      the      distribution      induced      by      the      reference      SDE      as      p™\\\\\\\\\\\\\\'(xp,-+-   \\n \\n,      9),      we define Qi\"      (xr,   \\n \\na   \\n \\n,      0)   \\n \\n=      p(x,      nr      Ly   \\n \\n|      L0)Pr      (Xo).</p><p>The      distribution      Q*°      above      is   \\n \\na      limiting      distribution      obtained      by      making      the      discretization      step small.</p><p>Training.\\nNow,      we      introduce   \\n \\na      new      SDE      parameterized      by   \\n \\n@      such      as      an      neural      network      to      align  \\n \\n  the      induced      distribution      P’      with      Q®\"*.\\nActually,      in      the      continuous      formulation,      the      RL-problem      (18) in      Section      4.2      is      equal      to      solving argmin      KL(P?||Q\"™).\\n \\n \\n  0 This      formalization      has      been      presented      in      Uehara      et      al.\\n(2024).\\nIntuitively,      as      we      see      in      Theorem      3,  \\n \\n  this      is      expected      because      we      see      that      the      posterior      distribution      induced      by      pre-trained      models:</p><p>ref   \\n \\n(      PD      (ar,-++      21   \\n \\n|      to)      (=      vey      (er|er_-1)p      Pp      (ar-1   \\n \\n|      er_2)      +++      pp\"      (x1   \\n \\n|      20), remains      preserved      after      fine-tuning      based      on      the      RL-formulation      (18).</p><p> \\n \\n  Comparison      with      flow-based      training.\\nIn      contrast      to      flow-based      training,      RL-based      fine-tuning  \\n \\n  minimizes      the      inverse      KL      divergence      rather      than      the      KL      divergence.\\nIntuitively,      this      is      because,  \\n \\n  unlike      Qt™*      or      Q*>   \\n \\n,      we      cannot      sample      from      Q*”      (recall      that   \\n \\na      marginal      distribution      at      time   \\n \\n0      in  \\n \\n  Qi,      i.e.,      p,,      is      unnormalized);      on      the      other      hand,      the      marginal      distributions      at      time   \\n \\n0      in      both      Q\\\\\\\\\\\\\\'™*  \\n \\n  and      Q*°°>      are      data      distributions.</p><p> \\n \\n  Remark   \\n \\n7      (Extension      to      f-divergence).\\nEach      of      these      methods      can      be      technically      extended      by  \\n \\n  incorporating      more      general      divergences      beyond      KL      divergence.\\nFor      instance,      in      the      context      of  \\n \\n  fine-tuning,      please      see      Tang      (2024).</p><p>10      Connection      with      Sampling      from      Unnormalized      Distributions</p><p> \\n \\n  Numerous      studies      delve      into      sampling      from      an      unnormalized      distribution      proportional      to      exp(r()),  \\n \\n  commonly      known      as      the      Gibbs      distribution,      extensively      discussed      in      the      computational      statistics  \\n \\n  and      statistical      physics      community      (Robert,      2014).\\nThis      issue      is      pertinent      to      our      work,      as      during  \\n \\n  fine-tuning,      the      target      distribution      is      also      formulated      in      the      Gibbs      distribution      form      in      (10),      which      is  \\n \\n  proportional      to      exp(r(a))p?\\n\"*      (x).</p><p> \\n \\n  The      literature      employs      two      main      approaches      to      tackle      this      challenge:      Markov      Chain      Monte  \\n \\n  Carlo      (MCMC)      and      RL-based      methods.\\nIn      particular,      the      RL-based      diffusion      model      fine-tuning  \\n \\n  we      have      discussed      so      far      is      closely      related      to      the      latter      approach.\\nIn      this      section,      we      explain      and  \\n \\n  compare      these      two      approaches.</p><h10>10.1      Markov      Chain      Monte      Carlo      (MCMC)</h10><p>CMC)  \\n \\n  In      MCMC,   \\n \\na      Markov      chain      is      constructed      to      approximate   \\n \\na      target      distribution      such      that      the      equilib-  \\n \\n  rium      distribution      of      the      Markov      chain      converges      to      the      target      distribution.\\nHowever,      dealing      with  \\n \\n  high-dimensional      domain      spaces      is      challenging      for      MCMC.\\n \\n \\nA      common      strategy      involves      leveraging  \\n \\n  gradient      information,      such      as      the      Metropolis-adjusted      Langevin      algorithm      (MALA)      (Besag,      1994)  \\n \\n  or      Hamiltonian      Monte      Carlo      (HMC)      (Neal      et      al.,      2011;      Girolami      and      Calderhead,      2011).\\nMALA  \\n \\n  shares      similarities      with      classifier      guidance      and      value-weighted      sampling,      as      both      methods      depend  \\n \\n  on      the      first-order      information      from      reward      (or      value)      functions.</p><p> \\n \\n  Can      we      use      MCMC      for      fine-tuning      diffusion      models?\\nIn      the      context      of      fine-tuning      dif-  \\n \\n  fusion      models,      while      it      may      seem      intuitive      to      sample      from      the      target      distribution      (10)      (i.e.,  \\n \\n  Dy   \\n \\n&      exp(r(x))pP\"*(x))      using      MCMC,      it’s      not      straightforward      for      the      following      reasons:</p><p>¢\\n \\n   We      lack      of      an      analytical      form      of      p?\\n\"*(-).<ul><li>*\\n \\n   Additionally,      even      if      we      can      estimate      p?\\n’°(-)      in      an      unbiased      manner,      as      in      Chen      et      al.\\n(2018),  \\n \\n  the      mixing      time      for      obtaining   \\n \\na      single      sample      in      MCMC      might      be      lengthy.</li></ul></p><p> \\n \\n  Therefore,      we      explore      creating   \\n \\na      generative      model      (i.e.,      diffusion      model)      for      this      task      using      RL-  \\n \\n  based      fine-tuning      so      that      we      can      easily      sample      from      p,(-)      during      inference      (i.e.,      simulating      the  \\n \\n  time-reversal      SDE      (2)      with      policies      from   \\n \\n¢   \\n \\n=      T\\\\\\\\\\\\\\'+   \\n \\n1      to   \\n \\nt   \\n \\n=      1),      without      relying      on      MCMC.\\nThis  \\n \\n  addresses      the      concern      of      MCMC      by      avoiding      the      need      to      estimate      p?\\n\"*(-)      and      minimizing      inference  \\n \\n  time,      albeit      at      the      expense      of      increased      training      time.</p><p>10.2}      RL-Based      Approaches  \\n \\n  While      MCMC      has      historically      been      widely      used      for      this      purpose,      recent      works      have      proposed      an  \\n \\n  RL-based      approach      or      its      variant      (e.g.,      Bernton      et      al.      (2019);      Heng      et      al.\\n(2020);      Huang      et      al.\\n(2023);</p><p> \\n \\n  Vargas      et      al.\\n(2023)).\\nFor      example,   \\n \\na      seminal      study      (Zhang      and      Chen,      2021)      introduced   \\n \\na      method  \\n \\n  very      similar      to      reward      backpropagation.\\nIn      their      work,      they      initially      define      Brownian      motion      as   \\n \\na       reference      SDE      (from      7’      to      0),      similar      to      the      pre-trained      diffusion      models      in      our      context.\\nThen,      by  \\n \\n  appropriately      setting      rewards,      Zhang      and      Chen      (2021)      aims      to      learn   \\n \\na      new      SDE      (from      T\\\\\\\\\\\\\\'      to      0)      such  \\n \\n  that      we      can      sample      from      the      target      distribution      at      the      endpoint      0.</p><p> \\n \\n  These      approaches      offer      advantages      over      MCMC      due      to      their      ability      to      minimize      inference      time  \\n \\n  (amortization)      by      constructing   \\n \\na      generative      model      comprising      policies      in      the      training      time      and      solely  \\n \\n  executing      learned      policies      during      inference.\\nUnlike      MCMC,      these      methods      significantly      reduce      the  \\n \\n  inference      cost      as      we      don’t      need      to      be      concerned      about      the      mixing      time.\\nAdditionally,      RL-based  \\n \\n  approaches      have      the      potential      to      effectively      handle      high-dimensional,      complex      (i.e.,      multi-modal)  \\n \\n  distributions      by      harnessing      the      considerable      expressive      power      arising      from      latent      states.</p><p> \\n \\n  Despite      these      advantages,      it      is      often      unclear      how      to      select   \\n \\na      reference      SDE      in      these      works.\\nIn  \\n \\n  contrast,      in      RL-based      fine-tuning      of      diffusion      models,      we      directly      leverage      pre-trained      diffusion  \\n \\n  models      as      the      reference      SDEs,      i.e.,      as      our      prior      knowledge.</p><p> \\n \\n  Key      message.\\nFinally,      it      is      worth      noting      that      we      can      technically      utilize      any      off-the-shelf      RL      algo-  \\n \\n  rithms      for      training.\\nFor      example,      Zhang      and      Chen      (2021)      employs      reward      backpropagation,      while  \\n \\n  works      in      Gflownets      typically      leverage      PCL,      as      we      mention      in      Section      6.3.\\nHowever,      additionally,      it  \\n \\n  is      possible      to      use      PPO      or      reward-weighted      MLE      mentioned      in      Section      4.2.</p><p>11      Closely      Related      Directions</p><p>Lastly,      we      highlight      several      closely      related      directions      we      have      not      yet      discussed.</p><p> \\n \\n  Aligning      text-to-image      models.\\nWe      recognize      that      current      efforts      in      fine-tuning      diffusion      models  \\n \\n  primarily      revolve      around      aligning      text-to-image      models      using      human      feedback      (Lee      et      al.,      2023;  \\n \\n  Wallace      et      al.,      2023;      Wu      et      al.,      2023;      Yang      et      al.,      2023).\\nIn      this      paper,      our      goal      is      to      consider   \\n \\na      more  \\n \\n  general      and      fundamental      formulation      that      is      not      tailored      to      any      particular      task.</p><p> \\n \\n  Diffusion      models      for      RL.\\nDiffusion      models      are      well-suited      for      specific      reinforcement      learning  \\n \\n  applications      due      to      their      ability      to      model      complex      and      multimodal      distributions      as      polices      (Janner  \\n \\n  et      al.,      2022;      Ajay      et      al.,      2023;      Wang      et      al.,      2022;      Hansen-Estruch      et      al.,      2023;      Du      et      al.,      2024;      Zhu  \\n \\n  et      al.,      2023).\\nFor      example,      Wang      et      al.\\n(2022)      use      conditional      diffusion      models      as      policies      and  \\n \\n  demonstrate      their      effectiveness      in      typical      offline      RL      benchmarks.\\nWhile      RL-based      fine-tuning      also  \\n \\n  aims      to      maximize      certain      reward      functions,      the      primary      focus      of      this      tutorial      covers      methods      that  \\n \\n  leverage      pre-trained      diffusion      models      for      this      purpose.</p><p> \\n \\n  RL      from      human      feedback      (RLHF)      for      language      models.\\nRLHF      is      widely      discussed      in      the  \\n \\n  context      of      language      models,      where      many      algorithms      share      similarities      with      those      used      in      diffusion  \\n \\n  models      (Ouyang      et      al.,      2022;      Bai      et      al.,      2022;      Casper      et      al.,      2023).\\nFor      instance,      both      domains  \\n \\n  commonly      employ      PPO      as   \\n \\na      standard      approach,      and      reward-weighted      training      (or      reward-weighted  \\n \\n  MLE)      often      serves      as   \\n \\na      basic      baseline      approach.\\nDespite      these      similarities,      nuanced      differences  \\n \\n  exist      mainly      because      diffusion      models      typically      operate      in   \\n \\na      continuous      input      space      rather      than  \\n \\n  a      discrete      one.\\nMoreover,      techniques      like      value-weighted      sampling      emerge      uniquely      within      the  \\n \\n  context      of      diffusion      models.</p><p>12      Summary</p><p> \\n \\n  In      this      article,      we      comprehensively      explain      how      fine-tuning      diffusion      models      to      maximize      down-  \\n \\n  stream      reward      functions      can      be      formalized      as   \\n \\na      reinforcement      learning      (RL)      problem      in      entropy-  \\n \\n  regularized      Markov      decision      processes      (MDPs).\\nBased      on      this      viewpoint,      we      elaborate      on      the  \\n \\n  application      of      various      RL      algorithms,      such      as      PPO      and      reward-weighted      MLE,      specifically      tailored  \\n \\n  for      fine-tuning      diffusion      models.\\nAdditionally,      we      categorize      different      scenarios      based      on      how  \\n \\n  reward      feedback      is      obtained.\\nWhile      this      categorization      is      not      always      explicitly      mentioned      in      many  \\n \\n  existing      works,      it      is      crucial      when      selecting      appropriate      algorithms.\\nFinally,      we      discuss      the      rela-  \\n \\n  tionships      with      several      related      topics,      including      classifier      guidance,      Gflownets,      path      integral      control  \\n \\n  theory,      and      sampling      from      unnormalized      distributions.</p><h7>8.2.      RL-Based      Fine-Tuning      for      Conditional      Generation</h7><p>tion  \\n \\n  In      this      section,      following      Zhao      et      al.\\n(2024),      we      explain      an      RL-based      fine-tuning      approach      for  \\n \\n  conditional      generation.\\nThe      core      idea      is      that      setting      r(x,      y)   \\n \\n=      log      p(y|x)      allows      us      to      sample      from  \\n \\n  the      target      conditional      distribution      (37),      drawing      on      the      insights      discussed      in      Section      4.2.  \\n \\n  More      precisely,      introducing   \\n \\na      distribution      over   \\n \\ny      as      q(-),      we      formulate      the      following      RL      problem:</p><p>{Di      ft   \\n \\n=      argmax      Egy,      (-\\\\\\\\\\\\\\\\-y)}.yott      log      p(y|xo)   \\n \\n—      NeopyaKL(pe(-|e,      y)      ||P      (late).\\n \\n \\n  {prE[(V,.X)PA(A)]      Heyy (38)</p><p> \\n \\n  Compared      to      the      objective      function      (18)      in      Section      4.2,      the      main      differences      are:      (1)      policy      space  \\n \\n  is      expanded      to      accommodate      additional      control      (i.e.,      y):      pz(-|-,-)   \\n \\nC      [R¢   \\n \\nx   \\n \\nY   \\n \\n+      A(R®)],      (2)      the  \\n \\n  fine-tuning      objectives      are      set      as      log-likelihoods:      log      p(y|z).\\nThen,      following      Theorem      1,      we      can  \\n \\n  establish      the      following      result.</p><p>Corollary      2.\\nThe      distribution      induced      by      {p*}1_74      (ue.   \\n \\n[      {Teri      Ph      (ae-1   \\n \\n|      te,      y)}deur      is      the target      conditional      distribution      (37).</p><p> \\n \\n  The      above      corollary      suggests      that      for      conditional      generation,      we      can      utilize      various      off-the-shelf  \\n \\n  algorithms      discussed      in      Section      4.2      and      6,      such      as      PPO,      reward      backpropagation,      and      reward-  \\n \\n  weighted      MLE,      as      well      as      value-weighted      sampling.</p><p>9\\n \\n   Connection      with      Flow-Based      Diffusion      Models</p><p> \\n \\n  We      elucidate      the      close      relationship      between      bridge      matching      and      fine-tuning,      as      summarized      in  \\n \\n  Table      2.\\nBridge      (flow)      matching      (Liu      et      al.,      2022;      Tong      et      al.,      2023;      Lipman      et      al.,      2023;      Liu      et      al.,  \\n \\n  2022;      Shi      et      al.,      2024;      Albergo      and      Vanden-Eijnden,      2022),      has      recently      gained      popularity      as   \\n \\na       distinct      type      of      diffusion      model      from      score-matching-based      ones.\\nThese      works      suggest      that      training  \\n \\n  and      inference      speed      will      be      accelerated      due      to      the      ability      to      define   \\n \\na      more      direct      trajectory      from  \\n \\n  noise      to      data      distribution      than      score-matching-based      diffusion      models.</p><p> \\n \\n  The      connection      between      bridge      matching      and      fine-tuning      becomes      evident      in      the      continuous-  \\n \\n  time      formulation.\\nTo      begin,      we      first      provide   \\n \\na      brief      overview      of      the      continuous-time      formulation      of  \\n \\n  score-matching-based      diffusion      models.</p><p> \\n \\n  Table      2:      Comparison      between      training      score/flow-based      diffusion      models      and      fine-tuning.\\nWe  \\n \\n  optimize      the      parameter   \\n \\n6      in      P’,      which      represents      the      induced      distribution      over      trajectories      associated  \\n \\n  with      6.</p><h8>Loss      Note      of      Q</h8><p>Score-based      KL(Q\\\\\\\\\\\\\\'™¢|P?)      Qtime:      Time-reversal      of      reference      SDE      (reference training      SDE      is      defined      from   \\n \\n0      to      T’      where      the      initial      dis-  \\n \\n  tribute      at   \\n \\n0      is      data      distribution)</p><p> \\n \\n  Flow-based      KL(Q*°>      ||P?)      @Q*°°h:      Coupling      between      the      reference      SDE      from  \\n \\n  training   \\n \\nT      to   \\n \\n0      conditioning      at   \\n \\n0      (dist.      over      x.;|7o)      and  \\n \\n  a      data      distribution      at      time   \\n \\n0      (dis.      over      x).\\nNote  \\n \\n  that      reference      SDE      is      defined      from      7’      to      0.</p><p> \\n \\n  Fine-tuning      KL(P?||Q*°)      @*:      Coupling      between      the      pre-trained      SDE  \\n \\n  from   \\n \\nT      to   \\n \\n0      conditioned      on   \\n \\n0      (dist.      over      x7.1|X0)  \\n \\n  and   \\n \\na      data      distribution      at      time   \\n \\n0      (dis.      over      Xo) 9.1      Continuous      Time      Formulation      of      Score-Based      diffusion      models</p><p> \\n \\n  We      provide   \\n \\na      brief      overview      of      the      continuous-time      formulation      of      diffusion      models      (Song      et      al.,  \\n \\n  2021),      further      expanding      Section      1.1.1.\\nIn      this      formulation,      we      initially      define      the      forward      reference  \\n \\n  SDE,      spanning      from      time   \\n \\n0      to      7’      (e.g.,      variance      exploding      (VE)      process      or      variance      preserving      (VP)  \\n \\n  process),      by      setting      an      initial      distribution      at   \\n \\n0      as      the      data      distribution.\\nAdditionally,      the      reference  \\n \\n  SDE      is      tailored      to      converge      to      an      easy      distribution      at      time      7’,      such      as      the      normal      distribution.</p><p> \\n \\n  Subsequently,      we      consider      the      time-reversal      SDE      (Anderson,      1982),      progressing      from      T\\\\\\\\\\\\\\'      to      0.\\nIf  \\n \\n  we      could      learn      this      time-reversal      SDE,      it      would      enable      us      to      sample      from      the      data      distribution      by  \\n \\n  starting      with      the      easy      initial      distribution      (at      7’)      and      following      the      time-reversal      SDE      from   \\n \\nT      to   \\n \\n0      at  \\n \\n  inference      time.</p><p> \\n \\n  Training.\\nNow,      the      remaining      question      is      how      to      learn      this      time-reversal      SDE.\\nHere,      by      repre-  \\n \\n  senting      the      induced      distribution      over      trajectories      from   \\n \\nT      to   \\n \\n0      with      this      time-reversal      SDE      as      Q\\\\\\\\\\\\\\'™®,  \\n \\n  our      goal      is      to      train   \\n \\na      new      SDE,      parameterized      by   \\n \\n@      in      the      neural      network,      such      that      the      induced  \\n \\n  distribution      P®      over      trajectories      associated      with   \\n \\n6      aligns      with      Q\"™*.\\nSpecifically,      the      objective      is      to minimize      the      following      losses:</p><p> \\n \\n  argmin      KL(Q\"™||P*).\\n(39)  \\n \\n  0  \\n \\n  With      some      algebra,      leveraging      the      observation      that      the      time-reversal      SDE      comprises      the      original  \\n \\n  forward      SDE      and      the      scoring      term      of      the      marginal      distribution      (Anderson,      1982)      as      in      (2),      Song  \\n \\n  et      al.\\n(2021)      demonstrates      that      this      KL      loss      (39)      is      approximated      as Dore   \\n \\n=      argmin      Eeuni((0,7))01~qyo0(weleo)co~pP\"e      Fe      Fl      Vey      log      jo      (Xt   \\n \\n|      Xo)   \\n \\n_      so(Xt,      t)|I?\\n],      (40)  \\n \\n  which      is      equal      to      (4)      where      the      weight      A(t)      is      {o?\\n}?\\n(for      notation,      refer      to      Section      1.1.1).</p><h9>9.2      Flow-Based      (Bridge-Based)      Diffusion      Models</h9><p>dels We      adopt      the      explanation      outlined      in      Liu      et      al.\\n(2022).\\nIn      this      framework,      we      begin      by      considering  \\n \\n  a      reference      SDE      (from      T\\\\\\\\\\\\\\'      to      0),      x;   \\n \\n=      zr_;      where      z;      is      defined      as      Brownian      motion:</p><p>dz,   \\n \\n=      dwt,      te      (0,      7).\\n(41)</p><p> \\n \\n  Subsequently,      we      consider      the      coupling      of      the      reference      SDE      conditioned      on      state      at      time   \\n \\n0      (i.e.,  \\n \\n  distribution      of      xo.7      conditional      on      x)      and      the      data      distribution      at      time      0,      denoting      the      resulting  \\n \\n  distribution      as      Q*°?,      Informally,      this      Q*°°?\\nis      characterized      as      follows.</p><p>Informal      characterization      of      Q*”.\\nWhen      discretizing      the      time      step,      by      denoting      the      distribution  \\n \\n  induced      by      the      reference      SDE      as      p\"(x7,--+   \\n \\n,      29),      we      define QF      (a7,      tae   \\n \\n,      20)   \\n \\n=      p(x,      ay   \\n \\n|      r9)pP\"°      (x9).</p><p>The      distribution      Q*”      above      is   \\n \\na      limiting      distribution      obtained      by      making      the      discretization      step small.</p><p>Training.\\nIn      bridge-based      diffusion      models,      our      objective      is      to      train   \\n \\na      new      SDE,      parametrized      by  \\n \\n  6      (e.g.,   \\n \\na      neural      network),      to      align      the      induced      distribution      P®      with      Q*°?.\\nSpecifically,      bridge-based training      aims      to      minimize aremin      KL(Q*>||P’).\\n(42)  \\n \\n  0  \\n \\n  While      the      method      to      minimize      the      above      is      currently      unclear,      we      can      derive   \\n \\na      trainable      objective  \\n \\n  through      algebraic      manipulation.\\nInitially,      we      observe      that      the      reference      SDE      (41),      conditioned      on  \\n \\n  state      xo      at      time   \\n \\n0      (zr   \\n \\n=      D),      is      expressed      as:</p><p>b\\n \\n \\n  b—      2;</p><p>t\\n \\n   €\\n \\n   [0,T];dz?\\n \\n \\n=      g?(z?,t)dt      +du,,  \\n \\n  g(z?,t)   \\n \\n=   \\n \\na       (43)</p><p>using      the      seminal      “Doob’s      h-transform”      (Rogers      and      Williams,      2000).\\nThis      SDE      is      commonly  \\n \\n  known      as      the      Brownian      bridge.\\nNow,      we      consider   \\n \\na      new      SDE      parameterized      by      0:</p><p>t\\n \\n   €\\n \\n   [0,7];      dz   \\n \\n=      s(z,t;      @)dt   \\n \\n+      dw.\\n(44)</p><p>Then,      we      can      demonstrate      that      minimizing      the      KL      loss      in      (42)      is      equivalent      to argmin      E,W      unil0,7],1~4\"°      cop?\\n[Ilg”      (2,      t)   \\n \\n_      s(21,      t;      0)      M3]      (45)</p><p>where      q;°      denotes      the      induced      distribution      at      time   \\n \\nt      following      SDE      (43)      conditioned      on      xo,      and  \\n \\n  s:      Xx      |0,T]   \\n \\n>   \\n \\n&      is      the      model      we      aim      to      optimize.</p><p> \\n \\n  Flow-based      diffusion      models.\\nThe      aforementioned      formulation      has      been      originally      proposed      as   \\n \\na       bridge-based      SDE      (Liu      et      al.,      2022)      because      the      conditional      SDE      is      often      referred      to      as   \\n \\na      Brownian  \\n \\n  bridge.\\nFlow-based      diffusion      models      proposed      in      Lipman      et      al.\\n(2023);      Tong      et      al.\\n(2023)      share   \\n \\na       close      relationship      with      bridge-based      diffusion      models.\\nThese      flow-based      models      use   \\n \\na      target      SDE (43)      and      an      SDE      with   \\n \\na      parameter      (44)      without      stochastic      terms      (1.e.,      no      dw,)      and      aim      to      minimize  \\n \\n  the      loss      function      (45).</p><p> \\n \\n  Comparison      with      score-based      diffusion      models.\\nBoth      score-based      diffusion      models      and      bridge-  \\n \\n  based      diffusion      models      aim      to      minimize      KL      divergences.\\nDespite      its      similarity,      in      contrast      to  \\n \\n  score-based      diffusion      models,      bridge-based      diffusion      models      target      Q*°>      rather      than      the      time-  \\n \\n  reversal      SDE      Q\"™®.\\nThe      distribution      Q*”      is      often      considered      preferable      because      it      can      impose      an  \\n \\n  SDE      that      navigates      efficiently      from   \\n \\n0      to      7’      (e.g.,   \\n \\na      Brownian      bridge      with      minimal      noise).\\nThis      can  \\n \\n  expedite      the      learning      process      since      we      lack      direct      control      over      the      time-reversal      SDE      in      score-based  \\n \\n  diffusion      models.</p><h10>9.3.      Connection      with      RL-Based      Fine-Tuning</h10><p>ning  \\n \\n  RL-based      fine-tuning      shares   \\n \\na      close      relationship      with      flow-based      training.\\nTo      illustrate      this,      consider  \\n \\n  the      reference      SDE      (from      7’      to      0)      following      the      pre-trained      diffusion      model.\\nThen,      akin      to      flow-  \\n \\n  based      training,      we      consider      the      coupling      of      the      reference      SDE      conditioned      on      state      at      time   \\n \\n0       (i.e.,      distribution      over      xo-r      conditioned      on      x9)      and      the      target      distribution      at      time   \\n \\n0      (i.e.,      p,(-)   \\n \\n«       exp(r(-)/a)p?\\n\"°(-)),      denoting      the      induced      distribution      as      Q*™.</p><p>Informal      characterization      of      Q™°*.\\nInformally,      Q*\"°      is      introduced      as      follows.\\nWhen      discretizing  \\n \\n  the      time      step,      by      denoting      the      distribution      induced      by      the      reference      SDE      as      p™\\\\\\\\\\\\\\'(xp,-+-   \\n \\n,      9),      we define Qi\"      (xr,   \\n \\na   \\n \\n,      0)   \\n \\n=      p(x,      nr      Ly   \\n \\n|      L0)Pr      (Xo).</p><p>The      distribution      Q*°      above      is   \\n \\na      limiting      distribution      obtained      by      making      the      discretization      step small.</p><p>Training.\\nNow,      we      introduce   \\n \\na      new      SDE      parameterized      by   \\n \\n@      such      as      an      neural      network      to      align  \\n \\n  the      induced      distribution      P’      with      Q®\"*.\\nActually,      in      the      continuous      formulation,      the      RL-problem      (18) in      Section      4.2      is      equal      to      solving argmin      KL(P?||Q\"™).\\n \\n \\n  0 This      formalization      has      been      presented      in      Uehara      et      al.\\n(2024).\\nIntuitively,      as      we      see      in      Theorem      3,  \\n \\n  this      is      expected      because      we      see      that      the      posterior      distribution      induced      by      pre-trained      models:</p><p>ref   \\n \\n(      PD      (ar,-++      21   \\n \\n|      to)      (=      vey      (er|er_-1)p      Pp      (ar-1   \\n \\n|      er_2)      +++      pp\"      (x1   \\n \\n|      20), remains      preserved      after      fine-tuning      based      on      the      RL-formulation      (18).</p><p> \\n \\n  Comparison      with      flow-based      training.\\nIn      contrast      to      flow-based      training,      RL-based      fine-tuning  \\n \\n  minimizes      the      inverse      KL      divergence      rather      than      the      KL      divergence.\\nIntuitively,      this      is      because,  \\n \\n  unlike      Qt™*      or      Q*>   \\n \\n,      we      cannot      sample      from      Q*”      (recall      that   \\n \\na      marginal      distribution      at      time   \\n \\n0      in  \\n \\n  Qi,      i.e.,      p,,      is      unnormalized);      on      the      other      hand,      the      marginal      distributions      at      time   \\n \\n0      in      both      Q\\\\\\\\\\\\\\'™*  \\n \\n  and      Q*°°>      are      data      distributions.</p><p> \\n \\n  Remark   \\n \\n7      (Extension      to      f-divergence).\\nEach      of      these      methods      can      be      technically      extended      by  \\n \\n  incorporating      more      general      divergences      beyond      KL      divergence.\\nFor      instance,      in      the      context      of  \\n \\n  fine-tuning,      please      see      Tang      (2024).</p><p>10      Connection      with      Sampling      from      Unnormalized      Distributions</p><p> \\n \\n  Numerous      studies      delve      into      sampling      from      an      unnormalized      distribution      proportional      to      exp(r()),  \\n \\n  commonly      known      as      the      Gibbs      distribution,      extensively      discussed      in      the      computational      statistics  \\n \\n  and      statistical      physics      community      (Robert,      2014).\\nThis      issue      is      pertinent      to      our      work,      as      during  \\n \\n  fine-tuning,      the      target      distribution      is      also      formulated      in      the      Gibbs      distribution      form      in      (10),      which      is  \\n \\n  proportional      to      exp(r(a))p?\\n\"*      (x).</p><p> \\n \\n  The      literature      employs      two      main      approaches      to      tackle      this      challenge:      Markov      Chain      Monte  \\n \\n  Carlo      (MCMC)      and      RL-based      methods.\\nIn      particular,      the      RL-based      diffusion      model      fine-tuning  \\n \\n  we      have      discussed      so      far      is      closely      related      to      the      latter      approach.\\nIn      this      section,      we      explain      and  \\n \\n  compare      these      two      approaches.</p><h10>10.1      Markov      Chain      Monte      Carlo      (MCMC)</h10><p>CMC)  \\n \\n  In      MCMC,   \\n \\na      Markov      chain      is      constructed      to      approximate   \\n \\na      target      distribution      such      that      the      equilib-  \\n \\n  rium      distribution      of      the      Markov      chain      converges      to      the      target      distribution.\\nHowever,      dealing      with  \\n \\n  high-dimensional      domain      spaces      is      challenging      for      MCMC.\\n \\n \\nA      common      strategy      involves      leveraging  \\n \\n  gradient      information,      such      as      the      Metropolis-adjusted      Langevin      algorithm      (MALA)      (Besag,      1994)  \\n \\n  or      Hamiltonian      Monte      Carlo      (HMC)      (Neal      et      al.,      2011;      Girolami      and      Calderhead,      2011).\\nMALA  \\n \\n  shares      similarities      with      classifier      guidance      and      value-weighted      sampling,      as      both      methods      depend  \\n \\n  on      the      first-order      information      from      reward      (or      value)      functions.</p><p> \\n \\n  Can      we      use      MCMC      for      fine-tuning      diffusion      models?\\nIn      the      context      of      fine-tuning      dif-  \\n \\n  fusion      models,      while      it      may      seem      intuitive      to      sample      from      the      target      distribution      (10)      (i.e.,  \\n \\n  Dy   \\n \\n&      exp(r(x))pP\"*(x))      using      MCMC,      it’s      not      straightforward      for      the      following      reasons:</p><p>¢\\n \\n   We      lack      of      an      analytical      form      of      p?\\n\"*(-).<ul><li>*\\n \\n   Additionally,      even      if      we      can      estimate      p?\\n’°(-)      in      an      unbiased      manner,      as      in      Chen      et      al.\\n(2018),  \\n \\n  the      mixing      time      for      obtaining   \\n \\na      single      sample      in      MCMC      might      be      lengthy.</li></ul></p><p> \\n \\n  Therefore,      we      explore      creating   \\n \\na      generative      model      (i.e.,      diffusion      model)      for      this      task      using      RL-  \\n \\n  based      fine-tuning      so      that      we      can      easily      sample      from      p,(-)      during      inference      (i.e.,      simulating      the  \\n \\n  time-reversal      SDE      (2)      with      policies      from   \\n \\n¢   \\n \\n=      T\\\\\\\\\\\\\\'+   \\n \\n1      to   \\n \\nt   \\n \\n=      1),      without      relying      on      MCMC.\\nThis  \\n \\n  addresses      the      concern      of      MCMC      by      avoiding      the      need      to      estimate      p?\\n\"*(-)      and      minimizing      inference  \\n \\n  time,      albeit      at      the      expense      of      increased      training      time.</p><p>10.2}      RL-Based      Approaches  \\n \\n  While      MCMC      has      historically      been      widely      used      for      this      purpose,      recent      works      have      proposed      an  \\n \\n  RL-based      approach      or      its      variant      (e.g.,      Bernton      et      al.      (2019);      Heng      et      al.\\n(2020);      Huang      et      al.\\n(2023);</p><p> \\n \\n  Vargas      et      al.\\n(2023)).\\nFor      example,   \\n \\na      seminal      study      (Zhang      and      Chen,      2021)      introduced   \\n \\na      method  \\n \\n  very      similar      to      reward      backpropagation.\\nIn      their      work,      they      initially      define      Brownian      motion      as   \\n \\na       reference      SDE      (from      7’      to      0),      similar      to      the      pre-trained      diffusion      models      in      our      context.\\nThen,      by  \\n \\n  appropriately      setting      rewards,      Zhang      and      Chen      (2021)      aims      to      learn   \\n \\na      new      SDE      (from      T\\\\\\\\\\\\\\'      to      0)      such  \\n \\n  that      we      can      sample      from      the      target      distribution      at      the      endpoint      0.</p><p> \\n \\n  These      approaches      offer      advantages      over      MCMC      due      to      their      ability      to      minimize      inference      time  \\n \\n  (amortization)      by      constructing   \\n \\na      generative      model      comprising      policies      in      the      training      time      and      solely  \\n \\n  executing      learned      policies      during      inference.\\nUnlike      MCMC,      these      methods      significantly      reduce      the  \\n \\n  inference      cost      as      we      don’t      need      to      be      concerned      about      the      mixing      time.\\nAdditionally,      RL-based  \\n \\n  approaches      have      the      potential      to      effectively      handle      high-dimensional,      complex      (i.e.,      multi-modal)  \\n \\n  distributions      by      harnessing      the      considerable      expressive      power      arising      from      latent      states.</p><p> \\n \\n  Despite      these      advantages,      it      is      often      unclear      how      to      select   \\n \\na      reference      SDE      in      these      works.\\nIn  \\n \\n  contrast,      in      RL-based      fine-tuning      of      diffusion      models,      we      directly      leverage      pre-trained      diffusion  \\n \\n  models      as      the      reference      SDEs,      i.e.,      as      our      prior      knowledge.</p><p> \\n \\n  Key      message.\\nFinally,      it      is      worth      noting      that      we      can      technically      utilize      any      off-the-shelf      RL      algo-  \\n \\n  rithms      for      training.\\nFor      example,      Zhang      and      Chen      (2021)      employs      reward      backpropagation,      while  \\n \\n  works      in      Gflownets      typically      leverage      PCL,      as      we      mention      in      Section      6.3.\\nHowever,      additionally,      it  \\n \\n  is      possible      to      use      PPO      or      reward-weighted      MLE      mentioned      in      Section      4.2.</p><p>11      Closely      Related      Directions</p><p>Lastly,      we      highlight      several      closely      related      directions      we      have      not      yet      discussed.</p><p> \\n \\n  Aligning      text-to-image      models.\\nWe      recognize      that      current      efforts      in      fine-tuning      diffusion      models  \\n \\n  primarily      revolve      around      aligning      text-to-image      models      using      human      feedback      (Lee      et      al.,      2023;  \\n \\n  Wallace      et      al.,      2023;      Wu      et      al.,      2023;      Yang      et      al.,      2023).\\nIn      this      paper,      our      goal      is      to      consider   \\n \\na      more  \\n \\n  general      and      fundamental      formulation      that      is      not      tailored      to      any      particular      task.</p><p> \\n \\n  Diffusion      models      for      RL.\\nDiffusion      models      are      well-suited      for      specific      reinforcement      learning  \\n \\n  applications      due      to      their      ability      to      model      complex      and      multimodal      distributions      as      polices      (Janner  \\n \\n  et      al.,      2022;      Ajay      et      al.,      2023;      Wang      et      al.,      2022;      Hansen-Estruch      et      al.,      2023;      Du      et      al.,      2024;      Zhu  \\n \\n  et      al.,      2023).\\nFor      example,      Wang      et      al.\\n(2022)      use      conditional      diffusion      models      as      policies      and  \\n \\n  demonstrate      their      effectiveness      in      typical      offline      RL      benchmarks.\\nWhile      RL-based      fine-tuning      also  \\n \\n  aims      to      maximize      certain      reward      functions,      the      primary      focus      of      this      tutorial      covers      methods      that  \\n \\n  leverage      pre-trained      diffusion      models      for      this      purpose.</p><p> \\n \\n  RL      from      human      feedback      (RLHF)      for      language      models.\\nRLHF      is      widely      discussed      in      the  \\n \\n  context      of      language      models,      where      many      algorithms      share      similarities      with      those      used      in      diffusion  \\n \\n  models      (Ouyang      et      al.,      2022;      Bai      et      al.,      2022;      Casper      et      al.,      2023).\\nFor      instance,      both      domains  \\n \\n  commonly      employ      PPO      as   \\n \\na      standard      approach,      and      reward-weighted      training      (or      reward-weighted  \\n \\n  MLE)      often      serves      as   \\n \\na      basic      baseline      approach.\\nDespite      these      similarities,      nuanced      differences  \\n \\n  exist      mainly      because      diffusion      models      typically      operate      in   \\n \\na      continuous      input      space      rather      than  \\n \\n  a      discrete      one.\\nMoreover,      techniques      like      value-weighted      sampling      emerge      uniquely      within      the  \\n \\n  context      of      diffusion      models.</p><p>12      Summary</p><p> \\n \\n  In      this      article,      we      comprehensively      explain      how      fine-tuning      diffusion      models      to      maximize      down-  \\n \\n  stream      reward      functions      can      be      formalized      as   \\n \\na      reinforcement      learning      (RL)      problem      in      entropy-  \\n \\n  regularized      Markov      decision      processes      (MDPs).\\nBased      on      this      viewpoint,      we      elaborate      on      the  \\n \\n  application      of      various      RL      algorithms,      such      as      PPO      and      reward-weighted      MLE,      specifically      tailored  \\n \\n  for      fine-tuning      diffusion      models.\\nAdditionally,      we      categorize      different      scenarios      based      on      how  \\n \\n  reward      feedback      is      obtained.\\nWhile      this      categorization      is      not      always      explicitly      mentioned      in      many  \\n \\n  existing      works,      it      is      crucial      when      selecting      appropriate      algorithms.\\nFinally,      we      discuss      the      rela-  \\n \\n  tionships      with      several      related      topics,      including      classifier      guidance,      Gflownets,      path      integral      control  \\n \\n  theory,      and      sampling      from      unnormalized      distributions.</p><h8>Loss      Note      of      Q</h8><p>Score-based      KL(Q\\\\\\\\\\\\\\'™¢|P?)      Qtime:      Time-reversal      of      reference      SDE      (reference training      SDE      is      defined      from   \\n \\n0      to      T’      where      the      initial      dis-  \\n \\n  tribute      at   \\n \\n0      is      data      distribution)</p><p> \\n \\n  Flow-based      KL(Q*°>      ||P?)      @Q*°°h:      Coupling      between      the      reference      SDE      from  \\n \\n  training   \\n \\nT      to   \\n \\n0      conditioning      at   \\n \\n0      (dist.      over      x.;|7o)      and  \\n \\n  a      data      distribution      at      time   \\n \\n0      (dis.      over      x).\\nNote  \\n \\n  that      reference      SDE      is      defined      from      7’      to      0.</p><p> \\n \\n  Fine-tuning      KL(P?||Q*°)      @*:      Coupling      between      the      pre-trained      SDE  \\n \\n  from   \\n \\nT      to   \\n \\n0      conditioned      on   \\n \\n0      (dist.      over      x7.1|X0)  \\n \\n  and   \\n \\na      data      distribution      at      time   \\n \\n0      (dis.      over      Xo) 9.1      Continuous      Time      Formulation      of      Score-Based      diffusion      models</p><p> \\n \\n  We      provide   \\n \\na      brief      overview      of      the      continuous-time      formulation      of      diffusion      models      (Song      et      al.,  \\n \\n  2021),      further      expanding      Section      1.1.1.\\nIn      this      formulation,      we      initially      define      the      forward      reference  \\n \\n  SDE,      spanning      from      time   \\n \\n0      to      7’      (e.g.,      variance      exploding      (VE)      process      or      variance      preserving      (VP)  \\n \\n  process),      by      setting      an      initial      distribution      at   \\n \\n0      as      the      data      distribution.\\nAdditionally,      the      reference  \\n \\n  SDE      is      tailored      to      converge      to      an      easy      distribution      at      time      7’,      such      as      the      normal      distribution.</p><p> \\n \\n  Subsequently,      we      consider      the      time-reversal      SDE      (Anderson,      1982),      progressing      from      T\\\\\\\\\\\\\\'      to      0.\\nIf  \\n \\n  we      could      learn      this      time-reversal      SDE,      it      would      enable      us      to      sample      from      the      data      distribution      by  \\n \\n  starting      with      the      easy      initial      distribution      (at      7’)      and      following      the      time-reversal      SDE      from   \\n \\nT      to   \\n \\n0      at  \\n \\n  inference      time.</p><p> \\n \\n  Training.\\nNow,      the      remaining      question      is      how      to      learn      this      time-reversal      SDE.\\nHere,      by      repre-  \\n \\n  senting      the      induced      distribution      over      trajectories      from   \\n \\nT      to   \\n \\n0      with      this      time-reversal      SDE      as      Q\\\\\\\\\\\\\\'™®,  \\n \\n  our      goal      is      to      train   \\n \\na      new      SDE,      parameterized      by   \\n \\n@      in      the      neural      network,      such      that      the      induced  \\n \\n  distribution      P®      over      trajectories      associated      with   \\n \\n6      aligns      with      Q\"™*.\\nSpecifically,      the      objective      is      to minimize      the      following      losses:</p><p> \\n \\n  argmin      KL(Q\"™||P*).\\n(39)  \\n \\n  0  \\n \\n  With      some      algebra,      leveraging      the      observation      that      the      time-reversal      SDE      comprises      the      original  \\n \\n  forward      SDE      and      the      scoring      term      of      the      marginal      distribution      (Anderson,      1982)      as      in      (2),      Song  \\n \\n  et      al.\\n(2021)      demonstrates      that      this      KL      loss      (39)      is      approximated      as Dore   \\n \\n=      argmin      Eeuni((0,7))01~qyo0(weleo)co~pP\"e      Fe      Fl      Vey      log      jo      (Xt   \\n \\n|      Xo)   \\n \\n_      so(Xt,      t)|I?\\n],      (40)  \\n \\n  which      is      equal      to      (4)      where      the      weight      A(t)      is      {o?\\n}?\\n(for      notation,      refer      to      Section      1.1.1).</p><h9>9.2      Flow-Based      (Bridge-Based)      Diffusion      Models</h9><p>dels We      adopt      the      explanation      outlined      in      Liu      et      al.\\n(2022).\\nIn      this      framework,      we      begin      by      considering  \\n \\n  a      reference      SDE      (from      T\\\\\\\\\\\\\\'      to      0),      x;   \\n \\n=      zr_;      where      z;      is      defined      as      Brownian      motion:</p><p>dz,   \\n \\n=      dwt,      te      (0,      7).\\n(41)</p><p> \\n \\n  Subsequently,      we      consider      the      coupling      of      the      reference      SDE      conditioned      on      state      at      time   \\n \\n0      (i.e.,  \\n \\n  distribution      of      xo.7      conditional      on      x)      and      the      data      distribution      at      time      0,      denoting      the      resulting  \\n \\n  distribution      as      Q*°?,      Informally,      this      Q*°°?\\nis      characterized      as      follows.</p><p>Informal      characterization      of      Q*”.\\nWhen      discretizing      the      time      step,      by      denoting      the      distribution  \\n \\n  induced      by      the      reference      SDE      as      p\"(x7,--+   \\n \\n,      29),      we      define QF      (a7,      tae   \\n \\n,      20)   \\n \\n=      p(x,      ay   \\n \\n|      r9)pP\"°      (x9).</p><p>The      distribution      Q*”      above      is   \\n \\na      limiting      distribution      obtained      by      making      the      discretization      step small.</p><p>Training.\\nIn      bridge-based      diffusion      models,      our      objective      is      to      train   \\n \\na      new      SDE,      parametrized      by  \\n \\n  6      (e.g.,   \\n \\na      neural      network),      to      align      the      induced      distribution      P®      with      Q*°?.\\nSpecifically,      bridge-based training      aims      to      minimize aremin      KL(Q*>||P’).\\n(42)  \\n \\n  0  \\n \\n  While      the      method      to      minimize      the      above      is      currently      unclear,      we      can      derive   \\n \\na      trainable      objective  \\n \\n  through      algebraic      manipulation.\\nInitially,      we      observe      that      the      reference      SDE      (41),      conditioned      on  \\n \\n  state      xo      at      time   \\n \\n0      (zr   \\n \\n=      D),      is      expressed      as:</p><p>b\\n \\n \\n  b—      2;</p><p>t\\n \\n   €\\n \\n   [0,T];dz?\\n \\n \\n=      g?(z?,t)dt      +du,,  \\n \\n  g(z?,t)   \\n \\n=   \\n \\na       (43)</p><p>using      the      seminal      “Doob’s      h-transform”      (Rogers      and      Williams,      2000).\\nThis      SDE      is      commonly  \\n \\n  known      as      the      Brownian      bridge.\\nNow,      we      consider   \\n \\na      new      SDE      parameterized      by      0:</p><p>t\\n \\n   €\\n \\n   [0,7];      dz   \\n \\n=      s(z,t;      @)dt   \\n \\n+      dw.\\n(44)</p><p>Then,      we      can      demonstrate      that      minimizing      the      KL      loss      in      (42)      is      equivalent      to argmin      E,W      unil0,7],1~4\"°      cop?\\n[Ilg”      (2,      t)   \\n \\n_      s(21,      t;      0)      M3]      (45)</p><p>where      q;°      denotes      the      induced      distribution      at      time   \\n \\nt      following      SDE      (43)      conditioned      on      xo,      and  \\n \\n  s:      Xx      |0,T]   \\n \\n>   \\n \\n&      is      the      model      we      aim      to      optimize.</p><p> \\n \\n  Flow-based      diffusion      models.\\nThe      aforementioned      formulation      has      been      originally      proposed      as   \\n \\na       bridge-based      SDE      (Liu      et      al.,      2022)      because      the      conditional      SDE      is      often      referred      to      as   \\n \\na      Brownian  \\n \\n  bridge.\\nFlow-based      diffusion      models      proposed      in      Lipman      et      al.\\n(2023);      Tong      et      al.\\n(2023)      share   \\n \\na       close      relationship      with      bridge-based      diffusion      models.\\nThese      flow-based      models      use   \\n \\na      target      SDE (43)      and      an      SDE      with   \\n \\na      parameter      (44)      without      stochastic      terms      (1.e.,      no      dw,)      and      aim      to      minimize  \\n \\n  the      loss      function      (45).</p><p> \\n \\n  Comparison      with      score-based      diffusion      models.\\nBoth      score-based      diffusion      models      and      bridge-  \\n \\n  based      diffusion      models      aim      to      minimize      KL      divergences.\\nDespite      its      similarity,      in      contrast      to  \\n \\n  score-based      diffusion      models,      bridge-based      diffusion      models      target      Q*°>      rather      than      the      time-  \\n \\n  reversal      SDE      Q\"™®.\\nThe      distribution      Q*”      is      often      considered      preferable      because      it      can      impose      an  \\n \\n  SDE      that      navigates      efficiently      from   \\n \\n0      to      7’      (e.g.,   \\n \\na      Brownian      bridge      with      minimal      noise).\\nThis      can  \\n \\n  expedite      the      learning      process      since      we      lack      direct      control      over      the      time-reversal      SDE      in      score-based  \\n \\n  diffusion      models.</p><h10>9.3.      Connection      with      RL-Based      Fine-Tuning</h10><p>ning  \\n \\n  RL-based      fine-tuning      shares   \\n \\na      close      relationship      with      flow-based      training.\\nTo      illustrate      this,      consider  \\n \\n  the      reference      SDE      (from      7’      to      0)      following      the      pre-trained      diffusion      model.\\nThen,      akin      to      flow-  \\n \\n  based      training,      we      consider      the      coupling      of      the      reference      SDE      conditioned      on      state      at      time   \\n \\n0       (i.e.,      distribution      over      xo-r      conditioned      on      x9)      and      the      target      distribution      at      time   \\n \\n0      (i.e.,      p,(-)   \\n \\n«       exp(r(-)/a)p?\\n\"°(-)),      denoting      the      induced      distribution      as      Q*™.</p><p>Informal      characterization      of      Q™°*.\\nInformally,      Q*\"°      is      introduced      as      follows.\\nWhen      discretizing  \\n \\n  the      time      step,      by      denoting      the      distribution      induced      by      the      reference      SDE      as      p™\\\\\\\\\\\\\\'(xp,-+-   \\n \\n,      9),      we define Qi\"      (xr,   \\n \\na   \\n \\n,      0)   \\n \\n=      p(x,      nr      Ly   \\n \\n|      L0)Pr      (Xo).</p><p>The      distribution      Q*°      above      is   \\n \\na      limiting      distribution      obtained      by      making      the      discretization      step small.</p><p>Training.\\nNow,      we      introduce   \\n \\na      new      SDE      parameterized      by   \\n \\n@      such      as      an      neural      network      to      align  \\n \\n  the      induced      distribution      P’      with      Q®\"*.\\nActually,      in      the      continuous      formulation,      the      RL-problem      (18) in      Section      4.2      is      equal      to      solving argmin      KL(P?||Q\"™).\\n \\n \\n  0 This      formalization      has      been      presented      in      Uehara      et      al.\\n(2024).\\nIntuitively,      as      we      see      in      Theorem      3,  \\n \\n  this      is      expected      because      we      see      that      the      posterior      distribution      induced      by      pre-trained      models:</p><p>ref   \\n \\n(      PD      (ar,-++      21   \\n \\n|      to)      (=      vey      (er|er_-1)p      Pp      (ar-1   \\n \\n|      er_2)      +++      pp\"      (x1   \\n \\n|      20), remains      preserved      after      fine-tuning      based      on      the      RL-formulation      (18).</p><p> \\n \\n  Comparison      with      flow-based      training.\\nIn      contrast      to      flow-based      training,      RL-based      fine-tuning  \\n \\n  minimizes      the      inverse      KL      divergence      rather      than      the      KL      divergence.\\nIntuitively,      this      is      because,  \\n \\n  unlike      Qt™*      or      Q*>   \\n \\n,      we      cannot      sample      from      Q*”      (recall      that   \\n \\na      marginal      distribution      at      time   \\n \\n0      in  \\n \\n  Qi,      i.e.,      p,,      is      unnormalized);      on      the      other      hand,      the      marginal      distributions      at      time   \\n \\n0      in      both      Q\\\\\\\\\\\\\\'™*  \\n \\n  and      Q*°°>      are      data      distributions.</p><p> \\n \\n  Remark   \\n \\n7      (Extension      to      f-divergence).\\nEach      of      these      methods      can      be      technically      extended      by  \\n \\n  incorporating      more      general      divergences      beyond      KL      divergence.\\nFor      instance,      in      the      context      of  \\n \\n  fine-tuning,      please      see      Tang      (2024).</p><p>10      Connection      with      Sampling      from      Unnormalized      Distributions</p><p> \\n \\n  Numerous      studies      delve      into      sampling      from      an      unnormalized      distribution      proportional      to      exp(r()),  \\n \\n  commonly      known      as      the      Gibbs      distribution,      extensively      discussed      in      the      computational      statistics  \\n \\n  and      statistical      physics      community      (Robert,      2014).\\nThis      issue      is      pertinent      to      our      work,      as      during  \\n \\n  fine-tuning,      the      target      distribution      is      also      formulated      in      the      Gibbs      distribution      form      in      (10),      which      is  \\n \\n  proportional      to      exp(r(a))p?\\n\"*      (x).</p><p> \\n \\n  The      literature      employs      two      main      approaches      to      tackle      this      challenge:      Markov      Chain      Monte  \\n \\n  Carlo      (MCMC)      and      RL-based      methods.\\nIn      particular,      the      RL-based      diffusion      model      fine-tuning  \\n \\n  we      have      discussed      so      far      is      closely      related      to      the      latter      approach.\\nIn      this      section,      we      explain      and  \\n \\n  compare      these      two      approaches.</p><h10>10.1      Markov      Chain      Monte      Carlo      (MCMC)</h10><p>CMC)  \\n \\n  In      MCMC,   \\n \\na      Markov      chain      is      constructed      to      approximate   \\n \\na      target      distribution      such      that      the      equilib-  \\n \\n  rium      distribution      of      the      Markov      chain      converges      to      the      target      distribution.\\nHowever,      dealing      with  \\n \\n  high-dimensional      domain      spaces      is      challenging      for      MCMC.\\n \\n \\nA      common      strategy      involves      leveraging  \\n \\n  gradient      information,      such      as      the      Metropolis-adjusted      Langevin      algorithm      (MALA)      (Besag,      1994)  \\n \\n  or      Hamiltonian      Monte      Carlo      (HMC)      (Neal      et      al.,      2011;      Girolami      and      Calderhead,      2011).\\nMALA  \\n \\n  shares      similarities      with      classifier      guidance      and      value-weighted      sampling,      as      both      methods      depend  \\n \\n  on      the      first-order      information      from      reward      (or      value)      functions.</p><p> \\n \\n  Can      we      use      MCMC      for      fine-tuning      diffusion      models?\\nIn      the      context      of      fine-tuning      dif-  \\n \\n  fusion      models,      while      it      may      seem      intuitive      to      sample      from      the      target      distribution      (10)      (i.e.,  \\n \\n  Dy   \\n \\n&      exp(r(x))pP\"*(x))      using      MCMC,      it’s      not      straightforward      for      the      following      reasons:</p><p>¢\\n \\n   We      lack      of      an      analytical      form      of      p?\\n\"*(-).<ul><li>*\\n \\n   Additionally,      even      if      we      can      estimate      p?\\n’°(-)      in      an      unbiased      manner,      as      in      Chen      et      al.\\n(2018),  \\n \\n  the      mixing      time      for      obtaining   \\n \\na      single      sample      in      MCMC      might      be      lengthy.</li></ul></p><p> \\n \\n  Therefore,      we      explore      creating   \\n \\na      generative      model      (i.e.,      diffusion      model)      for      this      task      using      RL-  \\n \\n  based      fine-tuning      so      that      we      can      easily      sample      from      p,(-)      during      inference      (i.e.,      simulating      the  \\n \\n  time-reversal      SDE      (2)      with      policies      from   \\n \\n¢   \\n \\n=      T\\\\\\\\\\\\\\'+   \\n \\n1      to   \\n \\nt   \\n \\n=      1),      without      relying      on      MCMC.\\nThis  \\n \\n  addresses      the      concern      of      MCMC      by      avoiding      the      need      to      estimate      p?\\n\"*(-)      and      minimizing      inference  \\n \\n  time,      albeit      at      the      expense      of      increased      training      time.</p><p>10.2}      RL-Based      Approaches  \\n \\n  While      MCMC      has      historically      been      widely      used      for      this      purpose,      recent      works      have      proposed      an  \\n \\n  RL-based      approach      or      its      variant      (e.g.,      Bernton      et      al.      (2019);      Heng      et      al.\\n(2020);      Huang      et      al.\\n(2023);</p><p> \\n \\n  Vargas      et      al.\\n(2023)).\\nFor      example,   \\n \\na      seminal      study      (Zhang      and      Chen,      2021)      introduced   \\n \\na      method  \\n \\n  very      similar      to      reward      backpropagation.\\nIn      their      work,      they      initially      define      Brownian      motion      as   \\n \\na       reference      SDE      (from      7’      to      0),      similar      to      the      pre-trained      diffusion      models      in      our      context.\\nThen,      by  \\n \\n  appropriately      setting      rewards,      Zhang      and      Chen      (2021)      aims      to      learn   \\n \\na      new      SDE      (from      T\\\\\\\\\\\\\\'      to      0)      such  \\n \\n  that      we      can      sample      from      the      target      distribution      at      the      endpoint      0.</p><p> \\n \\n  These      approaches      offer      advantages      over      MCMC      due      to      their      ability      to      minimize      inference      time  \\n \\n  (amortization)      by      constructing   \\n \\na      generative      model      comprising      policies      in      the      training      time      and      solely  \\n \\n  executing      learned      policies      during      inference.\\nUnlike      MCMC,      these      methods      significantly      reduce      the  \\n \\n  inference      cost      as      we      don’t      need      to      be      concerned      about      the      mixing      time.\\nAdditionally,      RL-based  \\n \\n  approaches      have      the      potential      to      effectively      handle      high-dimensional,      complex      (i.e.,      multi-modal)  \\n \\n  distributions      by      harnessing      the      considerable      expressive      power      arising      from      latent      states.</p><p> \\n \\n  Despite      these      advantages,      it      is      often      unclear      how      to      select   \\n \\na      reference      SDE      in      these      works.\\nIn  \\n \\n  contrast,      in      RL-based      fine-tuning      of      diffusion      models,      we      directly      leverage      pre-trained      diffusion  \\n \\n  models      as      the      reference      SDEs,      i.e.,      as      our      prior      knowledge.</p><p> \\n \\n  Key      message.\\nFinally,      it      is      worth      noting      that      we      can      technically      utilize      any      off-the-shelf      RL      algo-  \\n \\n  rithms      for      training.\\nFor      example,      Zhang      and      Chen      (2021)      employs      reward      backpropagation,      while  \\n \\n  works      in      Gflownets      typically      leverage      PCL,      as      we      mention      in      Section      6.3.\\nHowever,      additionally,      it  \\n \\n  is      possible      to      use      PPO      or      reward-weighted      MLE      mentioned      in      Section      4.2.</p><p>11      Closely      Related      Directions</p><p>Lastly,      we      highlight      several      closely      related      directions      we      have      not      yet      discussed.</p><p> \\n \\n  Aligning      text-to-image      models.\\nWe      recognize      that      current      efforts      in      fine-tuning      diffusion      models  \\n \\n  primarily      revolve      around      aligning      text-to-image      models      using      human      feedback      (Lee      et      al.,      2023;  \\n \\n  Wallace      et      al.,      2023;      Wu      et      al.,      2023;      Yang      et      al.,      2023).\\nIn      this      paper,      our      goal      is      to      consider   \\n \\na      more  \\n \\n  general      and      fundamental      formulation      that      is      not      tailored      to      any      particular      task.</p><p> \\n \\n  Diffusion      models      for      RL.\\nDiffusion      models      are      well-suited      for      specific      reinforcement      learning  \\n \\n  applications      due      to      their      ability      to      model      complex      and      multimodal      distributions      as      polices      (Janner  \\n \\n  et      al.,      2022;      Ajay      et      al.,      2023;      Wang      et      al.,      2022;      Hansen-Estruch      et      al.,      2023;      Du      et      al.,      2024;      Zhu  \\n \\n  et      al.,      2023).\\nFor      example,      Wang      et      al.\\n(2022)      use      conditional      diffusion      models      as      policies      and  \\n \\n  demonstrate      their      effectiveness      in      typical      offline      RL      benchmarks.\\nWhile      RL-based      fine-tuning      also  \\n \\n  aims      to      maximize      certain      reward      functions,      the      primary      focus      of      this      tutorial      covers      methods      that  \\n \\n  leverage      pre-trained      diffusion      models      for      this      purpose.</p><p> \\n \\n  RL      from      human      feedback      (RLHF)      for      language      models.\\nRLHF      is      widely      discussed      in      the  \\n \\n  context      of      language      models,      where      many      algorithms      share      similarities      with      those      used      in      diffusion  \\n \\n  models      (Ouyang      et      al.,      2022;      Bai      et      al.,      2022;      Casper      et      al.,      2023).\\nFor      instance,      both      domains  \\n \\n  commonly      employ      PPO      as   \\n \\na      standard      approach,      and      reward-weighted      training      (or      reward-weighted  \\n \\n  MLE)      often      serves      as   \\n \\na      basic      baseline      approach.\\nDespite      these      similarities,      nuanced      differences  \\n \\n  exist      mainly      because      diffusion      models      typically      operate      in   \\n \\na      continuous      input      space      rather      than  \\n \\n  a      discrete      one.\\nMoreover,      techniques      like      value-weighted      sampling      emerge      uniquely      within      the  \\n \\n  context      of      diffusion      models.</p><p>12      Summary</p><p> \\n \\n  In      this      article,      we      comprehensively      explain      how      fine-tuning      diffusion      models      to      maximize      down-  \\n \\n  stream      reward      functions      can      be      formalized      as   \\n \\na      reinforcement      learning      (RL)      problem      in      entropy-  \\n \\n  regularized      Markov      decision      processes      (MDPs).\\nBased      on      this      viewpoint,      we      elaborate      on      the  \\n \\n  application      of      various      RL      algorithms,      such      as      PPO      and      reward-weighted      MLE,      specifically      tailored  \\n \\n  for      fine-tuning      diffusion      models.\\nAdditionally,      we      categorize      different      scenarios      based      on      how  \\n \\n  reward      feedback      is      obtained.\\nWhile      this      categorization      is      not      always      explicitly      mentioned      in      many  \\n \\n  existing      works,      it      is      crucial      when      selecting      appropriate      algorithms.\\nFinally,      we      discuss      the      rela-  \\n \\n  tionships      with      several      related      topics,      including      classifier      guidance,      Gflownets,      path      integral      control  \\n \\n  theory,      and      sampling      from      unnormalized      distributions.</p><h9>9.2      Flow-Based      (Bridge-Based)      Diffusion      Models</h9><p>dels We      adopt      the      explanation      outlined      in      Liu      et      al.\\n(2022).\\nIn      this      framework,      we      begin      by      considering  \\n \\n  a      reference      SDE      (from      T\\\\\\\\\\\\\\'      to      0),      x;   \\n \\n=      zr_;      where      z;      is      defined      as      Brownian      motion:</p><p>dz,   \\n \\n=      dwt,      te      (0,      7).\\n(41)</p><p> \\n \\n  Subsequently,      we      consider      the      coupling      of      the      reference      SDE      conditioned      on      state      at      time   \\n \\n0      (i.e.,  \\n \\n  distribution      of      xo.7      conditional      on      x)      and      the      data      distribution      at      time      0,      denoting      the      resulting  \\n \\n  distribution      as      Q*°?,      Informally,      this      Q*°°?\\nis      characterized      as      follows.</p><p>Informal      characterization      of      Q*”.\\nWhen      discretizing      the      time      step,      by      denoting      the      distribution  \\n \\n  induced      by      the      reference      SDE      as      p\"(x7,--+   \\n \\n,      29),      we      define QF      (a7,      tae   \\n \\n,      20)   \\n \\n=      p(x,      ay   \\n \\n|      r9)pP\"°      (x9).</p><p>The      distribution      Q*”      above      is   \\n \\na      limiting      distribution      obtained      by      making      the      discretization      step small.</p><p>Training.\\nIn      bridge-based      diffusion      models,      our      objective      is      to      train   \\n \\na      new      SDE,      parametrized      by  \\n \\n  6      (e.g.,   \\n \\na      neural      network),      to      align      the      induced      distribution      P®      with      Q*°?.\\nSpecifically,      bridge-based training      aims      to      minimize aremin      KL(Q*>||P’).\\n(42)  \\n \\n  0  \\n \\n  While      the      method      to      minimize      the      above      is      currently      unclear,      we      can      derive   \\n \\na      trainable      objective  \\n \\n  through      algebraic      manipulation.\\nInitially,      we      observe      that      the      reference      SDE      (41),      conditioned      on  \\n \\n  state      xo      at      time   \\n \\n0      (zr   \\n \\n=      D),      is      expressed      as:</p><p>b\\n \\n \\n  b—      2;</p><p>t\\n \\n   €\\n \\n   [0,T];dz?\\n \\n \\n=      g?(z?,t)dt      +du,,  \\n \\n  g(z?,t)   \\n \\n=   \\n \\na       (43)</p><p>using      the      seminal      “Doob’s      h-transform”      (Rogers      and      Williams,      2000).\\nThis      SDE      is      commonly  \\n \\n  known      as      the      Brownian      bridge.\\nNow,      we      consider   \\n \\na      new      SDE      parameterized      by      0:</p><p>t\\n \\n   €\\n \\n   [0,7];      dz   \\n \\n=      s(z,t;      @)dt   \\n \\n+      dw.\\n(44)</p><p>Then,      we      can      demonstrate      that      minimizing      the      KL      loss      in      (42)      is      equivalent      to argmin      E,W      unil0,7],1~4\"°      cop?\\n[Ilg”      (2,      t)   \\n \\n_      s(21,      t;      0)      M3]      (45)</p><p>where      q;°      denotes      the      induced      distribution      at      time   \\n \\nt      following      SDE      (43)      conditioned      on      xo,      and  \\n \\n  s:      Xx      |0,T]   \\n \\n>   \\n \\n&      is      the      model      we      aim      to      optimize.</p><p> \\n \\n  Flow-based      diffusion      models.\\nThe      aforementioned      formulation      has      been      originally      proposed      as   \\n \\na       bridge-based      SDE      (Liu      et      al.,      2022)      because      the      conditional      SDE      is      often      referred      to      as   \\n \\na      Brownian  \\n \\n  bridge.\\nFlow-based      diffusion      models      proposed      in      Lipman      et      al.\\n(2023);      Tong      et      al.\\n(2023)      share   \\n \\na       close      relationship      with      bridge-based      diffusion      models.\\nThese      flow-based      models      use   \\n \\na      target      SDE (43)      and      an      SDE      with   \\n \\na      parameter      (44)      without      stochastic      terms      (1.e.,      no      dw,)      and      aim      to      minimize  \\n \\n  the      loss      function      (45).</p><p> \\n \\n  Comparison      with      score-based      diffusion      models.\\nBoth      score-based      diffusion      models      and      bridge-  \\n \\n  based      diffusion      models      aim      to      minimize      KL      divergences.\\nDespite      its      similarity,      in      contrast      to  \\n \\n  score-based      diffusion      models,      bridge-based      diffusion      models      target      Q*°>      rather      than      the      time-  \\n \\n  reversal      SDE      Q\"™®.\\nThe      distribution      Q*”      is      often      considered      preferable      because      it      can      impose      an  \\n \\n  SDE      that      navigates      efficiently      from   \\n \\n0      to      7’      (e.g.,   \\n \\na      Brownian      bridge      with      minimal      noise).\\nThis      can  \\n \\n  expedite      the      learning      process      since      we      lack      direct      control      over      the      time-reversal      SDE      in      score-based  \\n \\n  diffusion      models.</p><h10>9.3.      Connection      with      RL-Based      Fine-Tuning</h10><p>ning  \\n \\n  RL-based      fine-tuning      shares   \\n \\na      close      relationship      with      flow-based      training.\\nTo      illustrate      this,      consider  \\n \\n  the      reference      SDE      (from      7’      to      0)      following      the      pre-trained      diffusion      model.\\nThen,      akin      to      flow-  \\n \\n  based      training,      we      consider      the      coupling      of      the      reference      SDE      conditioned      on      state      at      time   \\n \\n0       (i.e.,      distribution      over      xo-r      conditioned      on      x9)      and      the      target      distribution      at      time   \\n \\n0      (i.e.,      p,(-)   \\n \\n«       exp(r(-)/a)p?\\n\"°(-)),      denoting      the      induced      distribution      as      Q*™.</p><p>Informal      characterization      of      Q™°*.\\nInformally,      Q*\"°      is      introduced      as      follows.\\nWhen      discretizing  \\n \\n  the      time      step,      by      denoting      the      distribution      induced      by      the      reference      SDE      as      p™\\\\\\\\\\\\\\'(xp,-+-   \\n \\n,      9),      we define Qi\"      (xr,   \\n \\na   \\n \\n,      0)   \\n \\n=      p(x,      nr      Ly   \\n \\n|      L0)Pr      (Xo).</p><p>The      distribution      Q*°      above      is   \\n \\na      limiting      distribution      obtained      by      making      the      discretization      step small.</p><p>Training.\\nNow,      we      introduce   \\n \\na      new      SDE      parameterized      by   \\n \\n@      such      as      an      neural      network      to      align  \\n \\n  the      induced      distribution      P’      with      Q®\"*.\\nActually,      in      the      continuous      formulation,      the      RL-problem      (18) in      Section      4.2      is      equal      to      solving argmin      KL(P?||Q\"™).\\n \\n \\n  0 This      formalization      has      been      presented      in      Uehara      et      al.\\n(2024).\\nIntuitively,      as      we      see      in      Theorem      3,  \\n \\n  this      is      expected      because      we      see      that      the      posterior      distribution      induced      by      pre-trained      models:</p><p>ref   \\n \\n(      PD      (ar,-++      21   \\n \\n|      to)      (=      vey      (er|er_-1)p      Pp      (ar-1   \\n \\n|      er_2)      +++      pp\"      (x1   \\n \\n|      20), remains      preserved      after      fine-tuning      based      on      the      RL-formulation      (18).</p><p> \\n \\n  Comparison      with      flow-based      training.\\nIn      contrast      to      flow-based      training,      RL-based      fine-tuning  \\n \\n  minimizes      the      inverse      KL      divergence      rather      than      the      KL      divergence.\\nIntuitively,      this      is      because,  \\n \\n  unlike      Qt™*      or      Q*>   \\n \\n,      we      cannot      sample      from      Q*”      (recall      that   \\n \\na      marginal      distribution      at      time   \\n \\n0      in  \\n \\n  Qi,      i.e.,      p,,      is      unnormalized);      on      the      other      hand,      the      marginal      distributions      at      time   \\n \\n0      in      both      Q\\\\\\\\\\\\\\'™*  \\n \\n  and      Q*°°>      are      data      distributions.</p><p> \\n \\n  Remark   \\n \\n7      (Extension      to      f-divergence).\\nEach      of      these      methods      can      be      technically      extended      by  \\n \\n  incorporating      more      general      divergences      beyond      KL      divergence.\\nFor      instance,      in      the      context      of  \\n \\n  fine-tuning,      please      see      Tang      (2024).</p><p>10      Connection      with      Sampling      from      Unnormalized      Distributions</p><p> \\n \\n  Numerous      studies      delve      into      sampling      from      an      unnormalized      distribution      proportional      to      exp(r()),  \\n \\n  commonly      known      as      the      Gibbs      distribution,      extensively      discussed      in      the      computational      statistics  \\n \\n  and      statistical      physics      community      (Robert,      2014).\\nThis      issue      is      pertinent      to      our      work,      as      during  \\n \\n  fine-tuning,      the      target      distribution      is      also      formulated      in      the      Gibbs      distribution      form      in      (10),      which      is  \\n \\n  proportional      to      exp(r(a))p?\\n\"*      (x).</p><p> \\n \\n  The      literature      employs      two      main      approaches      to      tackle      this      challenge:      Markov      Chain      Monte  \\n \\n  Carlo      (MCMC)      and      RL-based      methods.\\nIn      particular,      the      RL-based      diffusion      model      fine-tuning  \\n \\n  we      have      discussed      so      far      is      closely      related      to      the      latter      approach.\\nIn      this      section,      we      explain      and  \\n \\n  compare      these      two      approaches.</p><h10>10.1      Markov      Chain      Monte      Carlo      (MCMC)</h10><p>CMC)  \\n \\n  In      MCMC,   \\n \\na      Markov      chain      is      constructed      to      approximate   \\n \\na      target      distribution      such      that      the      equilib-  \\n \\n  rium      distribution      of      the      Markov      chain      converges      to      the      target      distribution.\\nHowever,      dealing      with  \\n \\n  high-dimensional      domain      spaces      is      challenging      for      MCMC.\\n \\n \\nA      common      strategy      involves      leveraging  \\n \\n  gradient      information,      such      as      the      Metropolis-adjusted      Langevin      algorithm      (MALA)      (Besag,      1994)  \\n \\n  or      Hamiltonian      Monte      Carlo      (HMC)      (Neal      et      al.,      2011;      Girolami      and      Calderhead,      2011).\\nMALA  \\n \\n  shares      similarities      with      classifier      guidance      and      value-weighted      sampling,      as      both      methods      depend  \\n \\n  on      the      first-order      information      from      reward      (or      value)      functions.</p><p> \\n \\n  Can      we      use      MCMC      for      fine-tuning      diffusion      models?\\nIn      the      context      of      fine-tuning      dif-  \\n \\n  fusion      models,      while      it      may      seem      intuitive      to      sample      from      the      target      distribution      (10)      (i.e.,  \\n \\n  Dy   \\n \\n&      exp(r(x))pP\"*(x))      using      MCMC,      it’s      not      straightforward      for      the      following      reasons:</p><p>¢\\n \\n   We      lack      of      an      analytical      form      of      p?\\n\"*(-).<ul><li>*\\n \\n   Additionally,      even      if      we      can      estimate      p?\\n’°(-)      in      an      unbiased      manner,      as      in      Chen      et      al.\\n(2018),  \\n \\n  the      mixing      time      for      obtaining   \\n \\na      single      sample      in      MCMC      might      be      lengthy.</li></ul></p><p> \\n \\n  Therefore,      we      explore      creating   \\n \\na      generative      model      (i.e.,      diffusion      model)      for      this      task      using      RL-  \\n \\n  based      fine-tuning      so      that      we      can      easily      sample      from      p,(-)      during      inference      (i.e.,      simulating      the  \\n \\n  time-reversal      SDE      (2)      with      policies      from   \\n \\n¢   \\n \\n=      T\\\\\\\\\\\\\\'+   \\n \\n1      to   \\n \\nt   \\n \\n=      1),      without      relying      on      MCMC.\\nThis  \\n \\n  addresses      the      concern      of      MCMC      by      avoiding      the      need      to      estimate      p?\\n\"*(-)      and      minimizing      inference  \\n \\n  time,      albeit      at      the      expense      of      increased      training      time.</p><p>10.2}      RL-Based      Approaches  \\n \\n  While      MCMC      has      historically      been      widely      used      for      this      purpose,      recent      works      have      proposed      an  \\n \\n  RL-based      approach      or      its      variant      (e.g.,      Bernton      et      al.      (2019);      Heng      et      al.\\n(2020);      Huang      et      al.\\n(2023);</p><p> \\n \\n  Vargas      et      al.\\n(2023)).\\nFor      example,   \\n \\na      seminal      study      (Zhang      and      Chen,      2021)      introduced   \\n \\na      method  \\n \\n  very      similar      to      reward      backpropagation.\\nIn      their      work,      they      initially      define      Brownian      motion      as   \\n \\na       reference      SDE      (from      7’      to      0),      similar      to      the      pre-trained      diffusion      models      in      our      context.\\nThen,      by  \\n \\n  appropriately      setting      rewards,      Zhang      and      Chen      (2021)      aims      to      learn   \\n \\na      new      SDE      (from      T\\\\\\\\\\\\\\'      to      0)      such  \\n \\n  that      we      can      sample      from      the      target      distribution      at      the      endpoint      0.</p><p> \\n \\n  These      approaches      offer      advantages      over      MCMC      due      to      their      ability      to      minimize      inference      time  \\n \\n  (amortization)      by      constructing   \\n \\na      generative      model      comprising      policies      in      the      training      time      and      solely  \\n \\n  executing      learned      policies      during      inference.\\nUnlike      MCMC,      these      methods      significantly      reduce      the  \\n \\n  inference      cost      as      we      don’t      need      to      be      concerned      about      the      mixing      time.\\nAdditionally,      RL-based  \\n \\n  approaches      have      the      potential      to      effectively      handle      high-dimensional,      complex      (i.e.,      multi-modal)  \\n \\n  distributions      by      harnessing      the      considerable      expressive      power      arising      from      latent      states.</p><p> \\n \\n  Despite      these      advantages,      it      is      often      unclear      how      to      select   \\n \\na      reference      SDE      in      these      works.\\nIn  \\n \\n  contrast,      in      RL-based      fine-tuning      of      diffusion      models,      we      directly      leverage      pre-trained      diffusion  \\n \\n  models      as      the      reference      SDEs,      i.e.,      as      our      prior      knowledge.</p><p> \\n \\n  Key      message.\\nFinally,      it      is      worth      noting      that      we      can      technically      utilize      any      off-the-shelf      RL      algo-  \\n \\n  rithms      for      training.\\nFor      example,      Zhang      and      Chen      (2021)      employs      reward      backpropagation,      while  \\n \\n  works      in      Gflownets      typically      leverage      PCL,      as      we      mention      in      Section      6.3.\\nHowever,      additionally,      it  \\n \\n  is      possible      to      use      PPO      or      reward-weighted      MLE      mentioned      in      Section      4.2.</p><p>11      Closely      Related      Directions</p><p>Lastly,      we      highlight      several      closely      related      directions      we      have      not      yet      discussed.</p><p> \\n \\n  Aligning      text-to-image      models.\\nWe      recognize      that      current      efforts      in      fine-tuning      diffusion      models  \\n \\n  primarily      revolve      around      aligning      text-to-image      models      using      human      feedback      (Lee      et      al.,      2023;  \\n \\n  Wallace      et      al.,      2023;      Wu      et      al.,      2023;      Yang      et      al.,      2023).\\nIn      this      paper,      our      goal      is      to      consider   \\n \\na      more  \\n \\n  general      and      fundamental      formulation      that      is      not      tailored      to      any      particular      task.</p><p> \\n \\n  Diffusion      models      for      RL.\\nDiffusion      models      are      well-suited      for      specific      reinforcement      learning  \\n \\n  applications      due      to      their      ability      to      model      complex      and      multimodal      distributions      as      polices      (Janner  \\n \\n  et      al.,      2022;      Ajay      et      al.,      2023;      Wang      et      al.,      2022;      Hansen-Estruch      et      al.,      2023;      Du      et      al.,      2024;      Zhu  \\n \\n  et      al.,      2023).\\nFor      example,      Wang      et      al.\\n(2022)      use      conditional      diffusion      models      as      policies      and  \\n \\n  demonstrate      their      effectiveness      in      typical      offline      RL      benchmarks.\\nWhile      RL-based      fine-tuning      also  \\n \\n  aims      to      maximize      certain      reward      functions,      the      primary      focus      of      this      tutorial      covers      methods      that  \\n \\n  leverage      pre-trained      diffusion      models      for      this      purpose.</p><p> \\n \\n  RL      from      human      feedback      (RLHF)      for      language      models.\\nRLHF      is      widely      discussed      in      the  \\n \\n  context      of      language      models,      where      many      algorithms      share      similarities      with      those      used      in      diffusion  \\n \\n  models      (Ouyang      et      al.,      2022;      Bai      et      al.,      2022;      Casper      et      al.,      2023).\\nFor      instance,      both      domains  \\n \\n  commonly      employ      PPO      as   \\n \\na      standard      approach,      and      reward-weighted      training      (or      reward-weighted  \\n \\n  MLE)      often      serves      as   \\n \\na      basic      baseline      approach.\\nDespite      these      similarities,      nuanced      differences  \\n \\n  exist      mainly      because      diffusion      models      typically      operate      in   \\n \\na      continuous      input      space      rather      than  \\n \\n  a      discrete      one.\\nMoreover,      techniques      like      value-weighted      sampling      emerge      uniquely      within      the  \\n \\n  context      of      diffusion      models.</p><p>12      Summary</p><p> \\n \\n  In      this      article,      we      comprehensively      explain      how      fine-tuning      diffusion      models      to      maximize      down-  \\n \\n  stream      reward      functions      can      be      formalized      as   \\n \\na      reinforcement      learning      (RL)      problem      in      entropy-  \\n \\n  regularized      Markov      decision      processes      (MDPs).\\nBased      on      this      viewpoint,      we      elaborate      on      the  \\n \\n  application      of      various      RL      algorithms,      such      as      PPO      and      reward-weighted      MLE,      specifically      tailored  \\n \\n  for      fine-tuning      diffusion      models.\\nAdditionally,      we      categorize      different      scenarios      based      on      how  \\n \\n  reward      feedback      is      obtained.\\nWhile      this      categorization      is      not      always      explicitly      mentioned      in      many  \\n \\n  existing      works,      it      is      crucial      when      selecting      appropriate      algorithms.\\nFinally,      we      discuss      the      rela-  \\n \\n  tionships      with      several      related      topics,      including      classifier      guidance,      Gflownets,      path      integral      control  \\n \\n  theory,      and      sampling      from      unnormalized      distributions.</p><h10>9.3.      Connection      with      RL-Based      Fine-Tuning</h10><p>ning  \\n \\n  RL-based      fine-tuning      shares   \\n \\na      close      relationship      with      flow-based      training.\\nTo      illustrate      this,      consider  \\n \\n  the      reference      SDE      (from      7’      to      0)      following      the      pre-trained      diffusion      model.\\nThen,      akin      to      flow-  \\n \\n  based      training,      we      consider      the      coupling      of      the      reference      SDE      conditioned      on      state      at      time   \\n \\n0       (i.e.,      distribution      over      xo-r      conditioned      on      x9)      and      the      target      distribution      at      time   \\n \\n0      (i.e.,      p,(-)   \\n \\n«       exp(r(-)/a)p?\\n\"°(-)),      denoting      the      induced      distribution      as      Q*™.</p><p>Informal      characterization      of      Q™°*.\\nInformally,      Q*\"°      is      introduced      as      follows.\\nWhen      discretizing  \\n \\n  the      time      step,      by      denoting      the      distribution      induced      by      the      reference      SDE      as      p™\\\\\\\\\\\\\\'(xp,-+-   \\n \\n,      9),      we define Qi\"      (xr,   \\n \\na   \\n \\n,      0)   \\n \\n=      p(x,      nr      Ly   \\n \\n|      L0)Pr      (Xo).</p><p>The      distribution      Q*°      above      is   \\n \\na      limiting      distribution      obtained      by      making      the      discretization      step small.</p><p>Training.\\nNow,      we      introduce   \\n \\na      new      SDE      parameterized      by   \\n \\n@      such      as      an      neural      network      to      align  \\n \\n  the      induced      distribution      P’      with      Q®\"*.\\nActually,      in      the      continuous      formulation,      the      RL-problem      (18) in      Section      4.2      is      equal      to      solving argmin      KL(P?||Q\"™).\\n \\n \\n  0 This      formalization      has      been      presented      in      Uehara      et      al.\\n(2024).\\nIntuitively,      as      we      see      in      Theorem      3,  \\n \\n  this      is      expected      because      we      see      that      the      posterior      distribution      induced      by      pre-trained      models:</p><p>ref   \\n \\n(      PD      (ar,-++      21   \\n \\n|      to)      (=      vey      (er|er_-1)p      Pp      (ar-1   \\n \\n|      er_2)      +++      pp\"      (x1   \\n \\n|      20), remains      preserved      after      fine-tuning      based      on      the      RL-formulation      (18).</p><p> \\n \\n  Comparison      with      flow-based      training.\\nIn      contrast      to      flow-based      training,      RL-based      fine-tuning  \\n \\n  minimizes      the      inverse      KL      divergence      rather      than      the      KL      divergence.\\nIntuitively,      this      is      because,  \\n \\n  unlike      Qt™*      or      Q*>   \\n \\n,      we      cannot      sample      from      Q*”      (recall      that   \\n \\na      marginal      distribution      at      time   \\n \\n0      in  \\n \\n  Qi,      i.e.,      p,,      is      unnormalized);      on      the      other      hand,      the      marginal      distributions      at      time   \\n \\n0      in      both      Q\\\\\\\\\\\\\\'™*  \\n \\n  and      Q*°°>      are      data      distributions.</p><p> \\n \\n  Remark   \\n \\n7      (Extension      to      f-divergence).\\nEach      of      these      methods      can      be      technically      extended      by  \\n \\n  incorporating      more      general      divergences      beyond      KL      divergence.\\nFor      instance,      in      the      context      of  \\n \\n  fine-tuning,      please      see      Tang      (2024).</p><p>10      Connection      with      Sampling      from      Unnormalized      Distributions</p><p> \\n \\n  Numerous      studies      delve      into      sampling      from      an      unnormalized      distribution      proportional      to      exp(r()),  \\n \\n  commonly      known      as      the      Gibbs      distribution,      extensively      discussed      in      the      computational      statistics  \\n \\n  and      statistical      physics      community      (Robert,      2014).\\nThis      issue      is      pertinent      to      our      work,      as      during  \\n \\n  fine-tuning,      the      target      distribution      is      also      formulated      in      the      Gibbs      distribution      form      in      (10),      which      is  \\n \\n  proportional      to      exp(r(a))p?\\n\"*      (x).</p><p> \\n \\n  The      literature      employs      two      main      approaches      to      tackle      this      challenge:      Markov      Chain      Monte  \\n \\n  Carlo      (MCMC)      and      RL-based      methods.\\nIn      particular,      the      RL-based      diffusion      model      fine-tuning  \\n \\n  we      have      discussed      so      far      is      closely      related      to      the      latter      approach.\\nIn      this      section,      we      explain      and  \\n \\n  compare      these      two      approaches.</p><h10>10.1      Markov      Chain      Monte      Carlo      (MCMC)</h10><p>CMC)  \\n \\n  In      MCMC,   \\n \\na      Markov      chain      is      constructed      to      approximate   \\n \\na      target      distribution      such      that      the      equilib-  \\n \\n  rium      distribution      of      the      Markov      chain      converges      to      the      target      distribution.\\nHowever,      dealing      with  \\n \\n  high-dimensional      domain      spaces      is      challenging      for      MCMC.\\n \\n \\nA      common      strategy      involves      leveraging  \\n \\n  gradient      information,      such      as      the      Metropolis-adjusted      Langevin      algorithm      (MALA)      (Besag,      1994)  \\n \\n  or      Hamiltonian      Monte      Carlo      (HMC)      (Neal      et      al.,      2011;      Girolami      and      Calderhead,      2011).\\nMALA  \\n \\n  shares      similarities      with      classifier      guidance      and      value-weighted      sampling,      as      both      methods      depend  \\n \\n  on      the      first-order      information      from      reward      (or      value)      functions.</p><p> \\n \\n  Can      we      use      MCMC      for      fine-tuning      diffusion      models?\\nIn      the      context      of      fine-tuning      dif-  \\n \\n  fusion      models,      while      it      may      seem      intuitive      to      sample      from      the      target      distribution      (10)      (i.e.,  \\n \\n  Dy   \\n \\n&      exp(r(x))pP\"*(x))      using      MCMC,      it’s      not      straightforward      for      the      following      reasons:</p><p>¢\\n \\n   We      lack      of      an      analytical      form      of      p?\\n\"*(-).<ul><li>*\\n \\n   Additionally,      even      if      we      can      estimate      p?\\n’°(-)      in      an      unbiased      manner,      as      in      Chen      et      al.\\n(2018),  \\n \\n  the      mixing      time      for      obtaining   \\n \\na      single      sample      in      MCMC      might      be      lengthy.</li></ul></p><p> \\n \\n  Therefore,      we      explore      creating   \\n \\na      generative      model      (i.e.,      diffusion      model)      for      this      task      using      RL-  \\n \\n  based      fine-tuning      so      that      we      can      easily      sample      from      p,(-)      during      inference      (i.e.,      simulating      the  \\n \\n  time-reversal      SDE      (2)      with      policies      from   \\n \\n¢   \\n \\n=      T\\\\\\\\\\\\\\'+   \\n \\n1      to   \\n \\nt   \\n \\n=      1),      without      relying      on      MCMC.\\nThis  \\n \\n  addresses      the      concern      of      MCMC      by      avoiding      the      need      to      estimate      p?\\n\"*(-)      and      minimizing      inference  \\n \\n  time,      albeit      at      the      expense      of      increased      training      time.</p><p>10.2}      RL-Based      Approaches  \\n \\n  While      MCMC      has      historically      been      widely      used      for      this      purpose,      recent      works      have      proposed      an  \\n \\n  RL-based      approach      or      its      variant      (e.g.,      Bernton      et      al.      (2019);      Heng      et      al.\\n(2020);      Huang      et      al.\\n(2023);</p><p> \\n \\n  Vargas      et      al.\\n(2023)).\\nFor      example,   \\n \\na      seminal      study      (Zhang      and      Chen,      2021)      introduced   \\n \\na      method  \\n \\n  very      similar      to      reward      backpropagation.\\nIn      their      work,      they      initially      define      Brownian      motion      as   \\n \\na       reference      SDE      (from      7’      to      0),      similar      to      the      pre-trained      diffusion      models      in      our      context.\\nThen,      by  \\n \\n  appropriately      setting      rewards,      Zhang      and      Chen      (2021)      aims      to      learn   \\n \\na      new      SDE      (from      T\\\\\\\\\\\\\\'      to      0)      such  \\n \\n  that      we      can      sample      from      the      target      distribution      at      the      endpoint      0.</p><p> \\n \\n  These      approaches      offer      advantages      over      MCMC      due      to      their      ability      to      minimize      inference      time  \\n \\n  (amortization)      by      constructing   \\n \\na      generative      model      comprising      policies      in      the      training      time      and      solely  \\n \\n  executing      learned      policies      during      inference.\\nUnlike      MCMC,      these      methods      significantly      reduce      the  \\n \\n  inference      cost      as      we      don’t      need      to      be      concerned      about      the      mixing      time.\\nAdditionally,      RL-based  \\n \\n  approaches      have      the      potential      to      effectively      handle      high-dimensional,      complex      (i.e.,      multi-modal)  \\n \\n  distributions      by      harnessing      the      considerable      expressive      power      arising      from      latent      states.</p><p> \\n \\n  Despite      these      advantages,      it      is      often      unclear      how      to      select   \\n \\na      reference      SDE      in      these      works.\\nIn  \\n \\n  contrast,      in      RL-based      fine-tuning      of      diffusion      models,      we      directly      leverage      pre-trained      diffusion  \\n \\n  models      as      the      reference      SDEs,      i.e.,      as      our      prior      knowledge.</p><p> \\n \\n  Key      message.\\nFinally,      it      is      worth      noting      that      we      can      technically      utilize      any      off-the-shelf      RL      algo-  \\n \\n  rithms      for      training.\\nFor      example,      Zhang      and      Chen      (2021)      employs      reward      backpropagation,      while  \\n \\n  works      in      Gflownets      typically      leverage      PCL,      as      we      mention      in      Section      6.3.\\nHowever,      additionally,      it  \\n \\n  is      possible      to      use      PPO      or      reward-weighted      MLE      mentioned      in      Section      4.2.</p><p>11      Closely      Related      Directions</p><p>Lastly,      we      highlight      several      closely      related      directions      we      have      not      yet      discussed.</p><p> \\n \\n  Aligning      text-to-image      models.\\nWe      recognize      that      current      efforts      in      fine-tuning      diffusion      models  \\n \\n  primarily      revolve      around      aligning      text-to-image      models      using      human      feedback      (Lee      et      al.,      2023;  \\n \\n  Wallace      et      al.,      2023;      Wu      et      al.,      2023;      Yang      et      al.,      2023).\\nIn      this      paper,      our      goal      is      to      consider   \\n \\na      more  \\n \\n  general      and      fundamental      formulation      that      is      not      tailored      to      any      particular      task.</p><p> \\n \\n  Diffusion      models      for      RL.\\nDiffusion      models      are      well-suited      for      specific      reinforcement      learning  \\n \\n  applications      due      to      their      ability      to      model      complex      and      multimodal      distributions      as      polices      (Janner  \\n \\n  et      al.,      2022;      Ajay      et      al.,      2023;      Wang      et      al.,      2022;      Hansen-Estruch      et      al.,      2023;      Du      et      al.,      2024;      Zhu  \\n \\n  et      al.,      2023).\\nFor      example,      Wang      et      al.\\n(2022)      use      conditional      diffusion      models      as      policies      and  \\n \\n  demonstrate      their      effectiveness      in      typical      offline      RL      benchmarks.\\nWhile      RL-based      fine-tuning      also  \\n \\n  aims      to      maximize      certain      reward      functions,      the      primary      focus      of      this      tutorial      covers      methods      that  \\n \\n  leverage      pre-trained      diffusion      models      for      this      purpose.</p><p> \\n \\n  RL      from      human      feedback      (RLHF)      for      language      models.\\nRLHF      is      widely      discussed      in      the  \\n \\n  context      of      language      models,      where      many      algorithms      share      similarities      with      those      used      in      diffusion  \\n \\n  models      (Ouyang      et      al.,      2022;      Bai      et      al.,      2022;      Casper      et      al.,      2023).\\nFor      instance,      both      domains  \\n \\n  commonly      employ      PPO      as   \\n \\na      standard      approach,      and      reward-weighted      training      (or      reward-weighted  \\n \\n  MLE)      often      serves      as   \\n \\na      basic      baseline      approach.\\nDespite      these      similarities,      nuanced      differences  \\n \\n  exist      mainly      because      diffusion      models      typically      operate      in   \\n \\na      continuous      input      space      rather      than  \\n \\n  a      discrete      one.\\nMoreover,      techniques      like      value-weighted      sampling      emerge      uniquely      within      the  \\n \\n  context      of      diffusion      models.</p><p>12      Summary</p><p> \\n \\n  In      this      article,      we      comprehensively      explain      how      fine-tuning      diffusion      models      to      maximize      down-  \\n \\n  stream      reward      functions      can      be      formalized      as   \\n \\na      reinforcement      learning      (RL)      problem      in      entropy-  \\n \\n  regularized      Markov      decision      processes      (MDPs).\\nBased      on      this      viewpoint,      we      elaborate      on      the  \\n \\n  application      of      various      RL      algorithms,      such      as      PPO      and      reward-weighted      MLE,      specifically      tailored  \\n \\n  for      fine-tuning      diffusion      models.\\nAdditionally,      we      categorize      different      scenarios      based      on      how  \\n \\n  reward      feedback      is      obtained.\\nWhile      this      categorization      is      not      always      explicitly      mentioned      in      many  \\n \\n  existing      works,      it      is      crucial      when      selecting      appropriate      algorithms.\\nFinally,      we      discuss      the      rela-  \\n \\n  tionships      with      several      related      topics,      including      classifier      guidance,      Gflownets,      path      integral      control  \\n \\n  theory,      and      sampling      from      unnormalized      distributions.</p><h4>References</h4><p>Agarwal,      A.,      N.      Jiang,      S.      M.      Kakade,      and      W.      Sun      (2019).\\nReinforcement      learning:      Theory      and  \\n \\n  algorithms.\\nCS      Dept.,      UW      Seattle,      Seattle,      WA,      USA,      Tech.\\nRep,      10-4.</p><p>Agarwal,      V.      and      D.      R.      Kelley      (2022).\\nThe      genetic      and      biochemical      determinants      of      mrna      degradation  \\n \\n  rates      in      mammals.\\nGenome      biology      23(1),      245.</p><p> \\n \\n  Ajay,      A.,      Y.      Du,      A.      Gupta,      J.      B.      Tenenbaum,      T.      S.      Jaakkola,      and      P.      Agrawal      (2023).\\nIs      conditional  \\n \\n  generative      modeling      all      you      need      for      decision      making?\\nIn      The      Eleventh      International      Conference  \\n \\n  on      Learning      Representations.</p><p>Albergo,      M.      S.      and      E.      Vanden-Eijnden      (2022).\\nBuilding      normalizing      flows      with      stochastic      inter-  \\n \\n  polants.\\narXiv      preprint      arXiv:2209.15571.</p><p>Anderson,      B.      D.      (1982).\\nReverse-time      diffusion      equation      models.\\nStochastic      Processes      and      their  \\n \\n  Applications      12(3),      313-326.</p><p> \\n \\n  Austin,      J.,      D.      D.      Johnson,      J.      Ho,      D.      Tarlow,      and      R.      Van      Den      Berg      (2021).\\nStructured      denoising  \\n \\n  diffusion      models      in      discrete      state-spaces.\\nAdvances      in      Neural      Information      Processing      Systems      34,  \\n \\n  17981-17993.</p><p>Avdeyey,      P.,      C.      Shi,      Y.      Tan,      K.      Dudnyk,      and      J.      Zhou      (2023).\\nDirichlet      diffusion      score      model      for  \\n \\n  biological      sequence      generation.\\narXiv      preprint      arXiv:2305.\\n10699.</p><p>Bai,      Y.,      S.      Kadavath,      S.      Kundu,      A.      Askell,      J.      Kernion,      A.      Jones,      A.      Chen,      A.      Goldie,      A.      Mirhoseini, C.      McKinnon,      et      al.\\n(2022).\\nConstitutional      ai:      Harmlessness      from      ai      feedback.\\narXiv      preprint  \\n \\n  arXiv:2212.08073.</p><p> \\n \\n  Bansal,      A.,      H.-M.\\nChu,      A.      Schwarzschild,      S.      Sengupta,      M.      Goldblum,      J.      Geiping,      and      T.      Goldstein  \\n \\n  (2023).\\nUniversal      guidance      for      diffusion      models.\\nIn      Proceedings      of      the      IEEE/CVF      Conference  \\n \\n  on      Computer      Vision      and      Pattern      Recognition,      pp.\\n843-852.</p><p>Bengio,      Y.,      S.      Lahlou,      T.      Deleu,      E.      J.      Hu,      M.      Tiwari,      and      E.      Bengio      (2023).\\nGflownet      foundations.\\n \\n \\n  Journal      of      Machine      Learning      Research      24(210),      1-55.</p><p> \\n \\n  Benton,      J.,      Y.      Shi,      V.      De      Bortoli,      G.      Deligiannidis,      and      A.      Doucet      (2024).\\nFrom      denoising      diffusions  \\n \\n  to      denoising      markov      models.\\nJournal      of      the      Royal      Statistical      Society      Series      B:      Statistical  \\n \\n  Methodology      86(2),      286-301.</p><p>Bernton,      E.,      J.      Heng,      A.      Doucet,      and      P.      E.      Jacob      (2019).\\nSchr\\\\\\\\\\\\\\\\””      odinger      bridge      samplers.\\narXiv  \\n \\n  preprint      arXiv:      1912.13170.</p><p>Besag,      J.\\n(1994).\\nComments      on      “representations      of      knowledge      in      complex      systems”      by      u.      grenander  \\n \\n  and      mi      miller.\\nJ.      Roy.\\nStatist.\\nSoc.      Ser.\\n \\n \\nB      56(591-592),      4.</p><p>Black,      K.,      M.      Janner,      Y.      Du,      I.      Kostrikov,      and      S.      Levine      (2023).\\nTraining      diffusion      models      with  \\n \\n  reinforcement      learning.\\narXiv      preprint      arXiv:2305.13301.</p><p> \\n \\n  Campbell,      A.,      J.      Benton,      V.      De      Bortoli,      T.      Rainforth,      G.      Deligiannidis,      and      A.      Doucet      (2022).\\n \\n \\nA       continuous      time      framework      for      discrete      denoising      models.\\nAdvances      in      Neural      Information  \\n \\n  Processing      Systems      35,      28266-28279.</p><p> \\n \\n  Campbell,      A.,      J.      Yim,      R.      Barzilay,      T.      Rainforth,      and      T.      Jaakkola      (2024).\\nGenerative      flows      on  \\n \\n  discrete      state-spaces:      Enabling      multimodal      flows      with      applications      to      protein      co-design.\\narXiv  \\n \\n  preprint      arXiv:2402.04997.</p><p>Cao,      H.,      C.      Tan,      Z.      Gao,      Y.      Xu,      G.      Chen,      P.-A.\\nHeng,      and      S.      Z.      Li      (2024).\\n \\n \\nA      survey      on      generative  \\n \\n  diffusion      models.\\n[EEE      Transactions      on      Knowledge      and      Data      Engineering.</p><p>Casper,      S.,      X.      Davies,      C.      Shi,      T.      K.      Gilbert,      J.      Scheurer,      J.      Rando,      R.      Freedman,      T.      Korbak, D.      Lindner,   \\n \\nP P.      Freire,      et      al.\\n(2023).\\nOpen      problems      and      fundamental      limitations      of      reinforcement  \\n \\n  learning      from      human      feedback.\\narXiv      preprint      arXiv:2307.15217.</p><p>Castillo-Hair,      S.      M.      and      G.      Seelig      (2021).\\nMachine      learning      for      designing      next-generation      mrna  \\n \\n  therapeutics.\\nAccounts      of      Chemical      Research      55(1),      24-34.</p><p>Chang,      J.      D.,      M.      Uehara,      D.      Sreenivas,      R.      Kidambi,      and      W.      Sun      (2021).\\nMitigating      covariate      shift  \\n \\n  in      imitation      learning      via      offline      data      without      great      coverage.\\narXiv      preprint      arXiv:2106.03207.</p><p>Chen,      M.,      S.      Mei,      J.      Fan,      and      M.      Wang      (2024).\\nAn      overview      of      diffusion      models:      Applications,  \\n \\n  guided      generation,      statistical      rates      and      optimization.\\narXiv      preprint      arXiv:2404.07771.</p><p>Chen,      R.      T.,      Y.      Rubanova,      J.      Bettencourt,      and      D.      K.      Duvenaud      (2018).\\nNeural      ordinary      differential  \\n \\n  equations.\\nAdvances      in      neural      information      processing      systems      31.</p><p>Chung,      H.,      J.      Kim,      M.      T.      Mccann,      M.      L.      Klasky,      and      J.      C.      Ye      (2022).\\nDiffusion      posterior      sampling  \\n \\n  for      general      noisy      inverse      problems.\\narXiv      preprint      arXiv:2209.\\n14687.</p><p> \\n \\n  Chung,      H.,      B.      Sim,      D.      Ryu,      and      J.      C.      Ye      (2022).\\nImproving      diffusion      models      for      inverse      problems  \\n \\n  using      manifold      constraints.\\nAdvances      in      Neural      Information      Processing      Systems      35,      25683-—  \\n \\n  25696.</p><p>Clark,      K.,      P.      Vicol,      K.      Swersky,      and      D.      J.      Fleet      (2023).\\nDirectly      fine-tuning      diffusion      models      on  \\n \\n  differentiable      rewards.\\narXiv      preprint      arXiv:2309.\\n17400.</p><p> \\n \\n  De      Bortoli,      V.,      E.      Mathieu,      M.      Hutchinson,      J.      Thornton,      Y.      W.      Teh,      and      A.      Doucet      (2022).\\nRieman-  \\n \\n  nian      score-based      generative      modelling.\\nAdvances      in      Neural      Information      Processing      Systems      35,  \\n \\n  2406-2422.</p><p>Deleu,      T.,      P.      Nouri,      N.      Malkin,      D.      Precup,      and      Y.      Bengio      (2024).\\nDiscrete      probabilistic      inference      as  \\n \\n  control      in      multi-path      environments.\\narXiv      preprint      arXiv:2402.10309.</p><p>Dhariwal,      P.      and      A.      Nichol      (2021).\\nDiffusion      models      beat      gans      on      image      synthesis.\\nAdvances      in  \\n \\n  neural      information      processing      systems      34,      8780-8794.</p><p> \\n \\n  Du,      Y.,      S.      Yang,      B.      Dai,      H.      Dai,      O.      Nachum,      J.      Tenenbaum,      D.      Schuurmans,      and      P.      Abbeel      (2024).\\n \\n \\n  Learning      universal      policies      via      text-guided      video      generation.\\nAdvances      in      Neural      Information  \\n \\n  Processing      Systems      36.</p><p>Fan,      Y.,      O.      Watkins,      Y.      Du,      H.      Liu,      M.      Ryu,      C.      Boutilier,      P.      Abbeel,      M.      Ghavamzadeh,      K.      Lee,      and K.      Lee      (2023) 023).\\nDPOK:      Reinforcement      learning      for      fine-tuning      text-to-image      diffusion      models.\\n \\n \\n  arXiv      preprint      arXiv:2305.16381.</p><p>Finn,      C.,      S.      Levine,      and      P.      Abbeel      (2016).\\nGuided      cost      learning:      Deep      inverse      optimal      control      via  \\n \\n  policy      optimization.\\nIn      International      conference      on      machine      learning,      pp.\\n49-58.\\nPMLR.</p><p>Fox,      R.,      A.      Pakman,      and      N.      Tishby      (2015).\\nTaming      the      noise      in      reinforcement      learning      via      soft  \\n \\n  updates.\\narXiv      preprint      arXiv:      1512.08562.</p><p>Frey,      N.      C.,      D.      Berenberg,      K.      Zadorozhny,      J.      Kleinhenz,      J.      Lafrance-Vanasse,      I.      Hotzel,      Y.      Wu, S.      Ra,   \\n \\nR R.      Bonneau,      K.      Cho,      et      al.\\n(2023).\\nProtein      discovery      with      discrete      walk-jump      sampling.\\n \\n \\n  arXiv      preprint      arXiv:2306.\\n12360.</p><p>Geist,      M.,      B.      Scherrer,      and      O.      Pietquin      (2019).\\n \\n \\nA      theory      of      regularized      markov      decision      processes.\\n \\n \\n  In      International      Conference      on      Machine      Learning,      pp.\\n2160-2169.\\nPMLR.</p><p> \\n \\n  Girolami,      M.      and      B.      Calderhead      (2011).\\nRiemann      manifold      langevin      and      hamiltonian      monte  \\n \\n  carlo      methods.\\nJournal      of      the      Royal      Statistical      Society      Series      B:      Statistical      Methodology      73(2),  \\n \\n  123-214.</p><p>Gosai,      S.      J.,      R.      I.      Castro,      N.      Fuentes,      J.      C.      Butts,      S.      Kales,      R.      R.      Noche,      K.      Mouri,      P.      C.      Sabeti, S.   \\n \\nK K.      Reilly,      and      R.      Tewhey      (2023).\\nMachine-guided      design      of      synthetic      cell      type-specific  \\n \\n  cis-regulatory      elements.\\nbioRxiv.</p><p>Haarnoja,      T.,      H.      Tang,      P.      Abbeel,      and      S.      Levine      (2017).\\nReinforcement      learning      with      deep      energy-  \\n \\n  based      policies.\\nIn      International      conference      on      machine      learning,      pp.\\n1352-1361.\\nPMLR.</p><p>Hansen-Estruch,      P.,      I.      Kostrikov,      M.      Janner,      J.      G.      Kuba,      and      S.      Levine      (2023).\\nIdql:      Implicit  \\n \\n  q-learning      as      an      actor-critic      method      with      diffusion      policies.\\narXiv      preprint      arXiv:2304.10573.</p><p>Heng,      J.,      A.      N.      Bishop,      G.      Deligiannidis,      and      A.      Doucet      (2020).\\nControlled      sequential      monte      carlo.</p><p>Ho,      J.,      A.      Jain,      and      P.      Abbeel      (2020).\\nDenoising      diffusion      probabilistic      models.\\nAdvances      in      neural  \\n \\n  information      processing      systems      33,      6840-6851.</p><p>Ho,      J.      and      T.      Salimans      (2022).\\nClassifier-free      diffusion      guidance.\\narXiv      preprint      arXiv:2207.12598.</p><p>Ho,      J.,      T.      Salimans,      A.      Gritsenko,      W.      Chan,      M.      Norouzi,      and      D.      J.      Fleet      (2022).\\nVideo      diffusion  \\n \\n  models.\\nAdvances      in      Neural      Information      Processing      Systems      35,      8633-8646.</p><p> \\n \\n  Hoogeboom,      E.,      V.      G.      Satorras,      C.      Vignac,      and      M.      Welling      (2022).\\nEquivariant      diffusion      for  \\n \\n  molecule      generation      in      3d.\\nIn      International      conference      on      machine      learning,      pp.\\n8867-8887.</p><h4>      PMLR.</h4><p>Huang,      X.,      H.      Dong,      H.      Yifan,      Y.      Ma,      and      T.      Zhang      (2023).\\nReverse      diffusion      monte      carlo.\\nIn      The  \\n \\n  Twelfth      International      Conference      on      Learning      Representations.</p><p>Janner,      M.,      Y.      Du,      J.      B.      Tenenbaum,      and      S.      Levine      (2022).\\nPlanning      with      diffusion      for      flexible  \\n \\n  behavior      synthesis.\\narXiv      preprint      arXiv:2205.09991.</p><p> \\n \\n  Jo,      J.,      S.      Lee,      and      S.      J.      Hwang      (2022).\\nScore-based      generative      modeling      of      graphs      via      the      system  \\n \\n  of      stochastic      differential      equations.\\nIn      International      Conference      on      Machine      Learning,      pp.\\n \\n \\n  10362-10383.\\nPMLR.</p><p> \\n \\n  Kazim,      M.,      J.      Hong,      M.-G.\\nKim,      and      K.-K.\\nK.      Kim      (2024).\\nRecent      advances      in      path      integral      control  \\n \\n  for      trajectory      optimization:      An      overview      in      theoretical      and      algorithmic      perspectives.\\nAnnual  \\n \\n  Reviews      in      Control      57,      100931.</p><p>Krishnamoorthy,      S.,      S.      M.      Mashkaria,      and      A.      Grover      (2023).\\nDiffusion      models      for      black-box  \\n \\n  optimization.\\narXiv      preprint      arXiv:2306.07180.</p><p>Kumar,      A.,      A.      Zhou,      G.      Tucker,      and      S.      Levine      (2020).\\nConservative      q-learning      for      offline      reinforce-  \\n \\n  ment      learning.\\nAdvances      in      Neural      Information      Processing      Systems      33,      1179-1191.</p><p>Lal,      A.,      D.      Garfield,      T.      Biancalani,      and      G.      Eraslan      (2024).\\nreglm:      Designing      realistic      regulatory  \\n \\n  dna      with      autoregressive      language      models.\\nbioRxiv,      2024-02.</p><p>Lee,      K.,      H.      Liu,      M.      Ryu,      O.      Watkins,      Y.      Du,      C.      Boutilier,      P.      Abbeel,      M.      Ghavamzadeh,      and      S.      S.      Gu  \\n \\n  (2023).\\nAligning      text-to-image      models      using      human      feedback.\\narXiv      preprint      arXiv:2302.12192.</p><p>Levine,      S.      (2018).\\nReinforcement      learning      and      control      as      probabilistic      inference:      Tutorial      and  \\n \\n  review.\\narXiv      preprint      arXiv:      1805.00909.</p><p>Li,      Z.,      Y.      Ni,      T.      A.      B.      Huygelen,      A.      Das,      G.      Xia,      G.-B.\\nStan,      and      Y.      Zhao      (2023).\\nLatent      diffusion  \\n \\n  model      for      dna      sequence      generation.\\narXiv      preprint      arXiv:2310.06150.</p><p>Lipman,      Y.,      R.      T.      Chen,      H.      Ben-Hamu,      M.      Nickel,      and      M.      Le      (2023).\\nFlow      matching      for      generative  \\n \\n  modeling.\\nJCLR      2023.</p><p>Liu,      X.,      C.      Gong,      and      Q.      Liu      (2022).\\nFlow      straight      and      fast:      Learning      to      generate      and      transfer      data  \\n \\n  with      rectified      flow.\\narXiv      preprint      arXiv:2209.03003.</p><p>Liu,      X.,      L.      Wu,      M.      Ye,      and      Q.      Liu      (2022).\\nLet      us      build      bridges:      Understanding      and      extending  \\n \\n  diffusion      generative      models.\\narXiv      preprint      arXiv:2208.14699.</p><p>Lou,      A.,      C.      Meng,      and      S.      Ermon      (2023).\\nDiscrete      diffusion      language      modeling      by      estimating      the  \\n \\n  ratios      of      the      data      distribution.\\narXiv      preprint      arXiv:2310.16834.</p><p>Malkin,      N.,      M.      Jain,      E.      Bengio,      C.      Sun,      and      Y.      Bengio      (2022).\\nTrajectory      balance:      Improved      credit  \\n \\n  assignment      in      gflownets.\\nAdvances      in      Neural      Information      Processing      Systems      35,      5955-5967.</p><p>Mohammadpour,      S.,      E.      Bengio,      E.      Frejinger,      and      P.-L.\\nBacon      (2023).\\nMaximum      entropy      gflownets  \\n \\n  with      soft      q-learning.\\narXiv      preprint      arXiv:2312.14331.</p><p>Nachum,      O.,      M.      Norouzi,      K.      Xu,      and      D.      Schuurmans      (2017).\\nBridging      the      gap      between      value      and  \\n \\n  policy      based      reinforcement      learning.\\nAdvances      in      neural      information      processing      systems      30.</p><p>Neal,      R.      M.      et      al.\\n(2011).\\nMcmce      using      hamiltonian      dynamics.\\nHandbook      of      markov      chain      monte  \\n \\n  carlo      2(11),      2.</p><p>Neu,      G.,      A.      Jonsson,      and      V.      Gémez      (2017).\\n \\n \\nA      unified      view      of      entropy-regularized      markov      decision  \\n \\n  processes.\\narXiv      preprint      arXiv:      1705.07798.</p><p>Ouyang,      L.,      J.      Wu,      X.      Jiang,      D.      Almeida,      C.      Wainwright,      P.      Mishkin,      C.      Zhang,      S.      Agarwal, K.      Slama,      A A.      Ray,      et      al.\\n(2022).\\nTraining      language      models      to      follow      instructions      with      human  \\n \\n  feedback.\\nAdvances      in      Neural      Information      Processing      Systems      35,      27730-27744.</p><p>Pattanaik,      L.      and      C.      W.      Coley      (2020).\\nMolecular      representation:      going      long      on      fingerprints.\\n \\n \\n  Chem      6(6),      1204-1207.</p><p>Peng,      X.      B.,      A.      Kumar,      G.      Zhang,      and      S.      Levine      (2019).\\nAdvantage-weighted      regression:      Simple  \\n \\n  and      scalable      off-policy      reinforcement      learning.\\narXiv      preprint      arXiv:1910.00177.</p><p>Peters,      J.,      K.      Mulling,      and      Y.      Altun      (2010).\\nRelative      entropy      policy      search.\\nIn      Proceedings      of      the  \\n \\n  AAAI      Conference      on      Artificial      Intelligence,      Volume      24,      pp.\\n1607-1612.</p><p> \\n \\n  Podell,      D.,      Z.      English,      K.      Lacey,      A.      Blattmann,      T.      Dockhorn,      J.      Miiller,      J.      Penna,      and      R.      Rombach  \\n \\n  (2023).\\nSdxl:      Improving      latent      diffusion      models      for      high-resolution      image      synthesis.\\narXiv  \\n \\n  preprint      arXiv:2307.01952.</p><p>Prabhudesai,      M.,      A.      Goyal,      D.      Pathak,      and      K.      Fragkiadaki      (2023).\\nAligning      text-to-image      diffusion  \\n \\n  models      with      reward      backpropagation.\\narXiv      preprint      arXiv:2310.03739.</p><p>Ramesh,      A.,      P.      Dhariwal,      A.      Nichol,      C.      Chu,      and      M.      Chen      (2022).\\nHierarchical      text-conditional  \\n \\n  image      generation      with      clip      latents.\\narXiv      preprint      arXiv:2204.06125      1(2),      3.</p><p>Rigter,      M.,      B.      Lacerda,      and      N.      Hawes      (2022).\\nRambo-rl:      Robust      adversarial      model-based      offline  \\n \\n  reinforcement      learning.\\nAdvances      in      neural      information      processing      systems      35,      16082-16097.</p><p>Robert,      C.      (2014).\\nStatistical      modeling      and      computation.</p><p>Rogers,      L.      C.      G.      and      D.      Williams      (2000).\\nDiffusions,      Markov      processes      and      martingales:      Volume      2,  \\n \\n  It6é      calculus,      Volume      2.\\nCambridge      university      press.</p><p> \\n \\n  Rombach,      R.,      A.      Blattmann,      D.      Lorenz,      P.      Esser,      and      B.      Ommer      (2022,      June).\\nHigh-resolution  \\n \\n  image      synthesis      with      latent      diffusion      models.\\nIn      Proceedings      of      the      IEEE/CVF      Conference      on  \\n \\n  Computer      Vision      and      Pattern      Recognition      (CVPR),      pp.\\n10684-10695.</p><p>Sarkar,      A.,      Z.      Tang,      C.      Zhao,      and      P.      Koo      (2024).\\nDesigning      dna      with      tunable      regulatory      activity  \\n \\n  using      discrete      diffusion.\\nbioRxiv,      2024-05.</p><p>Schulman,      J.,      X.      Chen,      and      P.      Abbeel      (2017).\\nEquivalence      between      policy      gradients      and      soft  \\n \\n  q-learning.\\narXiv      preprint      arXiv:      1704.06440.</p><p>Schulman,      J.,      S.      Levine,      P.      Abbeel,      M.      Jordan,      and      P.      Moritz      (2015).\\nTrust      region      policy      optimization.\\n \\n \\n  In      International      conference      on      machine      learning,      pp.\\n1889-1897.\\nPMLR.</p><p>Schulman,      J.,      F.      Wolski,      P.      Dhariwal,      A.      Radford,      and      O.      Klimov      (2017).\\nProximal      policy      optimiza-  \\n \\n  tion      algorithms.\\narXiv      preprint      arXiv:      1707.06347.</p><p>Shi,      Y.,      V.      De      Bortoli,      A.      Campbell,      and      A.      Doucet      (2024).\\nDiffusion      schrédinger      bridge      matching.\\n \\n \\n  Advances      in      Neural      Information      Processing      Systems      36.</p><p> \\n \\n  Sohl-Dickstein,      J.,      E.      Weiss,      N.      Maheswaranathan,      and      S.      Ganguli      (2015).\\nDeep      unsupervised  \\n \\n  learning      using      nonequilibrium      thermodynamics.\\nIn      /nternational      conference      on      machine      learning,  \\n \\n  pp.\\n2256-2265.\\nPMLR.</p><p>Song,      J.,      C.      Meng,      and      S.      Ermon      (2020).\\nDenoising      diffusion      implicit      models.\\narXiv      preprint  \\n \\n  arXiv:2010.02502.</p><p>Song,      Y.,      C.      Durkan,      I.      Murray,      and      S.      Ermon      (2021).\\nMaximum      likelihood      training      of      score-based  \\n \\n  diffusion      models.\\nAdvances      in      neural      information      processing      systems      34,      1415-1428.</p><p>Stark,      H.,      B.      Jing,      C.      Wang,      G.      Corso,      B.      Berger,      R.      Barzilay,      and      T.      Jaakkola      (2024).\\nDirichlet  \\n \\n  flow      matching      with      applications      to      dna      sequence      design.\\narXiv      preprint      arXiv:2402.05841.</p><p>Sutton,      R.      S.      and      A.      G.      Barto      (2018).\\nReinforcement      learning:      An      introduction.\\nMIT      press.</p><p>Tang,      W.      (2024).\\nFine-tuning      of      diffusion      models      via      stochastic      control:      entropy      regularization      and  \\n \\n  beyond.\\narXiv      preprint      arXiv:2403.06279.</p><p>Tang,      W.      and      H.      Zhao      (2024).\\nScore-based      diffusion      models      via      stochastic      differential      equations—a  \\n \\n  technical      tutorial.\\narXiv      preprint      arXiv:2402.07487.</p><p>Theodorou,      E.,      J.      Buchli,      and      S.      Schaal      (2010).\\n \\n \\nA      generalized      path      integral      control      approach      to  \\n \\n  reinforcement      learning.\\nThe      Journal      of      Machine      Learning      Research      11,      3137-3181.</p><p>Tiapkin,      D.,      N.      Morozov,      A.      Naumov,      and      D.      Vetrov      (2023).\\nGenerative      flow      networks      as      entropy-  \\n \\n  regularized      rl.\\narXiv      preprint      arXiv:2310.12934.</p><p> \\n \\n  Tong,      A.,      N.      Malkin,      G.      Huguet,      Y.      Zhang,      J.      Rector-Brooks,      K.      Fatras,      G.      Wolf,      and      Y.      Bengio  \\n \\n  (2023).\\nConditional      flow      matching:      Simulation-free      dynamic      optimal      transport.\\narXiv      preprint  \\n \\n  arXiv:2302.00482.</p><p>Uehara,      M.      and      W.      Sun      (2021).\\nPessimistic      model-based      offline      reinforcement      learning      under  \\n \\n  partial      coverage.\\narXiv      preprint      arXiv:2107.06226.</p><p>Uehara,      M.,      Y.      Zhao,      K.      Black,      E.      Hajiramezanali,      G.      Scalia,      N.      L.      Diamant,      A.      M.      Tseng, T.      Biancalani,      and      S S.      Levine      (2024).\\nFine-tuning      of      continuous-time      diffusion      models      as      entropy-  \\n \\n  regularized      control.\\narXiv      preprint      arXiv:2402.15194.</p><p> \\n \\n  Uehara,      M.,      Y.      Zhao,      K.      Black,      E.      Hajiramezanali,      G.      Scalia,      N.      L.      Diamant,      A.      M.      Tseng,      S.      Levine,  \\n \\n  and      T.      Biancalani      (2024).\\nFeedback      efficient      online      fine-tuning      of      diffusion      models.\\narXiv      preprint  \\n \\n  arXiv:2402.\\n16359.</p><p> \\n \\n  Uehara,      M.,      Y.      Zhao,      E.      Hajiramezanali,      G.      Scalia,      G.      Eraslan,      A.      Lal,      S.      Levine,      and      T.      Biancalani  \\n \\n  (2024).\\nBridging      model-based      optimization      and      generative      modeling      via      conservative      fine-tuning  \\n \\n  of      diffusion      models.\\narXiv      preprint      arXiv:2405.19673.</p><p>Vargas,      F.,      W.      Grathwohl,      and      A.      Doucet      (2023).\\nDenoising      diffusion      samplers.\\narXiv      preprint  \\n \\n  arXiv:2302.\\n13834.</p><p> \\n \\n  Wallace,      B.,      M.      Dang,      R.      Rafailov,      L.      Zhou,      A.      Lou,      S.      Purushwalkam,      S.      Ermon,      C.      Xiong,      S.      Joty,  \\n \\n  and      N.      Naik      (2023).\\nDiffusion      model      alignment      using      direct      preference      optimization.\\narXiv  \\n \\n  preprint      arXiv:2311.12908.</p><p>Wang,      Z.,      J.      J.      Hunt,      and      M.      Zhou      (2022).\\nDiffusion      policies      as      an      expressive      policy      class      for      offline  \\n \\n  reinforcement      learning.\\narXiv      preprint      arXiv:2208.06193.</p><p>Widatalla,      T.,      R.      Rafailov,      and      B.      Hie      (2024).\\nAligning      protein      generative      models      with      experimental  \\n \\n  fitness      via      direct      preference      optimization.\\nbioRxiv,      2024-05.</p><p>Williams,      G.,      A.      Aldrich,      and      E.      A.      Theodorou      (2017).\\nModel      predictive      path      integral      control:      From  \\n \\n  theory      to      parallel      computation.\\nJournal      of      Guidance,      Control,      and      Dynamics      40(2),      344-357.</p><p>Wu,      X.,      K.      Sun,      F.      Zhu,      R.      Zhao,      and      H.      Li      (2023).\\nBetter      aligning      text-to-image      models      with  \\n \\n  human      preference.\\narXiv      preprint      arXiv:2303.14420.</p><p>Wu,      Y.,      G.      Tucker,      and      O.      Nachum      (2019).\\nBehavior      regularized      offline      reinforcement      learning.\\n \\n \\n  arXiv      preprint      arXiv:      1911.11361.</p><p>Wulfmeier,      M.,      P.      Ondruska,      and      I.      Posner      (2015).\\nMaximum      entropy      deep      inverse      reinforcement  \\n \\n  learning.\\narXiv      preprint      arXiv:      1507.04888.</p><p>Xie,      T.,      C.-A.\\nCheng,      N.      Jiang,      P.      Mineiro,      and      A.      Agarwal      (2021).\\nBellman-consistent      pessimism  \\n \\n  for      offline      reinforcement      learning.\\nAdvances      in      neural      information      processing      systems      34.</p><p>Xu,      M.,      L.      Yu,      Y.      Song,      C.      Shi,      S.      Ermon,      and      J.      Tang      (2022).\\nGeodiff:   \\n \\nA      geometric      diffusion  \\n \\n  model      for      molecular      conformation      generation.\\narXiv      preprint      arXiv:2203.02923.</p><p> \\n \\n  Yang,      L.,      Z.      Zhang,      Y.      Song,      S.      Hong,      R.      Xu,      Y.      Zhao,      W.      Zhang,      B.      Cui,      and      M.-H.\\nYang      (2023).\\n \\n \\n  Diffusion      models:   \\n \\nA      comprehensive      survey      of      methods      and      applications.\\nACM      Computing  \\n \\n  Surveys      56(4),      1-39.</p><p> \\n \\n  Yu,      T.,      G.      Thomas,      L.      Yu,      S.      Ermon,      J.      Y.      Zou,      S.      Levine,      C.      Finn,      and      T.      Ma      (2020).\\nMopo:  \\n \\n  Model-based      offline      policy      optimization.\\nAdvances      in      Neural      Information      Processing      Systems      33,  \\n \\n  14129-14142.</p><p> \\n \\n  Yuan,      H.,      K.      Huang,      C.      Ni,      M.      Chen,      and      M.      Wang      (2023).\\nReward-directed      conditional      diffusion:  \\n \\n  Provable      distribution      estimation      and      reward      improvement.\\nIn      Thirty-seventh      Conference      on  \\n \\n  Neural      Information      Processing      Systems.</p><p>Zhang,      H.      and      T.      Xu      (2023).\\nTowards      controllable      diffusion      models      via      reward-guided      exploration.\\n \\n \\n  arXiv      preprint      arXiv:2304.07132.</p><p>Zhang,      Q.      and      Y.      Chen      (2021).\\nPath      integral      sampler:   \\n \\na      stochastic      control      approach      for      sampling.\\n \\n \\n  arXiv      preprint      arXiv:2111,15141.</p><p> \\n \\n  Zhao,      Y.,      M.      Uehara,      G.      Scalia,      T.      Biancalani,      S.      Levine,      and      E.      Hajiramezanali      (2024).\\n \\n \\n  Adding      conditional      control      to      diffusion      models      with      reinforcement      learning.\\narXiv      preprint  \\n \\n  arXiv:2406.\\n12120.</p><p>Zhou,      Z.,      S.      Kearnes,      L.      Li,      R.      N.      Zare,      and      P.      Riley      (2019).\\nOptimization      of      molecules      via      deep  \\n \\n  reinforcement      learning.\\nScientific      reports      9(1),      10752.</p><p>Zhu,      Z.,      H.      Zhao,      H.      He,      Y.      Zhong,      S.      Zhang,      Y.      Yu,      and      W.      Zhang      (2023).\\nDiffusion      models      for  \\n \\n  reinforcement      learning:   \\n \\nA      survey.\\narXiv      preprint      arXiv:2311.01223.</p><p>Ziebart,      B.      D.,      A.      L.      Maas,      J.      A.      Bagnell,      A.      K.      Dey,      et      al.\\n(2008).\\nMaximum      entropy      inverse  \\n \\n  reinforcement      learning.\\nIn      Aaai,      Volume      8,      pp.\\n1433-1438.\\nChicago,      IL,      USA.</p></html>')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '/home/ubuntu/T_RAG/testing/2407.13734v1.pdf'}, page_content='<html><h1>Understanding      Reinforcement      Learning-Based</h1><h1>Fine-Tuning      of      Diffusion      Models:      A      Tutorial      and      Review</h1><p>Masatoshi      Uehara*!,      Yulai      Zhao\\\\\\\\\\\\\\'*,      Tommaso      Biancalani!,      and      Sergey      Levine?</p><p>‘Genentech</p><li>*Princeton      University</li><p>>University      of      California,      Berkeley</p><h2>July      19,      2024</h2><h2>Abstract</h2><p> \\n \\n  This      tutorial      provides   \\n \\na      comprehensive      survey      of      methods      for      fine-tuning      diffusion      models  \\n \\n  to      optimize      downstream      reward      functions.\\nWhile      diffusion      models      are      widely      known      to      provide  \\n \\n  excellent      generative      modeling      capability,      practical      applications      in      domains      such      as      biology  \\n \\n  require      generating      samples      that      maximize      some      desired      metric      (e.g.,      translation      efficiency      in  \\n \\n  RNA,      docking      score      in      molecules,      stability      in      protein).\\nIn      these      cases,      the      diffusion      model      can  \\n \\n  be      optimized      not      only      to      generate      realistic      samples      but      also      to      maximize      the      measure      of      interest  \\n \\n  explicitly.\\nSuch      methods      are      based      on      concepts      from      reinforcement      learning      (RL).\\nWe      explain  \\n \\n  the      application      of      various      RL      algorithms,      including      PPO,      differentiable      optimization,      reward-  \\n \\n  weighted      MLE,      value-weighted      sampling,      and      path      consistency      learning,      tailored      specifically      for  \\n \\n  fine-tuning      diffusion      models.\\nWe      aim      to      explore      fundamental      aspects      such      as      the      strengths      and  \\n \\n  limitations      of      different      RL-based      fine-tuning      algorithms      across      various      scenarios,      the      benefits  \\n \\n  of      RL-based      fine-tuning      compared      to      non-RL-based      approaches,      and      the      formal      objectives      of  \\n \\n  RL-based      fine-tuning      (target      distributions).\\nAdditionally,      we      aim      to      examine      their      connections  \\n \\n  with      related      topics      such      as      classifier      guidance,      Gflownets,      flow-based      diffusion      models,      path  \\n \\n  integral      control      theory,      and      sampling      from      unnormalized      distributions      such      as      MCMC.\\nThe  \\n \\n  code      of      this      tutorial      is      available      at      https://github.com/masa-ue/RLfinetuning_Diffusion_Bioseq.</p><p>arXiv:2407.13734v1      [cs.LG]      18      Jul      2024</p><h2>July      19,      2024</h2><h2>Abstract</h2><p> \\n \\n  This      tutorial      provides   \\n \\na      comprehensive      survey      of      methods      for      fine-tuning      diffusion      models  \\n \\n  to      optimize      downstream      reward      functions.\\nWhile      diffusion      models      are      widely      known      to      provide  \\n \\n  excellent      generative      modeling      capability,      practical      applications      in      domains      such      as      biology  \\n \\n  require      generating      samples      that      maximize      some      desired      metric      (e.g.,      translation      efficiency      in  \\n \\n  RNA,      docking      score      in      molecules,      stability      in      protein).\\nIn      these      cases,      the      diffusion      model      can  \\n \\n  be      optimized      not      only      to      generate      realistic      samples      but      also      to      maximize      the      measure      of      interest  \\n \\n  explicitly.\\nSuch      methods      are      based      on      concepts      from      reinforcement      learning      (RL).\\nWe      explain  \\n \\n  the      application      of      various      RL      algorithms,      including      PPO,      differentiable      optimization,      reward-  \\n \\n  weighted      MLE,      value-weighted      sampling,      and      path      consistency      learning,      tailored      specifically      for  \\n \\n  fine-tuning      diffusion      models.\\nWe      aim      to      explore      fundamental      aspects      such      as      the      strengths      and  \\n \\n  limitations      of      different      RL-based      fine-tuning      algorithms      across      various      scenarios,      the      benefits  \\n \\n  of      RL-based      fine-tuning      compared      to      non-RL-based      approaches,      and      the      formal      objectives      of  \\n \\n  RL-based      fine-tuning      (target      distributions).\\nAdditionally,      we      aim      to      examine      their      connections  \\n \\n  with      related      topics      such      as      classifier      guidance,      Gflownets,      flow-based      diffusion      models,      path  \\n \\n  integral      control      theory,      and      sampling      from      unnormalized      distributions      such      as      MCMC.\\nThe  \\n \\n  code      of      this      tutorial      is      available      at      https://github.com/masa-ue/RLfinetuning_Diffusion_Bioseq.</p><p>arXiv:2407.13734v1      [cs.LG]      18      Jul      2024</p><h1>Introduction</h1><p> \\n \\n  Diffusion      models      (Sohl-Dickstein      et      al.,      2015;      Ho      et      al.,      2020;      Song      et      al.,      2020)      are      widely      rec-  \\n \\n  ognized      as      powerful      tools      for      generative      modeling.\\nThey      are      able      to      accurately      model      complex  \\n \\n  distributions      by      closely      emulating      the      characteristics      of      the      training      data.\\nThere      are      many      applica-  \\n \\n  tions      of      diffusion      models      in      various      fields,      including      computer      vision      (Podell      et      al.,      2023),      natural  \\n \\n  language      processing      (Austin      et      al.,      2021),      biology      (Avdeyev      et      al.,      2023;      Stark      et      al.,      2024;      Li      et      al., *“uehara.masatoshi@gene.com  \\n \\n  tyulaiz@princeton.\\nedu.\\nEqual      contribution.</p><p> \\n \\n  Reward      models  \\n \\n  Images   \\n \\n—      Aesthetic      score   \\n \\n|      Images   \\n \\n—      Aesthetic      score      Images   \\n \\n—      Aesthetic      score  \\n \\n  Molecules   \\n \\n~      QED      Molecules   \\n \\n—      QED      Molecules   \\n \\n—      QED  \\n \\n  DNAs   \\n \\n=      Cell-specificity   \\n \\n[      DNAs   \\n \\n=      Cell-specificity      DNAs   \\n \\n—      Cell-specificity freely      f      rel      f      rel      ainsoms.</p><p>es   \\n \\n(   \\n \\n_   \\n \\n|   \\n \\n) Images      with      high  \\n \\n  aesthetic      score</p><h2>Images</h2><p> \\n \\n \\ng   \\n \\n|   \\n \\na      ge   \\n \\nz       evi      do}      f      g,   \\n \\n2      3°      DNAs      with      high  \\n \\n  aor   \\n \\na   \\n \\n-      e      e       DNAs      io      si   \\n \\ni      og   \\n \\n+   \\n \\na      i:      cell-specificity</p><p>Figure      1:      Illustrative      examples      of      RL-based      fine-tuning,      aimed      at      optimizing      pre-trained      diffusion  \\n \\n  models      to      maximize      downstream      reward      functions.</p><p> \\n \\n  2023),      chemistry      (Jo      et      al.,      2022;      Xu      et      al.,      2022;      Hoogeboom      et      al.,      2022),      and      biology      (Avdeyev  \\n \\n  et      al.,      2023;      Stark      et      al.,      2024;      Campbell      et      al.,      2024).</p><p> \\n \\n  While      diffusion      models      exhibit      significant      power      in      capturing      the      training      data      distribution,  \\n \\n  there’s      often   \\n \\na      need      to      customize      these      models      for      particular      downstream      reward      functions.\\nFor  \\n \\n  instance,      in      computer      vision,      Stable      Diffusion      (Rombach      et      al.,      2022)      serves      as   \\n \\na      strong      backbone  \\n \\n  pre-trained      model.\\nHowever,      we      may      want      to      fine-tune      it      further      by      optimizing      downstream      reward  \\n \\n  functions      such      as      aesthetic      scores      or      human-alignment      scores      (Black      et      al.,      2023;      Fan      et      al.,      2023).\\n \\n \\n  Similarly,      in      fields      such      as      biology      and      chemistry,      various      sophisticated      diffusion      models      have  \\n \\n  been      developed      for      DNA,      RNA,      protein      sequences,      and      molecules,      effectively      modeling      biological  \\n \\n  and      chemical      spaces.\\nNonetheless,      biologists      and      chemists      typically      aim      to      optimize      specific  \\n \\n  downstream      objectives      such      as      cell-specific      expression      in      DNA      sequences      (Gosai      et      al.,      2023;      Lal  \\n \\n  et      al.,      2024;      Sarkar      et      al.,      2024),      translational      efficiency/stability      of      RNA      sequences      (Castillo-Hair  \\n \\n  and      Seelig,      2021;      Agarwal      and      Kelley,      2022),      stability/bioactivity      of      protein      sequence      (Frey      et      al.,  \\n \\n  2023;      Widatalla      et      al.,      2024)      or      QED/SA      scores      of      molecules      (Zhou      et      al.,      2019).</p><p> \\n \\n  To      achieve      this      goal,      numerous      algorithms      have      been      proposed      for      fine-tuning      diffusion      models  \\n \\n  via      reinforcement      learning      (RL)      (e.g.,      Black      et      al.      (2023);      Fan      et      al.\\n(2023);      Clark      et      al.\\n(2023);  \\n \\n  Prabhudesai      et      al.\\n(2023);      Uehara      et      al.\\n(2024)),      aiming      to      optimize      downstream      reward      functions.\\n \\n \\n  RL      is   \\n \\na      machine      learning      paradigm      where      agents      learn      to      make      sequential      decisions      to      maximize  \\n \\n  reward      signals      (Sutton      and      Barto,      2018;      Agarwal      et      al.,      2019).\\nIn      our      context,      RL      naturally      emerges  \\n \\n  as   \\n \\na      Suitable      approach      due      to      the      sequential      structure      inherent      in      diffusion      models,      where      each      time  \\n \\n  step      involves   \\n \\na      “decision”      corresponding      to      how      the      sample      is      denoised      at      that      step.\\nThis      tutorial  \\n \\n  aims      to      review      recent      works      for      readers      interested      in      understanding      the      fundamentals      of      RL-based  \\n \\n  fine-tuning      from   \\n \\na      holistic      perspective,      including      the      advantages      of      RL-based      fine-tuning      over  \\n \\n  non-RL      approaches,      the      pros      and      cons      of      different      RL-based      fine-tuning      algorithms,      the      formalized  \\n \\n  goal      of      RL-based      fine-tuning,      and      its      connections      with      related      topics      such      as      classifier      guidance.</p><p> \\n \\n  The      content      of      this      tutorial      is      primarily      divided      into      three      parts.\\nIn      addition,      as      an      implementation  \\n \\n  example,      we      also      release      the      code      that      employs      RL-based      fine-tuning      for      guided      biological      sequences  \\n \\n  (DNA/RNA)      generation      at      https://github.com/masa-ue/RLfinetuning_Diffusion_Bioseq.</p><li>1.      We      aim      to      provide   \\n \\na      comprehensive      overview      of      current      algorithms.\\nNotably,      given      the  \\n \\n  sequential      nature      of      diffusion      models,      we      can      naturally      frame      fine-tuning      as   \\n \\na      reinforcement  \\n \\n  learning      (RL)      problem      within      Markov      Decision      Processes      (MDPs),      as      detailed      in      Section   \\n \\n3       and      4.\\nTherefore,      we      can      employ      any      off-the-shelf      RL      algorithms      such      as      PPO      (Schulman  \\n \\n  et      al.,      2017),      differentiable      optimization      (direct      reward      backpropagation),      weighted      MLE  \\n \\n  (Peters      et      al.,      2010;      Peng      et      al.,      2019),      value-weighted      sampling      (close      to      classifier      guidance  \\n \\n  in      Dhariwal      and      Nichol      (2021)),      and      path      consistency      learning      (Nachum      et      al.,      2017).\\nWe  \\n \\n  discuss      these      algorithms      in      detail      in      Section      4.2      and      6.\\nInstead      of      merely      outlining      each  \\n \\n  algorithm,      we      aim      to      present      both      their      advantages      and      disadvantages      so      readers      can      select  \\n \\n  the      most      suitable      algorithms      for      their      specific      purposes.</li><li>2.      We      categorize      various      fine-tuning      scenarios      based      on      how      reward      feedback      is      acquired      in  \\n \\n  Section      7.\\nThis      distinction      is      pivotal      for      practical      algorithm      design.\\nFor      example,      if      we      can  \\n \\n  access      accurate      reward      functions,      computational      efficiency      would      become      our      primary      focus.\\n \\n \\n  However,      in      cases      where      reward      functions      are      unknown,      it      is      essential      to      learn      them      from  \\n \\n  data      with      reward      feedback,      leading      us      to      take      feedback      efficiency      and      distributional      shift      into  \\n \\n  consideration      as      well.\\nSpecifically,      when      reward      functions      need      to      be      learned      from      static  \\n \\n  offline      data      without      any      online      interactions,      we      must      address      the      issue      of      overoptimization,  \\n \\n  where      fine-tuned      models      are      misled      by      out-of-distribution      samples,      and      generate      samples  \\n \\n  with      low      genuine      rewards.\\nThis      is      crucial      because,      in      an      offline      scenario,      the      coverage      of  \\n \\n  offline      data      distribution      with      feedback      is      limited;      hence,      the      out-of-distribution      region      could  \\n \\n  be      extensive      (Uehara      et      al.,      2024).</li><li>3.      We      provide   \\n \\na      detailed      discussion      on      the      relationship      between      RL-based      fine-tuning      methods  \\n \\n  and      closely      related      methods      in      the      literature,      such      as      classifier      guidance      (Dhariwal      and      Nichol,  \\n \\n  2021)      in      Section      8,      flow-based      diffusion      models      (Liu      et      al.,      2022;      Lipman      et      al.,      2023;      Tong  \\n \\n  et      al.,      2023)      in      Section      9,      sampling      from      unnormalized      distributions      (Zhang      and      Chen,      2021)  \\n \\n  in      Section      10,      Gflownets      (Bengio      et      al.,      2023)      in      Section      6.3,      and      path      integral      control      theory  \\n \\n  (Theodorou      et      al.,      2010;      Williams      et      al.,      2017;      Kazim      et      al.,      2024)      in      Section      6.2.3.      We  \\n \\n  summarize      the      key      messages      as      follows.</li><p>¢\\n \\n   Section      6.3:      The      losses      used      in      Gflownets      are      fundamentally      equivalent      to      those      derived  \\n \\n  from   \\n \\na      specific      RL      algorithm      called      path      consistency      learning.</p><p> \\n \\n \\n¢      Section      8:      Classifier      guidance      employed      in      conditional      generation      is      regarded      as   \\n \\na       specific      RL-based      fine-tuning      method,      which      we      call      value-weighted      sampling.\\nAs  \\n \\n  formalized      in      Zhao      et      al.\\n(2024),      this      observation      indicates      that      any      off-the-shelf      RL-  \\n \\n  based      fine-tuning      algorithms      (e.g.,      PPO      and      differentiable      optimization)      can      be      applied  \\n \\n  to      conditional      generation.</p><p> \\n \\n \\n¢      Section      10:      Sampling      from      unnormalized      distributions,      often      referred      to      as      Gibbs  \\n \\n  distributions,      is      an      important      and      challenging      problem      in      diverse      domains.\\nWhile  \\n \\n  MCMC      methods      are      traditionally      used      for      this      task,      recognizing      its      similarity      to      the  \\n \\n  objectives      of      RL-based      fine-tuning      suggests      that      off-the-shelf      RL      algorithms      can      also  \\n \\n  effectively      address      the      challenge      of      sampling      from      unnormalized      distributions.</p><h2>Preliminaries</h2><li>1.1 DiffusionModels 2 2.20.\\n00 2 ee ee</li><table><th><td colSpan=1>1.1.1 Score-Based Diffusion Models (Optional)</td><td colSpan=1></td><td colSpan=1></td></th><tr><td colSpan=1>1.2 Fine-Tuning Diffusion Models withRL</td><td colSpan=1>0.00.0.</td><td colSpan=1></td></tr><tr><td colSpan=1>1.2.1. Brief Overview: Fine-tuning withRL 1.2.2 Motivation for Using RL over Non-RL Alternatives.</td><td colSpan=1></td><td colSpan=1>2.</td></tr><tr><td>Brief      Overview      of      Entropy-Regularized      MDPs</td></tr><tr><td>Fine-Tuning      Diffusion      Models      with      RL      in      Entropy      Regularized      MDPs</td></tr><tr><td>Theory      of      RL-Based      Fine-Tuning</td></tr></table><h2>RL-Based      Fine-Tuning      Algorithms      1:      Non-Distribution-Constrained      Approaches</h2><h2>RL-Based      Fine-Tuning      Algorithms      2:      Distribution-Constrained      Approaches</h2><h2>Fine-Tuning      Settings      Taxonomy</h2><h2>Connection      with      Classifier      Guidance</h2><p> \\n \\n  Connection      with      Flow-Based      Diffusion      Models  \\n \\n  10  \\n \\n  10  \\n \\n  11 12  \\n \\n  14  \\n \\n  14  \\n \\n  15  \\n \\n  16  \\n \\n  16  \\n \\n  17  \\n \\n  18  \\n \\n  19  \\n \\n  21  \\n \\n  21  \\n \\n  23  \\n \\n  24  \\n \\n  25  \\n \\n  25  \\n \\n  28  \\n \\n  28  \\n \\n  28</p><p>10      Connection      with      Sampling      from      Unnormalized      Distributions      35 10.1 Markov Chain Monte Carlo(MCMC) 2.00000 eee ene 35 10.2 RL-Based Approaches 0 2.0.0 pee ee 36</p><p>11      Closely      Related      Directions      36</p><p>12      Summary      37</p><p>1\\n \\n   Preliminaries</p><p>In      this      section,      we      outline      the      fundamentals      of      diffusion      models      and      elucidate      the      objective      of  \\n \\n  fine-tuning      them.</p><h3>1.1.      Diffusion      Models</h3><p>dels  \\n \\n  We      present      an      overview      of      denoising      diffusion      probabilistic      models      (DDPM)      (Ho      et      al.,      2020).\\n \\n \\n  For      more      details,      refer      to      Yang      et      al.\\n(2023);      Cao      et      al.\\n(2024);      Chen      et      al.\\n(2024);      Tang      and      Zhao  \\n \\n  (2024).</p><p> \\n \\n  In      diffusion      models,      the      objective      is      to      develop   \\n \\na      deep      generative      model      that      accurately      captures  \\n \\n  the      true      data      distribution.\\nSpecifically,      denoting      the      data      distribution      by      pr.\\n \\n \\n€      A(4’)      where   \\n \\n¥      is      an  \\n \\n  input      space,   \\n \\na      DDPM      aims      to      approximate      p,,.\\nusing   \\n \\na      parametric      model      structured      as 1 p(xo3      8)   \\n \\n=   \\n \\n[      rlco      O)dx1.r,      where      p(%o:7;      9)   \\n \\n=      pr+i(@7;      9)      [[      peter:      0).</p><p>t=T When   \\n \\n4      is      an      Euclidean      space      (in      R®),      the      forward      process      is      modeled      as      the      following      dynamics:</p><p>pra(er)      =N(0,1),      pe(ve-r)2;      9)   \\n \\n=      N(p(a1,      t;      8),      07      (4)   \\n \\nx      D),  \\n \\n  where      N(-,-)      denotes   \\n \\na      normal      distribution,   \\n \\nJ      is      an      identity      matrix      and      p   \\n \\n:      R¢   \\n \\nx      [0,7]   \\n \\n>      R¢.\\nIn  \\n \\n  DDPMs,      we      aim      to      obtain   \\n \\na      set      of      policies      (i.e.,      denoising      process)      {p;}{_741,      pr:   \\n \\n&   \\n \\n—      A(A&)      such  \\n \\n  that      p(x;      0)   \\n \\n©      Ppre(#o).\\nIndeed,      by      optimizing      the      variational      bound      on      the      negative      log-likelihood,  \\n \\n  we      can      derive      such   \\n \\na      set      of      policies.\\nFor      more      details,      refer      to      Section      1.1.1.  \\n \\n  Hereafter,      we      consider   \\n \\na      situation      where      we      have   \\n \\na      pre-trained      diffusion      model      that      is      already  \\n \\n  trained      on   \\n \\na      large      dataset,      such      that      the      model      can      accurately      capture      the      underlying      data      distribution.</p><p> \\n \\n  pre We      refer      to      the      pre-trained      policies      as      {p?\\n\"*(-|-)}{_7,,,      and      to      the      marginal      distribution      at   \\n \\nt   \\n \\n=   \\n \\n0       induced      by      the      pre-trained      diffusion      model      as      pp.\\nIn      other      words, 1\\n \\n    pr      (x0)   \\n \\n=      [iler)      [[°@eledderr.</p><p>t=T  \\n \\n  Remark   \\n \\n1      (Non-Euclidean      space).\\nFor      simplicity,      we      typically      assume      that      the      domain      space      is  \\n \\n  Euclidean.\\nHowever,      we      can      easily      extend      most      of      the      discussion      to   \\n \\na      more      general      space,      such      as  \\n \\n  a      Riemannian      manifold      (De      Bortoli      et      al.,      2022)      or      discrete      space      (Austin      et      al.,      2021;      Campbell  \\n \\n  et      al.,      2022;      Benton      et      al.,      2024;      Lou      et      al.,      2023).</p><p> \\n \\n  Remark   \\n \\n2      (Conditional      generative      models).\\nPre-trained      models      can      be      conditional      diffusion      models,  \\n \\n  such      as      text-to-image      diffusion      models      (Ramesh      et      al.,      2022).\\nThe      extension      is      straightforward:  \\n \\n  augmenting      the      input      spaces      of      policies      with      an      additional      space      on      which      we      want      to      condition.\\n \\n \\n  More      specifically,      by      denoting      that      space      by   \\n \\nc   \\n \\n€      C,      each      policy      becomes      p;(x¢|%1,      650)   \\n \\n:   \\n \\n&   \\n \\nx   \\n \\nC   \\n \\n>       A(X).</p><p> \\n \\n  Remark   \\n \\n3      (Extension      to      Continuous-time      diffusion      models).\\nJn      this      tutorial,      our      discussion      on  \\n \\n  fine-tuning      diffusion      models      will      be      primarily      formulated      on      the      discrete-time      formulation,      as      we      did  \\n \\n  above      Nonetheless,      much      of      our      discussion      is      also      applicable      to      continuous-time      diffusion      models,  \\n \\n  as      formalized      in      Uehara      et      al.\\n(2024)</p><h4>1.1.1      Score-Based      Diffusion      Models      (Optional)</h4><p>nal)  \\n \\n  We      briefly      discuss      how      to      train      diffusion      models      in   \\n \\na      continuous-time      framework      (Song      et      al.,  \\n \\n  2021).\\nIn      our      tutorial,      this      section      is      mainly      used      to      discuss      several      algorithms      (e.g.,      value-weighted  \\n \\n  sampling      in      Section      6.2)      and      their      relationship      with      flow-based      diffusion      models      (Section      9)      later.</p><p> \\n \\n  Therefore,      readers      may      skip      it      based      on      their      individual      needs.</p><p> \\n \\n  The      training      process      of      diffusion      models      can      be      summarized      as      follows.\\nOur      objective      is      to      train  \\n \\n  a      sequential      mapping      from   \\n \\na      known      noise      distribution      to   \\n \\na      data      distribution,      formalized      through  \\n \\n  a      stochastic      differential      equation      (SDE).\\nFirstly,      we      define   \\n \\na      forward      (fixed)      SDE      that      maps      from  \\n \\n  the      data      distribution      to      the      noise      distribution.\\nThen,   \\n \\na      time-reversal      SDE      is      expressed      as      an      SDE,  \\n \\n  which      includes      the      (unknown)      score      function.\\nNow,      by      learning      this      unknown      score      function      from  \\n \\n  the      training      data,      the      time-reversal      SDE      can      be      utilized      as   \\n \\na      generative      model.\\nHere      are      the      details.</p><p>Forward      and      time-reversal      SDE.\\nFirstly,      we      introduce   \\n \\na      forward      (a.k.a.,      reference)      Stochastic  \\n \\n  differential      equations      (SDE)      from   \\n \\n0      to      7’.\\n \\n \\nA      common      choice      is   \\n \\na      variance-preserving      (VP)      process:</p><p>t\\n \\n   €\\n \\n   [0,7];      dx,   \\n \\n=      —0.52,dt   \\n \\n+      du;,,      x9   \\n \\n~      p(x),      (1)</p><p>where      dw;      represents      standard      Brownian      motion.\\nHere      are      two      crucial      observations:</p><p>¢\\n \\n   As   \\n \\nT      approaches      oo,      the      limiting      distribution      is      \\\\\\\\\\\\\\\\V(0,      I).</p><p>¢\\n \\n   The      time-reversal      SDE      (Anderson,      1982),      which      preserves      the      marginal      distribution,      is  \\n \\n  expressed      as      follows:</p><p>t\\n \\n   €\\n \\n   [0,7];      dz,   \\n \\n=      [0.5z,   \\n \\n+      V      log      qr_s(%)|dt   \\n \\n+      du,.\\n(2) Here,      g,   \\n \\n€      A(R“)      denotes      the      marginal      distribution      at      time   \\n \\nt      induced      by      the      reference      SDE (1).\\nNotably,      the      marginal      distribution      of      zr_;      is      the      same      as      the      one      of      x;      induced      by      the  \\n \\n  reference      SDE.</p><p> \\n \\n  These      observations      suggest      that      with      sufficiently      large      T,      starting      from      V      (0,      I)      and      following      the  \\n \\n  time-reversal      SDE      (2)   \\n \\n,      we      are      able      to      sample      from      the      data      distribution      (i.e.,      p?\\n\"°)      at      the      terminal  \\n \\n  time      point      7’.</p><p>Training      score      functions      from      the      training      data.\\nNow,      the      remaining      question      is      how      to      estimate  \\n \\n  the      marginal      score      function      V      log      q(-)      in      (2).\\nThis      can      be      computed      using      the      following      principles:</p><p>°\\n \\n   We      have      V      log      (a+)   \\n \\n=      Exgxppre[Va,      log      qyo(ae   \\n \\n|      2o)],      where      q@jo      represents      the      conditional  \\n \\n  distribution      of      x;      given      Xo      induced      by      the      reference      SDE.</p><p>¢\\n \\n   The      conditional      score      function      V,,,      log      dejo      (Xe   \\n \\n|      xo)      can      be      analytically      derived      as  \\n \\n  °</p><h5>      Myo      —      Ut</h5><p> \\n \\n  {ory      ue   \\n \\n=      exp(—0.5t)xo,      {o?\\n}?\\n \\n \\n=   \\n \\n1   \\n \\n—      exp(—0.5t).\\n(3) t Subsequently,      by      defining   \\n \\na      parameterized      model      sg   \\n \\n:      R¢   \\n \\nx      [0,7]   \\n \\n>      R¢      and      utilizing      the      following weighted      regression,      the      marginal      score      is      estimated:</p><p>A\\n \\n \\n  Dore   \\n \\n=      argmin      Eywuni((0,7))      ce~aejo(w+|ar0),c0~pPr®      [A(t)      ||      Ver,      log      jo      (Le   \\n \\n|      ro)   \\n \\n_      So(£t,      t)      17)      (4)</p><p> \\n \\n  where   \\n \\n2   \\n \\n:      [0,7]   \\n \\n+      R      is   \\n \\na      weighted      function.\\n \\n \\n  Note      we      typically      use      another      parametrization      well.\\nFrom      (3),      we      can      easily      see      that      this      score      is  \\n \\n  estimated      by      introducing      networks      €9   \\n \\n:      R@   \\n \\nx      [0,7]   \\n \\n—      R®,      which      aims      to      estimate      the      noise      Bere      eO</p><p>Ore   \\n \\n=      argmin      Ey.\\nuni({0,7]),cr~p2xo+o?\\ne~N      (0,1)      ,co~pPte      [A(t)/{op}?\\nlle   \\n \\n—      €9      (1,      t)      7)   \\n \\n:      (5)</p><p>Then,      by      denoting      the      training      data      as      {a}      and      setting      A(t)   \\n \\n=      {a7      }”,      the      actual      loss      function      from the      training      data      is argmin   \\n \\n)      Ey      ai(o.r))a   \\n \\nO      peal      +e02,e~N      (0,1)      [Ile   \\n \\n—      €o(ae?,      t)      [17].\\n(6) i=1 Inference      time.\\nOnce      this      Bore      (or      Ore)      is      learned,      we      insert      it      into      the      time-reversal      SDE      (2)      and use      it      as   \\n \\na      generative      model.\\nUltimately,      with      standard      discretization,      we      obtain:</p><p>p(      xe,      t;      a)   \\n \\n=      24+      (0.54,   \\n \\n+      Sire      (,,T      —t)|(ot),      {o?(t)}?\\n \\n \\n=      (6t).</p><p>Equivalently,      this      is (21,      t;      Ore)   \\n \\n=   \\n \\n2   \\n \\n+      [0.52   \\n \\n—      1/o?\\n \\n \\nx      e%,,,(a1,T   \\n \\n—      t)]|      (6t),      (7)</p><p>where      (dt)      denotes      the      discretization      step.</p><p> \\n \\n  Equivalence      to      DDPM.\\nThe      objective      function      derived      here      is      equivalent      to      the      one      formulated  \\n \\n  based      on      variational      inference      in      the      discretized      formulation,      which      is      commonly      referred      to      as  \\n \\n  DDPMs      (Ho      et      al.,      2020).\\nIn      DDPMs      (Ho      et      al.,      2020),      we      often      see      the      following      form:</p><p> \\n \\n \\n1      1-      (ey;</p><p> \\n \\n  it;      0)   \\n \\n=      ~~   \\n \\n’      ),  \\n \\n  p(xy   \\n \\n)      Xt      Jina      ae      [om</p><h6>Vor</h6><p>When      a;   \\n \\n=   \\n \\n1   \\n \\n—      (dt),      the      above      is      equivalent      to      (7)      when      dt      goes      to   \\n \\n0      by      noting      1/,/a;   \\n \\n©      1+0.5(d¢).</p><h3>1.2      Fine-Tuning      Diffusion      Models      with      RL</h3><p>RL  \\n \\n  Importantly,      our      focus      on      RL-based      fine-tuning      distinguishes      itself      from      the      standard      fine-tuning  \\n \\n  methods.\\nStandard      fine-tuning      typically      involves      scenarios      where      we      have      pre-trained      models  \\n \\n  (e.g.,      diffusion      models)      and      new      training      data      {2,      y}.\\nIn      such      cases,      the      common      approach  \\n \\n  for      fine-tuning      is      to      retrain      diffusion      models      with      the      new      training      data      using      the      same      loss  \\n \\n  function      employed      during      pre-training.\\nIn      sharp      contrast,      RL-based      fine-tuning      directly      employs  \\n \\n  the      downstream      reward      functions      as      the      primary      optimization      objectives,      making      the      loss      functions  \\n \\n  different      from      those      used      in      pre-training.</p><p> \\n \\n  Hereafter,      we      start      with   \\n \\na      concise      overview      of      RL-based      fine-tuning.\\nThen,      before      delving      into  \\n \\n  specifics,      we      discuss      simpler      non-RL      alternatives      to      provide      motivation      for      adopting      RL-based  \\n \\n  fine-tuning.</p><li>1.2.1      Brief      Overview:      Fine-tuning      with      RL</li><p>In      this      article,      we      explore      the      fine-tuning      of      pre-trained      diffusion      models      to      optimize      downstream  \\n \\n  reward      functions      r   \\n \\n:      R¢   \\n \\n+      R.      In      domains      such      as      images,      these      backbone      diffusion      models      to      be  \\n \\n  fine-tuned      include      Stable      Diffusion      (Rombach      et      al.,      2022),      while      the      reward      functions      are      aesthetic  \\n \\n  scores      and      alignment      scores      (Clark      et      al.,      2023;      Black      et      al.,      2023;      Fan      et      al.,      2023).\\nMore      examples  \\n \\n  are      detailed      in      the      introduction.\\nThese      rewards      are      often      unknown,      necessitating      learning      from  \\n \\n  data      with      feedback:      {2      r(x™)}.\\nWe      will      explore      this      aspect      further      in      Section      7.\\nUntil      then,      we  \\n \\n  assume      r      is      known.\\n \\n \\n  Now,      readers      may      wonder      about      the      objectives      we      aim      to      achieve      during      the      fine-tuning      process.\\n \\n \\n  A      natural      approach      is      to      define      the      optimization      problem:</p><p>argmax      E,.\\n[r()|      (8)  \\n \\n  qeA(¥)  \\n \\n  where   \\n \\nq      is      initialized      with   \\n \\na      pre-trained      diffusion      model      p?\\n*®   \\n \\n€      A(2).\\nIn      this      tutorial,      we      will      detail  \\n \\n  the      procedure      of      solving      (8)      with      RL      in      the      upcoming      sections.\\nIn      essence,      we      leverage      the      fact  \\n \\n  that      diffusion      models      are      formulated      as   \\n \\na      sequential      decision-making      problem,      where      each      decision  \\n \\n  corresponds      to      how      samples      are      denoised.</p><p> \\n \\n  Although      the      above      objective      function      (8)      is      reasonable,      the      resulting      distribution      might      deviate  \\n \\n  too      much      from      the      pre-trained      diffusion      model.\\nTo      circumvent      this      issue,   \\n \\na      natural      way      is      to      add  \\n \\n  penalization      against      pre-trained      diffusion      models.\\nThen,      the      target      distribution      is      defined      as:</p><p>argmax      E,.4[r(x)]   \\n \\n—      aKL(q||p’*).\\n(9)  \\n \\n  qeA(¥) Notably,      (9)      reduces      to      the      following      distribution:</p><li>(10)</li><p>5\\n \\n   exp(r()/a)p\"\"()  \\n \\n  Pr)   \\n \\n=      Fra)      apa)</p><p> \\n \\n  Here,      the      first      term      in      (9)      corresponds      to      the      mean      reward,      which      we      want      to      optimize      in      the  \\n \\n  fine-tuning      process.\\nThe      second      term      in      (10)      serves      as   \\n \\na      penalty      term,      indicating      the      deviation      of   \\n \\nq       from      the      pre-trained      model.\\nThe      parameter   \\n \\na      controls      the      strength      of      this      regularization      term.\\nThe  \\n \\n  proper      choice      of   \\n \\na      depends      on      the      task      we      are      interested      in.</p><h6>1.2.2      Motivation      for      Using      RL      over      Non-RL      Alternatives</h6><p>ives  \\n \\n  To      achieve      our      goal      of      maximizing      downstream      reward      functions      with      diffusion      models,      readers  \\n \\n  may      question      whether      alternative      approaches      can      be      employed      apart      from      RL.\\nHere,      we      investigate  \\n \\n  these      potential      alternatives      and      explain      why      RL      approaches      may      offer      advantages      over      them.</p><p> \\n \\n  Rejection      sampling.\\nOne      approach      involves      generating      multiple      samples      from      pre-trained      diffu-  \\n \\n  sion      models      and      selecting      only      those      with      high      rewards.\\nThis      method,      called      rejection      sampling,  \\n \\n  operates      without      needing      fine-tuning.\\nHowever,      rejection      sampling      is      effective      primarily      when      the  \\n \\n  pre-trained      model      already      has   \\n \\na      high      probability      of      producing      high-reward      samples.\\nIt      resembles  \\n \\n  sampling      from   \\n \\na      prior      distribution      to      obtain      posterior      samples      (in      this      case,      high-reward      points).\\n \\n \\n  This      approach      works      efficiently      when      the      posterior      closely      matches      the      prior      but      can      become      highly  \\n \\n  inefficient      otherwise.\\nIn      contrast,      by      explicitly      updating      weight      in      diffusion      models,      RL-based      fine-  \\n \\n  tuning      allows      us      to      obtain      these      high-reward      samples,      which      are      seldom      generated      by      pre-trained  \\n \\n  models.</p><p> \\n \\n  Conditional      diffusion      models      (classifier-free      guidance).\\nIn      conditional      generative      models,      the  \\n \\n  general      goal      is      to      sample      from      p(|c),      where   \\n \\nx      is      the      output      and   \\n \\nc      denotes      the      conditioning      variable.\\n \\n \\n  For      example,      in      text-to-language      diffusion      models,   \\n \\nc      is   \\n \\na      text,      and   \\n \\nx      is      the      generated      image.\\nSimilarly,  \\n \\n  in      the      context      of      protein      engineering      for      addressing      inverse      folding      problems,      models      often      define   \\n \\nc       as      the      protein      backbone      structure      and   \\n \\nx      as      the      corresponding      amino      acid      sequence.\\nHere,      using      the  \\n \\n  training      data      {c,      ¢},      the      model      is      trained      by      using      the      loss      function:</p><p>argmin   \\n \\n»      E,uni(l0,7)2)      ~upa      +o?\\nsewN\\\\\\\\\\\\\\'(O)      [lle   \\n \\n—      co(2”,      }),      t)      I\"), t where      the      denoising      function      €g      additionally      receives      the      conditioning      information   \\n \\nc      as      input.\\nIn  \\n \\n  practice,   \\n \\na      variety      of      improvements      such      as      classifier-free      guidance      (Ho      and      Salimans,      2022)      can  \\n \\n  further      improve      the      model’s      ability      to      learn      the      conditional      distribution      p(|c).</p><p> \\n \\n  These      conditional      generative      models      can      be      used      to      optimize      down-stream      rewards      by      condi-  \\n \\n  tioning      on      the      reward      values,      then      sampling   \\n \\nx      conditioned      on      high      reward      values      (Krishnamoorthy  \\n \\n  et      al.,      2023;      Yuan      et      al.,      2023).\\nWhile      this      method      is,      in      principle,      capable      of      generating      plausible x\\n \\n   values      across   \\n \\na      range      of      reward      levels      within      the      training      data      distribution,      it      is      not      the      most  \\n \\n  effective      optimization      strategy.\\nThis      is      primarily      because      high-reward      inputs      frequently      reside  \\n \\n  in      the      tails      of      the      training      distribution      or      even      beyond      it.\\nConsequently,      this      method      may      not  \\n \\n  effectively      generate      high-reward      samples      that      lie      outside      the      training      data      distribution.\\nIn      contrast,  \\n \\n  RL-based      fine-tuning      has      the      capability      to      generate      samples      with      higher      rewards      beyond      the      training  \\n \\n  data.\\nThis      is      achieved      by      explicitly      maximizing      reward      models      learned      from      the      training      data  \\n \\n  and      leveraging      their      extrapolative      capabilities      of      reward      models,      as      theoretically      formalized      and  \\n \\n  empirically      observed      in      Uehara      et      al.\\n(2024).</p><p>Reward-weighted      training.\\nAnother      alternative      approach      is      to      use   \\n \\na      reward-weighted      version  \\n \\n  of      the      standard      training      loss      for      diffusion      models.\\nSuppose      that      we      have      data      {x      r(a)}.\\nThen, after      learning   \\n \\na      reward   \\n \\n7   \\n \\n:   \\n \\n¥   \\n \\n—      R      with      regression      from      the      data,      to      achieve      our      goal,      it      looks  \\n \\n  natural      to      use   \\n \\na      reward-weighted      version      of      the      training      loss      for      diffusion      models      (5),      1.e.,.\\na7      (4)      i)   \\n \\n2 areaun   \\n \\n»      E,uni((0,2))      peel?\\n+eo¢,enncoal      (to      ile   \\n \\n—      co(ay?,t)      |\") i\\n \\n    There      are      two      potential      drawbacks      to      this      approach.\\nFirst,      in      practice,      it      may      struggle      to      generate  \\n \\n  samples      with      higher      rewards      beyond      the      training      data.\\nAs      we      will      explain      later,      many      RL      algorithms  \\n \\n  are      more      directly      focused      on      optimizing      reward      functions,      which      are      expected      to      excel      in      obtaining  \\n \\n  samples      with      high      rewards      not      observed      in      the      original      data,      as      empirically      observed      in      Black      et      al.\\n \\n \\n  (2023).\\nSecond,      when      fine-tuning   \\n \\na      conditional      diffusion      model      p(x|c),      the      alternative      approach  \\n \\n  here      requires   \\n \\na      pair      of      {c,      «}      during      fine-tuning      to      ensure      the      validity      of      the      loss      function.\\nWhen  \\n \\n  we      only      have      data      {2      r(x      )}      but      not      {c,      2,      r(a)},      this      implies      that      we      might      need      to      solve an      inverse      problem      from   \\n \\nz      to      c,      which      can      often      be      challenging.\\nIn      contrast,      in      these      scenarios,      RL  \\n \\n  algorithms,      which      we      will      introduce      later,      can      operate      without      needing      such      pairs      {c,      x},      as  \\n \\n  long      as      we      have      learned      reward      functions      7.  \\n \\n  Finally,      it      should      be      noted      that      reward-weighted      training      technically      falls      under      the      broader  \\n \\n  category      of      RL      methods.\\nIt      shares   \\n \\na      close      connection      with      “reward-weighted      MLE”      introduced      in  \\n \\n  Section      6.1,      as      discussed      later.\\nEmploying      this      reward-weighted      MLE      helps      address      the      second  \\n \\n  concern      of      “reward-weighted      training”      mentioned      earlier.</p><p>2\\n \\n   Brief      Overview      of      Entropy-Regularized      MDPs</p><p> \\n \\n  In      this      tutorial,      we      explain      how      fine-tuning      diffusion      models      can      be      naturally      formulated      as      an      RL  \\n \\n  problem      in      entropy-regularized      MDPs.\\nThis      perspective      is      natural      because      RL      involves      sequential  \\n \\n  decision-making,      and   \\n \\na      diffusion      model      is      formulated      as   \\n \\na      sequential      problem      where      each      denoising  \\n \\n  step      is   \\n \\na      decision-making      process.\\nTo      connect      diffusion      models      with      RL,      we      begin      with   \\n \\na      concise  \\n \\n  overview      of      RL      in      standard      entropy-regularized      MDPs      (Haarnoja      et      al.,      2017;      Neu      et      al.,      2017;</p><p> \\n \\n  Geist      et      al.,      2019;      Schulman      et      al.,      2017).</p><li>2.1      MDPs</li><p> \\n \\n  An      MDP      is      defined      as      follows:      {S,A,      {P\"*}/_5,      {r:}-9,      po}      where      S      is      the      state      space,   \\n \\nA      is      the  \\n \\n  action      space,      P;\"*      is   \\n \\na      transition      dynamic      mapping:      S   \\n \\nx   \\n \\nA   \\n \\n—      A(S),      7,   \\n \\n:      S   \\n \\nx   \\n \\nA   \\n \\n>      R      denotes  \\n \\n  reward      received      at   \\n \\nt      and      po      is      an      initial      distribution      over      S.   \\n \\nA      policy      7,   \\n \\n:      S   \\n \\n—      A(A)      is   \\n \\na      map      from  \\n \\n  any      state      s   \\n \\n€      S      to      the      distribution      over      actions.\\nThe      standard      goal      in      RL      is      to      solve</p><h2>Images</h2><p> \\n \\n \\ng   \\n \\n|   \\n \\na      ge   \\n \\nz       evi      do}      f      g,   \\n \\n2      3°      DNAs      with      high  \\n \\n  aor   \\n \\na   \\n \\n-      e      e       DNAs      io      si   \\n \\ni      og   \\n \\n+   \\n \\na      i:      cell-specificity</p><p>Figure      1:      Illustrative      examples      of      RL-based      fine-tuning,      aimed      at      optimizing      pre-trained      diffusion  \\n \\n  models      to      maximize      downstream      reward      functions.</p><p> \\n \\n  2023),      chemistry      (Jo      et      al.,      2022;      Xu      et      al.,      2022;      Hoogeboom      et      al.,      2022),      and      biology      (Avdeyev  \\n \\n  et      al.,      2023;      Stark      et      al.,      2024;      Campbell      et      al.,      2024).</p><p> \\n \\n  While      diffusion      models      exhibit      significant      power      in      capturing      the      training      data      distribution,  \\n \\n  there’s      often   \\n \\na      need      to      customize      these      models      for      particular      downstream      reward      functions.\\nFor  \\n \\n  instance,      in      computer      vision,      Stable      Diffusion      (Rombach      et      al.,      2022)      serves      as   \\n \\na      strong      backbone  \\n \\n  pre-trained      model.\\nHowever,      we      may      want      to      fine-tune      it      further      by      optimizing      downstream      reward  \\n \\n  functions      such      as      aesthetic      scores      or      human-alignment      scores      (Black      et      al.,      2023;      Fan      et      al.,      2023).\\n \\n \\n  Similarly,      in      fields      such      as      biology      and      chemistry,      various      sophisticated      diffusion      models      have  \\n \\n  been      developed      for      DNA,      RNA,      protein      sequences,      and      molecules,      effectively      modeling      biological  \\n \\n  and      chemical      spaces.\\nNonetheless,      biologists      and      chemists      typically      aim      to      optimize      specific  \\n \\n  downstream      objectives      such      as      cell-specific      expression      in      DNA      sequences      (Gosai      et      al.,      2023;      Lal  \\n \\n  et      al.,      2024;      Sarkar      et      al.,      2024),      translational      efficiency/stability      of      RNA      sequences      (Castillo-Hair  \\n \\n  and      Seelig,      2021;      Agarwal      and      Kelley,      2022),      stability/bioactivity      of      protein      sequence      (Frey      et      al.,  \\n \\n  2023;      Widatalla      et      al.,      2024)      or      QED/SA      scores      of      molecules      (Zhou      et      al.,      2019).</p><p> \\n \\n  To      achieve      this      goal,      numerous      algorithms      have      been      proposed      for      fine-tuning      diffusion      models  \\n \\n  via      reinforcement      learning      (RL)      (e.g.,      Black      et      al.      (2023);      Fan      et      al.\\n(2023);      Clark      et      al.\\n(2023);  \\n \\n  Prabhudesai      et      al.\\n(2023);      Uehara      et      al.\\n(2024)),      aiming      to      optimize      downstream      reward      functions.\\n \\n \\n  RL      is   \\n \\na      machine      learning      paradigm      where      agents      learn      to      make      sequential      decisions      to      maximize  \\n \\n  reward      signals      (Sutton      and      Barto,      2018;      Agarwal      et      al.,      2019).\\nIn      our      context,      RL      naturally      emerges  \\n \\n  as   \\n \\na      Suitable      approach      due      to      the      sequential      structure      inherent      in      diffusion      models,      where      each      time  \\n \\n  step      involves   \\n \\na      “decision”      corresponding      to      how      the      sample      is      denoised      at      that      step.\\nThis      tutorial  \\n \\n  aims      to      review      recent      works      for      readers      interested      in      understanding      the      fundamentals      of      RL-based  \\n \\n  fine-tuning      from   \\n \\na      holistic      perspective,      including      the      advantages      of      RL-based      fine-tuning      over  \\n \\n  non-RL      approaches,      the      pros      and      cons      of      different      RL-based      fine-tuning      algorithms,      the      formalized  \\n \\n  goal      of      RL-based      fine-tuning,      and      its      connections      with      related      topics      such      as      classifier      guidance.</p><p> \\n \\n  The      content      of      this      tutorial      is      primarily      divided      into      three      parts.\\nIn      addition,      as      an      implementation  \\n \\n  example,      we      also      release      the      code      that      employs      RL-based      fine-tuning      for      guided      biological      sequences  \\n \\n  (DNA/RNA)      generation      at      https://github.com/masa-ue/RLfinetuning_Diffusion_Bioseq.</p><li>1.      We      aim      to      provide   \\n \\na      comprehensive      overview      of      current      algorithms.\\nNotably,      given      the  \\n \\n  sequential      nature      of      diffusion      models,      we      can      naturally      frame      fine-tuning      as   \\n \\na      reinforcement  \\n \\n  learning      (RL)      problem      within      Markov      Decision      Processes      (MDPs),      as      detailed      in      Section   \\n \\n3       and      4.\\nTherefore,      we      can      employ      any      off-the-shelf      RL      algorithms      such      as      PPO      (Schulman  \\n \\n  et      al.,      2017),      differentiable      optimization      (direct      reward      backpropagation),      weighted      MLE  \\n \\n  (Peters      et      al.,      2010;      Peng      et      al.,      2019),      value-weighted      sampling      (close      to      classifier      guidance  \\n \\n  in      Dhariwal      and      Nichol      (2021)),      and      path      consistency      learning      (Nachum      et      al.,      2017).\\nWe  \\n \\n  discuss      these      algorithms      in      detail      in      Section      4.2      and      6.\\nInstead      of      merely      outlining      each  \\n \\n  algorithm,      we      aim      to      present      both      their      advantages      and      disadvantages      so      readers      can      select  \\n \\n  the      most      suitable      algorithms      for      their      specific      purposes.</li><li>2.      We      categorize      various      fine-tuning      scenarios      based      on      how      reward      feedback      is      acquired      in  \\n \\n  Section      7.\\nThis      distinction      is      pivotal      for      practical      algorithm      design.\\nFor      example,      if      we      can  \\n \\n  access      accurate      reward      functions,      computational      efficiency      would      become      our      primary      focus.\\n \\n \\n  However,      in      cases      where      reward      functions      are      unknown,      it      is      essential      to      learn      them      from  \\n \\n  data      with      reward      feedback,      leading      us      to      take      feedback      efficiency      and      distributional      shift      into  \\n \\n  consideration      as      well.\\nSpecifically,      when      reward      functions      need      to      be      learned      from      static  \\n \\n  offline      data      without      any      online      interactions,      we      must      address      the      issue      of      overoptimization,  \\n \\n  where      fine-tuned      models      are      misled      by      out-of-distribution      samples,      and      generate      samples  \\n \\n  with      low      genuine      rewards.\\nThis      is      crucial      because,      in      an      offline      scenario,      the      coverage      of  \\n \\n  offline      data      distribution      with      feedback      is      limited;      hence,      the      out-of-distribution      region      could  \\n \\n  be      extensive      (Uehara      et      al.,      2024).</li><li>3.      We      provide   \\n \\na      detailed      discussion      on      the      relationship      between      RL-based      fine-tuning      methods  \\n \\n  and      closely      related      methods      in      the      literature,      such      as      classifier      guidance      (Dhariwal      and      Nichol,  \\n \\n  2021)      in      Section      8,      flow-based      diffusion      models      (Liu      et      al.,      2022;      Lipman      et      al.,      2023;      Tong  \\n \\n  et      al.,      2023)      in      Section      9,      sampling      from      unnormalized      distributions      (Zhang      and      Chen,      2021)  \\n \\n  in      Section      10,      Gflownets      (Bengio      et      al.,      2023)      in      Section      6.3,      and      path      integral      control      theory  \\n \\n  (Theodorou      et      al.,      2010;      Williams      et      al.,      2017;      Kazim      et      al.,      2024)      in      Section      6.2.3.      We  \\n \\n  summarize      the      key      messages      as      follows.</li><p>¢\\n \\n   Section      6.3:      The      losses      used      in      Gflownets      are      fundamentally      equivalent      to      those      derived  \\n \\n  from   \\n \\na      specific      RL      algorithm      called      path      consistency      learning.</p><p> \\n \\n \\n¢      Section      8:      Classifier      guidance      employed      in      conditional      generation      is      regarded      as   \\n \\na       specific      RL-based      fine-tuning      method,      which      we      call      value-weighted      sampling.\\nAs  \\n \\n  formalized      in      Zhao      et      al.\\n(2024),      this      observation      indicates      that      any      off-the-shelf      RL-  \\n \\n  based      fine-tuning      algorithms      (e.g.,      PPO      and      differentiable      optimization)      can      be      applied  \\n \\n  to      conditional      generation.</p><p> \\n \\n \\n¢      Section      10:      Sampling      from      unnormalized      distributions,      often      referred      to      as      Gibbs  \\n \\n  distributions,      is      an      important      and      challenging      problem      in      diverse      domains.\\nWhile  \\n \\n  MCMC      methods      are      traditionally      used      for      this      task,      recognizing      its      similarity      to      the  \\n \\n  objectives      of      RL-based      fine-tuning      suggests      that      off-the-shelf      RL      algorithms      can      also  \\n \\n  effectively      address      the      challenge      of      sampling      from      unnormalized      distributions.</p><h2>Preliminaries</h2><li>1.1 DiffusionModels 2 2.20.\\n00 2 ee ee</li><table><th><td colSpan=1>1.1.1 Score-Based Diffusion Models (Optional)</td><td colSpan=1></td><td colSpan=1></td></th><tr><td colSpan=1>1.2 Fine-Tuning Diffusion Models withRL</td><td colSpan=1>0.00.0.</td><td colSpan=1></td></tr><tr><td colSpan=1>1.2.1. Brief Overview: Fine-tuning withRL 1.2.2 Motivation for Using RL over Non-RL Alternatives.</td><td colSpan=1></td><td colSpan=1>2.</td></tr><tr><td>Brief      Overview      of      Entropy-Regularized      MDPs</td></tr><tr><td>Fine-Tuning      Diffusion      Models      with      RL      in      Entropy      Regularized      MDPs</td></tr><tr><td>Theory      of      RL-Based      Fine-Tuning</td></tr></table><h2>RL-Based      Fine-Tuning      Algorithms      1:      Non-Distribution-Constrained      Approaches</h2><h2>RL-Based      Fine-Tuning      Algorithms      2:      Distribution-Constrained      Approaches</h2><h2>Fine-Tuning      Settings      Taxonomy</h2><h2>Connection      with      Classifier      Guidance</h2><p> \\n \\n  Connection      with      Flow-Based      Diffusion      Models  \\n \\n  10  \\n \\n  10  \\n \\n  11 12  \\n \\n  14  \\n \\n  14  \\n \\n  15  \\n \\n  16  \\n \\n  16  \\n \\n  17  \\n \\n  18  \\n \\n  19  \\n \\n  21  \\n \\n  21  \\n \\n  23  \\n \\n  24  \\n \\n  25  \\n \\n  25  \\n \\n  28  \\n \\n  28  \\n \\n  28</p><p>10      Connection      with      Sampling      from      Unnormalized      Distributions      35 10.1 Markov Chain Monte Carlo(MCMC) 2.00000 eee ene 35 10.2 RL-Based Approaches 0 2.0.0 pee ee 36</p><p>11      Closely      Related      Directions      36</p><p>12      Summary      37</p><p>1\\n \\n   Preliminaries</p><p>In      this      section,      we      outline      the      fundamentals      of      diffusion      models      and      elucidate      the      objective      of  \\n \\n  fine-tuning      them.</p><h3>1.1.      Diffusion      Models</h3><p>dels  \\n \\n  We      present      an      overview      of      denoising      diffusion      probabilistic      models      (DDPM)      (Ho      et      al.,      2020).\\n \\n \\n  For      more      details,      refer      to      Yang      et      al.\\n(2023);      Cao      et      al.\\n(2024);      Chen      et      al.\\n(2024);      Tang      and      Zhao  \\n \\n  (2024).</p><p> \\n \\n  In      diffusion      models,      the      objective      is      to      develop   \\n \\na      deep      generative      model      that      accurately      captures  \\n \\n  the      true      data      distribution.\\nSpecifically,      denoting      the      data      distribution      by      pr.\\n \\n \\n€      A(4’)      where   \\n \\n¥      is      an  \\n \\n  input      space,   \\n \\na      DDPM      aims      to      approximate      p,,.\\nusing   \\n \\na      parametric      model      structured      as 1 p(xo3      8)   \\n \\n=   \\n \\n[      rlco      O)dx1.r,      where      p(%o:7;      9)   \\n \\n=      pr+i(@7;      9)      [[      peter:      0).</p><p>t=T When   \\n \\n4      is      an      Euclidean      space      (in      R®),      the      forward      process      is      modeled      as      the      following      dynamics:</p><p>pra(er)      =N(0,1),      pe(ve-r)2;      9)   \\n \\n=      N(p(a1,      t;      8),      07      (4)   \\n \\nx      D),  \\n \\n  where      N(-,-)      denotes   \\n \\na      normal      distribution,   \\n \\nJ      is      an      identity      matrix      and      p   \\n \\n:      R¢   \\n \\nx      [0,7]   \\n \\n>      R¢.\\nIn  \\n \\n  DDPMs,      we      aim      to      obtain   \\n \\na      set      of      policies      (i.e.,      denoising      process)      {p;}{_741,      pr:   \\n \\n&   \\n \\n—      A(A&)      such  \\n \\n  that      p(x;      0)   \\n \\n©      Ppre(#o).\\nIndeed,      by      optimizing      the      variational      bound      on      the      negative      log-likelihood,  \\n \\n  we      can      derive      such   \\n \\na      set      of      policies.\\nFor      more      details,      refer      to      Section      1.1.1.  \\n \\n  Hereafter,      we      consider   \\n \\na      situation      where      we      have   \\n \\na      pre-trained      diffusion      model      that      is      already  \\n \\n  trained      on   \\n \\na      large      dataset,      such      that      the      model      can      accurately      capture      the      underlying      data      distribution.</p><p> \\n \\n  pre We      refer      to      the      pre-trained      policies      as      {p?\\n\"*(-|-)}{_7,,,      and      to      the      marginal      distribution      at   \\n \\nt   \\n \\n=   \\n \\n0       induced      by      the      pre-trained      diffusion      model      as      pp.\\nIn      other      words, 1\\n \\n    pr      (x0)   \\n \\n=      [iler)      [[°@eledderr.</p><p>t=T  \\n \\n  Remark   \\n \\n1      (Non-Euclidean      space).\\nFor      simplicity,      we      typically      assume      that      the      domain      space      is  \\n \\n  Euclidean.\\nHowever,      we      can      easily      extend      most      of      the      discussion      to   \\n \\na      more      general      space,      such      as  \\n \\n  a      Riemannian      manifold      (De      Bortoli      et      al.,      2022)      or      discrete      space      (Austin      et      al.,      2021;      Campbell  \\n \\n  et      al.,      2022;      Benton      et      al.,      2024;      Lou      et      al.,      2023).</p><p> \\n \\n  Remark   \\n \\n2      (Conditional      generative      models).\\nPre-trained      models      can      be      conditional      diffusion      models,  \\n \\n  such      as      text-to-image      diffusion      models      (Ramesh      et      al.,      2022).\\nThe      extension      is      straightforward:  \\n \\n  augmenting      the      input      spaces      of      policies      with      an      additional      space      on      which      we      want      to      condition.\\n \\n \\n  More      specifically,      by      denoting      that      space      by   \\n \\nc   \\n \\n€      C,      each      policy      becomes      p;(x¢|%1,      650)   \\n \\n:   \\n \\n&   \\n \\nx   \\n \\nC   \\n \\n>       A(X).</p><p> \\n \\n  Remark   \\n \\n3      (Extension      to      Continuous-time      diffusion      models).\\nJn      this      tutorial,      our      discussion      on  \\n \\n  fine-tuning      diffusion      models      will      be      primarily      formulated      on      the      discrete-time      formulation,      as      we      did  \\n \\n  above      Nonetheless,      much      of      our      discussion      is      also      applicable      to      continuous-time      diffusion      models,  \\n \\n  as      formalized      in      Uehara      et      al.\\n(2024)</p><h4>1.1.1      Score-Based      Diffusion      Models      (Optional)</h4><p>nal)  \\n \\n  We      briefly      discuss      how      to      train      diffusion      models      in   \\n \\na      continuous-time      framework      (Song      et      al.,  \\n \\n  2021).\\nIn      our      tutorial,      this      section      is      mainly      used      to      discuss      several      algorithms      (e.g.,      value-weighted  \\n \\n  sampling      in      Section      6.2)      and      their      relationship      with      flow-based      diffusion      models      (Section      9)      later.</p><p> \\n \\n  Therefore,      readers      may      skip      it      based      on      their      individual      needs.</p><p> \\n \\n  The      training      process      of      diffusion      models      can      be      summarized      as      follows.\\nOur      objective      is      to      train  \\n \\n  a      sequential      mapping      from   \\n \\na      known      noise      distribution      to   \\n \\na      data      distribution,      formalized      through  \\n \\n  a      stochastic      differential      equation      (SDE).\\nFirstly,      we      define   \\n \\na      forward      (fixed)      SDE      that      maps      from  \\n \\n  the      data      distribution      to      the      noise      distribution.\\nThen,   \\n \\na      time-reversal      SDE      is      expressed      as      an      SDE,  \\n \\n  which      includes      the      (unknown)      score      function.\\nNow,      by      learning      this      unknown      score      function      from  \\n \\n  the      training      data,      the      time-reversal      SDE      can      be      utilized      as   \\n \\na      generative      model.\\nHere      are      the      details.</p><p>Forward      and      time-reversal      SDE.\\nFirstly,      we      introduce   \\n \\na      forward      (a.k.a.,      reference)      Stochastic  \\n \\n  differential      equations      (SDE)      from   \\n \\n0      to      7’.\\n \\n \\nA      common      choice      is   \\n \\na      variance-preserving      (VP)      process:</p><p>t\\n \\n   €\\n \\n   [0,7];      dx,   \\n \\n=      —0.52,dt   \\n \\n+      du;,,      x9   \\n \\n~      p(x),      (1)</p><p>where      dw;      represents      standard      Brownian      motion.\\nHere      are      two      crucial      observations:</p><p>¢\\n \\n   As   \\n \\nT      approaches      oo,      the      limiting      distribution      is      \\\\\\\\\\\\\\\\V(0,      I).</p><p>¢\\n \\n   The      time-reversal      SDE      (Anderson,      1982),      which      preserves      the      marginal      distribution,      is  \\n \\n  expressed      as      follows:</p><p>t\\n \\n   €\\n \\n   [0,7];      dz,   \\n \\n=      [0.5z,   \\n \\n+      V      log      qr_s(%)|dt   \\n \\n+      du,.\\n(2) Here,      g,   \\n \\n€      A(R“)      denotes      the      marginal      distribution      at      time   \\n \\nt      induced      by      the      reference      SDE (1).\\nNotably,      the      marginal      distribution      of      zr_;      is      the      same      as      the      one      of      x;      induced      by      the  \\n \\n  reference      SDE.</p><p> \\n \\n  These      observations      suggest      that      with      sufficiently      large      T,      starting      from      V      (0,      I)      and      following      the  \\n \\n  time-reversal      SDE      (2)   \\n \\n,      we      are      able      to      sample      from      the      data      distribution      (i.e.,      p?\\n\"°)      at      the      terminal  \\n \\n  time      point      7’.</p><p>Training      score      functions      from      the      training      data.\\nNow,      the      remaining      question      is      how      to      estimate  \\n \\n  the      marginal      score      function      V      log      q(-)      in      (2).\\nThis      can      be      computed      using      the      following      principles:</p><p>°\\n \\n   We      have      V      log      (a+)   \\n \\n=      Exgxppre[Va,      log      qyo(ae   \\n \\n|      2o)],      where      q@jo      represents      the      conditional  \\n \\n  distribution      of      x;      given      Xo      induced      by      the      reference      SDE.</p><p>¢\\n \\n   The      conditional      score      function      V,,,      log      dejo      (Xe   \\n \\n|      xo)      can      be      analytically      derived      as  \\n \\n  °</p><h5>      Myo      —      Ut</h5><p> \\n \\n  {ory      ue   \\n \\n=      exp(—0.5t)xo,      {o?\\n}?\\n \\n \\n=   \\n \\n1   \\n \\n—      exp(—0.5t).\\n(3) t Subsequently,      by      defining   \\n \\na      parameterized      model      sg   \\n \\n:      R¢   \\n \\nx      [0,7]   \\n \\n>      R¢      and      utilizing      the      following weighted      regression,      the      marginal      score      is      estimated:</p><p>A\\n \\n \\n  Dore   \\n \\n=      argmin      Eywuni((0,7))      ce~aejo(w+|ar0),c0~pPr®      [A(t)      ||      Ver,      log      jo      (Le   \\n \\n|      ro)   \\n \\n_      So(£t,      t)      17)      (4)</p><p> \\n \\n  where   \\n \\n2   \\n \\n:      [0,7]   \\n \\n+      R      is   \\n \\na      weighted      function.\\n \\n \\n  Note      we      typically      use      another      parametrization      well.\\nFrom      (3),      we      can      easily      see      that      this      score      is  \\n \\n  estimated      by      introducing      networks      €9   \\n \\n:      R@   \\n \\nx      [0,7]   \\n \\n—      R®,      which      aims      to      estimate      the      noise      Bere      eO</p><p>Ore   \\n \\n=      argmin      Ey.\\nuni({0,7]),cr~p2xo+o?\\ne~N      (0,1)      ,co~pPte      [A(t)/{op}?\\nlle   \\n \\n—      €9      (1,      t)      7)   \\n \\n:      (5)</p><p>Then,      by      denoting      the      training      data      as      {a}      and      setting      A(t)   \\n \\n=      {a7      }”,      the      actual      loss      function      from the      training      data      is argmin   \\n \\n)      Ey      ai(o.r))a   \\n \\nO      peal      +e02,e~N      (0,1)      [Ile   \\n \\n—      €o(ae?,      t)      [17].\\n(6) i=1 Inference      time.\\nOnce      this      Bore      (or      Ore)      is      learned,      we      insert      it      into      the      time-reversal      SDE      (2)      and use      it      as   \\n \\na      generative      model.\\nUltimately,      with      standard      discretization,      we      obtain:</p><p>p(      xe,      t;      a)   \\n \\n=      24+      (0.54,   \\n \\n+      Sire      (,,T      —t)|(ot),      {o?(t)}?\\n \\n \\n=      (6t).</p><p>Equivalently,      this      is (21,      t;      Ore)   \\n \\n=   \\n \\n2   \\n \\n+      [0.52   \\n \\n—      1/o?\\n \\n \\nx      e%,,,(a1,T   \\n \\n—      t)]|      (6t),      (7)</p><p>where      (dt)      denotes      the      discretization      step.</p><p> \\n \\n  Equivalence      to      DDPM.\\nThe      objective      function      derived      here      is      equivalent      to      the      one      formulated  \\n \\n  based      on      variational      inference      in      the      discretized      formulation,      which      is      commonly      referred      to      as  \\n \\n  DDPMs      (Ho      et      al.,      2020).\\nIn      DDPMs      (Ho      et      al.,      2020),      we      often      see      the      following      form:</p><p> \\n \\n \\n1      1-      (ey;</p><p> \\n \\n  it;      0)   \\n \\n=      ~~   \\n \\n’      ),  \\n \\n  p(xy   \\n \\n)      Xt      Jina      ae      [om</p><h6>Vor</h6><p>When      a;   \\n \\n=   \\n \\n1   \\n \\n—      (dt),      the      above      is      equivalent      to      (7)      when      dt      goes      to   \\n \\n0      by      noting      1/,/a;   \\n \\n©      1+0.5(d¢).</p><h3>1.2      Fine-Tuning      Diffusion      Models      with      RL</h3><p>RL  \\n \\n  Importantly,      our      focus      on      RL-based      fine-tuning      distinguishes      itself      from      the      standard      fine-tuning  \\n \\n  methods.\\nStandard      fine-tuning      typically      involves      scenarios      where      we      have      pre-trained      models  \\n \\n  (e.g.,      diffusion      models)      and      new      training      data      {2,      y}.\\nIn      such      cases,      the      common      approach  \\n \\n  for      fine-tuning      is      to      retrain      diffusion      models      with      the      new      training      data      using      the      same      loss  \\n \\n  function      employed      during      pre-training.\\nIn      sharp      contrast,      RL-based      fine-tuning      directly      employs  \\n \\n  the      downstream      reward      functions      as      the      primary      optimization      objectives,      making      the      loss      functions  \\n \\n  different      from      those      used      in      pre-training.</p><p> \\n \\n  Hereafter,      we      start      with   \\n \\na      concise      overview      of      RL-based      fine-tuning.\\nThen,      before      delving      into  \\n \\n  specifics,      we      discuss      simpler      non-RL      alternatives      to      provide      motivation      for      adopting      RL-based  \\n \\n  fine-tuning.</p><li>1.2.1      Brief      Overview:      Fine-tuning      with      RL</li><p>In      this      article,      we      explore      the      fine-tuning      of      pre-trained      diffusion      models      to      optimize      downstream  \\n \\n  reward      functions      r   \\n \\n:      R¢   \\n \\n+      R.      In      domains      such      as      images,      these      backbone      diffusion      models      to      be  \\n \\n  fine-tuned      include      Stable      Diffusion      (Rombach      et      al.,      2022),      while      the      reward      functions      are      aesthetic  \\n \\n  scores      and      alignment      scores      (Clark      et      al.,      2023;      Black      et      al.,      2023;      Fan      et      al.,      2023).\\nMore      examples  \\n \\n  are      detailed      in      the      introduction.\\nThese      rewards      are      often      unknown,      necessitating      learning      from  \\n \\n  data      with      feedback:      {2      r(x™)}.\\nWe      will      explore      this      aspect      further      in      Section      7.\\nUntil      then,      we  \\n \\n  assume      r      is      known.\\n \\n \\n  Now,      readers      may      wonder      about      the      objectives      we      aim      to      achieve      during      the      fine-tuning      process.\\n \\n \\n  A      natural      approach      is      to      define      the      optimization      problem:</p><p>argmax      E,.\\n[r()|      (8)  \\n \\n  qeA(¥)  \\n \\n  where   \\n \\nq      is      initialized      with   \\n \\na      pre-trained      diffusion      model      p?\\n*®   \\n \\n€      A(2).\\nIn      this      tutorial,      we      will      detail  \\n \\n  the      procedure      of      solving      (8)      with      RL      in      the      upcoming      sections.\\nIn      essence,      we      leverage      the      fact  \\n \\n  that      diffusion      models      are      formulated      as   \\n \\na      sequential      decision-making      problem,      where      each      decision  \\n \\n  corresponds      to      how      samples      are      denoised.</p><p> \\n \\n  Although      the      above      objective      function      (8)      is      reasonable,      the      resulting      distribution      might      deviate  \\n \\n  too      much      from      the      pre-trained      diffusion      model.\\nTo      circumvent      this      issue,   \\n \\na      natural      way      is      to      add  \\n \\n  penalization      against      pre-trained      diffusion      models.\\nThen,      the      target      distribution      is      defined      as:</p><p>argmax      E,.4[r(x)]   \\n \\n—      aKL(q||p’*).\\n(9)  \\n \\n  qeA(¥) Notably,      (9)      reduces      to      the      following      distribution:</p><li>(10)</li><p>5\\n \\n   exp(r()/a)p\"\"()  \\n \\n  Pr)   \\n \\n=      Fra)      apa)</p><p> \\n \\n  Here,      the      first      term      in      (9)      corresponds      to      the      mean      reward,      which      we      want      to      optimize      in      the  \\n \\n  fine-tuning      process.\\nThe      second      term      in      (10)      serves      as   \\n \\na      penalty      term,      indicating      the      deviation      of   \\n \\nq       from      the      pre-trained      model.\\nThe      parameter   \\n \\na      controls      the      strength      of      this      regularization      term.\\nThe  \\n \\n  proper      choice      of   \\n \\na      depends      on      the      task      we      are      interested      in.</p><h6>1.2.2      Motivation      for      Using      RL      over      Non-RL      Alternatives</h6><p>ives  \\n \\n  To      achieve      our      goal      of      maximizing      downstream      reward      functions      with      diffusion      models,      readers  \\n \\n  may      question      whether      alternative      approaches      can      be      employed      apart      from      RL.\\nHere,      we      investigate  \\n \\n  these      potential      alternatives      and      explain      why      RL      approaches      may      offer      advantages      over      them.</p><p> \\n \\n  Rejection      sampling.\\nOne      approach      involves      generating      multiple      samples      from      pre-trained      diffu-  \\n \\n  sion      models      and      selecting      only      those      with      high      rewards.\\nThis      method,      called      rejection      sampling,  \\n \\n  operates      without      needing      fine-tuning.\\nHowever,      rejection      sampling      is      effective      primarily      when      the  \\n \\n  pre-trained      model      already      has   \\n \\na      high      probability      of      producing      high-reward      samples.\\nIt      resembles  \\n \\n  sampling      from   \\n \\na      prior      distribution      to      obtain      posterior      samples      (in      this      case,      high-reward      points).\\n \\n \\n  This      approach      works      efficiently      when      the      posterior      closely      matches      the      prior      but      can      become      highly  \\n \\n  inefficient      otherwise.\\nIn      contrast,      by      explicitly      updating      weight      in      diffusion      models,      RL-based      fine-  \\n \\n  tuning      allows      us      to      obtain      these      high-reward      samples,      which      are      seldom      generated      by      pre-trained  \\n \\n  models.</p><p> \\n \\n  Conditional      diffusion      models      (classifier-free      guidance).\\nIn      conditional      generative      models,      the  \\n \\n  general      goal      is      to      sample      from      p(|c),      where   \\n \\nx      is      the      output      and   \\n \\nc      denotes      the      conditioning      variable.\\n \\n \\n  For      example,      in      text-to-language      diffusion      models,   \\n \\nc      is   \\n \\na      text,      and   \\n \\nx      is      the      generated      image.\\nSimilarly,  \\n \\n  in      the      context      of      protein      engineering      for      addressing      inverse      folding      problems,      models      often      define   \\n \\nc       as      the      protein      backbone      structure      and   \\n \\nx      as      the      corresponding      amino      acid      sequence.\\nHere,      using      the  \\n \\n  training      data      {c,      ¢},      the      model      is      trained      by      using      the      loss      function:</p><p>argmin   \\n \\n»      E,uni(l0,7)2)      ~upa      +o?\\nsewN\\\\\\\\\\\\\\'(O)      [lle   \\n \\n—      co(2”,      }),      t)      I\"), t where      the      denoising      function      €g      additionally      receives      the      conditioning      information   \\n \\nc      as      input.\\nIn  \\n \\n  practice,   \\n \\na      variety      of      improvements      such      as      classifier-free      guidance      (Ho      and      Salimans,      2022)      can  \\n \\n  further      improve      the      model’s      ability      to      learn      the      conditional      distribution      p(|c).</p><p> \\n \\n  These      conditional      generative      models      can      be      used      to      optimize      down-stream      rewards      by      condi-  \\n \\n  tioning      on      the      reward      values,      then      sampling   \\n \\nx      conditioned      on      high      reward      values      (Krishnamoorthy  \\n \\n  et      al.,      2023;      Yuan      et      al.,      2023).\\nWhile      this      method      is,      in      principle,      capable      of      generating      plausible x\\n \\n   values      across   \\n \\na      range      of      reward      levels      within      the      training      data      distribution,      it      is      not      the      most  \\n \\n  effective      optimization      strategy.\\nThis      is      primarily      because      high-reward      inputs      frequently      reside  \\n \\n  in      the      tails      of      the      training      distribution      or      even      beyond      it.\\nConsequently,      this      method      may      not  \\n \\n  effectively      generate      high-reward      samples      that      lie      outside      the      training      data      distribution.\\nIn      contrast,  \\n \\n  RL-based      fine-tuning      has      the      capability      to      generate      samples      with      higher      rewards      beyond      the      training  \\n \\n  data.\\nThis      is      achieved      by      explicitly      maximizing      reward      models      learned      from      the      training      data  \\n \\n  and      leveraging      their      extrapolative      capabilities      of      reward      models,      as      theoretically      formalized      and  \\n \\n  empirically      observed      in      Uehara      et      al.\\n(2024).</p><p>Reward-weighted      training.\\nAnother      alternative      approach      is      to      use   \\n \\na      reward-weighted      version  \\n \\n  of      the      standard      training      loss      for      diffusion      models.\\nSuppose      that      we      have      data      {x      r(a)}.\\nThen, after      learning   \\n \\na      reward   \\n \\n7   \\n \\n:   \\n \\n¥   \\n \\n—      R      with      regression      from      the      data,      to      achieve      our      goal,      it      looks  \\n \\n  natural      to      use   \\n \\na      reward-weighted      version      of      the      training      loss      for      diffusion      models      (5),      1.e.,.\\na7      (4)      i)   \\n \\n2 areaun   \\n \\n»      E,uni((0,2))      peel?\\n+eo¢,enncoal      (to      ile   \\n \\n—      co(ay?,t)      |\") i\\n \\n    There      are      two      potential      drawbacks      to      this      approach.\\nFirst,      in      practice,      it      may      struggle      to      generate  \\n \\n  samples      with      higher      rewards      beyond      the      training      data.\\nAs      we      will      explain      later,      many      RL      algorithms  \\n \\n  are      more      directly      focused      on      optimizing      reward      functions,      which      are      expected      to      excel      in      obtaining  \\n \\n  samples      with      high      rewards      not      observed      in      the      original      data,      as      empirically      observed      in      Black      et      al.\\n \\n \\n  (2023).\\nSecond,      when      fine-tuning   \\n \\na      conditional      diffusion      model      p(x|c),      the      alternative      approach  \\n \\n  here      requires   \\n \\na      pair      of      {c,      «}      during      fine-tuning      to      ensure      the      validity      of      the      loss      function.\\nWhen  \\n \\n  we      only      have      data      {2      r(x      )}      but      not      {c,      2,      r(a)},      this      implies      that      we      might      need      to      solve an      inverse      problem      from   \\n \\nz      to      c,      which      can      often      be      challenging.\\nIn      contrast,      in      these      scenarios,      RL  \\n \\n  algorithms,      which      we      will      introduce      later,      can      operate      without      needing      such      pairs      {c,      x},      as  \\n \\n  long      as      we      have      learned      reward      functions      7.  \\n \\n  Finally,      it      should      be      noted      that      reward-weighted      training      technically      falls      under      the      broader  \\n \\n  category      of      RL      methods.\\nIt      shares   \\n \\na      close      connection      with      “reward-weighted      MLE”      introduced      in  \\n \\n  Section      6.1,      as      discussed      later.\\nEmploying      this      reward-weighted      MLE      helps      address      the      second  \\n \\n  concern      of      “reward-weighted      training”      mentioned      earlier.</p><p>2\\n \\n   Brief      Overview      of      Entropy-Regularized      MDPs</p><p> \\n \\n  In      this      tutorial,      we      explain      how      fine-tuning      diffusion      models      can      be      naturally      formulated      as      an      RL  \\n \\n  problem      in      entropy-regularized      MDPs.\\nThis      perspective      is      natural      because      RL      involves      sequential  \\n \\n  decision-making,      and   \\n \\na      diffusion      model      is      formulated      as   \\n \\na      sequential      problem      where      each      denoising  \\n \\n  step      is   \\n \\na      decision-making      process.\\nTo      connect      diffusion      models      with      RL,      we      begin      with   \\n \\na      concise  \\n \\n  overview      of      RL      in      standard      entropy-regularized      MDPs      (Haarnoja      et      al.,      2017;      Neu      et      al.,      2017;</p><p> \\n \\n  Geist      et      al.,      2019;      Schulman      et      al.,      2017).</p><li>2.1      MDPs</li><p> \\n \\n  An      MDP      is      defined      as      follows:      {S,A,      {P\"*}/_5,      {r:}-9,      po}      where      S      is      the      state      space,   \\n \\nA      is      the  \\n \\n  action      space,      P;\"*      is   \\n \\na      transition      dynamic      mapping:      S   \\n \\nx   \\n \\nA   \\n \\n—      A(S),      7,   \\n \\n:      S   \\n \\nx   \\n \\nA   \\n \\n>      R      denotes  \\n \\n  reward      received      at   \\n \\nt      and      po      is      an      initial      distribution      over      S.   \\n \\nA      policy      7,   \\n \\n:      S   \\n \\n—      A(A)      is   \\n \\na      map      from  \\n \\n  any      state      s   \\n \\n€      S      to      the      distribution      over      actions.\\nThe      standard      goal      in      RL      is      to      solve</p><h3>1.1.      Diffusion      Models</h3><p>dels  \\n \\n  We      present      an      overview      of      denoising      diffusion      probabilistic      models      (DDPM)      (Ho      et      al.,      2020).\\n \\n \\n  For      more      details,      refer      to      Yang      et      al.\\n(2023);      Cao      et      al.\\n(2024);      Chen      et      al.\\n(2024);      Tang      and      Zhao  \\n \\n  (2024).</p><p> \\n \\n  In      diffusion      models,      the      objective      is      to      develop   \\n \\na      deep      generative      model      that      accurately      captures  \\n \\n  the      true      data      distribution.\\nSpecifically,      denoting      the      data      distribution      by      pr.\\n \\n \\n€      A(4’)      where   \\n \\n¥      is      an  \\n \\n  input      space,   \\n \\na      DDPM      aims      to      approximate      p,,.\\nusing   \\n \\na      parametric      model      structured      as 1 p(xo3      8)   \\n \\n=   \\n \\n[      rlco      O)dx1.r,      where      p(%o:7;      9)   \\n \\n=      pr+i(@7;      9)      [[      peter:      0).</p><p>t=T When   \\n \\n4      is      an      Euclidean      space      (in      R®),      the      forward      process      is      modeled      as      the      following      dynamics:</p><p>pra(er)      =N(0,1),      pe(ve-r)2;      9)   \\n \\n=      N(p(a1,      t;      8),      07      (4)   \\n \\nx      D),  \\n \\n  where      N(-,-)      denotes   \\n \\na      normal      distribution,   \\n \\nJ      is      an      identity      matrix      and      p   \\n \\n:      R¢   \\n \\nx      [0,7]   \\n \\n>      R¢.\\nIn  \\n \\n  DDPMs,      we      aim      to      obtain   \\n \\na      set      of      policies      (i.e.,      denoising      process)      {p;}{_741,      pr:   \\n \\n&   \\n \\n—      A(A&)      such  \\n \\n  that      p(x;      0)   \\n \\n©      Ppre(#o).\\nIndeed,      by      optimizing      the      variational      bound      on      the      negative      log-likelihood,  \\n \\n  we      can      derive      such   \\n \\na      set      of      policies.\\nFor      more      details,      refer      to      Section      1.1.1.  \\n \\n  Hereafter,      we      consider   \\n \\na      situation      where      we      have   \\n \\na      pre-trained      diffusion      model      that      is      already  \\n \\n  trained      on   \\n \\na      large      dataset,      such      that      the      model      can      accurately      capture      the      underlying      data      distribution.</p><p> \\n \\n  pre We      refer      to      the      pre-trained      policies      as      {p?\\n\"*(-|-)}{_7,,,      and      to      the      marginal      distribution      at   \\n \\nt   \\n \\n=   \\n \\n0       induced      by      the      pre-trained      diffusion      model      as      pp.\\nIn      other      words, 1\\n \\n    pr      (x0)   \\n \\n=      [iler)      [[°@eledderr.</p><p>t=T  \\n \\n  Remark   \\n \\n1      (Non-Euclidean      space).\\nFor      simplicity,      we      typically      assume      that      the      domain      space      is  \\n \\n  Euclidean.\\nHowever,      we      can      easily      extend      most      of      the      discussion      to   \\n \\na      more      general      space,      such      as  \\n \\n  a      Riemannian      manifold      (De      Bortoli      et      al.,      2022)      or      discrete      space      (Austin      et      al.,      2021;      Campbell  \\n \\n  et      al.,      2022;      Benton      et      al.,      2024;      Lou      et      al.,      2023).</p><p> \\n \\n  Remark   \\n \\n2      (Conditional      generative      models).\\nPre-trained      models      can      be      conditional      diffusion      models,  \\n \\n  such      as      text-to-image      diffusion      models      (Ramesh      et      al.,      2022).\\nThe      extension      is      straightforward:  \\n \\n  augmenting      the      input      spaces      of      policies      with      an      additional      space      on      which      we      want      to      condition.\\n \\n \\n  More      specifically,      by      denoting      that      space      by   \\n \\nc   \\n \\n€      C,      each      policy      becomes      p;(x¢|%1,      650)   \\n \\n:   \\n \\n&   \\n \\nx   \\n \\nC   \\n \\n>       A(X).</p><p> \\n \\n  Remark   \\n \\n3      (Extension      to      Continuous-time      diffusion      models).\\nJn      this      tutorial,      our      discussion      on  \\n \\n  fine-tuning      diffusion      models      will      be      primarily      formulated      on      the      discrete-time      formulation,      as      we      did  \\n \\n  above      Nonetheless,      much      of      our      discussion      is      also      applicable      to      continuous-time      diffusion      models,  \\n \\n  as      formalized      in      Uehara      et      al.\\n(2024)</p><h4>1.1.1      Score-Based      Diffusion      Models      (Optional)</h4><p>nal)  \\n \\n  We      briefly      discuss      how      to      train      diffusion      models      in   \\n \\na      continuous-time      framework      (Song      et      al.,  \\n \\n  2021).\\nIn      our      tutorial,      this      section      is      mainly      used      to      discuss      several      algorithms      (e.g.,      value-weighted  \\n \\n  sampling      in      Section      6.2)      and      their      relationship      with      flow-based      diffusion      models      (Section      9)      later.</p><p> \\n \\n  Therefore,      readers      may      skip      it      based      on      their      individual      needs.</p><p> \\n \\n  The      training      process      of      diffusion      models      can      be      summarized      as      follows.\\nOur      objective      is      to      train  \\n \\n  a      sequential      mapping      from   \\n \\na      known      noise      distribution      to   \\n \\na      data      distribution,      formalized      through  \\n \\n  a      stochastic      differential      equation      (SDE).\\nFirstly,      we      define   \\n \\na      forward      (fixed)      SDE      that      maps      from  \\n \\n  the      data      distribution      to      the      noise      distribution.\\nThen,   \\n \\na      time-reversal      SDE      is      expressed      as      an      SDE,  \\n \\n  which      includes      the      (unknown)      score      function.\\nNow,      by      learning      this      unknown      score      function      from  \\n \\n  the      training      data,      the      time-reversal      SDE      can      be      utilized      as   \\n \\na      generative      model.\\nHere      are      the      details.</p><p>Forward      and      time-reversal      SDE.\\nFirstly,      we      introduce   \\n \\na      forward      (a.k.a.,      reference)      Stochastic  \\n \\n  differential      equations      (SDE)      from   \\n \\n0      to      7’.\\n \\n \\nA      common      choice      is   \\n \\na      variance-preserving      (VP)      process:</p><p>t\\n \\n   €\\n \\n   [0,7];      dx,   \\n \\n=      —0.52,dt   \\n \\n+      du;,,      x9   \\n \\n~      p(x),      (1)</p><p>where      dw;      represents      standard      Brownian      motion.\\nHere      are      two      crucial      observations:</p><p>¢\\n \\n   As   \\n \\nT      approaches      oo,      the      limiting      distribution      is      \\\\\\\\\\\\\\\\V(0,      I).</p><p>¢\\n \\n   The      time-reversal      SDE      (Anderson,      1982),      which      preserves      the      marginal      distribution,      is  \\n \\n  expressed      as      follows:</p><p>t\\n \\n   €\\n \\n   [0,7];      dz,   \\n \\n=      [0.5z,   \\n \\n+      V      log      qr_s(%)|dt   \\n \\n+      du,.\\n(2) Here,      g,   \\n \\n€      A(R“)      denotes      the      marginal      distribution      at      time   \\n \\nt      induced      by      the      reference      SDE (1).\\nNotably,      the      marginal      distribution      of      zr_;      is      the      same      as      the      one      of      x;      induced      by      the  \\n \\n  reference      SDE.</p><p> \\n \\n  These      observations      suggest      that      with      sufficiently      large      T,      starting      from      V      (0,      I)      and      following      the  \\n \\n  time-reversal      SDE      (2)   \\n \\n,      we      are      able      to      sample      from      the      data      distribution      (i.e.,      p?\\n\"°)      at      the      terminal  \\n \\n  time      point      7’.</p><p>Training      score      functions      from      the      training      data.\\nNow,      the      remaining      question      is      how      to      estimate  \\n \\n  the      marginal      score      function      V      log      q(-)      in      (2).\\nThis      can      be      computed      using      the      following      principles:</p><p>°\\n \\n   We      have      V      log      (a+)   \\n \\n=      Exgxppre[Va,      log      qyo(ae   \\n \\n|      2o)],      where      q@jo      represents      the      conditional  \\n \\n  distribution      of      x;      given      Xo      induced      by      the      reference      SDE.</p><p>¢\\n \\n   The      conditional      score      function      V,,,      log      dejo      (Xe   \\n \\n|      xo)      can      be      analytically      derived      as  \\n \\n  °</p><h5>      Myo      —      Ut</h5><p> \\n \\n  {ory      ue   \\n \\n=      exp(—0.5t)xo,      {o?\\n}?\\n \\n \\n=   \\n \\n1   \\n \\n—      exp(—0.5t).\\n(3) t Subsequently,      by      defining   \\n \\na      parameterized      model      sg   \\n \\n:      R¢   \\n \\nx      [0,7]   \\n \\n>      R¢      and      utilizing      the      following weighted      regression,      the      marginal      score      is      estimated:</p><p>A\\n \\n \\n  Dore   \\n \\n=      argmin      Eywuni((0,7))      ce~aejo(w+|ar0),c0~pPr®      [A(t)      ||      Ver,      log      jo      (Le   \\n \\n|      ro)   \\n \\n_      So(£t,      t)      17)      (4)</p><p> \\n \\n  where   \\n \\n2   \\n \\n:      [0,7]   \\n \\n+      R      is   \\n \\na      weighted      function.\\n \\n \\n  Note      we      typically      use      another      parametrization      well.\\nFrom      (3),      we      can      easily      see      that      this      score      is  \\n \\n  estimated      by      introducing      networks      €9   \\n \\n:      R@   \\n \\nx      [0,7]   \\n \\n—      R®,      which      aims      to      estimate      the      noise      Bere      eO</p><p>Ore   \\n \\n=      argmin      Ey.\\nuni({0,7]),cr~p2xo+o?\\ne~N      (0,1)      ,co~pPte      [A(t)/{op}?\\nlle   \\n \\n—      €9      (1,      t)      7)   \\n \\n:      (5)</p><p>Then,      by      denoting      the      training      data      as      {a}      and      setting      A(t)   \\n \\n=      {a7      }”,      the      actual      loss      function      from the      training      data      is argmin   \\n \\n)      Ey      ai(o.r))a   \\n \\nO      peal      +e02,e~N      (0,1)      [Ile   \\n \\n—      €o(ae?,      t)      [17].\\n(6) i=1 Inference      time.\\nOnce      this      Bore      (or      Ore)      is      learned,      we      insert      it      into      the      time-reversal      SDE      (2)      and use      it      as   \\n \\na      generative      model.\\nUltimately,      with      standard      discretization,      we      obtain:</p><p>p(      xe,      t;      a)   \\n \\n=      24+      (0.54,   \\n \\n+      Sire      (,,T      —t)|(ot),      {o?(t)}?\\n \\n \\n=      (6t).</p><p>Equivalently,      this      is (21,      t;      Ore)   \\n \\n=   \\n \\n2   \\n \\n+      [0.52   \\n \\n—      1/o?\\n \\n \\nx      e%,,,(a1,T   \\n \\n—      t)]|      (6t),      (7)</p><p>where      (dt)      denotes      the      discretization      step.</p><p> \\n \\n  Equivalence      to      DDPM.\\nThe      objective      function      derived      here      is      equivalent      to      the      one      formulated  \\n \\n  based      on      variational      inference      in      the      discretized      formulation,      which      is      commonly      referred      to      as  \\n \\n  DDPMs      (Ho      et      al.,      2020).\\nIn      DDPMs      (Ho      et      al.,      2020),      we      often      see      the      following      form:</p><p> \\n \\n \\n1      1-      (ey;</p><p> \\n \\n  it;      0)   \\n \\n=      ~~   \\n \\n’      ),  \\n \\n  p(xy   \\n \\n)      Xt      Jina      ae      [om</p><h6>Vor</h6><p>When      a;   \\n \\n=   \\n \\n1   \\n \\n—      (dt),      the      above      is      equivalent      to      (7)      when      dt      goes      to   \\n \\n0      by      noting      1/,/a;   \\n \\n©      1+0.5(d¢).</p><h4>1.1.1      Score-Based      Diffusion      Models      (Optional)</h4><p>nal)  \\n \\n  We      briefly      discuss      how      to      train      diffusion      models      in   \\n \\na      continuous-time      framework      (Song      et      al.,  \\n \\n  2021).\\nIn      our      tutorial,      this      section      is      mainly      used      to      discuss      several      algorithms      (e.g.,      value-weighted  \\n \\n  sampling      in      Section      6.2)      and      their      relationship      with      flow-based      diffusion      models      (Section      9)      later.</p><p> \\n \\n  Therefore,      readers      may      skip      it      based      on      their      individual      needs.</p><p> \\n \\n  The      training      process      of      diffusion      models      can      be      summarized      as      follows.\\nOur      objective      is      to      train  \\n \\n  a      sequential      mapping      from   \\n \\na      known      noise      distribution      to   \\n \\na      data      distribution,      formalized      through  \\n \\n  a      stochastic      differential      equation      (SDE).\\nFirstly,      we      define   \\n \\na      forward      (fixed)      SDE      that      maps      from  \\n \\n  the      data      distribution      to      the      noise      distribution.\\nThen,   \\n \\na      time-reversal      SDE      is      expressed      as      an      SDE,  \\n \\n  which      includes      the      (unknown)      score      function.\\nNow,      by      learning      this      unknown      score      function      from  \\n \\n  the      training      data,      the      time-reversal      SDE      can      be      utilized      as   \\n \\na      generative      model.\\nHere      are      the      details.</p><p>Forward      and      time-reversal      SDE.\\nFirstly,      we      introduce   \\n \\na      forward      (a.k.a.,      reference)      Stochastic  \\n \\n  differential      equations      (SDE)      from   \\n \\n0      to      7’.\\n \\n \\nA      common      choice      is   \\n \\na      variance-preserving      (VP)      process:</p><p>t\\n \\n   €\\n \\n   [0,7];      dx,   \\n \\n=      —0.52,dt   \\n \\n+      du;,,      x9   \\n \\n~      p(x),      (1)</p><p>where      dw;      represents      standard      Brownian      motion.\\nHere      are      two      crucial      observations:</p><p>¢\\n \\n   As   \\n \\nT      approaches      oo,      the      limiting      distribution      is      \\\\\\\\\\\\\\\\V(0,      I).</p><p>¢\\n \\n   The      time-reversal      SDE      (Anderson,      1982),      which      preserves      the      marginal      distribution,      is  \\n \\n  expressed      as      follows:</p><p>t\\n \\n   €\\n \\n   [0,7];      dz,   \\n \\n=      [0.5z,   \\n \\n+      V      log      qr_s(%)|dt   \\n \\n+      du,.\\n(2) Here,      g,   \\n \\n€      A(R“)      denotes      the      marginal      distribution      at      time   \\n \\nt      induced      by      the      reference      SDE (1).\\nNotably,      the      marginal      distribution      of      zr_;      is      the      same      as      the      one      of      x;      induced      by      the  \\n \\n  reference      SDE.</p><p> \\n \\n  These      observations      suggest      that      with      sufficiently      large      T,      starting      from      V      (0,      I)      and      following      the  \\n \\n  time-reversal      SDE      (2)   \\n \\n,      we      are      able      to      sample      from      the      data      distribution      (i.e.,      p?\\n\"°)      at      the      terminal  \\n \\n  time      point      7’.</p><p>Training      score      functions      from      the      training      data.\\nNow,      the      remaining      question      is      how      to      estimate  \\n \\n  the      marginal      score      function      V      log      q(-)      in      (2).\\nThis      can      be      computed      using      the      following      principles:</p><p>°\\n \\n   We      have      V      log      (a+)   \\n \\n=      Exgxppre[Va,      log      qyo(ae   \\n \\n|      2o)],      where      q@jo      represents      the      conditional  \\n \\n  distribution      of      x;      given      Xo      induced      by      the      reference      SDE.</p><p>¢\\n \\n   The      conditional      score      function      V,,,      log      dejo      (Xe   \\n \\n|      xo)      can      be      analytically      derived      as  \\n \\n  °</p><h5>      Myo      —      Ut</h5><p> \\n \\n  {ory      ue   \\n \\n=      exp(—0.5t)xo,      {o?\\n}?\\n \\n \\n=   \\n \\n1   \\n \\n—      exp(—0.5t).\\n(3) t Subsequently,      by      defining   \\n \\na      parameterized      model      sg   \\n \\n:      R¢   \\n \\nx      [0,7]   \\n \\n>      R¢      and      utilizing      the      following weighted      regression,      the      marginal      score      is      estimated:</p><p>A\\n \\n \\n  Dore   \\n \\n=      argmin      Eywuni((0,7))      ce~aejo(w+|ar0),c0~pPr®      [A(t)      ||      Ver,      log      jo      (Le   \\n \\n|      ro)   \\n \\n_      So(£t,      t)      17)      (4)</p><p> \\n \\n  where   \\n \\n2   \\n \\n:      [0,7]   \\n \\n+      R      is   \\n \\na      weighted      function.\\n \\n \\n  Note      we      typically      use      another      parametrization      well.\\nFrom      (3),      we      can      easily      see      that      this      score      is  \\n \\n  estimated      by      introducing      networks      €9   \\n \\n:      R@   \\n \\nx      [0,7]   \\n \\n—      R®,      which      aims      to      estimate      the      noise      Bere      eO</p><p>Ore   \\n \\n=      argmin      Ey.\\nuni({0,7]),cr~p2xo+o?\\ne~N      (0,1)      ,co~pPte      [A(t)/{op}?\\nlle   \\n \\n—      €9      (1,      t)      7)   \\n \\n:      (5)</p><p>Then,      by      denoting      the      training      data      as      {a}      and      setting      A(t)   \\n \\n=      {a7      }”,      the      actual      loss      function      from the      training      data      is argmin   \\n \\n)      Ey      ai(o.r))a   \\n \\nO      peal      +e02,e~N      (0,1)      [Ile   \\n \\n—      €o(ae?,      t)      [17].\\n(6) i=1 Inference      time.\\nOnce      this      Bore      (or      Ore)      is      learned,      we      insert      it      into      the      time-reversal      SDE      (2)      and use      it      as   \\n \\na      generative      model.\\nUltimately,      with      standard      discretization,      we      obtain:</p><p>p(      xe,      t;      a)   \\n \\n=      24+      (0.54,   \\n \\n+      Sire      (,,T      —t)|(ot),      {o?(t)}?\\n \\n \\n=      (6t).</p><p>Equivalently,      this      is (21,      t;      Ore)   \\n \\n=   \\n \\n2   \\n \\n+      [0.52   \\n \\n—      1/o?\\n \\n \\nx      e%,,,(a1,T   \\n \\n—      t)]|      (6t),      (7)</p><p>where      (dt)      denotes      the      discretization      step.</p><p> \\n \\n  Equivalence      to      DDPM.\\nThe      objective      function      derived      here      is      equivalent      to      the      one      formulated  \\n \\n  based      on      variational      inference      in      the      discretized      formulation,      which      is      commonly      referred      to      as  \\n \\n  DDPMs      (Ho      et      al.,      2020).\\nIn      DDPMs      (Ho      et      al.,      2020),      we      often      see      the      following      form:</p><p> \\n \\n \\n1      1-      (ey;</p><p> \\n \\n  it;      0)   \\n \\n=      ~~   \\n \\n’      ),  \\n \\n  p(xy   \\n \\n)      Xt      Jina      ae      [om</p><h6>Vor</h6><p>When      a;   \\n \\n=   \\n \\n1   \\n \\n—      (dt),      the      above      is      equivalent      to      (7)      when      dt      goes      to   \\n \\n0      by      noting      1/,/a;   \\n \\n©      1+0.5(d¢).</p><h5>      Myo      —      Ut</h5><p> \\n \\n  {ory      ue   \\n \\n=      exp(—0.5t)xo,      {o?\\n}?\\n \\n \\n=   \\n \\n1   \\n \\n—      exp(—0.5t).\\n(3) t Subsequently,      by      defining   \\n \\na      parameterized      model      sg   \\n \\n:      R¢   \\n \\nx      [0,7]   \\n \\n>      R¢      and      utilizing      the      following weighted      regression,      the      marginal      score      is      estimated:</p><p>A\\n \\n \\n  Dore   \\n \\n=      argmin      Eywuni((0,7))      ce~aejo(w+|ar0),c0~pPr®      [A(t)      ||      Ver,      log      jo      (Le   \\n \\n|      ro)   \\n \\n_      So(£t,      t)      17)      (4)</p><p> \\n \\n  where   \\n \\n2   \\n \\n:      [0,7]   \\n \\n+      R      is   \\n \\na      weighted      function.\\n \\n \\n  Note      we      typically      use      another      parametrization      well.\\nFrom      (3),      we      can      easily      see      that      this      score      is  \\n \\n  estimated      by      introducing      networks      €9   \\n \\n:      R@   \\n \\nx      [0,7]   \\n \\n—      R®,      which      aims      to      estimate      the      noise      Bere      eO</p><p>Ore   \\n \\n=      argmin      Ey.\\nuni({0,7]),cr~p2xo+o?\\ne~N      (0,1)      ,co~pPte      [A(t)/{op}?\\nlle   \\n \\n—      €9      (1,      t)      7)   \\n \\n:      (5)</p><p>Then,      by      denoting      the      training      data      as      {a}      and      setting      A(t)   \\n \\n=      {a7      }”,      the      actual      loss      function      from the      training      data      is argmin   \\n \\n)      Ey      ai(o.r))a   \\n \\nO      peal      +e02,e~N      (0,1)      [Ile   \\n \\n—      €o(ae?,      t)      [17].\\n(6) i=1 Inference      time.\\nOnce      this      Bore      (or      Ore)      is      learned,      we      insert      it      into      the      time-reversal      SDE      (2)      and use      it      as   \\n \\na      generative      model.\\nUltimately,      with      standard      discretization,      we      obtain:</p><p>p(      xe,      t;      a)   \\n \\n=      24+      (0.54,   \\n \\n+      Sire      (,,T      —t)|(ot),      {o?(t)}?\\n \\n \\n=      (6t).</p><p>Equivalently,      this      is (21,      t;      Ore)   \\n \\n=   \\n \\n2   \\n \\n+      [0.52   \\n \\n—      1/o?\\n \\n \\nx      e%,,,(a1,T   \\n \\n—      t)]|      (6t),      (7)</p><p>where      (dt)      denotes      the      discretization      step.</p><p> \\n \\n  Equivalence      to      DDPM.\\nThe      objective      function      derived      here      is      equivalent      to      the      one      formulated  \\n \\n  based      on      variational      inference      in      the      discretized      formulation,      which      is      commonly      referred      to      as  \\n \\n  DDPMs      (Ho      et      al.,      2020).\\nIn      DDPMs      (Ho      et      al.,      2020),      we      often      see      the      following      form:</p><p> \\n \\n \\n1      1-      (ey;</p><p> \\n \\n  it;      0)   \\n \\n=      ~~   \\n \\n’      ),  \\n \\n  p(xy   \\n \\n)      Xt      Jina      ae      [om</p><h6>Vor</h6><p>When      a;   \\n \\n=   \\n \\n1   \\n \\n—      (dt),      the      above      is      equivalent      to      (7)      when      dt      goes      to   \\n \\n0      by      noting      1/,/a;   \\n \\n©      1+0.5(d¢).</p><h6>Vor</h6><p>When      a;   \\n \\n=   \\n \\n1   \\n \\n—      (dt),      the      above      is      equivalent      to      (7)      when      dt      goes      to   \\n \\n0      by      noting      1/,/a;   \\n \\n©      1+0.5(d¢).</p><h3>1.2      Fine-Tuning      Diffusion      Models      with      RL</h3><p>RL  \\n \\n  Importantly,      our      focus      on      RL-based      fine-tuning      distinguishes      itself      from      the      standard      fine-tuning  \\n \\n  methods.\\nStandard      fine-tuning      typically      involves      scenarios      where      we      have      pre-trained      models  \\n \\n  (e.g.,      diffusion      models)      and      new      training      data      {2,      y}.\\nIn      such      cases,      the      common      approach  \\n \\n  for      fine-tuning      is      to      retrain      diffusion      models      with      the      new      training      data      using      the      same      loss  \\n \\n  function      employed      during      pre-training.\\nIn      sharp      contrast,      RL-based      fine-tuning      directly      employs  \\n \\n  the      downstream      reward      functions      as      the      primary      optimization      objectives,      making      the      loss      functions  \\n \\n  different      from      those      used      in      pre-training.</p><p> \\n \\n  Hereafter,      we      start      with   \\n \\na      concise      overview      of      RL-based      fine-tuning.\\nThen,      before      delving      into  \\n \\n  specifics,      we      discuss      simpler      non-RL      alternatives      to      provide      motivation      for      adopting      RL-based  \\n \\n  fine-tuning.</p><li>1.2.1      Brief      Overview:      Fine-tuning      with      RL</li><p>In      this      article,      we      explore      the      fine-tuning      of      pre-trained      diffusion      models      to      optimize      downstream  \\n \\n  reward      functions      r   \\n \\n:      R¢   \\n \\n+      R.      In      domains      such      as      images,      these      backbone      diffusion      models      to      be  \\n \\n  fine-tuned      include      Stable      Diffusion      (Rombach      et      al.,      2022),      while      the      reward      functions      are      aesthetic  \\n \\n  scores      and      alignment      scores      (Clark      et      al.,      2023;      Black      et      al.,      2023;      Fan      et      al.,      2023).\\nMore      examples  \\n \\n  are      detailed      in      the      introduction.\\nThese      rewards      are      often      unknown,      necessitating      learning      from  \\n \\n  data      with      feedback:      {2      r(x™)}.\\nWe      will      explore      this      aspect      further      in      Section      7.\\nUntil      then,      we  \\n \\n  assume      r      is      known.\\n \\n \\n  Now,      readers      may      wonder      about      the      objectives      we      aim      to      achieve      during      the      fine-tuning      process.\\n \\n \\n  A      natural      approach      is      to      define      the      optimization      problem:</p><p>argmax      E,.\\n[r()|      (8)  \\n \\n  qeA(¥)  \\n \\n  where   \\n \\nq      is      initialized      with   \\n \\na      pre-trained      diffusion      model      p?\\n*®   \\n \\n€      A(2).\\nIn      this      tutorial,      we      will      detail  \\n \\n  the      procedure      of      solving      (8)      with      RL      in      the      upcoming      sections.\\nIn      essence,      we      leverage      the      fact  \\n \\n  that      diffusion      models      are      formulated      as   \\n \\na      sequential      decision-making      problem,      where      each      decision  \\n \\n  corresponds      to      how      samples      are      denoised.</p><p> \\n \\n  Although      the      above      objective      function      (8)      is      reasonable,      the      resulting      distribution      might      deviate  \\n \\n  too      much      from      the      pre-trained      diffusion      model.\\nTo      circumvent      this      issue,   \\n \\na      natural      way      is      to      add  \\n \\n  penalization      against      pre-trained      diffusion      models.\\nThen,      the      target      distribution      is      defined      as:</p><p>argmax      E,.4[r(x)]   \\n \\n—      aKL(q||p’*).\\n(9)  \\n \\n  qeA(¥) Notably,      (9)      reduces      to      the      following      distribution:</p><li>(10)</li><p>5\\n \\n   exp(r()/a)p\"\"()  \\n \\n  Pr)   \\n \\n=      Fra)      apa)</p><p> \\n \\n  Here,      the      first      term      in      (9)      corresponds      to      the      mean      reward,      which      we      want      to      optimize      in      the  \\n \\n  fine-tuning      process.\\nThe      second      term      in      (10)      serves      as   \\n \\na      penalty      term,      indicating      the      deviation      of   \\n \\nq       from      the      pre-trained      model.\\nThe      parameter   \\n \\na      controls      the      strength      of      this      regularization      term.\\nThe  \\n \\n  proper      choice      of   \\n \\na      depends      on      the      task      we      are      interested      in.</p><h6>1.2.2      Motivation      for      Using      RL      over      Non-RL      Alternatives</h6><p>ives  \\n \\n  To      achieve      our      goal      of      maximizing      downstream      reward      functions      with      diffusion      models,      readers  \\n \\n  may      question      whether      alternative      approaches      can      be      employed      apart      from      RL.\\nHere,      we      investigate  \\n \\n  these      potential      alternatives      and      explain      why      RL      approaches      may      offer      advantages      over      them.</p><p> \\n \\n  Rejection      sampling.\\nOne      approach      involves      generating      multiple      samples      from      pre-trained      diffu-  \\n \\n  sion      models      and      selecting      only      those      with      high      rewards.\\nThis      method,      called      rejection      sampling,  \\n \\n  operates      without      needing      fine-tuning.\\nHowever,      rejection      sampling      is      effective      primarily      when      the  \\n \\n  pre-trained      model      already      has   \\n \\na      high      probability      of      producing      high-reward      samples.\\nIt      resembles  \\n \\n  sampling      from   \\n \\na      prior      distribution      to      obtain      posterior      samples      (in      this      case,      high-reward      points).\\n \\n \\n  This      approach      works      efficiently      when      the      posterior      closely      matches      the      prior      but      can      become      highly  \\n \\n  inefficient      otherwise.\\nIn      contrast,      by      explicitly      updating      weight      in      diffusion      models,      RL-based      fine-  \\n \\n  tuning      allows      us      to      obtain      these      high-reward      samples,      which      are      seldom      generated      by      pre-trained  \\n \\n  models.</p><p> \\n \\n  Conditional      diffusion      models      (classifier-free      guidance).\\nIn      conditional      generative      models,      the  \\n \\n  general      goal      is      to      sample      from      p(|c),      where   \\n \\nx      is      the      output      and   \\n \\nc      denotes      the      conditioning      variable.\\n \\n \\n  For      example,      in      text-to-language      diffusion      models,   \\n \\nc      is   \\n \\na      text,      and   \\n \\nx      is      the      generated      image.\\nSimilarly,  \\n \\n  in      the      context      of      protein      engineering      for      addressing      inverse      folding      problems,      models      often      define   \\n \\nc       as      the      protein      backbone      structure      and   \\n \\nx      as      the      corresponding      amino      acid      sequence.\\nHere,      using      the  \\n \\n  training      data      {c,      ¢},      the      model      is      trained      by      using      the      loss      function:</p><p>argmin   \\n \\n»      E,uni(l0,7)2)      ~upa      +o?\\nsewN\\\\\\\\\\\\\\'(O)      [lle   \\n \\n—      co(2”,      }),      t)      I\"), t where      the      denoising      function      €g      additionally      receives      the      conditioning      information   \\n \\nc      as      input.\\nIn  \\n \\n  practice,   \\n \\na      variety      of      improvements      such      as      classifier-free      guidance      (Ho      and      Salimans,      2022)      can  \\n \\n  further      improve      the      model’s      ability      to      learn      the      conditional      distribution      p(|c).</p><p> \\n \\n  These      conditional      generative      models      can      be      used      to      optimize      down-stream      rewards      by      condi-  \\n \\n  tioning      on      the      reward      values,      then      sampling   \\n \\nx      conditioned      on      high      reward      values      (Krishnamoorthy  \\n \\n  et      al.,      2023;      Yuan      et      al.,      2023).\\nWhile      this      method      is,      in      principle,      capable      of      generating      plausible x\\n \\n   values      across   \\n \\na      range      of      reward      levels      within      the      training      data      distribution,      it      is      not      the      most  \\n \\n  effective      optimization      strategy.\\nThis      is      primarily      because      high-reward      inputs      frequently      reside  \\n \\n  in      the      tails      of      the      training      distribution      or      even      beyond      it.\\nConsequently,      this      method      may      not  \\n \\n  effectively      generate      high-reward      samples      that      lie      outside      the      training      data      distribution.\\nIn      contrast,  \\n \\n  RL-based      fine-tuning      has      the      capability      to      generate      samples      with      higher      rewards      beyond      the      training  \\n \\n  data.\\nThis      is      achieved      by      explicitly      maximizing      reward      models      learned      from      the      training      data  \\n \\n  and      leveraging      their      extrapolative      capabilities      of      reward      models,      as      theoretically      formalized      and  \\n \\n  empirically      observed      in      Uehara      et      al.\\n(2024).</p><p>Reward-weighted      training.\\nAnother      alternative      approach      is      to      use   \\n \\na      reward-weighted      version  \\n \\n  of      the      standard      training      loss      for      diffusion      models.\\nSuppose      that      we      have      data      {x      r(a)}.\\nThen, after      learning   \\n \\na      reward   \\n \\n7   \\n \\n:   \\n \\n¥   \\n \\n—      R      with      regression      from      the      data,      to      achieve      our      goal,      it      looks  \\n \\n  natural      to      use   \\n \\na      reward-weighted      version      of      the      training      loss      for      diffusion      models      (5),      1.e.,.\\na7      (4)      i)   \\n \\n2 areaun   \\n \\n»      E,uni((0,2))      peel?\\n+eo¢,enncoal      (to      ile   \\n \\n—      co(ay?,t)      |\") i\\n \\n    There      are      two      potential      drawbacks      to      this      approach.\\nFirst,      in      practice,      it      may      struggle      to      generate  \\n \\n  samples      with      higher      rewards      beyond      the      training      data.\\nAs      we      will      explain      later,      many      RL      algorithms  \\n \\n  are      more      directly      focused      on      optimizing      reward      functions,      which      are      expected      to      excel      in      obtaining  \\n \\n  samples      with      high      rewards      not      observed      in      the      original      data,      as      empirically      observed      in      Black      et      al.\\n \\n \\n  (2023).\\nSecond,      when      fine-tuning   \\n \\na      conditional      diffusion      model      p(x|c),      the      alternative      approach  \\n \\n  here      requires   \\n \\na      pair      of      {c,      «}      during      fine-tuning      to      ensure      the      validity      of      the      loss      function.\\nWhen  \\n \\n  we      only      have      data      {2      r(x      )}      but      not      {c,      2,      r(a)},      this      implies      that      we      might      need      to      solve an      inverse      problem      from   \\n \\nz      to      c,      which      can      often      be      challenging.\\nIn      contrast,      in      these      scenarios,      RL  \\n \\n  algorithms,      which      we      will      introduce      later,      can      operate      without      needing      such      pairs      {c,      x},      as  \\n \\n  long      as      we      have      learned      reward      functions      7.  \\n \\n  Finally,      it      should      be      noted      that      reward-weighted      training      technically      falls      under      the      broader  \\n \\n  category      of      RL      methods.\\nIt      shares   \\n \\na      close      connection      with      “reward-weighted      MLE”      introduced      in  \\n \\n  Section      6.1,      as      discussed      later.\\nEmploying      this      reward-weighted      MLE      helps      address      the      second  \\n \\n  concern      of      “reward-weighted      training”      mentioned      earlier.</p><p>2\\n \\n   Brief      Overview      of      Entropy-Regularized      MDPs</p><p> \\n \\n  In      this      tutorial,      we      explain      how      fine-tuning      diffusion      models      can      be      naturally      formulated      as      an      RL  \\n \\n  problem      in      entropy-regularized      MDPs.\\nThis      perspective      is      natural      because      RL      involves      sequential  \\n \\n  decision-making,      and   \\n \\na      diffusion      model      is      formulated      as   \\n \\na      sequential      problem      where      each      denoising  \\n \\n  step      is   \\n \\na      decision-making      process.\\nTo      connect      diffusion      models      with      RL,      we      begin      with   \\n \\na      concise  \\n \\n  overview      of      RL      in      standard      entropy-regularized      MDPs      (Haarnoja      et      al.,      2017;      Neu      et      al.,      2017;</p><p> \\n \\n  Geist      et      al.,      2019;      Schulman      et      al.,      2017).</p><li>2.1      MDPs</li><p> \\n \\n  An      MDP      is      defined      as      follows:      {S,A,      {P\"*}/_5,      {r:}-9,      po}      where      S      is      the      state      space,   \\n \\nA      is      the  \\n \\n  action      space,      P;\"*      is   \\n \\na      transition      dynamic      mapping:      S   \\n \\nx   \\n \\nA   \\n \\n—      A(S),      7,   \\n \\n:      S   \\n \\nx   \\n \\nA   \\n \\n>      R      denotes  \\n \\n  reward      received      at   \\n \\nt      and      po      is      an      initial      distribution      over      S.   \\n \\nA      policy      7,   \\n \\n:      S   \\n \\n—      A(A)      is   \\n \\na      map      from  \\n \\n  any      state      s   \\n \\n€      S      to      the      distribution      over      actions.\\nThe      standard      goal      in      RL      is      to      solve</p><h6>1.2.2      Motivation      for      Using      RL      over      Non-RL      Alternatives</h6><p>ives  \\n \\n  To      achieve      our      goal      of      maximizing      downstream      reward      functions      with      diffusion      models,      readers  \\n \\n  may      question      whether      alternative      approaches      can      be      employed      apart      from      RL.\\nHere,      we      investigate  \\n \\n  these      potential      alternatives      and      explain      why      RL      approaches      may      offer      advantages      over      them.</p><p> \\n \\n  Rejection      sampling.\\nOne      approach      involves      generating      multiple      samples      from      pre-trained      diffu-  \\n \\n  sion      models      and      selecting      only      those      with      high      rewards.\\nThis      method,      called      rejection      sampling,  \\n \\n  operates      without      needing      fine-tuning.\\nHowever,      rejection      sampling      is      effective      primarily      when      the  \\n \\n  pre-trained      model      already      has   \\n \\na      high      probability      of      producing      high-reward      samples.\\nIt      resembles  \\n \\n  sampling      from   \\n \\na      prior      distribution      to      obtain      posterior      samples      (in      this      case,      high-reward      points).\\n \\n \\n  This      approach      works      efficiently      when      the      posterior      closely      matches      the      prior      but      can      become      highly  \\n \\n  inefficient      otherwise.\\nIn      contrast,      by      explicitly      updating      weight      in      diffusion      models,      RL-based      fine-  \\n \\n  tuning      allows      us      to      obtain      these      high-reward      samples,      which      are      seldom      generated      by      pre-trained  \\n \\n  models.</p><p> \\n \\n  Conditional      diffusion      models      (classifier-free      guidance).\\nIn      conditional      generative      models,      the  \\n \\n  general      goal      is      to      sample      from      p(|c),      where   \\n \\nx      is      the      output      and   \\n \\nc      denotes      the      conditioning      variable.\\n \\n \\n  For      example,      in      text-to-language      diffusion      models,   \\n \\nc      is   \\n \\na      text,      and   \\n \\nx      is      the      generated      image.\\nSimilarly,  \\n \\n  in      the      context      of      protein      engineering      for      addressing      inverse      folding      problems,      models      often      define   \\n \\nc       as      the      protein      backbone      structure      and   \\n \\nx      as      the      corresponding      amino      acid      sequence.\\nHere,      using      the  \\n \\n  training      data      {c,      ¢},      the      model      is      trained      by      using      the      loss      function:</p><p>argmin   \\n \\n»      E,uni(l0,7)2)      ~upa      +o?\\nsewN\\\\\\\\\\\\\\'(O)      [lle   \\n \\n—      co(2”,      }),      t)      I\"), t where      the      denoising      function      €g      additionally      receives      the      conditioning      information   \\n \\nc      as      input.\\nIn  \\n \\n  practice,   \\n \\na      variety      of      improvements      such      as      classifier-free      guidance      (Ho      and      Salimans,      2022)      can  \\n \\n  further      improve      the      model’s      ability      to      learn      the      conditional      distribution      p(|c).</p><p> \\n \\n  These      conditional      generative      models      can      be      used      to      optimize      down-stream      rewards      by      condi-  \\n \\n  tioning      on      the      reward      values,      then      sampling   \\n \\nx      conditioned      on      high      reward      values      (Krishnamoorthy  \\n \\n  et      al.,      2023;      Yuan      et      al.,      2023).\\nWhile      this      method      is,      in      principle,      capable      of      generating      plausible x\\n \\n   values      across   \\n \\na      range      of      reward      levels      within      the      training      data      distribution,      it      is      not      the      most  \\n \\n  effective      optimization      strategy.\\nThis      is      primarily      because      high-reward      inputs      frequently      reside  \\n \\n  in      the      tails      of      the      training      distribution      or      even      beyond      it.\\nConsequently,      this      method      may      not  \\n \\n  effectively      generate      high-reward      samples      that      lie      outside      the      training      data      distribution.\\nIn      contrast,  \\n \\n  RL-based      fine-tuning      has      the      capability      to      generate      samples      with      higher      rewards      beyond      the      training  \\n \\n  data.\\nThis      is      achieved      by      explicitly      maximizing      reward      models      learned      from      the      training      data  \\n \\n  and      leveraging      their      extrapolative      capabilities      of      reward      models,      as      theoretically      formalized      and  \\n \\n  empirically      observed      in      Uehara      et      al.\\n(2024).</p><p>Reward-weighted      training.\\nAnother      alternative      approach      is      to      use   \\n \\na      reward-weighted      version  \\n \\n  of      the      standard      training      loss      for      diffusion      models.\\nSuppose      that      we      have      data      {x      r(a)}.\\nThen, after      learning   \\n \\na      reward   \\n \\n7   \\n \\n:   \\n \\n¥   \\n \\n—      R      with      regression      from      the      data,      to      achieve      our      goal,      it      looks  \\n \\n  natural      to      use   \\n \\na      reward-weighted      version      of      the      training      loss      for      diffusion      models      (5),      1.e.,.\\na7      (4)      i)   \\n \\n2 areaun   \\n \\n»      E,uni((0,2))      peel?\\n+eo¢,enncoal      (to      ile   \\n \\n—      co(ay?,t)      |\") i\\n \\n    There      are      two      potential      drawbacks      to      this      approach.\\nFirst,      in      practice,      it      may      struggle      to      generate  \\n \\n  samples      with      higher      rewards      beyond      the      training      data.\\nAs      we      will      explain      later,      many      RL      algorithms  \\n \\n  are      more      directly      focused      on      optimizing      reward      functions,      which      are      expected      to      excel      in      obtaining  \\n \\n  samples      with      high      rewards      not      observed      in      the      original      data,      as      empirically      observed      in      Black      et      al.\\n \\n \\n  (2023).\\nSecond,      when      fine-tuning   \\n \\na      conditional      diffusion      model      p(x|c),      the      alternative      approach  \\n \\n  here      requires   \\n \\na      pair      of      {c,      «}      during      fine-tuning      to      ensure      the      validity      of      the      loss      function.\\nWhen  \\n \\n  we      only      have      data      {2      r(x      )}      but      not      {c,      2,      r(a)},      this      implies      that      we      might      need      to      solve an      inverse      problem      from   \\n \\nz      to      c,      which      can      often      be      challenging.\\nIn      contrast,      in      these      scenarios,      RL  \\n \\n  algorithms,      which      we      will      introduce      later,      can      operate      without      needing      such      pairs      {c,      x},      as  \\n \\n  long      as      we      have      learned      reward      functions      7.  \\n \\n  Finally,      it      should      be      noted      that      reward-weighted      training      technically      falls      under      the      broader  \\n \\n  category      of      RL      methods.\\nIt      shares   \\n \\na      close      connection      with      “reward-weighted      MLE”      introduced      in  \\n \\n  Section      6.1,      as      discussed      later.\\nEmploying      this      reward-weighted      MLE      helps      address      the      second  \\n \\n  concern      of      “reward-weighted      training”      mentioned      earlier.</p><p>2\\n \\n   Brief      Overview      of      Entropy-Regularized      MDPs</p><p> \\n \\n  In      this      tutorial,      we      explain      how      fine-tuning      diffusion      models      can      be      naturally      formulated      as      an      RL  \\n \\n  problem      in      entropy-regularized      MDPs.\\nThis      perspective      is      natural      because      RL      involves      sequential  \\n \\n  decision-making,      and   \\n \\na      diffusion      model      is      formulated      as   \\n \\na      sequential      problem      where      each      denoising  \\n \\n  step      is   \\n \\na      decision-making      process.\\nTo      connect      diffusion      models      with      RL,      we      begin      with   \\n \\na      concise  \\n \\n  overview      of      RL      in      standard      entropy-regularized      MDPs      (Haarnoja      et      al.,      2017;      Neu      et      al.,      2017;</p><p> \\n \\n  Geist      et      al.,      2019;      Schulman      et      al.,      2017).</p><li>2.1      MDPs</li><p> \\n \\n  An      MDP      is      defined      as      follows:      {S,A,      {P\"*}/_5,      {r:}-9,      po}      where      S      is      the      state      space,   \\n \\nA      is      the  \\n \\n  action      space,      P;\"*      is   \\n \\na      transition      dynamic      mapping:      S   \\n \\nx   \\n \\nA   \\n \\n—      A(S),      7,   \\n \\n:      S   \\n \\nx   \\n \\nA   \\n \\n>      R      denotes  \\n \\n  reward      received      at   \\n \\nt      and      po      is      an      initial      distribution      over      S.   \\n \\nA      policy      7,   \\n \\n:      S   \\n \\n—      A(A)      is   \\n \\na      map      from  \\n \\n  any      state      s   \\n \\n€      S      to      the      distribution      over      actions.\\nThe      standard      goal      in      RL      is      to      solve</p><h1>T</h1><p>So      ri(si,      2)      (11)  \\n \\n  argmax      Ey      ,,}  \\n \\n  {re}  \\n \\n  In      entropy-regularized      MDPs,      we      consider      the      following      regularized      objective      instead:</p><p>t=0  \\n \\n  where      E;,,}[-]      is      the      expectation      induced      both      policy   \\n \\n7      and      the      transition      dynamics      as      follows:  \\n \\n  So   \\n \\n~      Po,      do   \\n \\n~      To(-|80),      51   \\n \\n~      P§\"(-|S0,@0),°+:.\\nAs      we      will      soon      detail      in      the      next      section  \\n \\n  (Section      3),      diffusion      models      can      naturally      be      framed      as      MDPs      as      each      policy      corresponds      to   \\n \\na       denoising      process      in      diffusion      models.</p><p>T\\n \\n \\n  {iy}   \\n \\n=      argmax      Ein)      Son      (51,      a4)   \\n \\n—      AKL(m:(-|s¢),      7:      (-]54))      (12) mt}      t—0  \\n \\n  where      7’   \\n \\n:      S      —>      A(A)      is   \\n \\na      certain      reference      policy.\\nThe      arg      max      solution      is      often      called   \\n \\na      set      of      soft  \\n \\n  optimal      policies.\\nCompared      to   \\n \\na      standard      objective      (11),      here      we      add      KL      terms      against      reference  \\n \\n  policies.\\nThis      addition      aims      to      ensure      that      soft      optimal      policies      closely      align      with      the      reference  \\n \\n  policies.\\nIn      the      context      of      fine-tuning      diffusion      models,      these      reference      policies      correspond      to  \\n \\n  the      pre-trained      diffusion      models,      as      we      aim      to      maintain      similarity      between      the      fine-tuned      and  \\n \\n  pre-trained      models.</p><p> \\n \\n  This      entropy-regularized      objective      in      (12)      has      been      widely      employed      in      RL      literature      due      to  \\n \\n  several      benefits      (Levine,      2018).\\nFor      instance,      in      online      RL,      it      is      known      that      these      policies      have  \\n \\n  good      exploration      properties      by      setting      reference      policies      as      uniform      policies      (Fox      et      al.,      2015;</p><p> \\n \\n  Haarnoja      et      al.,      2017).\\nIn      offline      RL,      Wu      et      al.\\n(2019)      suggests      using      these      policies      as      conservative  \\n \\n  policies      by      setting      reference      policies      close      to      behavior      policies      (policies      used      to      collect      offline      data).\\n \\n \\n  Additionally,      in      inverse      RL,      this      soft      optimal      policy      is      used      as      an      expert      policy      in      scenarios      where  \\n \\n  rewards      are      unobservable,      only      trajectories      from      expert      policies      are      available      (typically      referred      to  \\n \\n  as      maximum      entropy      RL      as      Ziebart      et      al.      (2008);      Wulfmeier      et      al.\\n(2015);      Finn      et      al.\\n(2016)).</p><h6>2.2      Key      Concepts:      Soft      Q-functions,      Soft      Bellman      Equations</h6><p>ions.</p><p> \\n \\n  The      crucial      question      in      RL      is      how      to      devise      algorithms      that      effectively      solve      the      optimization  \\n \\n  problem      (12).\\nThese      algorithms      are      later      used      as      fine-tuning      algorithms      of      diffusion      models.\\nTo      see  \\n \\n  these      algorithms,      we      rely      on      several      critical      concepts      in      entropy-regularized      MDPs.\\nSpecifically,  \\n \\n  soft-optimal      policies      (i.e.,      solutions      to      (12))      can      be      expressed      analytically      as   \\n \\na      blend      of      soft      Q-  \\n \\n  functions      and      reference      policies.\\nFurthermore,      these      soft      Q-functions      are      defined      as      solutions      to  \\n \\n  equations      known      as      soft      Bellman      equations.\\nWe      elaborate      on      these      foundational      concepts      below.</p><p>Soft      Q-functions      and      soft      optimal      policies.\\nSoft      optimal      policies      are      expressed      as   \\n \\na      blend      of      soft  \\n \\n  Q-functions      and      reference      policies.\\nTo      see      it,      we      define      the      soft      Q-function      as      follows:</p><p>T\\n \\n \\n  q(se,      a)   \\n \\n=      Egnsy      So      ral      ($4,      4%)   \\n \\n—      OBL      (m1      (-|8e41)      ley      1(-/Se41))      [S45      Ge   \\n \\n|   \\n \\n-      (13) k=t Then,      by      comparing      (13)      and      (12),      we      clearly      have m\\n \\n   =\\n \\n   argmax      Eq,~n(s,)[Ge(se,      a)   \\n \\n—      AKL(a(-|5¢)|]77(-   \\n \\n|      S2)|s¢].\\n(14)  \\n \\n  TE[XA(X)] Hence,      by      calculating      the      above      explicitly,   \\n \\na      soft      optimal      policy      in      (12)      is      described      as      follows:</p><p> \\n \\n  we)      og      _£xBlan(s.)      fant)  \\n \\n  mi      CIS)   \\n \\n&      Fe      qi(s,      a)      (ayn      (als)da      as)  \\n \\n  Soft      Bellman      equations.\\nWe      have      already      defined      soft      Q-functions      in      (13).\\nHowever,      this      form  \\n \\n  includes      the      soft      optimal      policies.\\nActually,      without      using      soft      optimal      policies,      the      soft      Q-function  \\n \\n  satisfies      the      following      recursive      equation      (a.k.a.      soft      Bellman      equation):</p><p>4(      St,      At)   \\n \\n=      Epps}      [rosuay)   \\n \\n+      alog   \\n \\n{   \\n \\n[      explara(siara)/a}n{(alsis)aa}   \\n \\n|      sia    \\n.\\n     (16)</p><p>This      is      proven      by      noting      we      recursively      have a\\n \\n   (Se,      ar)   \\n \\n=      Bers}      [re(      Se,      at)   \\n \\n+      G41      St41;      di41)   \\n \\n_      akKL      (74,4      (-|Se41),      Tai      (-|Se41))[Se,      ar|</p><p>By      substituting      (15)      into      the      above,      we      obtain      the      soft      Bellman      equation      (16).</p><p> \\n \\n  Soft      value      functions.\\nSo      far,      we      have      defined      the      soft      Q-functions,      which      depend      on      both      states  \\n \\n  and      actions.\\nWe      can      now      introduce   \\n \\na      related      concept      that      depends      solely      on      states,      termed      the      soft  \\n \\n  value      function.\\nThe      soft      value      function      is      defined      as      follows:</p><li>v4      (Sz)   \\n \\n=      tary T Tk(Sk;      Qk)   \\n \\n—      onto)</li><p>k=t Then,      the      soft      optimal      policy      in      (14)      is      also      written      as r*(-|s)   \\n \\nx      exp(q(s,      -)/o)m      Cs)  \\n \\n  ‘      exp(u;(s)/a) (17)</p><p>because      we      have exp      (ee)   \\n \\n=   \\n \\n/      exp      (a   \\n \\n)      m(a   \\n \\n|      s)da.</p><p>Then,      substituting      the      above      in      the      soft      Bellman      equation      (16),      it      is      written      as a\\n \\n   (Se,      at)   \\n \\n=      Egrs}      [7      (Se;      at)   \\n \\n+      p41      (Se41)|S¢,      ar).</p><p> \\n \\n  Algorithms      in      entropy-regularized      MDPs.\\nAs      outlined      in      Levine      (2018),      to      solve      (12),      various  \\n \\n  well-known      algorithms      exist      in      the      literature      on      RL.\\nThe      abovementioned      concepts      are      useful      in  \\n \\n  constructing      these      algorithms.\\nThese      include      policy      gradients,      which      gradually      optimize   \\n \\na      policy  \\n \\n  using   \\n \\na      policy      neural      network;      soft      Q-learning      algorithms,      which      utilize      the      soft-Bellman      equation  \\n \\n  and      approximate      the      soft-value      function      with   \\n \\na      value      neural      network;      and      soft      actor-critic      algorithms  \\n \\n  that      leverage      both      policy      and      value      neural      networks.\\nWe      will      explore      how      these      algorithms      can      be  \\n \\n  applied      in      the      context      of      diffusion      models      shortly      in      Section      4.2      and      6.</p><p>3\\n \\n   Fine-Tuning      Diffusion      Models      with      RL      in      Entropy      Regular- ized      MDPs</p><p> \\n \\n  In      this      section,      as      done      in      Fan      et      al.\\n(2023);      Black      et      al.\\n(2023);      Uehara      et      al.\\n(2024),      we      illustrate  \\n \\n  how      fine-tuning      can      be      formulated      as      an      RL      problem      in      soft-entropy      regularized      MDPs,      where      each  \\n \\n  PT   \\n \\nx      PT-1      rp_9      PT-2      P2      r1      Pl      Lo  \\n \\n  LT      T-1</p><p>Figure      2:      Formulating      fine-tuning      in      diffusion      models      using      MDPs.</p><p> \\n \\n  denoising      step      of      diffusion      models      corresponds      to   \\n \\na      policy      in      RL.\\nFinally,      we      outline   \\n \\na      specific      RL  \\n \\n  problem      of      interest      in      our      context.</p><p> \\n \\n  To      cast      fine-tuning      diffusion      models      as      an      RL      problem,      we      start      with      defining      the      following  \\n \\n  MDP:</p><p>The      state      space      S      and      action      space   \\n \\nA      correspond      to      the      input      space      1.</p><p>The      transition      dynamics      at      time   \\n \\nt      (i.e.,      P;)      is      an      identity      map      6(5;4;   \\n \\n=      a;).</p><p>The      reward      at      time   \\n \\nt   \\n \\n€      |0,---   \\n \\n,      7]      (i.e.,      r¢)      is      provided      only      at   \\n \\nT      as      r      (down-stream      reward  \\n \\n  function);      but   \\n \\n0      at      other      time      steps.</p><p>The      policy      at      time   \\n \\nt      (i.e,      7)      corresponds      to      pr41;_,:   \\n \\n¥   \\n \\n>      A(X).</p><p>The      initial      distribution      at      time   \\n \\n0      corresponds      to      pr,   \\n \\n€      A(4).\\nWith      slight      abuse      of      notation,  \\n \\n  we      often      denote      it      by      pr41(-|-),      while      this      is      just      pr+4(-).</p><p> \\n \\n  The      reference      policy      at   \\n \\nt      (i.e.,      7;)      corresponds      to   \\n \\na      denoising      process      in      the      pre-trained      model  \\n \\n  pre  \\n \\n  Pr+i-t We      list      several      things      to      note.</p><p> \\n \\n \\n¢      We      reverse      the      time-evolving      process      to      adhere      to      the      standard      notation      in      diffusion      models,  \\n \\n  i.e.,      from   \\n \\nt   \\n \\n=      T\\\\\\\\\\\\\\'      tot   \\n \\n=      0.\\nHence,      s;      in      standard      MDPs      corresponds      to      x74      _;      in      diffusion  \\n \\n  models.</p><p>¢\\n \\n   In      our      context,      unlike      standard      RL      scenarios,      the      transition      dynamics      are      known.</p><p>Key      RL      Problem.\\nNow,      by      reformulating      the      original      objective      of      standard      RL      into      our      contexts,  \\n \\n  the      objective      function      in      (12)      reduces      to      the      following:</p><p> \\n \\n  {pi}:   \\n \\n=      argmax      Eg,   \\n \\n3      (r(x)   \\n \\na      Deep      Epp      [KL      (pe(-|24)      lve      (-|ee))]      8)  \\n \\n  {pie      [RES      A(RY)]      Hay  \\n \\n  Reward      KL      penalty where      the      expectation      E,,,,;[-]      is      taken      with      respect      to      Tierys      Di(@r-1|@1),      Le.\\n@r   \\n \\n~      prai(-),      7-1   \\n \\n~       pr-i(-   \\n \\n|      &r-1),r_-2   \\n \\n~      pr—a(-   \\n \\n|      er_2),--+.\\nIn      this      article,      we      set      this      as      an      objective      function      in  \\n \\n  fine-tuning      diffusion      models.\\nThis      objective      is      natural      as      it      seeks      to      optimize      sequential      denoising  \\n \\n  processes      to      maximize      downstream      rewards      while      maintaining      proximity      to      pre-trained      models.</p><p> \\n \\n  Subsequently,      we      investigate      several      algorithms      to      solve      (18).\\nBefore      discussing      these      algorithms,  \\n \\n  we      summarize      several      key      theoretical      properties      that      will      aid      their      derivation.</p><p>4\\n \\n   Theory      of      RL-Based      Fine-Tuning</p><p> \\n \\n  So      far,      we      have      introduced   \\n \\na      certain      RL      problem      (i.e.,      (18))      as   \\n \\na      fine-tuning      diffusion      model.\\nIn  \\n \\n  this      section,      we      explain      that      solving      this      RL      problem      allows      us      to      achieve      the      target      distribution  \\n \\n  discussed      in      Section      1.2.1.\\nAdditionally,      we      present      several      important      theoretical      properties,      such  \\n \\n  as      the      analytical      form      of      marginal      distributions      and      posterior      distributions      induced      by      fine-tuned  \\n \\n  models.\\nThis      formulation      is      also      instrumental      in      introducing      several      algorithms      (reward-weighted  \\n \\n  MLE,      value-weighted      sampling,      and      path      consistency      learning      in      Section      6),      and      establishing  \\n \\n  connections      with      related      areas      (classifier      guidance      in      Section      8,      and      flow-based      diffusion      models      in  \\n \\n  Section      9).\\nWe      start      with      several      key      concepts.</p><li>4.1      Key      Concepts:      Soft      Value      functions      and      Soft      Bellman      Equations.</li><p> \\n \\n  Now,      reflecting      on      how      soft      optimal      policies      are      expressed      using      soft      value      functions      in      Section   \\n \\n2      in  \\n \\n  the      context      of      standard      RL      problems,      we      derive      several      important      concepts      applicable      to      fine-tuning  \\n \\n  diffusion      models.\\nThese      concepts      are      later      useful      in      constructing      algorithms      to      solve      our      RL      problem  \\n \\n  (18).</p><p>      Firstly,      as      we      see      in      (15),      soft-optimal      policies      are      characterized      as: Pe      (-|e1) expt      a(/aypl(      a9       f      exp(vr-a(ve-1)/@)      pe      (ar-1      |      v4)dx1-1 where      soft-value      functions      are      defined      as ve(es)      =      Egppy[r(@o)      —      oD      KL(pe(-|ere)      lle      Cle)      |e), 1       (a1,      t1-1)      =      Egsy(r(@o)      —      a      $5      KL(pe(-|e)      Pee      (len)      [es      era]      =      era      (1-1). k=t+1 Secondly,      as      we      see      in      (16),      the      soft-value      functions      are      also      recursively      defined      by      the      soft       Bellman      equations:       7      (22)      _      fexp      (uae)      De      (X41      |      v,)dx,-1      (t      =T+1,---      ,1), vo(%o)      =      (x0). (20)</p><p>Now      substituting      the      above      in      (19),      we      obtain sg.y   \\n \\n=      ex      (trea      )/a)eP\"C   \\n \\n|      2)  \\n \\n  PEC|e)      exp(v;      (x)      /a)      :</p><p> \\n \\n  As      mentioned      earlier,      these      soft      value      functions      and      their      recursive      form      will      later      serve      as      the  \\n \\n  basis      for      constructing      several      concrete      fine-tuning      algorithms      (such      as      reward-weighted      MLE      and  \\n \\n  value-weighted      sampling      in      Section      6).</p><h6>2.2      Key      Concepts:      Soft      Q-functions,      Soft      Bellman      Equations</h6><p>ions.</p><p> \\n \\n  The      crucial      question      in      RL      is      how      to      devise      algorithms      that      effectively      solve      the      optimization  \\n \\n  problem      (12).\\nThese      algorithms      are      later      used      as      fine-tuning      algorithms      of      diffusion      models.\\nTo      see  \\n \\n  these      algorithms,      we      rely      on      several      critical      concepts      in      entropy-regularized      MDPs.\\nSpecifically,  \\n \\n  soft-optimal      policies      (i.e.,      solutions      to      (12))      can      be      expressed      analytically      as   \\n \\na      blend      of      soft      Q-  \\n \\n  functions      and      reference      policies.\\nFurthermore,      these      soft      Q-functions      are      defined      as      solutions      to  \\n \\n  equations      known      as      soft      Bellman      equations.\\nWe      elaborate      on      these      foundational      concepts      below.</p><p>Soft      Q-functions      and      soft      optimal      policies.\\nSoft      optimal      policies      are      expressed      as   \\n \\na      blend      of      soft  \\n \\n  Q-functions      and      reference      policies.\\nTo      see      it,      we      define      the      soft      Q-function      as      follows:</p><p>T\\n \\n \\n  q(se,      a)   \\n \\n=      Egnsy      So      ral      ($4,      4%)   \\n \\n—      OBL      (m1      (-|8e41)      ley      1(-/Se41))      [S45      Ge   \\n \\n|   \\n \\n-      (13) k=t Then,      by      comparing      (13)      and      (12),      we      clearly      have m\\n \\n   =\\n \\n   argmax      Eq,~n(s,)[Ge(se,      a)   \\n \\n—      AKL(a(-|5¢)|]77(-   \\n \\n|      S2)|s¢].\\n(14)  \\n \\n  TE[XA(X)] Hence,      by      calculating      the      above      explicitly,   \\n \\na      soft      optimal      policy      in      (12)      is      described      as      follows:</p><p> \\n \\n  we)      og      _£xBlan(s.)      fant)  \\n \\n  mi      CIS)   \\n \\n&      Fe      qi(s,      a)      (ayn      (als)da      as)  \\n \\n  Soft      Bellman      equations.\\nWe      have      already      defined      soft      Q-functions      in      (13).\\nHowever,      this      form  \\n \\n  includes      the      soft      optimal      policies.\\nActually,      without      using      soft      optimal      policies,      the      soft      Q-function  \\n \\n  satisfies      the      following      recursive      equation      (a.k.a.      soft      Bellman      equation):</p><p>4(      St,      At)   \\n \\n=      Epps}      [rosuay)   \\n \\n+      alog   \\n \\n{   \\n \\n[      explara(siara)/a}n{(alsis)aa}   \\n \\n|      sia    \\n.\\n     (16)</p><p>This      is      proven      by      noting      we      recursively      have a\\n \\n   (Se,      ar)   \\n \\n=      Bers}      [re(      Se,      at)   \\n \\n+      G41      St41;      di41)   \\n \\n_      akKL      (74,4      (-|Se41),      Tai      (-|Se41))[Se,      ar|</p><p>By      substituting      (15)      into      the      above,      we      obtain      the      soft      Bellman      equation      (16).</p><p> \\n \\n  Soft      value      functions.\\nSo      far,      we      have      defined      the      soft      Q-functions,      which      depend      on      both      states  \\n \\n  and      actions.\\nWe      can      now      introduce   \\n \\na      related      concept      that      depends      solely      on      states,      termed      the      soft  \\n \\n  value      function.\\nThe      soft      value      function      is      defined      as      follows:</p><li>v4      (Sz)   \\n \\n=      tary T Tk(Sk;      Qk)   \\n \\n—      onto)</li><p>k=t Then,      the      soft      optimal      policy      in      (14)      is      also      written      as r*(-|s)   \\n \\nx      exp(q(s,      -)/o)m      Cs)  \\n \\n  ‘      exp(u;(s)/a) (17)</p><p>because      we      have exp      (ee)   \\n \\n=   \\n \\n/      exp      (a   \\n \\n)      m(a   \\n \\n|      s)da.</p><p>Then,      substituting      the      above      in      the      soft      Bellman      equation      (16),      it      is      written      as a\\n \\n   (Se,      at)   \\n \\n=      Egrs}      [7      (Se;      at)   \\n \\n+      p41      (Se41)|S¢,      ar).</p><p> \\n \\n  Algorithms      in      entropy-regularized      MDPs.\\nAs      outlined      in      Levine      (2018),      to      solve      (12),      various  \\n \\n  well-known      algorithms      exist      in      the      literature      on      RL.\\nThe      abovementioned      concepts      are      useful      in  \\n \\n  constructing      these      algorithms.\\nThese      include      policy      gradients,      which      gradually      optimize   \\n \\na      policy  \\n \\n  using   \\n \\na      policy      neural      network;      soft      Q-learning      algorithms,      which      utilize      the      soft-Bellman      equation  \\n \\n  and      approximate      the      soft-value      function      with   \\n \\na      value      neural      network;      and      soft      actor-critic      algorithms  \\n \\n  that      leverage      both      policy      and      value      neural      networks.\\nWe      will      explore      how      these      algorithms      can      be  \\n \\n  applied      in      the      context      of      diffusion      models      shortly      in      Section      4.2      and      6.</p><p>3\\n \\n   Fine-Tuning      Diffusion      Models      with      RL      in      Entropy      Regular- ized      MDPs</p><p> \\n \\n  In      this      section,      as      done      in      Fan      et      al.\\n(2023);      Black      et      al.\\n(2023);      Uehara      et      al.\\n(2024),      we      illustrate  \\n \\n  how      fine-tuning      can      be      formulated      as      an      RL      problem      in      soft-entropy      regularized      MDPs,      where      each  \\n \\n  PT   \\n \\nx      PT-1      rp_9      PT-2      P2      r1      Pl      Lo  \\n \\n  LT      T-1</p><p>Figure      2:      Formulating      fine-tuning      in      diffusion      models      using      MDPs.</p><p> \\n \\n  denoising      step      of      diffusion      models      corresponds      to   \\n \\na      policy      in      RL.\\nFinally,      we      outline   \\n \\na      specific      RL  \\n \\n  problem      of      interest      in      our      context.</p><p> \\n \\n  To      cast      fine-tuning      diffusion      models      as      an      RL      problem,      we      start      with      defining      the      following  \\n \\n  MDP:</p><p>The      state      space      S      and      action      space   \\n \\nA      correspond      to      the      input      space      1.</p><p>The      transition      dynamics      at      time   \\n \\nt      (i.e.,      P;)      is      an      identity      map      6(5;4;   \\n \\n=      a;).</p><p>The      reward      at      time   \\n \\nt   \\n \\n€      |0,---   \\n \\n,      7]      (i.e.,      r¢)      is      provided      only      at   \\n \\nT      as      r      (down-stream      reward  \\n \\n  function);      but   \\n \\n0      at      other      time      steps.</p><p>The      policy      at      time   \\n \\nt      (i.e,      7)      corresponds      to      pr41;_,:   \\n \\n¥   \\n \\n>      A(X).</p><p>The      initial      distribution      at      time   \\n \\n0      corresponds      to      pr,   \\n \\n€      A(4).\\nWith      slight      abuse      of      notation,  \\n \\n  we      often      denote      it      by      pr41(-|-),      while      this      is      just      pr+4(-).</p><p> \\n \\n  The      reference      policy      at   \\n \\nt      (i.e.,      7;)      corresponds      to   \\n \\na      denoising      process      in      the      pre-trained      model  \\n \\n  pre  \\n \\n  Pr+i-t We      list      several      things      to      note.</p><p> \\n \\n \\n¢      We      reverse      the      time-evolving      process      to      adhere      to      the      standard      notation      in      diffusion      models,  \\n \\n  i.e.,      from   \\n \\nt   \\n \\n=      T\\\\\\\\\\\\\\'      tot   \\n \\n=      0.\\nHence,      s;      in      standard      MDPs      corresponds      to      x74      _;      in      diffusion  \\n \\n  models.</p><p>¢\\n \\n   In      our      context,      unlike      standard      RL      scenarios,      the      transition      dynamics      are      known.</p><p>Key      RL      Problem.\\nNow,      by      reformulating      the      original      objective      of      standard      RL      into      our      contexts,  \\n \\n  the      objective      function      in      (12)      reduces      to      the      following:</p><p> \\n \\n  {pi}:   \\n \\n=      argmax      Eg,   \\n \\n3      (r(x)   \\n \\na      Deep      Epp      [KL      (pe(-|24)      lve      (-|ee))]      8)  \\n \\n  {pie      [RES      A(RY)]      Hay  \\n \\n  Reward      KL      penalty where      the      expectation      E,,,,;[-]      is      taken      with      respect      to      Tierys      Di(@r-1|@1),      Le.\\n@r   \\n \\n~      prai(-),      7-1   \\n \\n~       pr-i(-   \\n \\n|      &r-1),r_-2   \\n \\n~      pr—a(-   \\n \\n|      er_2),--+.\\nIn      this      article,      we      set      this      as      an      objective      function      in  \\n \\n  fine-tuning      diffusion      models.\\nThis      objective      is      natural      as      it      seeks      to      optimize      sequential      denoising  \\n \\n  processes      to      maximize      downstream      rewards      while      maintaining      proximity      to      pre-trained      models.</p><p> \\n \\n  Subsequently,      we      investigate      several      algorithms      to      solve      (18).\\nBefore      discussing      these      algorithms,  \\n \\n  we      summarize      several      key      theoretical      properties      that      will      aid      their      derivation.</p><p>4\\n \\n   Theory      of      RL-Based      Fine-Tuning</p><p> \\n \\n  So      far,      we      have      introduced   \\n \\na      certain      RL      problem      (i.e.,      (18))      as   \\n \\na      fine-tuning      diffusion      model.\\nIn  \\n \\n  this      section,      we      explain      that      solving      this      RL      problem      allows      us      to      achieve      the      target      distribution  \\n \\n  discussed      in      Section      1.2.1.\\nAdditionally,      we      present      several      important      theoretical      properties,      such  \\n \\n  as      the      analytical      form      of      marginal      distributions      and      posterior      distributions      induced      by      fine-tuned  \\n \\n  models.\\nThis      formulation      is      also      instrumental      in      introducing      several      algorithms      (reward-weighted  \\n \\n  MLE,      value-weighted      sampling,      and      path      consistency      learning      in      Section      6),      and      establishing  \\n \\n  connections      with      related      areas      (classifier      guidance      in      Section      8,      and      flow-based      diffusion      models      in  \\n \\n  Section      9).\\nWe      start      with      several      key      concepts.</p><li>4.1      Key      Concepts:      Soft      Value      functions      and      Soft      Bellman      Equations.</li><p> \\n \\n  Now,      reflecting      on      how      soft      optimal      policies      are      expressed      using      soft      value      functions      in      Section   \\n \\n2      in  \\n \\n  the      context      of      standard      RL      problems,      we      derive      several      important      concepts      applicable      to      fine-tuning  \\n \\n  diffusion      models.\\nThese      concepts      are      later      useful      in      constructing      algorithms      to      solve      our      RL      problem  \\n \\n  (18).</p><p>      Firstly,      as      we      see      in      (15),      soft-optimal      policies      are      characterized      as: Pe      (-|e1) expt      a(/aypl(      a9       f      exp(vr-a(ve-1)/@)      pe      (ar-1      |      v4)dx1-1 where      soft-value      functions      are      defined      as ve(es)      =      Egppy[r(@o)      —      oD      KL(pe(-|ere)      lle      Cle)      |e), 1       (a1,      t1-1)      =      Egsy(r(@o)      —      a      $5      KL(pe(-|e)      Pee      (len)      [es      era]      =      era      (1-1). k=t+1 Secondly,      as      we      see      in      (16),      the      soft-value      functions      are      also      recursively      defined      by      the      soft       Bellman      equations:       7      (22)      _      fexp      (uae)      De      (X41      |      v,)dx,-1      (t      =T+1,---      ,1), vo(%o)      =      (x0). (20)</p><p>Now      substituting      the      above      in      (19),      we      obtain sg.y   \\n \\n=      ex      (trea      )/a)eP\"C   \\n \\n|      2)  \\n \\n  PEC|e)      exp(v;      (x)      /a)      :</p><p> \\n \\n  As      mentioned      earlier,      these      soft      value      functions      and      their      recursive      form      will      later      serve      as      the  \\n \\n  basis      for      constructing      several      concrete      fine-tuning      algorithms      (such      as      reward-weighted      MLE      and  \\n \\n  value-weighted      sampling      in      Section      6).</p><h3>4.2      Induced      Distributions      after      Fine-Tuning</h3><p>ning Now,      with      the      above      preparation,      we      can      show      that      the      induced      distribution      derived      by      this      soft  \\n \\n  optimal      policy      is      actually      equal      to      our      target      distribution.</p><p>Theorem   \\n \\n1      (Theorem   \\n \\n|      in      Uehara      et      al.      (2024)).\\nLet      p*(-)      be      an      induced      distribution      at      time   \\n \\n0 from optimal policies {pf }j_7 41) 1-e P*(%o) = Tera pt (a@1-1|@1) }dx1.r.\\nThe distribution p* is      equal      to      the      target      distribution      (10),      i.e., p*(a)   \\n \\n=      p,(2).</p><p> \\n \\n  This      theorem      states      that      after      solving      (18)      and      obtaining      soft      optimal      policies,      we      can      sample  \\n \\n  from      the      target      distribution      by      sequentially      running      policies      from      p7.,,      to      pj.\\nThus,      (18)      serves      as   \\n \\na       natural      objective      for      fine-tuning.\\nThis      fact      is      also      useful      in      deriving   \\n \\na      connection      with      classifier  \\n \\n  guidance      in      Section      8.</p><p>Marginal      distributions.\\nWe      can      derive      the      marginal      distribution      as      follows.</p><p>Theorem   \\n \\n2      (Theorem   \\n \\n2      in      Uehara      et      al.      (2024)      ).\\nLet      ps(x,)      be      the      marginal      distributions      at   \\n \\nt       induced      by      soft-optimal      policies      {p¥}}_741,      i.e.\\nDi(@0)   \\n \\n=      f(T]      Pe(@e—1zn)      }dvipir.\\nThen, x\\n \\n   _\\n \\n   exp(vi(a4)/a)pP      (zt)  \\n \\n  Pr      (xz)      —_   \\n \\nC      ’</p><p>where      v;(-)      is      the      soft-value      function.</p><p>Interestingly,      the      normalizing      constant      is      independent      of   \\n \\n¢      in      the      above      theorem.</p><p>Posterior      distributions.\\nWe      can      derive      the      posterior      distribution      as      follows.</p><p> \\n \\n  Theorem   \\n \\n3      (Theorem   \\n \\n3      in      Uehara      et      al.      (2024)).\\nDenote      the      posterior      distribution      of      x4      given  \\n \\n  24_1      for      the      distribution      induced      by      soft-optimal      policies      {py      }i_7,      by      {p*}°(-   \\n \\n|      -).      We      define      the  \\n \\n  analogous      objective      for   \\n \\na      pre-trained      policy      and      denote      it      by      p?\\\\\\\\\\\\\\'*(-   \\n \\n|      -).      Then,      we      get {p\"}}(@+|@1-1)   \\n \\n=      pe      (@4|X4-1)-</p><p> \\n \\n  This      theorem      indicates      that      after      solving      (18),      the      posterior      distribution      induced      by      pre-trained  \\n \\n  models      remains      preserved.\\nThis      property      plays      an      important      role      in      constructing      PCL      (path  \\n \\n  consistency      learning)      in      Section      6.3.  \\n \\n  Remark   \\n \\n4      (Continuous-time      formulation).\\nFor      simplicity,      our      explanation      is      generally      based      on  \\n \\n  the      discrete-time      formulation.\\nHowever,      as      training      of      diffusion      models      could      be      formulated      in  \\n \\n  the      continuous-time      formulation      (Song      et      al.,      2021),      we      can      still      extend      most      of      our      discussion      of  \\n \\n  fine-tuning      in      our      tutorial      in      the      continuous-time      formulation.\\nFor      example,      the      above      Theorems      are  \\n \\n  extended      to      the      continuous      time      formulation      in      Uehara      et      al.\\n(2024).</p><p> \\n \\n  Table      1:      Description      of      each      RL      algorithm      for      fine-tuning      diffusion      models      (note      that      value-  \\n \\n  weighted      sampling      is      technically      not   \\n \\na      fine-tuning      algorithm.)      Note      (1)      “Without      learning      value  \\n \\n  functions”      refers      to      the      capability      of      algorithms      to      directly      utilize      non-differentiable      black-box  \\n \\n  reward      feedback,      bypassing      the      necessity      to      train      differentiable      reward      functions,      (2)      “Distribution-  \\n \\n  constrained”      indicates      that      the      algorithms      are      designed      to      maintain      proximity      to      pre-trained      models.</p><p> \\n \\n  Based      on      it,      the      practical      recommendation      of      algorithms      is      summarized      in      Figure      3.</p><p>Memory      Computational      Without      learning      Distribution-  \\n \\n  efficiency   \\n \\n—      efficiency      value      functions      constrained</p><p>Soft      PPO      v      Vv</p><p>Reward      backpropagation      v      v</p><p>Reward-weighted      MLE      v      v      v</p><p>Value-weighted      sampling      v      v</p><p>Path      consistency      learning      v      v</p><p>5\\n \\n   RL-Based      Fine-Tuning      Algorithms      1:      Non-Distribution-Constrained</p><h2>Approaches</h2><p> \\n \\n  So      far,      we      have      explained      how      to      frame      fine-tuning      diffusion      models      as      the      RL      problem      in  \\n \\n  entropy-regularized      MDPs.\\nMoving      forward,      we      summarize      actual      algorithms      that      can      solve      the      RL  \\n \\n  problem      of      interest      described      by      Equation      (18).\\nIn      this      section,      we      introduce      two      algorithms:      PPO       and      direct      reward      backpropagation.\\n \\n \\n  These      algorithms      are      originally      designed      to      optimize      reward      functions      directly,      meaning      they  \\n \\n  operate      effectively      even      without      entropy      regularization      (i.e.,   \\n \\na   \\n \\n=      0).\\nConsequently,      they      are      well-  \\n \\n  suited      for      generating      samples      with      high      rewards      that      may      not      be      in      the      original      training      dataset.\\nMore  \\n \\n  distribution-constrained      algorithms      that      align      closely      with      pre-trained      models      will      be      discussed      in  \\n \\n  the      subsequent      section      (Section      4.2).\\nTherefore,      we      classify      algorithms      in      this      section      (i.e.,      PPO      and  \\n \\n  direct      reward      backpropagation)      as      non-distribution-constrained      approaches.\\nThe      whole      summary      is  \\n \\n  described      in      Table      1.</p><h3>5.1      Soft      Proximal      Policy      Optimization      (PPO)</h3><p>PPO)  \\n \\n  In      order      to      solve      Equation      (18),      Fan      et      al.\\n(2023);      Clark      et      al.\\n(2023)      propose      using      PPO      (Schulman  \\n \\n  et      al.,      2017).\\nPPO      has      been      widely      used      in      RL,      as      well      as,      in      the      literature      in      fine-tuning      LLMs,  \\n \\n  due      to      its      stability      and      simplicity.\\nIn      the      standard      context      of      RL,      this      is      especially      preferred      over  \\n \\n  Q-learning      when      the      action      space      is      high-dimensional.</p><p> \\n \\n  The      PPO      algorithm      is      described      in      Algorithm      1.\\nThis      is      an      iterative      procedure      of      updating  \\n \\n  a      parameter      7.\\nEach      iteration      comprises      two      steps:      firstly,      samples      are      generated      by      executing  \\n \\n  Algorithm   \\n \\n1      Soft      PPO</p><p> \\n \\n  1:      Require:      Pre-trained      model      {N(p(1,      t;      Opre),      77      (t))      }i_-74.1,      batch      size      m,      parameter   \\n \\na   \\n \\n€      R*,  \\n \\n  learning      rate  \\n \\n  2:      Initialize:      0;   \\n \\n=      Opre  \\n \\n  3:      for      s   \\n \\n€      [1,---      ,S]      do  \\n \\n  4:      Collect   \\n \\nm      samples      fa      (0)      }9_      from   \\n \\na      current      diffusion      model      (i.e.,      generating      by      sequen- tially      running      polices      {N(p(2z,      t;      9),      07(t))      }t_p4,      from   \\n \\nt   \\n \\n=   \\n \\nT   \\n \\n+   \\n \\n1      tot   \\n \\n=      1)  \\n \\n  5:      Update      as      follows      (several      times      if      needed):</p><h3>5.1      Soft      Proximal      Policy      Optimization      (PPO)</h3><p>PPO)  \\n \\n  In      order      to      solve      Equation      (18),      Fan      et      al.\\n(2023);      Clark      et      al.\\n(2023)      propose      using      PPO      (Schulman  \\n \\n  et      al.,      2017).\\nPPO      has      been      widely      used      in      RL,      as      well      as,      in      the      literature      in      fine-tuning      LLMs,  \\n \\n  due      to      its      stability      and      simplicity.\\nIn      the      standard      context      of      RL,      this      is      especially      preferred      over  \\n \\n  Q-learning      when      the      action      space      is      high-dimensional.</p><p> \\n \\n  The      PPO      algorithm      is      described      in      Algorithm      1.\\nThis      is      an      iterative      procedure      of      updating  \\n \\n  a      parameter      7.\\nEach      iteration      comprises      two      steps:      firstly,      samples      are      generated      by      executing  \\n \\n  Algorithm   \\n \\n1      Soft      PPO</p><p> \\n \\n  1:      Require:      Pre-trained      model      {N(p(1,      t;      Opre),      77      (t))      }i_-74.1,      batch      size      m,      parameter   \\n \\na   \\n \\n€      R*,  \\n \\n  learning      rate  \\n \\n  2:      Initialize:      0;   \\n \\n=      Opre  \\n \\n  3:      for      s   \\n \\n€      [1,---      ,S]      do  \\n \\n  4:      Collect   \\n \\nm      samples      fa      (0)      }9_      from   \\n \\na      current      diffusion      model      (i.e.,      generating      by      sequen- tially      running      polices      {N(p(2z,      t;      9),      07(t))      }t_p4,      from   \\n \\nt   \\n \\n=   \\n \\nT   \\n \\n+   \\n \\n1      tot   \\n \\n=      1)  \\n \\n  5:      Update      as      follows      (several      times      if      needed):</p><h2>      A541      <—      O5      —      7      Ve      )      )      i      {le      ))</h2><p>Co,      (i)      1).\\n \\n \\ng       F(a,      xt”))   \\n \\n-      Clip      Pere      so)      l—e,l+e</p><p>where  \\n \\n  1   \\n \\nm    \\n \\nY    \\n.\\n     Je,      (i)   \\n \\n4 t=T+1      i=1 p(x,      ja;      9) pai,      |al;      As) (21)</p><h3>P(x,      |e      ;      5)</h3><p>t;      0)   \\n \\n—      t:      9Pre)      ||2  \\n \\n  F4(Xo,      Lt)   \\n \\n=      —r(ap)   \\n \\n+      ole   \\n \\n?      ae   \\n \\n’      II     .</p><p>lo=0.></p><p>KL      term 6:      end      for  \\n \\n  7;      Output:      Policy      {p.(-   \\n \\n|      0s)      Jars</p><p> \\n \\n  current      policies      to      construct      the      loss      function      (inspired      by      policy      gradient      formulation);      secondly,  \\n \\n  the      parameter   \\n \\n6      is      updated      by      computing      the      gradient      of      the      loss      function.</p><p> \\n \\n  PPO      offers      several      advantages.\\nThe      approach      is      known      for      its      stability      and      relatively      straight-  \\n \\n  forward      implementation.\\nStability      comes      from      the      conservative      parameter      updates.\\nIndeed,      PPO       builds      upon      TRPO      (Schulman      et      al.,      2015),      where      parameters      are      conservatively      updated      with   \\n \\na      KL  \\n \\n  penalty      term      (between      @,,,      and      @,)      to      prevent      significant      deviation      from      the      current      parameter.\\nThis  \\n \\n  gives      us      stability      in      the      optimization      landscape.\\nFurthermore,      in      Algorithm      1,      we      do      not      necessarily  \\n \\n  need      to      rely      on      value      functions,      although      they      could      be      useful      for      variance      reduction.\\nAs      discussed  \\n \\n  in      the      subsequent      subsection,      this      can      be      advantageous      compared      to      other      methods,      especially      since  \\n \\n  learning      value      functions      can      be      challenging      in      high-dimensional      spaces,      particularly      within      the  \\n \\n  context      of      diffusion      models.</p><h3>5.2      Direct      Reward      Backpropagation</h3><p>tion  \\n \\n  Another      standard      approach      is   \\n \\na      differentiable      optimization      (Clark      et      al.,      2023;      Prabhudesai      et      al.,  \\n \\n  2023;      Uehara      et      al.,      2024),      where      gradients      are      directly      propagated      from      reward      functions      to      update  \\n \\n  policies.</p><p> \\n \\n  The      entire      algorithm      is      detailed      in      Algorithm      2.\\nThis      reward      backpropagation      entails      an      iterative  \\n \\n  process      of      updating   \\n \\na      parameter      0.\\nEach      iteration      comprises      two      steps;      firstly,      samples      are      generated  \\n \\n  by      executing      current      policies      to      approximate      the      expectation      in      the      loss      function,      which      is      directly  \\n \\n  derived      from      (18);      second,      the      current      parameter   \\n \\n@      is      updated      by      computing      the      gradient      of      the      loss  \\n \\n  Algorithm   \\n \\n2      Reward      backpropagation</p><p>1:      Require:      Pre-trained      model      {N(p(21,      t;      pre),      77      (t))      }i_-74.1,      batch      size      m,      parameter   \\n \\na   \\n \\n€      R*,  \\n \\n  learning      rate   \\n \\n7       2:      Train   \\n \\na      differentiable      reward      function      (if      reward      feedback      is      not      differentiable)  \\n \\n  3:      Initialize:      0;   \\n \\n=      Opre  \\n \\n  4:      for      s   \\n \\n€      {1,---   \\n \\n,      S|]      do  \\n \\n  5:      Collect   \\n \\nm      samples      {a      (0)      }9_»      from   \\n \\na      current      diffusion      model      (i.e.,      generating      by      sequen- tially      running      polices      {N(p(2,      t;      9),      07(t))      }i_74,      from   \\n \\nt   \\n \\n=   \\n \\nT   \\n \\n+      1tot   \\n \\n=      1)  \\n \\n  6:      Update      6,      to      0,11:</p><p> \\n \\n  Baar      Ba   \\n \\n|      ESF      (al(B)      —c      Sp      WALee      Bt)   \\n \\n—      vas\"):      Bore?\\nYY  \\n \\n  s+l      Ss      v   \\n \\nm   \\n \\n4      r      Lo   \\n \\na      20?\\n(t)      0=0.°  \\n \\n  i=1      t=T+1 (22)</p><p>7:      end      for  \\n \\n  8:      Output:      Policy      {~(-   \\n \\n|      -;4s)      inna</p><p>function.</p><p> \\n \\n  Advantages      over      PPO.      This      approach      offers      further      simplicity      in      implementation      in   \\n \\na      case      where  \\n \\n  we      already      have   \\n \\na      pre-trained      differentiable      reward      model.\\nFurthermore,      the      training      speed      is      much  \\n \\n  faster      since      we      are      directly      back-propagating      from      rewards.</p><p> \\n \\n  Potential      disadvantages      over      PPO.      Reward      backpropagation      may      face      memory      inefficiency  \\n \\n  issues.\\nHowever,      there      are      strategies      to      mitigate      this      challenge.\\nFirstly,      implementing      gradient  \\n \\n  accumulation      can      help      to      keep   \\n \\na      large      batch      size.\\nSecondly,      as      proposed      in      DRaFT      (Clark      et      al.,  \\n \\n  2023),      propagating      rewards      backward      from      time   \\n \\n0      to      k      (k      is      an      intermediate      step      smaller      than      7’)  \\n \\n  and      updating      policies      from      k      to   \\n \\n0      can      still      yield      high      performance.\\nThirdly,      drawing      from      insights      in  \\n \\n  the      literature      on      neural      SDE/ODE      (Chen      et      al.,      2018),      more      memory-efficient      advanced      techniques  \\n \\n  such      as      adjoint      methods      could      be      helpful.</p><p> \\n \\n  Another      potential      drawback      is      the      requirement      for      “differentiable”      reward      functions.\\nOften,  \\n \\n  reward      functions      are      obtained      in   \\n \\na      non-differentiable      black-box      way      (e.g.,      computational      feedback  \\n \\n  derived      from      physical      simulations).\\nIn      such      scenarios,      using      direct      backpropagation      necessitates  \\n \\n  the      learning      of      differentiable      reward      functions      even      if      accurate      reward      feedback      is      available.\\nThis  \\n \\n  learning      step      can      pose      challenges      as      it      involves      data      collection      and      constructing      suitable      reward  \\n \\n  models.</p><p>6\\n \\n   RL-Based      Fine-Tuning      Algorithms      2:      Distribution-Constrained</p><h3>P(x,      |e      ;      5)</h3><p>t;      0)   \\n \\n—      t:      9Pre)      ||2  \\n \\n  F4(Xo,      Lt)   \\n \\n=      —r(ap)   \\n \\n+      ole   \\n \\n?      ae   \\n \\n’      II     .</p><p>lo=0.></p><p>KL      term 6:      end      for  \\n \\n  7;      Output:      Policy      {p.(-   \\n \\n|      0s)      Jars</p><p> \\n \\n  current      policies      to      construct      the      loss      function      (inspired      by      policy      gradient      formulation);      secondly,  \\n \\n  the      parameter   \\n \\n6      is      updated      by      computing      the      gradient      of      the      loss      function.</p><p> \\n \\n  PPO      offers      several      advantages.\\nThe      approach      is      known      for      its      stability      and      relatively      straight-  \\n \\n  forward      implementation.\\nStability      comes      from      the      conservative      parameter      updates.\\nIndeed,      PPO       builds      upon      TRPO      (Schulman      et      al.,      2015),      where      parameters      are      conservatively      updated      with   \\n \\na      KL  \\n \\n  penalty      term      (between      @,,,      and      @,)      to      prevent      significant      deviation      from      the      current      parameter.\\nThis  \\n \\n  gives      us      stability      in      the      optimization      landscape.\\nFurthermore,      in      Algorithm      1,      we      do      not      necessarily  \\n \\n  need      to      rely      on      value      functions,      although      they      could      be      useful      for      variance      reduction.\\nAs      discussed  \\n \\n  in      the      subsequent      subsection,      this      can      be      advantageous      compared      to      other      methods,      especially      since  \\n \\n  learning      value      functions      can      be      challenging      in      high-dimensional      spaces,      particularly      within      the  \\n \\n  context      of      diffusion      models.</p><h3>5.2      Direct      Reward      Backpropagation</h3><p>tion  \\n \\n  Another      standard      approach      is   \\n \\na      differentiable      optimization      (Clark      et      al.,      2023;      Prabhudesai      et      al.,  \\n \\n  2023;      Uehara      et      al.,      2024),      where      gradients      are      directly      propagated      from      reward      functions      to      update  \\n \\n  policies.</p><p> \\n \\n  The      entire      algorithm      is      detailed      in      Algorithm      2.\\nThis      reward      backpropagation      entails      an      iterative  \\n \\n  process      of      updating   \\n \\na      parameter      0.\\nEach      iteration      comprises      two      steps;      firstly,      samples      are      generated  \\n \\n  by      executing      current      policies      to      approximate      the      expectation      in      the      loss      function,      which      is      directly  \\n \\n  derived      from      (18);      second,      the      current      parameter   \\n \\n@      is      updated      by      computing      the      gradient      of      the      loss  \\n \\n  Algorithm   \\n \\n2      Reward      backpropagation</p><p>1:      Require:      Pre-trained      model      {N(p(21,      t;      pre),      77      (t))      }i_-74.1,      batch      size      m,      parameter   \\n \\na   \\n \\n€      R*,  \\n \\n  learning      rate   \\n \\n7       2:      Train   \\n \\na      differentiable      reward      function      (if      reward      feedback      is      not      differentiable)  \\n \\n  3:      Initialize:      0;   \\n \\n=      Opre  \\n \\n  4:      for      s   \\n \\n€      {1,---   \\n \\n,      S|]      do  \\n \\n  5:      Collect   \\n \\nm      samples      {a      (0)      }9_»      from   \\n \\na      current      diffusion      model      (i.e.,      generating      by      sequen- tially      running      polices      {N(p(2,      t;      9),      07(t))      }i_74,      from   \\n \\nt   \\n \\n=   \\n \\nT   \\n \\n+      1tot   \\n \\n=      1)  \\n \\n  6:      Update      6,      to      0,11:</p><p> \\n \\n  Baar      Ba   \\n \\n|      ESF      (al(B)      —c      Sp      WALee      Bt)   \\n \\n—      vas\"):      Bore?\\nYY  \\n \\n  s+l      Ss      v   \\n \\nm   \\n \\n4      r      Lo   \\n \\na      20?\\n(t)      0=0.°  \\n \\n  i=1      t=T+1 (22)</p><p>7:      end      for  \\n \\n  8:      Output:      Policy      {~(-   \\n \\n|      -;4s)      inna</p><p>function.</p><p> \\n \\n  Advantages      over      PPO.      This      approach      offers      further      simplicity      in      implementation      in   \\n \\na      case      where  \\n \\n  we      already      have   \\n \\na      pre-trained      differentiable      reward      model.\\nFurthermore,      the      training      speed      is      much  \\n \\n  faster      since      we      are      directly      back-propagating      from      rewards.</p><p> \\n \\n  Potential      disadvantages      over      PPO.      Reward      backpropagation      may      face      memory      inefficiency  \\n \\n  issues.\\nHowever,      there      are      strategies      to      mitigate      this      challenge.\\nFirstly,      implementing      gradient  \\n \\n  accumulation      can      help      to      keep   \\n \\na      large      batch      size.\\nSecondly,      as      proposed      in      DRaFT      (Clark      et      al.,  \\n \\n  2023),      propagating      rewards      backward      from      time   \\n \\n0      to      k      (k      is      an      intermediate      step      smaller      than      7’)  \\n \\n  and      updating      policies      from      k      to   \\n \\n0      can      still      yield      high      performance.\\nThirdly,      drawing      from      insights      in  \\n \\n  the      literature      on      neural      SDE/ODE      (Chen      et      al.,      2018),      more      memory-efficient      advanced      techniques  \\n \\n  such      as      adjoint      methods      could      be      helpful.</p><p> \\n \\n  Another      potential      drawback      is      the      requirement      for      “differentiable”      reward      functions.\\nOften,  \\n \\n  reward      functions      are      obtained      in   \\n \\na      non-differentiable      black-box      way      (e.g.,      computational      feedback  \\n \\n  derived      from      physical      simulations).\\nIn      such      scenarios,      using      direct      backpropagation      necessitates  \\n \\n  the      learning      of      differentiable      reward      functions      even      if      accurate      reward      feedback      is      available.\\nThis  \\n \\n  learning      step      can      pose      challenges      as      it      involves      data      collection      and      constructing      suitable      reward  \\n \\n  models.</p><p>6\\n \\n   RL-Based      Fine-Tuning      Algorithms      2:      Distribution-Constrained</p><h2>Approaches</h2><p> \\n \\n  In      this      section,      following      Section      4.2,      we      present      three      additional      algorithms      (reward-weighted  \\n \\n  MLE,      value-weighted      sampling,      and      path      consistency      learning)      aimed      at      solving      the      RL      problem  \\n \\n  of      interest      defined      by      Equation      (18).\\nIn      the      context      of      standard      RL,      these      algorithms      are      tailored  \\n \\n  to      align      closely      with      reference      policies,      specifically      pre-trained      diffusion      models      in      our      context.\\n \\n \\n  Formally,      indeed,      all      algorithms      in      this      section      are      not      well-defined      when   \\n \\na   \\n \\n=   \\n \\n0      (1.e.,      without  \\n \\n  entropy      regularization).\\nHence,      we      categorize      these      three      algorithms      as      distribution-constrained  \\n \\n  approaches.</p><p> \\n \\n  The      algorithms      in      this      section      excel      in      preserving      the      characteristics      of      pre-trained      diffusion  \\n \\n  models.\\nPractically,      this      property      becomes      especially      crucial      when      reward      functions      are      learned      from  \\n \\n  training      data,      and      we      want      to      avoid      being      fooled      by      distribution      samples      (a.k.a.      overoptimization      as  \\n \\n  detailed      in      Section      7.3).\\nHowever,      as   \\n \\na      caveat,      this      property      might      also      pose      challenges      in      effectively  \\n \\n  generating      high-reward      samples      beyond      the      training      data.\\nThis      implies      that      these      approaches      may  \\n \\n  not      be      suitable      when      accurate      reward      feedback      is      readily      available      without      learning.\\nHence,      we  \\n \\n  generally      recommend      using      them      when      reward      functions      are      unknown.</p><h3>6.1      Reward-Weighted      MLE</h3><p>MLE  \\n \\n  Here,      we      elucidate      an      approach      based      on      reward-weighted      MLE      (Peters      et      al.,      2010),   \\n \\na      technique  \\n \\n  commonly      employed      in      offline      RL      (Peng      et      al.,      2019).\\nWhile      Fan      et      al.\\n(2023,      Algorithm      2)      and  \\n \\n  Zhang      and      Xu      (2023)      propose      variations      of      reward-weighted      MLE      for      diffusion      models,      the      specific  \\n \\n  formulation      of      reward-weighted      MLE      discussed      here      does      not      seem      to      have      been      explicitly      detailed  \\n \\n  previously.\\nTherefore,      unlike      the      previous      section,      we      start      by      outlining      the      detailed      rationale      for  \\n \\n  this      approach.\\nSubsequently,      we      provide   \\n \\na      comprehensive      explanation      of      the      algorithm.\\nFinally,      we  \\n \\n  delve      into      its      connection      with      the      original      training      loss      of      diffusion      models.</p><p>Motivation.\\nFirst,      from      Theorem      2,      recall      the      form      of      the      optimal      policy      p¥(x_1|x,):</p><p>exp(v¢—-1      (+1)      /a:)      pe      (a1      |) exp(u;(x;)/@)  \\n \\n  Py      (Lt-1|2t)   \\n \\n= Now,      we      have:</p><p>p=      argmin      Ey,xu,[KL(p;      (-|2+)||Pe(-|2+))|  \\n \\n  pt:X      A(X) where      u,   \\n \\n€      A(%)      is   \\n \\na      roll-in      distribution      encompassing      the      entire      space      V      This      can      be      reformulated  \\n \\n  as      value-weighted      MLE      as      follows.</p><p>Lemma   \\n \\n1      (Value-weighted      MLE).\\nWhen      II,   \\n \\n=      [¥   \\n \\n—      A(4)],      the      policy      pt      is      equal      to</p><p>Ut—-1\\\\\\\\\\\\\\\\      Tt-1  \\n \\n  PC)   \\n \\n=      Ar      gMAX      Bey,      sph      (ne)      seeme      lex»      (eaGey      ’)      log      r(e-aley)     .</p><p>t\\n \\n   t\\n \\n    This      lemma      illustrates      that      if      v,-;      is      known,      py      can      be      estimated      using      weighted      maximum  \\n \\n  likelihood      estimation      (MLE).\\nWhile      this      formulation      is      commonly      used      in      standard      RL      (Peng      et      al.,  \\n \\n  2019),      in      our      context      of      fine-tuning      diffusion      models,      learning   \\n \\na      value      function      is      often      challenging.\\n \\n \\n  Interestingly,      this      reward-weighted      MLE      can      be      performed      without      directly      estimating      the      soft      value  \\n \\n  function      after      proper      reformulation.\\nTo      demonstrate      this,      let’s      utilize      the      following      lemma:</p><h3>6.1      Reward-Weighted      MLE</h3><p>MLE  \\n \\n  Here,      we      elucidate      an      approach      based      on      reward-weighted      MLE      (Peters      et      al.,      2010),   \\n \\na      technique  \\n \\n  commonly      employed      in      offline      RL      (Peng      et      al.,      2019).\\nWhile      Fan      et      al.\\n(2023,      Algorithm      2)      and  \\n \\n  Zhang      and      Xu      (2023)      propose      variations      of      reward-weighted      MLE      for      diffusion      models,      the      specific  \\n \\n  formulation      of      reward-weighted      MLE      discussed      here      does      not      seem      to      have      been      explicitly      detailed  \\n \\n  previously.\\nTherefore,      unlike      the      previous      section,      we      start      by      outlining      the      detailed      rationale      for  \\n \\n  this      approach.\\nSubsequently,      we      provide   \\n \\na      comprehensive      explanation      of      the      algorithm.\\nFinally,      we  \\n \\n  delve      into      its      connection      with      the      original      training      loss      of      diffusion      models.</p><p>Motivation.\\nFirst,      from      Theorem      2,      recall      the      form      of      the      optimal      policy      p¥(x_1|x,):</p><p>exp(v¢—-1      (+1)      /a:)      pe      (a1      |) exp(u;(x;)/@)  \\n \\n  Py      (Lt-1|2t)   \\n \\n= Now,      we      have:</p><p>p=      argmin      Ey,xu,[KL(p;      (-|2+)||Pe(-|2+))|  \\n \\n  pt:X      A(X) where      u,   \\n \\n€      A(%)      is   \\n \\na      roll-in      distribution      encompassing      the      entire      space      V      This      can      be      reformulated  \\n \\n  as      value-weighted      MLE      as      follows.</p><p>Lemma   \\n \\n1      (Value-weighted      MLE).\\nWhen      II,   \\n \\n=      [¥   \\n \\n—      A(4)],      the      policy      pt      is      equal      to</p><p>Ut—-1\\\\\\\\\\\\\\\\      Tt-1  \\n \\n  PC)   \\n \\n=      Ar      gMAX      Bey,      sph      (ne)      seeme      lex»      (eaGey      ’)      log      r(e-aley)     .</p><p>t\\n \\n   t\\n \\n    This      lemma      illustrates      that      if      v,-;      is      known,      py      can      be      estimated      using      weighted      maximum  \\n \\n  likelihood      estimation      (MLE).\\nWhile      this      formulation      is      commonly      used      in      standard      RL      (Peng      et      al.,  \\n \\n  2019),      in      our      context      of      fine-tuning      diffusion      models,      learning   \\n \\na      value      function      is      often      challenging.\\n \\n \\n  Interestingly,      this      reward-weighted      MLE      can      be      performed      without      directly      estimating      the      soft      value  \\n \\n  function      after      proper      reformulation.\\nTo      demonstrate      this,      let’s      utilize      the      following      lemma:</p><h2>Algorithm      3      Reward-weighed      MLE</h2><p>:\\n \\n   Require:      Pre-trained      model      {N(p(+,      t;      Opre),      07      (t))      }i-741,      batch      size      m,      parameter   \\n \\na   \\n \\n€      Rt, learning      rate   \\n \\n7 — 2:      Initialize:      6;   \\n \\n=      Opre  \\n \\n  3:      for      s   \\n \\n€      [1,---      ,S]      do 4.      fork   \\n \\n€      [2      +1,---      ,1]      do  \\n \\n  5:      Collect   \\n \\nm      samples      {aye      from</p><p> \\n \\n \\na      policy      pr+i(:   \\n \\n|      -3Os)i-°+      Peer|5      9s),      PP      Cl).\\nPr      Cl).\\n \\n \\n  6:      end      for  \\n \\n  7:      Update      @,      to      6,1;      as      follows:</p><p>1\\n \\n   m\\n \\n   (i,t)      (i,t)      (i,t)      4,   \\n \\n2       r(x      XL,   \\n \\n1   \\n \\n—      play’,      t3   \\n \\n0 t=T+1      i=1   \\n \\no      tor}</p><p>8:      end      for  \\n \\n  9:      Output:      Policy      {p:(-   \\n \\n|      39s)      }ir41</p><p>Lemma   \\n \\n2      (Characterization      of      soft      optimal      value      functions).</p><p>v,(@e)      ro0) exp      (ee)   \\n \\n~      Bey      mpP      (a1),      tr_1      pe,      (et)      lex»   \\n \\n(   \\n \\na   \\n \\n-     .</p><p>Recall      Eg,py|-|24]      means      E       zo~pP\"®      (1)      ae-1p?™S      (e)      [|e]:</p><p>Proof.\\nThis      is      obtained      by      recursively      using      the      soft-Bellman      equation      (20):</p><p>en      (22D)   \\n \\n&      few      (2%)      spears      ates   \\n \\n=      >=      Bie      fxn      (2) (23)</p><h1>O</h1><p>Algorithm.\\nNow,      we      are      ready      to      present      the      algorithm.\\nBy      combining      Lemma   \\n \\n|      and      Lemma      2,  \\n \\n  we      obtain      the      following.</p><p>Lemma   \\n \\n3      (Reward-weighted      MLE).\\nWhen      Il,   \\n \\n=      [¥   \\n \\n>      A(X)], *\\n \\n   r(x)</p><p>Pye   \\n \\n=      ATBMAX      Ep   \\n \\ny      mph      (1)y--      ee      a~pP      (este      exw   \\n \\n(   \\n \\na   \\n \\n)      tor      r(eale)   \\n \\n,      (25) Pt   \\n \\nt       Proof.\\nUsing      Lemma      2,      we      have Up—-1(Lt-1)  \\n \\n  By,      ywpPP      (-[ae),cevur      lexp   \\n \\n(   \\n \\na   \\n \\n)      log      r(esale)</p><p>r(x)  \\n \\n  The      rest      of      the      proof      is      obvious      by      using      Lemma      1.   \\n \\nO 0  \\n \\n  =      Ba      spledirnn      [Bp      [exp      (TE)      joa]      town      (esal)</p><p>r(x) =\\n \\n   Legg?\"\\n(01)      yeep\",      (ae)      eewur      exw      (a)      log      r(es-al)      :</p><p> \\n \\n  Then,      after      approximating      the      expectation      in      (25),      by      using   \\n \\na      Gaussian      policy      class      with      the  \\n \\n  mean      parameterized      by      neural      networks      as   \\n \\na      policy      class      II;,      we      can      estimate      p;.\\nFinally,      the      entire  \\n \\n  algorithm      is      described      in      Algorithm      3.  \\n \\n  Here,      we      give      two      remarks.\\nThis      is      an      off-policy      algorithm.\\nHence,      we      can      use      any      roll-in  \\n \\n  policies      as      wu;      in      Lemma      3.\\nIn      Algorithm      3,      we      use      the      current      policy      as   \\n \\na      roll-in      policy.\\nAdditionally,  \\n \\n  in      Algorithm      3,      the      loss      function      (24)      is      derived      by      recalling      that,      up      to      constant,   \\n \\n—      log      p;(2_1|2:)  \\n \\n  is      equal      to   \\n \\nA      ;</p><p> \\n \\n  Advantages      and      potential      disadvantages.\\nLike      PPO      in      Section      5.1,      this      approach      is      expected      to  \\n \\n  be      memory      efficient,      and      does      not      require      learning      differentiable      reward      functions,      which      can      often  \\n \\n  be      challenging.\\nHowever,      compared      to      direct      reward      backpropagation      Section      5.2,      it      might      not      be  \\n \\n  as      computationally      efficient.\\nFurthermore,      unlike      PPO,      this      algorithm      is      not      effective      when   \\n \\na   \\n \\n=      0,  \\n \\n  potentially      limiting      its      ability      to      generate      samples      with      extremely      high      rewards.</p><h2>6.1.1      Relation      with      Loss      Functions      for      the      Original      Training      Objective</h2><p>tive In      this      subsection,      we      explore      the      connection      with      the      original      loss      function      of      pre-trained      diffusion  \\n \\n  models.\\nTo      see      that,      as      we      see      in      Section      1.1.1,      recall ol)   \\n \\n=      al   \\n \\n4      (0.50,   \\n \\n—      1/o%eg,,,      (a1,   \\n \\nT   \\n \\n—      t)](5t)      +e?\\n02,      €)   \\n \\n~      N(0,1).</p><p> \\n \\n \\n/       \\\\\\\\\\\\\\\\-  \\n \\n  (al?\\nt;Opre) Then,      the      loss      function      (24)      in      reward-weighted      MLE      reduces      to</p><table><th><td colSpan=1>      =      r(x”)       A,      —      Vo      S>      Ss\"      exp      |      ~~      —       t=T+1      i=1</td><td colSpan=1>      .      _      ;      2       (i,t)      (ay,      Tt;      Opre)      _      eal,      T      -t;      0)       ey      _       {or}?</td><td colSpan=1>|o=0</td></th><tr><td colSpan=1></td><td colSpan=1>      2</td><td colSpan=1>      (26)</td></tr></table><p>This      objective      function      closely      resembles      the      reward-weighted      version      of      the      loss      function      (5)      used  \\n \\n  for      training      pre-trained      diffusion      models.</p><h3>6.2      Value-Weighted      Sampling</h3><p>ling  \\n \\n  Thus      far,      we      have      discussed      methods      for      fine-tuning      pre-trained      diffusion      models.\\nNow,      let’s      delve  \\n \\n  into      an      alternative      approach      during      inference      that      aims      to      sample      from      the      target      distribution      p,.\\n \\n \\n  without      explicitly      fine-tuning      the      diffusion      models.\\nIn      essence,      this      approach      involves      incorporating  \\n \\n  gradients      of      value      functions      during      inference      alongside      the      denoising      process      in      pre-trained      diffusion  \\n \\n  models.\\nHence,      we      refer      to      this      approach      as      value-weighted      sampling.\\nWhile      it      seems      that      this  \\n \\n  method      has      not      been      explicitly      formalized      in      previous      literature,      the      value-weighted      sampling  \\n \\n  closely      connects      with      classifier      guidance,      as      discussed      in      Section      8.1.  \\n \\n  Before      delving      into      the      algorithmic      details,      we      outline      the      motivation.\\nSubsequently,      we      present  \\n \\n  the      concrete      algorithm      and      discuss      its      advantages—specifically,      its      capability      to      operate      without      the  \\n \\n  necessity      of      fine-tuning      diffusion      models—and      its      disadvantage,      which      involves      the      need      to      obtain  \\n \\n  differentiable      value      functions.</p><p>Algorithm   \\n \\n4      Value-weighted      sampling 1:      Require:      Pre-trained      model      {p?\\n\"*(2,_1|r1)      }:   \\n \\n=      {N(0(      22,      t;      Opre),      07(t))      be.\\n \\n \\n  2:      Estimate      v   \\n \\n:   \\n \\n¥   \\n \\nx      [0,7]   \\n \\n—      R      and      denote      it      by      i(.,      -) ¢\\n \\n   (1)      Monte-Carlo      approach      in      (28) e      (2)      Value      iteration      approach      (Soft      Q-learning)      in      (29)      in      Section      6.2.1 ¢\\n \\n   (3)      Approximation      using      Tweedie’s      formula      in      Section      6.2.2 3:      fort   \\n \\n€      [7      +1,---   \\n \\n,      1}      do  \\n \\n  4:      Set.\\na?\\n \\n \\nt      Ve0   \\n \\nx      xL=Lt  \\n \\n  P(t,      t)   \\n \\n=   \\n \\n(   \\n \\n)      \\\\\\\\\\\\\\\\   \\n \\nu   \\n \\n+      p(xz,t      Are)</p><p>5:      end      for  \\n \\n  6:      Output:      {N(p      O(      Le,      t),o7(t))      ery</p><p> \\n \\n  Motivation.\\nConsidering   \\n \\na      Gaussian      policy      7:1   \\n \\n~      N(/(2:,t),07(t)),      we      aim      to      determine  \\n \\n  P(x4,t;      0)      such      that      N(A(2x;,      t;      0),      0?(t))      closely      approximates      p*(-|x,).\\nHere,      typically,      we      have  \\n \\n  p(xz,t;0)   \\n \\n=      a,   \\n \\n+      (t)g(a;,t)      and      o?(t)   \\n \\n=      g(t)(ot)      for      certain      function   \\n \\ng   \\n \\n:   \\n \\nY   \\n \\nx      [0,7]   \\n \\n>      R¢  \\n \\n  and   \\n \\ng   \\n \\n:      [0,7]   \\n \\n—      R,      as      we      have      explained      in      Section      1.1.1.\\nThen,      using   \\n \\nx      as      equality      up      to      the  \\n \\n  normalizing      constant, Piles      |)      exp(eralar-s)/a)      exp      (BEE      OI)</p><p>2\\n \\n   exp(vy(am)   \\n \\n+      Ver(      ee)»      {a1   \\n \\n—      3)      exp      (—      “2      Clr      uF)</p><p>_\\n \\n   lve1   \\n \\n—      ae   \\n \\n—      (58)      9(t)      Vora)      /o   \\n \\n—      (6t)9(ae,      t))      |?\\n \\n \\n  esp      (-      0.59(#)      (6t)      )</p><h4>Hence,      we      can      approximate:</h4><p> \\n \\n  o°(t)      Vu;      (a4)  \\n \\n  a   \\n \\n+      p(      xz,      ti      Ore):      (27)  \\n \\n  P(x,      t)      od</p><p>Remark      5.\\nNote      while      the      above      derivation      is      heuristic,      it      is      formalized      in      Uehara      et      al.\\n(2024,  \\n \\n  Lemma      2)      in      the      continuous-time      formulation.</p><p> \\n \\n  Algorithm.\\nUtilizing      (27),      the      entire      algorithm      of      value-weighted      sampling      is      outlined      in      Algo-  \\n \\n  rithm      4.\\nThis      method      doesn’t      involve      updating      parameters      in   \\n \\na      diffusion      model;      hence,      it’s      not   \\n \\na       fine-tuning      method.</p><p> \\n \\n  Note      that      in      this      algorithm,      we      require   \\n \\na      form      of      v;(-)      to      compute      gradients.\\nThis      value      function  \\n \\n  can      be      estimated      through      regression      using      the      characterization      described      in      Lemma      2.\\nHere,      inspired  \\n \\n  by      Lemma      2,      we      use      the      following      loss      function:</p><p>1\\n \\n   om      (i,t)      (i,t)   \\n \\n°       o(2)   \\n \\n=      argmin      S-      S-      {es      (me)   \\n \\n—      exp   \\n \\n|   \\n \\n,      (28)</p><p>h:[R4,{0,T]]-R      t=T+1      i=1</p><p>where      the      data      is      collected      in      an      off-policy      way.\\nLater,      we      explain      two      alternative      approaches      in  \\n \\n  Section      6.2.1      and      Section      6.2.2.</p><p> \\n \\n  Advantages      and      potential      disadvantages.\\nThe      value-weighted      sampling      is      straightforward      and  \\n \\n  less      memory-intensive      since      it      avoids      fine-tuning.\\nTherefore,      it      presents      an      appealing      simple      option  \\n \\n  if      it      performs      well.\\nIndeed,      in      various      inverse      problems,      such      as      inpainting,      super-resolution,      and  \\n \\n  colorization,      setting      r()      as   \\n \\na      likelihood      of      the      measurement      model   \\n \\ny   \\n \\n=      g(a)      +e      (where   \\n \\ny      represents  \\n \\n  actual      measurements,   \\n \\n€      denotes      noise,      and   \\n \\ng      defines      the      measurement      function)      has      proven      highly  \\n \\n  successful      (Chung      et      al.,      2022;      Bansal      et      al.,      2023;      Chung      et      al.,      2022).</p><p> \\n \\n  The      potential      drawback      compared      to      fine-tuning      algorithms      so      far      (i.e.,      PPO      and      reward-  \\n \\n  weighted      MLE)      is      the      necessity      to      learn      differetiable      soft      value      functions      like      direct      reward      back-  \\n \\n  propagation.\\nThis      learning      process      is      often      not      straightforward      as      explained      in      Section      5.2.\\nThe  \\n \\n  previously      discussed      fine-tuning      algorithms      (i.e.,      PPO,      reward-weighted      MLE)      circumvent      this  \\n \\n  requirement      by      obtaining      rewards      to      go      via      Monte      Carlo      approaches,      thereby      avoiding      the      need      for  \\n \\n  soft      value      functions.</p><h2>6.2.1      Soft      Q-learning</h2><p>ning  \\n \\n  We      have      elucidated      that      leveraging      Lemma      2,      we      can      estimate      soft      value      functions      v;(-)      based  \\n \\n  on      Equation      (28)      in   \\n \\na      Monte      Carlo      way.\\nSubsequently,      these      soft      value      functions      are      used      in  \\n \\n  value-weighted      sampling      to      sample      from      the      target      distribution      p,.\\nAlternatively,      there      is      another  \\n \\n  method      that      involves      using      soft      Bellman      equations      to      estimate      soft      value      functions      v;(-).\\nThis  \\n \\n  technique      is      commonly      called      soft      Q-learning      in      the      context      of      standard      RL.</p><p> \\n \\n  First,      recalling      the      soft      Bellman      equations      in      (16),      we      have en      (22)   \\n \\n=      fey      (=)      pa      nd</p><p>Taking      the      logarithm,      we      obtain u(ay)   \\n \\n=      alog      f      exp      (eaen)      De      (@4-1   \\n \\n|      @4)dxy-1.</p><p>a</p><h3>Hence,</h3><p> \\n \\n \\n_   \\n \\n:      (xt)      Up-1(Lt-1)   \\n \\n°       vu   \\n \\n=      argmin      Ey,.u,   \\n \\n|   \\n \\n4      ——   \\n \\n—      log   \\n \\n|      exp   \\n \\n|      ————   \\n \\n]      Dpre(@e-1]      21)      da4_1  \\n \\n  a   \\n \\na h:X->R  \\n \\n  where      u,   \\n \\n€      A(%)      is      any      roll-in      distribution      that      covers      the      entire      space      V.      Using      this      relation      and  \\n \\n  replacing      the      expectation      E,,,.,,,      with      empirical      approximation,      we      are      able      to      estimate      soft      value  \\n \\n  functions      v;      in   \\n \\na      recursive      manner:</p><p> \\n \\n \\n1   \\n \\nm      ~(j—1)   \\n \\n°       aj   \\n \\n;   \\n \\ni      Ur      Ut   \\n \\ny       {6      (x)},   \\n \\n—       argmin   \\n \\ny      S      {ht   \\n \\n)   \\n \\n—      log   \\n \\n/      exp      (eee)      Ppre(@1—1|}      yan] h:      [R¢,[0,7]]|-R      t=T+1      i=1</p><p>where      the      data      is      collected      in      an      off-policy      way.</p><p> \\n \\n  Remark      6.\\nAlthough      soft      Q-learning      is      widely      used      in      standard      RL      (Schulman      et      al.,      2017),      it  \\n \\n  cannot      be      directly      applied      to      our      fine-tuning      context      without      resorting      to      value-weighted      sampling  \\n \\n  or      value-weighted      MLE.\\nThis      is      because,      even      if      we      estimate      soft-value      functions      as      %;,      substituting  \\n \\n  Uz      with      0,      in      the      soft-optimal      policy      results      in      an      unnormalized      policy.<ul><li>6.2.2.\\nApproximation      using      Tweedie’s      formula</li></ul></p><p> \\n \\n  So      far,      we      have      explained      two      approaches:   \\n \\na      Monte      Carlo      approach      and   \\n \\na      value      iteration      approach      to  \\n \\n  estimate      soft      value      functions.\\nHowever,      learning      value      functions      in      (28)      can      still      be      often      challenging  \\n \\n  in      practice.\\nTherefore,      we      can      employ      approximation      strategies      inspired      by      recent      literature      on  \\n \\n  classifier      guidance      (e.g.,      reconstruction      guidance      (Ho      et      al.,      2022),      manifold      constrained      gradients  \\n \\n  (Chung      et      al.,      2022),      universal      guidance      (Bansal      et      al.,      2023),      and      diffusion      posterior      sampling  \\n \\n  (Chung      et      al.,      2022)).</p><p> \\n \\n  Specifically,      we      adopt      the      following      approximation:</p><p>ur(xz)   \\n \\n=      alog      Egpry      lex      (“)      oy   \\n \\n=      alog      (/      exp      (om)      p      (colar      (30)</p><p>~\\n \\n   alog      (exw      (ceo)   \\n \\n)   \\n \\n»       £o(t1)   \\n \\n=      Egprey|xo   \\n \\n|      xe],      (31) a\\n \\n    =\\n \\n   1r(Xo(2t)).</p><p> \\n \\n  Here,      we      replace      the      integration      in      (30)      with   \\n \\na      Dirac      delta      distribution      with      the      posterior      mean.\\n \\n \\n  Importantly,      we      can      calculate      Zo      (x;)   \\n \\n=      E,»*-[xo   \\n \\n|      x4]      using      the      pre-trained      (score-based)      diffusion  \\n \\n  model      based      on      Tweedie’s      formula:</p><table><tr><td colSpan=1>      O12</td><td colSpan=1><p>]\\n \\n \\n  E,pre      [xo   \\n \\n|      x1   \\n \\n—      Ut   \\n \\n+      {or}      V      0g      Ge(&e)  \\n \\n  Le</p></td></tr></table><p>Recall      that      the      notation      p/?,      0?,      q,      are      defined      in      (3).\\nFinally,      by      recalling      V      log   \\n \\n@   \\n \\n=      Si      ore      (x;,t)      in  \\n \\n  score-based      diffusion      models,      we      can      approximate      Vv;(a)      with</p><h4>Le</h4><p> \\n \\n  and      plug      it      into      Algorithm   \\n \\n4      (i.e.,      value-weighted      sampling).\\n \\n \\n  As      mentioned      earlier,      this      approach      has      been      practically      widely      used      in      the      context      of      classifier  \\n \\n  guidance.\\nDespite      its      simplicity,      the      approximation      error      can      be      significant,      as      it      can      not      be  \\n \\n  diminished      even      with   \\n \\na      large      sample      size      because      the      discrepancy      from      (30)      to      (31)      is      not      merely   \\n \\na       statistical      error      that      diminishes      with      increasing      sample      size.</p><li>6.2.3.\\nZeroth-Order      Guidance      using      Path      Integral      Control</li><p> \\n \\n  In      the      previous      subsection      (Section      6.2.2),      we      discussed      an      approximation      technique      to      bypass      the  \\n \\n  necessity      of      learning      explicit      value      functions.\\nAnother      approach      to      bypass      this      requirement      is      to      use  \\n \\n  control-based      techniques      for      obtaining      soft-optimal      policies,      a.k.a.,      path      integral      control      (Theodorou  \\n \\n  et      al.,      2010;      Williams      et      al.,      2017;      Kazim      et      al.,      2024).</p><p> \\n \\n  Recall      that      we      have      p(x;,t;      6)   \\n \\n=      2;   \\n \\n+      (dt)g(x+,      t)      and      o°(t)   \\n \\n=      g(t)(dt),      the      motivation      behind this      approach      is      as      follows.\\nInitially,      we      have:</p><h5>Vo,      exp(v(#1)/@)</h5><p>=\\n \\n   Egppreyexp(vr_1      (22-1)      /@)   \\n \\n|      24]      (Soft      Bellman      equation) _\\n \\n   (1   \\n \\n—   \\n \\nx   \\n \\n—      (Ot)      G(a2,      t))?\\n=\\n \\n   Vz»,   \\n \\n{   \\n \\n[      explatena/ay)      exp      (-      0-5a(t)      (ot)   \\n \\n)      in,-}</p><p>=\\n \\n   Byes      [ese      a(era/a))      {ea      ~~      (61)g(nn,)}      EE      Cah</p><p>~\\n \\n   arp      Buty      lexr(era(ea/a))      Gera   \\n \\n—      21   \\n \\n~      (8t)ale      plod</p><p>_t   \\n \\n1       Ht)      ot)      Eg,prey      [E      ppp}      [exp(r(2o)/a)   \\n \\n|      T-1]      e|24|   \\n \\n=      Woon)      [exp(r(xo/a))e|x¢]      ;</p><h5>Now,      we      obtain:</h5><p>o7(t)Vnv(ae)   \\n \\n1      Eqppry      lexp(r(wo)/a)      eel    \\n \\nQ   \\n \\n~   \\n \\na   \\n \\n*      Egyprey      lexp(r(2o)/a))      |x]      (32)</p><p> \\n \\n  Importantly,      this      approach      does      not      require      any      differentiation.\\nTherefore,      by      running      policies  \\n \\n  from      pre-trained      models      and      simply      using      Monte      Carlo      approximation      in      both      the      denominator  \\n \\n  and      numerator,      we      can      estimate      the      above      quantity      (the      right-hand      side      of      (32))      without      making  \\n \\n  any      models      (classifiers),      unlike      classifier      guidance.\\nNote      while      this      approach      is      widely      used      in      the  \\n \\n  control      community,      it      may      not      be      feasible      for      diffusion      models      due      to      the      high      dimensionality      of  \\n \\n  the      input      space.</p><h3>6.3      Path      Consistency      Learning      (Losses      Often      Used      in      Gflownets)</h3><p>ets)  \\n \\n  Now,      we      explain      how      to      apply      path      consistency      learning      (PCL)      (Nachum      et      al.,      2017)      to      fine-  \\n \\n  tune      diffusion      models.\\nIn      the      Gflownets      literature      (Bengio      et      al.,      2023),      it      seems      that      this      variant      is  \\n \\n  utilized      as      either   \\n \\na      detailed      balance      or   \\n \\na      trajectory      balance      loss,      as      discussed      in      Mohammadpour  \\n \\n  et      al.\\n(2023);      Tiapkin      et      al.\\n(2023);      Deleu      et      al.\\n(2024).\\nHowever,      to      the      best      of      our      knowledge,  \\n \\n  the      precise      formulation      of      path      consistency      learning      in      the      context      of      fine-tuning      diffusion      models  \\n \\n  has      not      been      established.\\nTherefore,      we      start      by      elucidating      the      rationale      of      PCL.\\nSubsequently,  \\n \\n  we      provide   \\n \\na      comprehensive      explanation      of      the      PCL.\\nFinally,      we      discuss      its      connection      with      the  \\n \\n  literature      on      Gflownets.</p><p>Motivation.\\nHere,      we      present      the      fundamental      principles      of      the      PCL.\\nTo      start      with,      we      prove      the  \\n \\n  following      lemma,      which      characterizes      soft-value      functions      and      soft-optimal      policies      recursively.</p><p>Algorithm   \\n \\n5      Path      Consistency      Learning      (Training      with      detailed      balance      loss)  \\n \\n  1:      Require:      Diffusion-model      {N(p(2,t;),07(t))}i-74,,  \\n \\n  -pre-trained   \\n \\n=      model  \\n \\n  {N      (p(t,      t;      Opre),      07      (t))      }i_744,      batch      size      m,      parameter   \\n \\na   \\n \\n€      Rt,      learning      rate   \\n \\n7       2:      Set   \\n \\na      model      {v;(-;@)}      to      learn      optimal      soft      value      function,      and   \\n \\na      model      {p;(-|-;@)}      to      learn  \\n \\n  optimal      polices.\\n \\n \\n  3:      Initialize:      0;   \\n \\n=      Opre  \\n \\n  4:      for      s   \\n \\n=      {1,---      ,S}do  \\n \\n  5:      Collect   \\n \\nm      samples      {a      (0)      }?_7,,      from   \\n \\na      current      diffusion      model      (i.e.,      generating      by      sequen- tially      running      polices      {N(p(2z,      t;      8),      07(t))      }i_-74,      from   \\n \\nt   \\n \\n=   \\n \\nT      to   \\n \\nt   \\n \\n=      0)  \\n \\n  6:      Setuo=r</p><li>(i)   \\n \\n?       a!\\n \\n \\n3   \\n \\na   \\n \\n4      Up—-1(@      13      ds)      re  \\n \\n  bs41   \\n \\n—      bs   \\n \\n—      Vo   \\n \\n3   \\n \\na      9)   \\n \\n+      log      p:(a\\\\\\\\\\\\\\\\,      |a\\\\\\\\\\\\\\\\;0,)   \\n \\n-      EP   \\n \\n—      Jog      rl      >}      Ibe:</li><p>a\\n \\n \\n  t=T+1      i=1  \\n \\n  (c}      ite)      ()      via)      43s)   \\n \\n;       Os41   \\n \\n<      Os   \\n \\n—      Vo   \\n \\n3      TE      velee      ids)      —S<   \\n \\n+      log      pe(ay      2s      |p      0)      —§      log      pee      (ar?\\nifort?\\nJe      lass</p><p>t=T+1      i=1 7:      end      for  \\n \\n  8:      Output:      {p,(x1-1      [215      Os)      fe</p><h3>Lemma      4      (1-step      Consistency      Equation).</h3><li>(2)   \\n \\n+      log      pi      (aa      |a1)   \\n \\n=      (eaeen)   \\n \\n+      log      pp      (x+-1|2+)      (33)</li><p> \\n \\n  Proof.\\nWe      consider      the      marginal      distribution      with      respect      to      x,      and      2;_,      induced      by      the      soft-  \\n \\n  optimal      policy,      and      denote      it      [(z;,      2:1)   \\n \\n€      A(X   \\n \\nx      X).\\nThen,      it      is      clearly      I(x,)l(a,_1   \\n \\n|      x)   \\n \\n=       I(x4-1)l      (xz   \\n \\n|      a1).\\nNow,      from      Theorem   \\n \\n2      that      characterizes      marginal      distributions,      and      Theorem   \\n \\n3       that      characterizes      posterior      distributions,      this      results      in      :</p><p>1\\n \\n   UE      L      re   \\n \\nx   \\n \\n1      Otte      me  \\n \\n  exp   \\n \\n(   \\n \\ni      2)      Pe      (xt)   \\n \\nx      pe      (®e-1|@e)   \\n \\n=   \\n \\nB      exp      (faeev)      Pra      (@1-1)   \\n \\nX      Pe      (1/01) a\\n \\n   —_-_——”’   \\n \\nC   \\n \\na      ~~  \\n \\n  C  \\n \\n  we  \\n \\n  Optimal      policy  \\n \\n  “~   \\n \\n“      Posterior      distribution  \\n \\n  Marginal      distribution      at   \\n \\nt      Marginal      distribution      at      t-1 Rearranging      yields:</p><p>1\\n \\n   UL(&   \\n \\n1      Up—1      (Le      re  \\n \\n  G      exp   \\n \\n(      ))   \\n \\nx      Di      (@-1|%4)   \\n \\n=   \\n \\nG      exp      (eaeen)   \\n \\nx      pe      (@4-1|24)</p><p>Taking      the      logarithm,      the      statement      is      concluded.\\nO</p><p>Algorithm.\\nBeing      motivated      by      the      relation      in      (33),      after      initializing      v9   \\n \\n=      r,      we      obtain      the  \\n \\n  recursive      equation:</p><p>2 Olen      U_-1      (Lt  \\n \\n  (pp)   \\n \\n=      argmin   \\n \\n—      Egany   \\n \\n|      oO   \\n \\n4      tog   \\n \\ng      (arp      ng)   \\n \\n—      PEED)   \\n \\n—      dog      pP\"      (ya      er) g):%3R,g2):¥      A(X)   \\n \\na   \\n \\no U   \\n \\n_   \\n \\nx   \\n \\n_      re      re  \\n \\n  =      (MEY)      tog      peal      altiansa)      +++      Flog      pe      (esl),  \\n \\n  which      is      an      extension      of      (33).\\nThe      loss      function      based      on      the      above      k-step      consistency      equation  \\n \\n  could      make      training      faster      without      learning      value      functions      at      every      time      point,      as      noted      in      the  \\n \\n  literature      in      PCL.\\nIn      the      extreme      case      (i.e.,      when      we      recursively      apply      it      with   \\n \\nt   \\n \\n=      7’),      we      obtain      the  \\n \\n  following.</p><li>(34)  \\n \\n  where      u,   \\n \\n€      A(X)      is      any      exploratory      roll-in      distribution.\\nBased      on      this      algorithm,      we      outline      the  \\n \\n  entire      algorithm      in      Algorithm      5.  \\n \\n  We      make      several      important      remarks      regarding      Algorithm      5.\\nFirstly,      while      we      use      on-policy      data  \\n \\n  collection,      technically,      any      policy      can      be      used      in      this      off-policy      algorithm,      like      reward-weighted  \\n \\n  MLE.\\nSecondly,      in      practice,      it      might      be      preferable      to      utilize   \\n \\na      sub-trajectory      from      x;      to      x;_,      based  \\n \\n  on      the      following      expression:</li><p>UFZ  \\n \\n  log      pi_      pa      (Gee      |Vt—ng1)   \\n \\n+      +++   \\n \\n+      log      pi      (4-1      |a4)   \\n \\n+      (2)</p><h3>Corollary      1      (7-step      consistency).</h3><p>log      pf      (xo|21)   \\n \\n+      +++   \\n \\n+      log      pp(wr-a|er)   \\n \\n+      log      pp      (x7)      (35) r(x      re      re      re  \\n \\n  —      (=)   \\n \\n+      log      p?\\n*(xo|v1)      +--+   \\n \\n+      log      pr\"      (ar-i|er)   \\n \\n+      log      pe      (xr).</p><p>Proof.\\nWe      consider      the      marginal      distribution      with      respect      to      7,      +--+   \\n \\n,   \\n \\n%      induced      by      the      soft-optimal  \\n \\n  policy,      and      denote      it      I(vp,---      ,--+      ,%o)   \\n \\n€   \\n \\n¥      X--+   \\n \\nx      X.      We      have Uar)l(ar—1   \\n \\n|      Lr)      tee      I(xo|21)   \\n \\n=      U(x      )l(a4   \\n \\n|      Xo)      te      (ar   \\n \\n|      LT-1)      (36) From      Theorem   \\n \\n2      that      characterized      marginal      distributions,      and      Theorem   \\n \\n3      that      characterize      posterior  \\n \\n  distributions,      the      left      hand      side      of      (36)      is      equal      to *\\n \\n   *\\n \\n   *</p><p>Pr      (xr)   \\n \\nx      pp      (r|er-1)      X-++   \\n \\nx      pi      (Xo|@1) Marginal      distribution      atT      Optimal      policy      at   \\n \\nT   \\n \\n—   \\n \\n1      Optimal      policy      at   \\n \\n1 and      the      right-hand      side      in      (36)      is      equal      to exp(r      (xo)      /a)      pre      pre      pre  \\n \\n  aq      Po      (xo)   \\n \\nX      po      (a1   \\n \\n|      Zo)      X+++   \\n \\nX      prey      (@r   \\n \\n|      er-1)-.\\n \\n \\n|      ~~  \\n \\n  Marginal      distribution      at   \\n \\n0      Posterior      distribution      Posterior      distribution By      rearranging      the      term,      we      obtain      (35).\\nO</p><p> \\n \\n  Comparison      with      Gflownets.\\nIn      the      Gflownets      literature,      similar      losses      are      used.\\nFor      instance,  \\n \\n  the      loss      derived      from      (33)      or      (35)      is      commonly      known      as   \\n \\na      detailed      balance      loss      (Bengio      et      al.,  \\n \\n  2023)      or   \\n \\na      trajectory      loss      (Malkin      et      al.,      2022),      respectively.</p><p> \\n \\n  Note      in      general,      the      literature      in      Gflownets      primarily      focuses      on      sampling      from      unnormalized  \\n \\n  models      (distributions      proportional      to      exp(r(a))).\\nHence,      reference      policies      (i.e,      {p?\"\"}\\n \\n \\n)      or      latent  \\n \\n  states      (1.e.,      %7.;      before      x9)      are      introduced      without      relying      on      pre-trained      diffusion      models.\\nIn  \\n \\n  contrast,      in      our      context,      we      use      policies      derived      from      pre-trained      diffusion      models      as      reference  \\n \\n  policies,      leveraging      them      as      our      prior      knowledge.</p><p>7\\n \\n   Fine-Tuning      Settings      Taxonomy</p><p> \\n \\n  So      far,      we      implicitly      assume      we      have      access      to      reward      functions.\\nHowever,      these      functions      are      often  \\n \\n  unknown      and      need      to      be      learned      from      data.\\nWe      classify      several      settings      in      terms      of      whether      reward  \\n \\n  functions      are      available      or,      if      not,      how      they      could      be      learned.\\nThis      section      is      summarized      in      Figure      3.</p><h4>Direct      backpropagation      (Sec.      5.2)       _       Differentiable</h4><p>_\\n \\n   ——_      Non-differentiable PP      5.1</p><h4>Accurate      rewards      O      (Sec.      5.1)</h4><p> \\n \\n  feedback   \\n \\n;    \\n \\n—      Reward-weighted      MLE      (Sec.      6.1  \\n \\n  Xo      Unknown      reward      Memory      effi-      oe      eward-welgnte      (Sec.      6.1) unctions      —_\\\\\\\\\\\\\\\\_—      ciency</p><h4>No      fine-tuning      Computational</h4><p>efficiency —\\n \\n    NN      Direct      backpropagation      (Sec.      5.2)</p><h5>Value-weighted      sampling      (Sec.      6.2)</h5><p>Figure      3:      Practical      recommendation      of      RL-based      algorithms      to      optimize      downstream      reward  \\n \\n  functions.</p><h6>7.1      Fine-Tuning      with      Known,      Differentiable      Reward      Functions</h6><p>ions  \\n \\n  When      accurate      differentiable      reward      functions      are      available      (e.g.,      standard      in      computer      vision      tasks),  \\n \\n  our      primary      focus      is      typically      on      computational      and      memory      efficiency.\\nIn      general,      we      advocate      for  \\n \\n  employing      reward      backpropagation      (c.f.      Algorithm      2)      for      its      computational      efficiency.\\nAlternatively,  \\n \\n  if      memory      efficiency      is   \\n \\na      significant      concern,      we      suggest      using      PPO      due      to      its      stability.</p><h6>7.2      Fine-Tuning      with      Black-Box      Reward      Feedback</h6><p>back  \\n \\n  Here,      we      explore      scenarios      where      we      have      access      to      accurate      but      non-differentiable      (black-box)  \\n \\n  reward      feedback,      often      found      in      scientific      simulations.\\nThe      emphasis      still      remains      primarily      on  \\n \\n  computational      and      memory      efficiency.\\nIn      such      scenarios,      due      to      the      non-differentiability      of      reward  \\n \\n  feedback,      we      recommend      using      PPO      or      reward-weighted      MLE      for      the      following      reasons.</p><p> \\n \\n  In      the      preceding      section,      we      advocated      for      the      use      of      reward      backpropagation      due      to      its      com-  \\n \\n  putational      advantages.\\nHowever,      when      dealing      with      non-differentiable      feedback,      the      advantage  \\n \\n  of      reward      backpropagation      may      diminish.\\nThis      is      because      learning      from      such      feedback      requires  \\n \\n  learning      differentiable      reward      functions      to      make      the      algorithm      work      as      we      discuss      in      Section      5.2.  \\n \\n  However,      obtaining   \\n \\na      differentiable      reward      function      can      be      challenging      in      many      domains.\\nFor  \\n \\n  example,      in      molecular      property      prediction,      molecular      fingerprints      are      informative      features,      and  \\n \\n  accurate      reward      functions      can      be      derived      by      training      simple      neural      networks      on      these      features  \\n \\n  (Pattanaik      and      Coley,      2020).\\nYet,      these      mappings,      grounded      in      prior      scientific      understanding,      are  \\n \\n  non-differentiable,      thereby      restricting      their      applicability.</p><p> \\n \\n  In      contrast,      PPO      and      reward-weighted      MLE      allow      for      policy      updates      without      the      explicit  \\n \\n  requirement      to      learn   \\n \\na      differentiable      reward      function,      offering   \\n \\na      significant      advantage      over      reward  \\n \\n  backpropagation.</p><h6>7.3      Fine-Tuning      with      Unknown      Rewards      Functions</h6><p>ions  \\n \\n  When      reward      functions      (or      computational      proxies      in      Section      7.2)      are      unavailable,      we      must      learn  \\n \\n  from      data      with      reward      feedback.\\nIn      such      cases,      compared      to      the      previous      scenarios,      two      important  \\n \\n  considerations      arise:</p><p>¢\\n \\n   Not      only      computational      or      memory      efficiency      but      also      feedback      efficiency      (i.e.,      sample  \\n \\n  efficiency      regarding      reward      feedback)      is      crucial.</p><p> \\n \\n \\n¢      Given      that      learned      reward      feedback      may      not      generalize      well      outside      the      training      data      distri-  \\n \\n  butions,      it      is      essential      to      constrain      fine-tuned      models      to      prevent      significant      divergence      from  \\n \\n  diffusion      models.\\nIn      these      situations,      not      only      soft      PPO      and      direct      backpropagation      but      also  \\n \\n  methods      discussed      in      Section      6,      such      as      reward-weighted      MLE,      can      be      particularly      effective.</p><p>Further,      the      scenario      involving      unknown      reward      functions      can      be      broadly      categorized      into      two      cases,  \\n \\n  which      we      will      discuss      in      more      detail.</p><p>Offline      scenario      (Figure      4).\\nIn      many      scenarios,      even  \\n \\n  in      cases      where      reward      functions      are      unknown,      we      of-  \\n \\n.     .</p><p>Offline      data      with ten      have      access      to      offline      data      with      reward      feedback      reward      feedback</p><li>(i)      (i)   \\n \\nO   \\n \\n;   \\n \\ni       a”      ra  \\n \\n.      ne      straightforward      approach      is      to</li><p> \\n \\n \\n{   \\n \\n,   \\n \\n(   \\n \\n)    \\n.\\n    .\\n \\n   g\\n \\n   PP  \\n \\n.      <=>      Learning      reward  \\n \\n  perform      regression      with      neural      networks      and      build   \\n \\na      models  \\n \\n  learned      regressor      *.\\nHowever,      this      learned      reward      func-      SS  \\n \\n  tion   \\n \\n*      might      not      generalize      well      beyond      the      distribution      x)  \\n \\n  of      the      offline      data.\\nIn      regions      outside      this      distribution,   \\n \\n7       could      assign   \\n \\na      high      value      even      when      the      actual      reward  \\n \\n  r      is      low      due      to      high      uncertainty.\\nConsequently,      we  \\n \\n  could      be      easily      misled      by      out-of-distribution      samples  \\n \\n  if      we      solely      optimize      7.\\n|Fine-tuning      diffusion      models   \\n \\n|       One      approach      to      alleviate      this      issue      is      to      adopt   \\n \\na       pessimistic      (i.e.,      conservative)      strategy,      as      proposed      in      Figure      4:      Offline      scenario  \\n \\n  Uehara      et      al.\\n(2024).\\nThis      technique      has      been      widely      Collecting      data]   \\n \\nY      Learning      reward  \\n \\n  applied      in      the      field      of      offline      RL      (Levine,      2018).\\nThe      aa  \\n \\n  main      idea      is      to      penalize   \\n \\n7      outside      the      distributions      us-  \\n \\n  ing      techniques      such      as      adding      an      explicit      penalty      term  \\n \\n  (Yu      et      al.,      2020;      Chang      et      al.,      2021),      bootstrapping,  \\n \\n  or      more      sophisticated      methods      (Kumar      et      al.,      2020;  \\n \\n  Xie      et      al.,      2021;      Rigter      et      al.,      2022;      Uehara      and      Sun,  \\n \\n  2021).\\nBy      doing      so,      we      can      avoid      being      fooled      by  \\n \\n  out-to-distribution      regions      while      still      benefiting      from  \\n \\n  the      extrapolation      capabilities      of      reward      models.</p><p>S—      diffusion      models      |</p><p>Figure      5:      Online      scenario</p><p> \\n \\n  Online      scenario      (Figure      5).\\nConsider      situations  \\n \\n  where      reward      functions      are      unknown,      but      we      can      gather      data      online.\\nThese      scenarios      are      often  \\n \\n  referred      to      as   \\n \\na      lab-in-the-loop      setting.</p><p> \\n \\n  In      such      scenarios,      Uehara      et      al.\\n(2024)      proposes      an      iterative      procedure      comprising      three      steps:</p><li>(1)      collecting      feedback      data      by      exploring      new      areas      (i.e.,      obtaining   \\n \\nx      from      current      diffusion      models  \\n \\n  and      corresponding      r(x)),      (2)      learning      the      reward      function      from      the      collected      feedback      data,      and</li><li>(3)      fine-tuning      current      diffusion      models      by      maximizing      the      learned      reward      function      enhanced      by  \\n \\n  an      optimistic      bonus      term,      using      algorithms      discussed      in      Section      4.2.\\nIn      contrast      to      standard      online  \\n \\n  PPO,      where      feedback      data      is      directly      used      in      the      fine-tuning      process,      actual      reward      feedback      is  \\n \\n  only      used      in      step      (2)      and      not      in      step      (3).\\nAn      additional      key      aspect      is      the      use      of      an      optimistic  \\n \\n  reward      function      that      encourages      exploration      beyond      the      distributions      of      the      current      diffusion      model.\\n \\n \\n  This      deliberate      separation      between      reward      learning      and      fine-tuning      steps,      coupled      with      the      use      of  \\n \\n  optimism,      significantly      enhances      feedback      efficiency.</li><p>8\\n \\n   Connection      with      Classifier      Guidance</p><p> \\n \\n  Classifier      guidance      (Dhariwal      and      Nichol,      2021)      is      commonly      used      for      conditional      generation      in  \\n \\n  diffusion      models.\\nInterestingly,      RL-based      methods      share   \\n \\na      close      connection      with      classifier      guidance.\\n \\n \\n  More      specifically,      in      this      section,      following      Zhao      et      al.\\n(2024),      we      elucidate      how      RL-based      methods  \\n \\n  can      be      applied      to      conditional      generation.\\nFurthermore,      from      this      unified      viewpoint,      we      highlight  \\n \\n  that      classifier      guidance      is      seen      as   \\n \\na      value-weighted      sampling      in      Section      4.2.</p><h6>8.1      Classfier      Guidance</h6><p>ance  \\n \\n  In      this      section,      we      first      introduce      classifier      guidance.\\nClassifier      guidance      utilizes      an      unconditional  \\n \\n  pre-trained      model      for      conditional      generation.\\nMore      specifically,      we      consider   \\n \\na      scenario      where      we  \\n \\n  have   \\n \\na      pre-trained      diffusion      model,      which      enables      us      to      sample      from      p?\\n\"*(x),      and      data      with      feedback  \\n \\n  {x,y},      where   \\n \\ny      represents      the      new      label      we      want      to      condition      on.\\nOur      objective      here      is      to      sample  \\n \\n  from      p(-|y)   \\n \\n€      [Y   \\n \\n>      ACR), P(-   \\n \\n|      y)   \\n \\nx      py(yl-)pP*(-).\\n(37)</p><p>where      p,   \\n \\n:   \\n \\nX      —>      A(Y)      denotes      the      conditional      distribution      of   \\n \\ny      given      x.</p><p>Motivation.\\nIn      classifier      guidance,      we      consider      the      following      policy:</p><p>pe      (-lare,      y)   \\n \\n=   \\n \\nN      (p(22,      t;      Opre)   \\n \\n+      a(t)      Ve      log      Vy      (Xt;      t),      a*(t))   \\n \\n’ dy(x,t)   \\n \\n=      Egy.\\nyap,      (-leo)      L(Y   \\n \\n=      y)|a,   \\n \\n=      2].</p><p>Under      the      continuous-time      formulation      of      diffusion      models,      applying      Doob’s      h-transform      (Rogers  \\n \\n  and      Williams,      2000),      it      is      shown      that      we      can      sample      from      p(x|y)      by      sequentially      running      {p7\"\\\\\\\\\\\\\\'(-|7+,      y)      }i-r41-</p><p>Algorithm.\\nSpecifically,      the      algorithm      comprises      two      steps.\\nIt’s      important      to      note      that      this      is      not  \\n \\n  a      fine-tuning      method      but      rather      an      inference-time      technique      as      we      freeze      the      pre-trained      model.</p><li>1. Learning      q,(x,t)      through      regression      by      gathering      data      containing      {z;,      y}.\\nHere,      for      each  \\n \\n  (Xo,      y),      using   \\n \\na      forward      policy      for      the      pre-training      (from   \\n \\n0      to      T),      we      obtain      2;      starting      from</li><p>Lo.</p><li>2. During      inference,      sequentially      execute      {p2\"(-|a,,      y)      }1_>   \\n \\n1      by      leverage      the      fact      that      compared</li><p>gui to      the      policy      in      pre-trained      diffusion      models,      the      mean      of      {p?\\n\"\\\\\\\\\\\\\\'(-|71,      y)      }i-74,      is      just      shifted  \\n \\n  with      an      additional      term      V,,      log      q,(2,      t).</p><p> \\n \\n  Notably,      the      method      described      above      can      be      reduced      to      value-weighted      sampling      (cf.      Sec-  \\n \\n  tion      6.2),      when      r(x,      y)   \\n \\n=      log      p(y|x).\\nInspired      by      such      an      observation,      we      apply      an      RL-based  \\n \\n  fine-tuning      method      for      conditional      generation      as      below.</p><h7>8.2.      RL-Based      Fine-Tuning      for      Conditional      Generation</h7><p>tion  \\n \\n  In      this      section,      following      Zhao      et      al.\\n(2024),      we      explain      an      RL-based      fine-tuning      approach      for  \\n \\n  conditional      generation.\\nThe      core      idea      is      that      setting      r(x,      y)   \\n \\n=      log      p(y|x)      allows      us      to      sample      from  \\n \\n  the      target      conditional      distribution      (37),      drawing      on      the      insights      discussed      in      Section      4.2.  \\n \\n  More      precisely,      introducing   \\n \\na      distribution      over   \\n \\ny      as      q(-),      we      formulate      the      following      RL      problem:</p><p>{Di      ft   \\n \\n=      argmax      Egy,      (-\\\\\\\\\\\\\\\\-y)}.yott      log      p(y|xo)   \\n \\n—      NeopyaKL(pe(-|e,      y)      ||P      (late).\\n \\n \\n  {prE[(V,.X)PA(A)]      Heyy (38)</p><p> \\n \\n  Compared      to      the      objective      function      (18)      in      Section      4.2,      the      main      differences      are:      (1)      policy      space  \\n \\n  is      expanded      to      accommodate      additional      control      (i.e.,      y):      pz(-|-,-)   \\n \\nC      [R¢   \\n \\nx   \\n \\nY   \\n \\n+      A(R®)],      (2)      the  \\n \\n  fine-tuning      objectives      are      set      as      log-likelihoods:      log      p(y|z).\\nThen,      following      Theorem      1,      we      can  \\n \\n  establish      the      following      result.</p><p>Corollary      2.\\nThe      distribution      induced      by      {p*}1_74      (ue.   \\n \\n[      {Teri      Ph      (ae-1   \\n \\n|      te,      y)}deur      is      the target      conditional      distribution      (37).</p><p> \\n \\n  The      above      corollary      suggests      that      for      conditional      generation,      we      can      utilize      various      off-the-shelf  \\n \\n  algorithms      discussed      in      Section      4.2      and      6,      such      as      PPO,      reward      backpropagation,      and      reward-  \\n \\n  weighted      MLE,      as      well      as      value-weighted      sampling.</p><p>9\\n \\n   Connection      with      Flow-Based      Diffusion      Models</p><p> \\n \\n  We      elucidate      the      close      relationship      between      bridge      matching      and      fine-tuning,      as      summarized      in  \\n \\n  Table      2.\\nBridge      (flow)      matching      (Liu      et      al.,      2022;      Tong      et      al.,      2023;      Lipman      et      al.,      2023;      Liu      et      al.,  \\n \\n  2022;      Shi      et      al.,      2024;      Albergo      and      Vanden-Eijnden,      2022),      has      recently      gained      popularity      as   \\n \\na       distinct      type      of      diffusion      model      from      score-matching-based      ones.\\nThese      works      suggest      that      training  \\n \\n  and      inference      speed      will      be      accelerated      due      to      the      ability      to      define   \\n \\na      more      direct      trajectory      from  \\n \\n  noise      to      data      distribution      than      score-matching-based      diffusion      models.</p><p> \\n \\n  The      connection      between      bridge      matching      and      fine-tuning      becomes      evident      in      the      continuous-  \\n \\n  time      formulation.\\nTo      begin,      we      first      provide   \\n \\na      brief      overview      of      the      continuous-time      formulation      of  \\n \\n  score-matching-based      diffusion      models.</p><p> \\n \\n  Table      2:      Comparison      between      training      score/flow-based      diffusion      models      and      fine-tuning.\\nWe  \\n \\n  optimize      the      parameter   \\n \\n6      in      P’,      which      represents      the      induced      distribution      over      trajectories      associated  \\n \\n  with      6.</p><h8>Loss      Note      of      Q</h8><p>Score-based      KL(Q\\\\\\\\\\\\\\'™¢|P?)      Qtime:      Time-reversal      of      reference      SDE      (reference training      SDE      is      defined      from   \\n \\n0      to      T’      where      the      initial      dis-  \\n \\n  tribute      at   \\n \\n0      is      data      distribution)</p><p> \\n \\n  Flow-based      KL(Q*°>      ||P?)      @Q*°°h:      Coupling      between      the      reference      SDE      from  \\n \\n  training   \\n \\nT      to   \\n \\n0      conditioning      at   \\n \\n0      (dist.      over      x.;|7o)      and  \\n \\n  a      data      distribution      at      time   \\n \\n0      (dis.      over      x).\\nNote  \\n \\n  that      reference      SDE      is      defined      from      7’      to      0.</p><p> \\n \\n  Fine-tuning      KL(P?||Q*°)      @*:      Coupling      between      the      pre-trained      SDE  \\n \\n  from   \\n \\nT      to   \\n \\n0      conditioned      on   \\n \\n0      (dist.      over      x7.1|X0)  \\n \\n  and   \\n \\na      data      distribution      at      time   \\n \\n0      (dis.      over      Xo) 9.1      Continuous      Time      Formulation      of      Score-Based      diffusion      models</p><p> \\n \\n  We      provide   \\n \\na      brief      overview      of      the      continuous-time      formulation      of      diffusion      models      (Song      et      al.,  \\n \\n  2021),      further      expanding      Section      1.1.1.\\nIn      this      formulation,      we      initially      define      the      forward      reference  \\n \\n  SDE,      spanning      from      time   \\n \\n0      to      7’      (e.g.,      variance      exploding      (VE)      process      or      variance      preserving      (VP)  \\n \\n  process),      by      setting      an      initial      distribution      at   \\n \\n0      as      the      data      distribution.\\nAdditionally,      the      reference  \\n \\n  SDE      is      tailored      to      converge      to      an      easy      distribution      at      time      7’,      such      as      the      normal      distribution.</p><p> \\n \\n  Subsequently,      we      consider      the      time-reversal      SDE      (Anderson,      1982),      progressing      from      T\\\\\\\\\\\\\\'      to      0.\\nIf  \\n \\n  we      could      learn      this      time-reversal      SDE,      it      would      enable      us      to      sample      from      the      data      distribution      by  \\n \\n  starting      with      the      easy      initial      distribution      (at      7’)      and      following      the      time-reversal      SDE      from   \\n \\nT      to   \\n \\n0      at  \\n \\n  inference      time.</p><p> \\n \\n  Training.\\nNow,      the      remaining      question      is      how      to      learn      this      time-reversal      SDE.\\nHere,      by      repre-  \\n \\n  senting      the      induced      distribution      over      trajectories      from   \\n \\nT      to   \\n \\n0      with      this      time-reversal      SDE      as      Q\\\\\\\\\\\\\\'™®,  \\n \\n  our      goal      is      to      train   \\n \\na      new      SDE,      parameterized      by   \\n \\n@      in      the      neural      network,      such      that      the      induced  \\n \\n  distribution      P®      over      trajectories      associated      with   \\n \\n6      aligns      with      Q\"™*.\\nSpecifically,      the      objective      is      to minimize      the      following      losses:</p><p> \\n \\n  argmin      KL(Q\"™||P*).\\n(39)  \\n \\n  0  \\n \\n  With      some      algebra,      leveraging      the      observation      that      the      time-reversal      SDE      comprises      the      original  \\n \\n  forward      SDE      and      the      scoring      term      of      the      marginal      distribution      (Anderson,      1982)      as      in      (2),      Song  \\n \\n  et      al.\\n(2021)      demonstrates      that      this      KL      loss      (39)      is      approximated      as Dore   \\n \\n=      argmin      Eeuni((0,7))01~qyo0(weleo)co~pP\"e      Fe      Fl      Vey      log      jo      (Xt   \\n \\n|      Xo)   \\n \\n_      so(Xt,      t)|I?\\n],      (40)  \\n \\n  which      is      equal      to      (4)      where      the      weight      A(t)      is      {o?\\n}?\\n(for      notation,      refer      to      Section      1.1.1).</p><h9>9.2      Flow-Based      (Bridge-Based)      Diffusion      Models</h9><p>dels We      adopt      the      explanation      outlined      in      Liu      et      al.\\n(2022).\\nIn      this      framework,      we      begin      by      considering  \\n \\n  a      reference      SDE      (from      T\\\\\\\\\\\\\\'      to      0),      x;   \\n \\n=      zr_;      where      z;      is      defined      as      Brownian      motion:</p><p>dz,   \\n \\n=      dwt,      te      (0,      7).\\n(41)</p><p> \\n \\n  Subsequently,      we      consider      the      coupling      of      the      reference      SDE      conditioned      on      state      at      time   \\n \\n0      (i.e.,  \\n \\n  distribution      of      xo.7      conditional      on      x)      and      the      data      distribution      at      time      0,      denoting      the      resulting  \\n \\n  distribution      as      Q*°?,      Informally,      this      Q*°°?\\nis      characterized      as      follows.</p><p>Informal      characterization      of      Q*”.\\nWhen      discretizing      the      time      step,      by      denoting      the      distribution  \\n \\n  induced      by      the      reference      SDE      as      p\"(x7,--+   \\n \\n,      29),      we      define QF      (a7,      tae   \\n \\n,      20)   \\n \\n=      p(x,      ay   \\n \\n|      r9)pP\"°      (x9).</p><p>The      distribution      Q*”      above      is   \\n \\na      limiting      distribution      obtained      by      making      the      discretization      step small.</p><p>Training.\\nIn      bridge-based      diffusion      models,      our      objective      is      to      train   \\n \\na      new      SDE,      parametrized      by  \\n \\n  6      (e.g.,   \\n \\na      neural      network),      to      align      the      induced      distribution      P®      with      Q*°?.\\nSpecifically,      bridge-based training      aims      to      minimize aremin      KL(Q*>||P’).\\n(42)  \\n \\n  0  \\n \\n  While      the      method      to      minimize      the      above      is      currently      unclear,      we      can      derive   \\n \\na      trainable      objective  \\n \\n  through      algebraic      manipulation.\\nInitially,      we      observe      that      the      reference      SDE      (41),      conditioned      on  \\n \\n  state      xo      at      time   \\n \\n0      (zr   \\n \\n=      D),      is      expressed      as:</p><p>b\\n \\n \\n  b—      2;</p><p>t\\n \\n   €\\n \\n   [0,T];dz?\\n \\n \\n=      g?(z?,t)dt      +du,,  \\n \\n  g(z?,t)   \\n \\n=   \\n \\na       (43)</p><p>using      the      seminal      “Doob’s      h-transform”      (Rogers      and      Williams,      2000).\\nThis      SDE      is      commonly  \\n \\n  known      as      the      Brownian      bridge.\\nNow,      we      consider   \\n \\na      new      SDE      parameterized      by      0:</p><p>t\\n \\n   €\\n \\n   [0,7];      dz   \\n \\n=      s(z,t;      @)dt   \\n \\n+      dw.\\n(44)</p><p>Then,      we      can      demonstrate      that      minimizing      the      KL      loss      in      (42)      is      equivalent      to argmin      E,W      unil0,7],1~4\"°      cop?\\n[Ilg”      (2,      t)   \\n \\n_      s(21,      t;      0)      M3]      (45)</p><p>where      q;°      denotes      the      induced      distribution      at      time   \\n \\nt      following      SDE      (43)      conditioned      on      xo,      and  \\n \\n  s:      Xx      |0,T]   \\n \\n>   \\n \\n&      is      the      model      we      aim      to      optimize.</p><p> \\n \\n  Flow-based      diffusion      models.\\nThe      aforementioned      formulation      has      been      originally      proposed      as   \\n \\na       bridge-based      SDE      (Liu      et      al.,      2022)      because      the      conditional      SDE      is      often      referred      to      as   \\n \\na      Brownian  \\n \\n  bridge.\\nFlow-based      diffusion      models      proposed      in      Lipman      et      al.\\n(2023);      Tong      et      al.\\n(2023)      share   \\n \\na       close      relationship      with      bridge-based      diffusion      models.\\nThese      flow-based      models      use   \\n \\na      target      SDE (43)      and      an      SDE      with   \\n \\na      parameter      (44)      without      stochastic      terms      (1.e.,      no      dw,)      and      aim      to      minimize  \\n \\n  the      loss      function      (45).</p><p> \\n \\n  Comparison      with      score-based      diffusion      models.\\nBoth      score-based      diffusion      models      and      bridge-  \\n \\n  based      diffusion      models      aim      to      minimize      KL      divergences.\\nDespite      its      similarity,      in      contrast      to  \\n \\n  score-based      diffusion      models,      bridge-based      diffusion      models      target      Q*°>      rather      than      the      time-  \\n \\n  reversal      SDE      Q\"™®.\\nThe      distribution      Q*”      is      often      considered      preferable      because      it      can      impose      an  \\n \\n  SDE      that      navigates      efficiently      from   \\n \\n0      to      7’      (e.g.,   \\n \\na      Brownian      bridge      with      minimal      noise).\\nThis      can  \\n \\n  expedite      the      learning      process      since      we      lack      direct      control      over      the      time-reversal      SDE      in      score-based  \\n \\n  diffusion      models.</p><h10>9.3.      Connection      with      RL-Based      Fine-Tuning</h10><p>ning  \\n \\n  RL-based      fine-tuning      shares   \\n \\na      close      relationship      with      flow-based      training.\\nTo      illustrate      this,      consider  \\n \\n  the      reference      SDE      (from      7’      to      0)      following      the      pre-trained      diffusion      model.\\nThen,      akin      to      flow-  \\n \\n  based      training,      we      consider      the      coupling      of      the      reference      SDE      conditioned      on      state      at      time   \\n \\n0       (i.e.,      distribution      over      xo-r      conditioned      on      x9)      and      the      target      distribution      at      time   \\n \\n0      (i.e.,      p,(-)   \\n \\n«       exp(r(-)/a)p?\\n\"°(-)),      denoting      the      induced      distribution      as      Q*™.</p><p>Informal      characterization      of      Q™°*.\\nInformally,      Q*\"°      is      introduced      as      follows.\\nWhen      discretizing  \\n \\n  the      time      step,      by      denoting      the      distribution      induced      by      the      reference      SDE      as      p™\\\\\\\\\\\\\\'(xp,-+-   \\n \\n,      9),      we define Qi\"      (xr,   \\n \\na   \\n \\n,      0)   \\n \\n=      p(x,      nr      Ly   \\n \\n|      L0)Pr      (Xo).</p><p>The      distribution      Q*°      above      is   \\n \\na      limiting      distribution      obtained      by      making      the      discretization      step small.</p><p>Training.\\nNow,      we      introduce   \\n \\na      new      SDE      parameterized      by   \\n \\n@      such      as      an      neural      network      to      align  \\n \\n  the      induced      distribution      P’      with      Q®\"*.\\nActually,      in      the      continuous      formulation,      the      RL-problem      (18) in      Section      4.2      is      equal      to      solving argmin      KL(P?||Q\"™).\\n \\n \\n  0 This      formalization      has      been      presented      in      Uehara      et      al.\\n(2024).\\nIntuitively,      as      we      see      in      Theorem      3,  \\n \\n  this      is      expected      because      we      see      that      the      posterior      distribution      induced      by      pre-trained      models:</p><p>ref   \\n \\n(      PD      (ar,-++      21   \\n \\n|      to)      (=      vey      (er|er_-1)p      Pp      (ar-1   \\n \\n|      er_2)      +++      pp\"      (x1   \\n \\n|      20), remains      preserved      after      fine-tuning      based      on      the      RL-formulation      (18).</p><p> \\n \\n  Comparison      with      flow-based      training.\\nIn      contrast      to      flow-based      training,      RL-based      fine-tuning  \\n \\n  minimizes      the      inverse      KL      divergence      rather      than      the      KL      divergence.\\nIntuitively,      this      is      because,  \\n \\n  unlike      Qt™*      or      Q*>   \\n \\n,      we      cannot      sample      from      Q*”      (recall      that   \\n \\na      marginal      distribution      at      time   \\n \\n0      in  \\n \\n  Qi,      i.e.,      p,,      is      unnormalized);      on      the      other      hand,      the      marginal      distributions      at      time   \\n \\n0      in      both      Q\\\\\\\\\\\\\\'™*  \\n \\n  and      Q*°°>      are      data      distributions.</p><p> \\n \\n  Remark   \\n \\n7      (Extension      to      f-divergence).\\nEach      of      these      methods      can      be      technically      extended      by  \\n \\n  incorporating      more      general      divergences      beyond      KL      divergence.\\nFor      instance,      in      the      context      of  \\n \\n  fine-tuning,      please      see      Tang      (2024).</p><p>10      Connection      with      Sampling      from      Unnormalized      Distributions</p><p> \\n \\n  Numerous      studies      delve      into      sampling      from      an      unnormalized      distribution      proportional      to      exp(r()),  \\n \\n  commonly      known      as      the      Gibbs      distribution,      extensively      discussed      in      the      computational      statistics  \\n \\n  and      statistical      physics      community      (Robert,      2014).\\nThis      issue      is      pertinent      to      our      work,      as      during  \\n \\n  fine-tuning,      the      target      distribution      is      also      formulated      in      the      Gibbs      distribution      form      in      (10),      which      is  \\n \\n  proportional      to      exp(r(a))p?\\n\"*      (x).</p><p> \\n \\n  The      literature      employs      two      main      approaches      to      tackle      this      challenge:      Markov      Chain      Monte  \\n \\n  Carlo      (MCMC)      and      RL-based      methods.\\nIn      particular,      the      RL-based      diffusion      model      fine-tuning  \\n \\n  we      have      discussed      so      far      is      closely      related      to      the      latter      approach.\\nIn      this      section,      we      explain      and  \\n \\n  compare      these      two      approaches.</p><h10>10.1      Markov      Chain      Monte      Carlo      (MCMC)</h10><p>CMC)  \\n \\n  In      MCMC,   \\n \\na      Markov      chain      is      constructed      to      approximate   \\n \\na      target      distribution      such      that      the      equilib-  \\n \\n  rium      distribution      of      the      Markov      chain      converges      to      the      target      distribution.\\nHowever,      dealing      with  \\n \\n  high-dimensional      domain      spaces      is      challenging      for      MCMC.\\n \\n \\nA      common      strategy      involves      leveraging  \\n \\n  gradient      information,      such      as      the      Metropolis-adjusted      Langevin      algorithm      (MALA)      (Besag,      1994)  \\n \\n  or      Hamiltonian      Monte      Carlo      (HMC)      (Neal      et      al.,      2011;      Girolami      and      Calderhead,      2011).\\nMALA  \\n \\n  shares      similarities      with      classifier      guidance      and      value-weighted      sampling,      as      both      methods      depend  \\n \\n  on      the      first-order      information      from      reward      (or      value)      functions.</p><p> \\n \\n  Can      we      use      MCMC      for      fine-tuning      diffusion      models?\\nIn      the      context      of      fine-tuning      dif-  \\n \\n  fusion      models,      while      it      may      seem      intuitive      to      sample      from      the      target      distribution      (10)      (i.e.,  \\n \\n  Dy   \\n \\n&      exp(r(x))pP\"*(x))      using      MCMC,      it’s      not      straightforward      for      the      following      reasons:</p><p>¢\\n \\n   We      lack      of      an      analytical      form      of      p?\\n\"*(-).<ul><li>*\\n \\n   Additionally,      even      if      we      can      estimate      p?\\n’°(-)      in      an      unbiased      manner,      as      in      Chen      et      al.\\n(2018),  \\n \\n  the      mixing      time      for      obtaining   \\n \\na      single      sample      in      MCMC      might      be      lengthy.</li></ul></p><p> \\n \\n  Therefore,      we      explore      creating   \\n \\na      generative      model      (i.e.,      diffusion      model)      for      this      task      using      RL-  \\n \\n  based      fine-tuning      so      that      we      can      easily      sample      from      p,(-)      during      inference      (i.e.,      simulating      the  \\n \\n  time-reversal      SDE      (2)      with      policies      from   \\n \\n¢   \\n \\n=      T\\\\\\\\\\\\\\'+   \\n \\n1      to   \\n \\nt   \\n \\n=      1),      without      relying      on      MCMC.\\nThis  \\n \\n  addresses      the      concern      of      MCMC      by      avoiding      the      need      to      estimate      p?\\n\"*(-)      and      minimizing      inference  \\n \\n  time,      albeit      at      the      expense      of      increased      training      time.</p><p>10.2}      RL-Based      Approaches  \\n \\n  While      MCMC      has      historically      been      widely      used      for      this      purpose,      recent      works      have      proposed      an  \\n \\n  RL-based      approach      or      its      variant      (e.g.,      Bernton      et      al.      (2019);      Heng      et      al.\\n(2020);      Huang      et      al.\\n(2023);</p><p> \\n \\n  Vargas      et      al.\\n(2023)).\\nFor      example,   \\n \\na      seminal      study      (Zhang      and      Chen,      2021)      introduced   \\n \\na      method  \\n \\n  very      similar      to      reward      backpropagation.\\nIn      their      work,      they      initially      define      Brownian      motion      as   \\n \\na       reference      SDE      (from      7’      to      0),      similar      to      the      pre-trained      diffusion      models      in      our      context.\\nThen,      by  \\n \\n  appropriately      setting      rewards,      Zhang      and      Chen      (2021)      aims      to      learn   \\n \\na      new      SDE      (from      T\\\\\\\\\\\\\\'      to      0)      such  \\n \\n  that      we      can      sample      from      the      target      distribution      at      the      endpoint      0.</p><p> \\n \\n  These      approaches      offer      advantages      over      MCMC      due      to      their      ability      to      minimize      inference      time  \\n \\n  (amortization)      by      constructing   \\n \\na      generative      model      comprising      policies      in      the      training      time      and      solely  \\n \\n  executing      learned      policies      during      inference.\\nUnlike      MCMC,      these      methods      significantly      reduce      the  \\n \\n  inference      cost      as      we      don’t      need      to      be      concerned      about      the      mixing      time.\\nAdditionally,      RL-based  \\n \\n  approaches      have      the      potential      to      effectively      handle      high-dimensional,      complex      (i.e.,      multi-modal)  \\n \\n  distributions      by      harnessing      the      considerable      expressive      power      arising      from      latent      states.</p><p> \\n \\n  Despite      these      advantages,      it      is      often      unclear      how      to      select   \\n \\na      reference      SDE      in      these      works.\\nIn  \\n \\n  contrast,      in      RL-based      fine-tuning      of      diffusion      models,      we      directly      leverage      pre-trained      diffusion  \\n \\n  models      as      the      reference      SDEs,      i.e.,      as      our      prior      knowledge.</p><p> \\n \\n  Key      message.\\nFinally,      it      is      worth      noting      that      we      can      technically      utilize      any      off-the-shelf      RL      algo-  \\n \\n  rithms      for      training.\\nFor      example,      Zhang      and      Chen      (2021)      employs      reward      backpropagation,      while  \\n \\n  works      in      Gflownets      typically      leverage      PCL,      as      we      mention      in      Section      6.3.\\nHowever,      additionally,      it  \\n \\n  is      possible      to      use      PPO      or      reward-weighted      MLE      mentioned      in      Section      4.2.</p><p>11      Closely      Related      Directions</p><p>Lastly,      we      highlight      several      closely      related      directions      we      have      not      yet      discussed.</p><p> \\n \\n  Aligning      text-to-image      models.\\nWe      recognize      that      current      efforts      in      fine-tuning      diffusion      models  \\n \\n  primarily      revolve      around      aligning      text-to-image      models      using      human      feedback      (Lee      et      al.,      2023;  \\n \\n  Wallace      et      al.,      2023;      Wu      et      al.,      2023;      Yang      et      al.,      2023).\\nIn      this      paper,      our      goal      is      to      consider   \\n \\na      more  \\n \\n  general      and      fundamental      formulation      that      is      not      tailored      to      any      particular      task.</p><p> \\n \\n  Diffusion      models      for      RL.\\nDiffusion      models      are      well-suited      for      specific      reinforcement      learning  \\n \\n  applications      due      to      their      ability      to      model      complex      and      multimodal      distributions      as      polices      (Janner  \\n \\n  et      al.,      2022;      Ajay      et      al.,      2023;      Wang      et      al.,      2022;      Hansen-Estruch      et      al.,      2023;      Du      et      al.,      2024;      Zhu  \\n \\n  et      al.,      2023).\\nFor      example,      Wang      et      al.\\n(2022)      use      conditional      diffusion      models      as      policies      and  \\n \\n  demonstrate      their      effectiveness      in      typical      offline      RL      benchmarks.\\nWhile      RL-based      fine-tuning      also  \\n \\n  aims      to      maximize      certain      reward      functions,      the      primary      focus      of      this      tutorial      covers      methods      that  \\n \\n  leverage      pre-trained      diffusion      models      for      this      purpose.</p><p> \\n \\n  RL      from      human      feedback      (RLHF)      for      language      models.\\nRLHF      is      widely      discussed      in      the  \\n \\n  context      of      language      models,      where      many      algorithms      share      similarities      with      those      used      in      diffusion  \\n \\n  models      (Ouyang      et      al.,      2022;      Bai      et      al.,      2022;      Casper      et      al.,      2023).\\nFor      instance,      both      domains  \\n \\n  commonly      employ      PPO      as   \\n \\na      standard      approach,      and      reward-weighted      training      (or      reward-weighted  \\n \\n  MLE)      often      serves      as   \\n \\na      basic      baseline      approach.\\nDespite      these      similarities,      nuanced      differences  \\n \\n  exist      mainly      because      diffusion      models      typically      operate      in   \\n \\na      continuous      input      space      rather      than  \\n \\n  a      discrete      one.\\nMoreover,      techniques      like      value-weighted      sampling      emerge      uniquely      within      the  \\n \\n  context      of      diffusion      models.</p><p>12      Summary</p><p> \\n \\n  In      this      article,      we      comprehensively      explain      how      fine-tuning      diffusion      models      to      maximize      down-  \\n \\n  stream      reward      functions      can      be      formalized      as   \\n \\na      reinforcement      learning      (RL)      problem      in      entropy-  \\n \\n  regularized      Markov      decision      processes      (MDPs).\\nBased      on      this      viewpoint,      we      elaborate      on      the  \\n \\n  application      of      various      RL      algorithms,      such      as      PPO      and      reward-weighted      MLE,      specifically      tailored  \\n \\n  for      fine-tuning      diffusion      models.\\nAdditionally,      we      categorize      different      scenarios      based      on      how  \\n \\n  reward      feedback      is      obtained.\\nWhile      this      categorization      is      not      always      explicitly      mentioned      in      many  \\n \\n  existing      works,      it      is      crucial      when      selecting      appropriate      algorithms.\\nFinally,      we      discuss      the      rela-  \\n \\n  tionships      with      several      related      topics,      including      classifier      guidance,      Gflownets,      path      integral      control  \\n \\n  theory,      and      sampling      from      unnormalized      distributions.</p><h4>References</h4><p>Agarwal,      A.,      N.      Jiang,      S.      M.      Kakade,      and      W.      Sun      (2019).\\nReinforcement      learning:      Theory      and  \\n \\n  algorithms.\\nCS      Dept.,      UW      Seattle,      Seattle,      WA,      USA,      Tech.\\nRep,      10-4.</p><p>Agarwal,      V.      and      D.      R.      Kelley      (2022).\\nThe      genetic      and      biochemical      determinants      of      mrna      degradation  \\n \\n  rates      in      mammals.\\nGenome      biology      23(1),      245.</p><p> \\n \\n  Ajay,      A.,      Y.      Du,      A.      Gupta,      J.      B.      Tenenbaum,      T.      S.      Jaakkola,      and      P.      Agrawal      (2023).\\nIs      conditional  \\n \\n  generative      modeling      all      you      need      for      decision      making?\\nIn      The      Eleventh      International      Conference  \\n \\n  on      Learning      Representations.</p><p>Albergo,      M.      S.      and      E.      Vanden-Eijnden      (2022).\\nBuilding      normalizing      flows      with      stochastic      inter-  \\n \\n  polants.\\narXiv      preprint      arXiv:2209.15571.</p><p>Anderson,      B.      D.      (1982).\\nReverse-time      diffusion      equation      models.\\nStochastic      Processes      and      their  \\n \\n  Applications      12(3),      313-326.</p><p> \\n \\n  Austin,      J.,      D.      D.      Johnson,      J.      Ho,      D.      Tarlow,      and      R.      Van      Den      Berg      (2021).\\nStructured      denoising  \\n \\n  diffusion      models      in      discrete      state-spaces.\\nAdvances      in      Neural      Information      Processing      Systems      34,  \\n \\n  17981-17993.</p><p>Avdeyey,      P.,      C.      Shi,      Y.      Tan,      K.      Dudnyk,      and      J.      Zhou      (2023).\\nDirichlet      diffusion      score      model      for  \\n \\n  biological      sequence      generation.\\narXiv      preprint      arXiv:2305.\\n10699.</p><p>Bai,      Y.,      S.      Kadavath,      S.      Kundu,      A.      Askell,      J.      Kernion,      A.      Jones,      A.      Chen,      A.      Goldie,      A.      Mirhoseini, C.      McKinnon,      et      al.\\n(2022).\\nConstitutional      ai:      Harmlessness      from      ai      feedback.\\narXiv      preprint  \\n \\n  arXiv:2212.08073.</p><p> \\n \\n  Bansal,      A.,      H.-M.\\nChu,      A.      Schwarzschild,      S.      Sengupta,      M.      Goldblum,      J.      Geiping,      and      T.      Goldstein  \\n \\n  (2023).\\nUniversal      guidance      for      diffusion      models.\\nIn      Proceedings      of      the      IEEE/CVF      Conference  \\n \\n  on      Computer      Vision      and      Pattern      Recognition,      pp.\\n843-852.</p><p>Bengio,      Y.,      S.      Lahlou,      T.      Deleu,      E.      J.      Hu,      M.      Tiwari,      and      E.      Bengio      (2023).\\nGflownet      foundations.\\n \\n \\n  Journal      of      Machine      Learning      Research      24(210),      1-55.</p><p> \\n \\n  Benton,      J.,      Y.      Shi,      V.      De      Bortoli,      G.      Deligiannidis,      and      A.      Doucet      (2024).\\nFrom      denoising      diffusions  \\n \\n  to      denoising      markov      models.\\nJournal      of      the      Royal      Statistical      Society      Series      B:      Statistical  \\n \\n  Methodology      86(2),      286-301.</p><p>Bernton,      E.,      J.      Heng,      A.      Doucet,      and      P.      E.      Jacob      (2019).\\nSchr\\\\\\\\\\\\\\\\””      odinger      bridge      samplers.\\narXiv  \\n \\n  preprint      arXiv:      1912.13170.</p><p>Besag,      J.\\n(1994).\\nComments      on      “representations      of      knowledge      in      complex      systems”      by      u.      grenander  \\n \\n  and      mi      miller.\\nJ.      Roy.\\nStatist.\\nSoc.      Ser.\\n \\n \\nB      56(591-592),      4.</p><p>Black,      K.,      M.      Janner,      Y.      Du,      I.      Kostrikov,      and      S.      Levine      (2023).\\nTraining      diffusion      models      with  \\n \\n  reinforcement      learning.\\narXiv      preprint      arXiv:2305.13301.</p><p> \\n \\n  Campbell,      A.,      J.      Benton,      V.      De      Bortoli,      T.      Rainforth,      G.      Deligiannidis,      and      A.      Doucet      (2022).\\n \\n \\nA       continuous      time      framework      for      discrete      denoising      models.\\nAdvances      in      Neural      Information  \\n \\n  Processing      Systems      35,      28266-28279.</p><p> \\n \\n  Campbell,      A.,      J.      Yim,      R.      Barzilay,      T.      Rainforth,      and      T.      Jaakkola      (2024).\\nGenerative      flows      on  \\n \\n  discrete      state-spaces:      Enabling      multimodal      flows      with      applications      to      protein      co-design.\\narXiv  \\n \\n  preprint      arXiv:2402.04997.</p><p>Cao,      H.,      C.      Tan,      Z.      Gao,      Y.      Xu,      G.      Chen,      P.-A.\\nHeng,      and      S.      Z.      Li      (2024).\\n \\n \\nA      survey      on      generative  \\n \\n  diffusion      models.\\n[EEE      Transactions      on      Knowledge      and      Data      Engineering.</p><p>Casper,      S.,      X.      Davies,      C.      Shi,      T.      K.      Gilbert,      J.      Scheurer,      J.      Rando,      R.      Freedman,      T.      Korbak, D.      Lindner,   \\n \\nP P.      Freire,      et      al.\\n(2023).\\nOpen      problems      and      fundamental      limitations      of      reinforcement  \\n \\n  learning      from      human      feedback.\\narXiv      preprint      arXiv:2307.15217.</p><p>Castillo-Hair,      S.      M.      and      G.      Seelig      (2021).\\nMachine      learning      for      designing      next-generation      mrna  \\n \\n  therapeutics.\\nAccounts      of      Chemical      Research      55(1),      24-34.</p><p>Chang,      J.      D.,      M.      Uehara,      D.      Sreenivas,      R.      Kidambi,      and      W.      Sun      (2021).\\nMitigating      covariate      shift  \\n \\n  in      imitation      learning      via      offline      data      without      great      coverage.\\narXiv      preprint      arXiv:2106.03207.</p><p>Chen,      M.,      S.      Mei,      J.      Fan,      and      M.      Wang      (2024).\\nAn      overview      of      diffusion      models:      Applications,  \\n \\n  guided      generation,      statistical      rates      and      optimization.\\narXiv      preprint      arXiv:2404.07771.</p><p>Chen,      R.      T.,      Y.      Rubanova,      J.      Bettencourt,      and      D.      K.      Duvenaud      (2018).\\nNeural      ordinary      differential  \\n \\n  equations.\\nAdvances      in      neural      information      processing      systems      31.</p><p>Chung,      H.,      J.      Kim,      M.      T.      Mccann,      M.      L.      Klasky,      and      J.      C.      Ye      (2022).\\nDiffusion      posterior      sampling  \\n \\n  for      general      noisy      inverse      problems.\\narXiv      preprint      arXiv:2209.\\n14687.</p><p> \\n \\n  Chung,      H.,      B.      Sim,      D.      Ryu,      and      J.      C.      Ye      (2022).\\nImproving      diffusion      models      for      inverse      problems  \\n \\n  using      manifold      constraints.\\nAdvances      in      Neural      Information      Processing      Systems      35,      25683-—  \\n \\n  25696.</p><p>Clark,      K.,      P.      Vicol,      K.      Swersky,      and      D.      J.      Fleet      (2023).\\nDirectly      fine-tuning      diffusion      models      on  \\n \\n  differentiable      rewards.\\narXiv      preprint      arXiv:2309.\\n17400.</p><p> \\n \\n  De      Bortoli,      V.,      E.      Mathieu,      M.      Hutchinson,      J.      Thornton,      Y.      W.      Teh,      and      A.      Doucet      (2022).\\nRieman-  \\n \\n  nian      score-based      generative      modelling.\\nAdvances      in      Neural      Information      Processing      Systems      35,  \\n \\n  2406-2422.</p><p>Deleu,      T.,      P.      Nouri,      N.      Malkin,      D.      Precup,      and      Y.      Bengio      (2024).\\nDiscrete      probabilistic      inference      as  \\n \\n  control      in      multi-path      environments.\\narXiv      preprint      arXiv:2402.10309.</p><p>Dhariwal,      P.      and      A.      Nichol      (2021).\\nDiffusion      models      beat      gans      on      image      synthesis.\\nAdvances      in  \\n \\n  neural      information      processing      systems      34,      8780-8794.</p><p> \\n \\n  Du,      Y.,      S.      Yang,      B.      Dai,      H.      Dai,      O.      Nachum,      J.      Tenenbaum,      D.      Schuurmans,      and      P.      Abbeel      (2024).\\n \\n \\n  Learning      universal      policies      via      text-guided      video      generation.\\nAdvances      in      Neural      Information  \\n \\n  Processing      Systems      36.</p><p>Fan,      Y.,      O.      Watkins,      Y.      Du,      H.      Liu,      M.      Ryu,      C.      Boutilier,      P.      Abbeel,      M.      Ghavamzadeh,      K.      Lee,      and K.      Lee      (2023) 023).\\nDPOK:      Reinforcement      learning      for      fine-tuning      text-to-image      diffusion      models.\\n \\n \\n  arXiv      preprint      arXiv:2305.16381.</p><p>Finn,      C.,      S.      Levine,      and      P.      Abbeel      (2016).\\nGuided      cost      learning:      Deep      inverse      optimal      control      via  \\n \\n  policy      optimization.\\nIn      International      conference      on      machine      learning,      pp.\\n49-58.\\nPMLR.</p><p>Fox,      R.,      A.      Pakman,      and      N.      Tishby      (2015).\\nTaming      the      noise      in      reinforcement      learning      via      soft  \\n \\n  updates.\\narXiv      preprint      arXiv:      1512.08562.</p><p>Frey,      N.      C.,      D.      Berenberg,      K.      Zadorozhny,      J.      Kleinhenz,      J.      Lafrance-Vanasse,      I.      Hotzel,      Y.      Wu, S.      Ra,   \\n \\nR R.      Bonneau,      K.      Cho,      et      al.\\n(2023).\\nProtein      discovery      with      discrete      walk-jump      sampling.\\n \\n \\n  arXiv      preprint      arXiv:2306.\\n12360.</p><p>Geist,      M.,      B.      Scherrer,      and      O.      Pietquin      (2019).\\n \\n \\nA      theory      of      regularized      markov      decision      processes.\\n \\n \\n  In      International      Conference      on      Machine      Learning,      pp.\\n2160-2169.\\nPMLR.</p><p> \\n \\n  Girolami,      M.      and      B.      Calderhead      (2011).\\nRiemann      manifold      langevin      and      hamiltonian      monte  \\n \\n  carlo      methods.\\nJournal      of      the      Royal      Statistical      Society      Series      B:      Statistical      Methodology      73(2),  \\n \\n  123-214.</p><p>Gosai,      S.      J.,      R.      I.      Castro,      N.      Fuentes,      J.      C.      Butts,      S.      Kales,      R.      R.      Noche,      K.      Mouri,      P.      C.      Sabeti, S.   \\n \\nK K.      Reilly,      and      R.      Tewhey      (2023).\\nMachine-guided      design      of      synthetic      cell      type-specific  \\n \\n  cis-regulatory      elements.\\nbioRxiv.</p><p>Haarnoja,      T.,      H.      Tang,      P.      Abbeel,      and      S.      Levine      (2017).\\nReinforcement      learning      with      deep      energy-  \\n \\n  based      policies.\\nIn      International      conference      on      machine      learning,      pp.\\n1352-1361.\\nPMLR.</p><p>Hansen-Estruch,      P.,      I.      Kostrikov,      M.      Janner,      J.      G.      Kuba,      and      S.      Levine      (2023).\\nIdql:      Implicit  \\n \\n  q-learning      as      an      actor-critic      method      with      diffusion      policies.\\narXiv      preprint      arXiv:2304.10573.</p><p>Heng,      J.,      A.      N.      Bishop,      G.      Deligiannidis,      and      A.      Doucet      (2020).\\nControlled      sequential      monte      carlo.</p><p>Ho,      J.,      A.      Jain,      and      P.      Abbeel      (2020).\\nDenoising      diffusion      probabilistic      models.\\nAdvances      in      neural  \\n \\n  information      processing      systems      33,      6840-6851.</p><p>Ho,      J.      and      T.      Salimans      (2022).\\nClassifier-free      diffusion      guidance.\\narXiv      preprint      arXiv:2207.12598.</p><p>Ho,      J.,      T.      Salimans,      A.      Gritsenko,      W.      Chan,      M.      Norouzi,      and      D.      J.      Fleet      (2022).\\nVideo      diffusion  \\n \\n  models.\\nAdvances      in      Neural      Information      Processing      Systems      35,      8633-8646.</p><p> \\n \\n  Hoogeboom,      E.,      V.      G.      Satorras,      C.      Vignac,      and      M.      Welling      (2022).\\nEquivariant      diffusion      for  \\n \\n  molecule      generation      in      3d.\\nIn      International      conference      on      machine      learning,      pp.\\n8867-8887.</p><h4>      PMLR.</h4><p>Huang,      X.,      H.      Dong,      H.      Yifan,      Y.      Ma,      and      T.      Zhang      (2023).\\nReverse      diffusion      monte      carlo.\\nIn      The  \\n \\n  Twelfth      International      Conference      on      Learning      Representations.</p><p>Janner,      M.,      Y.      Du,      J.      B.      Tenenbaum,      and      S.      Levine      (2022).\\nPlanning      with      diffusion      for      flexible  \\n \\n  behavior      synthesis.\\narXiv      preprint      arXiv:2205.09991.</p><p> \\n \\n  Jo,      J.,      S.      Lee,      and      S.      J.      Hwang      (2022).\\nScore-based      generative      modeling      of      graphs      via      the      system  \\n \\n  of      stochastic      differential      equations.\\nIn      International      Conference      on      Machine      Learning,      pp.\\n \\n \\n  10362-10383.\\nPMLR.</p><p> \\n \\n  Kazim,      M.,      J.      Hong,      M.-G.\\nKim,      and      K.-K.\\nK.      Kim      (2024).\\nRecent      advances      in      path      integral      control  \\n \\n  for      trajectory      optimization:      An      overview      in      theoretical      and      algorithmic      perspectives.\\nAnnual  \\n \\n  Reviews      in      Control      57,      100931.</p><p>Krishnamoorthy,      S.,      S.      M.      Mashkaria,      and      A.      Grover      (2023).\\nDiffusion      models      for      black-box  \\n \\n  optimization.\\narXiv      preprint      arXiv:2306.07180.</p><p>Kumar,      A.,      A.      Zhou,      G.      Tucker,      and      S.      Levine      (2020).\\nConservative      q-learning      for      offline      reinforce-  \\n \\n  ment      learning.\\nAdvances      in      Neural      Information      Processing      Systems      33,      1179-1191.</p><p>Lal,      A.,      D.      Garfield,      T.      Biancalani,      and      G.      Eraslan      (2024).\\nreglm:      Designing      realistic      regulatory  \\n \\n  dna      with      autoregressive      language      models.\\nbioRxiv,      2024-02.</p><p>Lee,      K.,      H.      Liu,      M.      Ryu,      O.      Watkins,      Y.      Du,      C.      Boutilier,      P.      Abbeel,      M.      Ghavamzadeh,      and      S.      S.      Gu  \\n \\n  (2023).\\nAligning      text-to-image      models      using      human      feedback.\\narXiv      preprint      arXiv:2302.12192.</p><p>Levine,      S.      (2018).\\nReinforcement      learning      and      control      as      probabilistic      inference:      Tutorial      and  \\n \\n  review.\\narXiv      preprint      arXiv:      1805.00909.</p><p>Li,      Z.,      Y.      Ni,      T.      A.      B.      Huygelen,      A.      Das,      G.      Xia,      G.-B.\\nStan,      and      Y.      Zhao      (2023).\\nLatent      diffusion  \\n \\n  model      for      dna      sequence      generation.\\narXiv      preprint      arXiv:2310.06150.</p><p>Lipman,      Y.,      R.      T.      Chen,      H.      Ben-Hamu,      M.      Nickel,      and      M.      Le      (2023).\\nFlow      matching      for      generative  \\n \\n  modeling.\\nJCLR      2023.</p><p>Liu,      X.,      C.      Gong,      and      Q.      Liu      (2022).\\nFlow      straight      and      fast:      Learning      to      generate      and      transfer      data  \\n \\n  with      rectified      flow.\\narXiv      preprint      arXiv:2209.03003.</p><p>Liu,      X.,      L.      Wu,      M.      Ye,      and      Q.      Liu      (2022).\\nLet      us      build      bridges:      Understanding      and      extending  \\n \\n  diffusion      generative      models.\\narXiv      preprint      arXiv:2208.14699.</p><p>Lou,      A.,      C.      Meng,      and      S.      Ermon      (2023).\\nDiscrete      diffusion      language      modeling      by      estimating      the  \\n \\n  ratios      of      the      data      distribution.\\narXiv      preprint      arXiv:2310.16834.</p><p>Malkin,      N.,      M.      Jain,      E.      Bengio,      C.      Sun,      and      Y.      Bengio      (2022).\\nTrajectory      balance:      Improved      credit  \\n \\n  assignment      in      gflownets.\\nAdvances      in      Neural      Information      Processing      Systems      35,      5955-5967.</p><p>Mohammadpour,      S.,      E.      Bengio,      E.      Frejinger,      and      P.-L.\\nBacon      (2023).\\nMaximum      entropy      gflownets  \\n \\n  with      soft      q-learning.\\narXiv      preprint      arXiv:2312.14331.</p><p>Nachum,      O.,      M.      Norouzi,      K.      Xu,      and      D.      Schuurmans      (2017).\\nBridging      the      gap      between      value      and  \\n \\n  policy      based      reinforcement      learning.\\nAdvances      in      neural      information      processing      systems      30.</p><p>Neal,      R.      M.      et      al.\\n(2011).\\nMcmce      using      hamiltonian      dynamics.\\nHandbook      of      markov      chain      monte  \\n \\n  carlo      2(11),      2.</p><p>Neu,      G.,      A.      Jonsson,      and      V.      Gémez      (2017).\\n \\n \\nA      unified      view      of      entropy-regularized      markov      decision  \\n \\n  processes.\\narXiv      preprint      arXiv:      1705.07798.</p><p>Ouyang,      L.,      J.      Wu,      X.      Jiang,      D.      Almeida,      C.      Wainwright,      P.      Mishkin,      C.      Zhang,      S.      Agarwal, K.      Slama,      A A.      Ray,      et      al.\\n(2022).\\nTraining      language      models      to      follow      instructions      with      human  \\n \\n  feedback.\\nAdvances      in      Neural      Information      Processing      Systems      35,      27730-27744.</p><p>Pattanaik,      L.      and      C.      W.      Coley      (2020).\\nMolecular      representation:      going      long      on      fingerprints.\\n \\n \\n  Chem      6(6),      1204-1207.</p><p>Peng,      X.      B.,      A.      Kumar,      G.      Zhang,      and      S.      Levine      (2019).\\nAdvantage-weighted      regression:      Simple  \\n \\n  and      scalable      off-policy      reinforcement      learning.\\narXiv      preprint      arXiv:1910.00177.</p><p>Peters,      J.,      K.      Mulling,      and      Y.      Altun      (2010).\\nRelative      entropy      policy      search.\\nIn      Proceedings      of      the  \\n \\n  AAAI      Conference      on      Artificial      Intelligence,      Volume      24,      pp.\\n1607-1612.</p><p> \\n \\n  Podell,      D.,      Z.      English,      K.      Lacey,      A.      Blattmann,      T.      Dockhorn,      J.      Miiller,      J.      Penna,      and      R.      Rombach  \\n \\n  (2023).\\nSdxl:      Improving      latent      diffusion      models      for      high-resolution      image      synthesis.\\narXiv  \\n \\n  preprint      arXiv:2307.01952.</p><p>Prabhudesai,      M.,      A.      Goyal,      D.      Pathak,      and      K.      Fragkiadaki      (2023).\\nAligning      text-to-image      diffusion  \\n \\n  models      with      reward      backpropagation.\\narXiv      preprint      arXiv:2310.03739.</p><p>Ramesh,      A.,      P.      Dhariwal,      A.      Nichol,      C.      Chu,      and      M.      Chen      (2022).\\nHierarchical      text-conditional  \\n \\n  image      generation      with      clip      latents.\\narXiv      preprint      arXiv:2204.06125      1(2),      3.</p><p>Rigter,      M.,      B.      Lacerda,      and      N.      Hawes      (2022).\\nRambo-rl:      Robust      adversarial      model-based      offline  \\n \\n  reinforcement      learning.\\nAdvances      in      neural      information      processing      systems      35,      16082-16097.</p><p>Robert,      C.      (2014).\\nStatistical      modeling      and      computation.</p><p>Rogers,      L.      C.      G.      and      D.      Williams      (2000).\\nDiffusions,      Markov      processes      and      martingales:      Volume      2,  \\n \\n  It6é      calculus,      Volume      2.\\nCambridge      university      press.</p><p> \\n \\n  Rombach,      R.,      A.      Blattmann,      D.      Lorenz,      P.      Esser,      and      B.      Ommer      (2022,      June).\\nHigh-resolution  \\n \\n  image      synthesis      with      latent      diffusion      models.\\nIn      Proceedings      of      the      IEEE/CVF      Conference      on  \\n \\n  Computer      Vision      and      Pattern      Recognition      (CVPR),      pp.\\n10684-10695.</p><p>Sarkar,      A.,      Z.      Tang,      C.      Zhao,      and      P.      Koo      (2024).\\nDesigning      dna      with      tunable      regulatory      activity  \\n \\n  using      discrete      diffusion.\\nbioRxiv,      2024-05.</p><p>Schulman,      J.,      X.      Chen,      and      P.      Abbeel      (2017).\\nEquivalence      between      policy      gradients      and      soft  \\n \\n  q-learning.\\narXiv      preprint      arXiv:      1704.06440.</p><p>Schulman,      J.,      S.      Levine,      P.      Abbeel,      M.      Jordan,      and      P.      Moritz      (2015).\\nTrust      region      policy      optimization.\\n \\n \\n  In      International      conference      on      machine      learning,      pp.\\n1889-1897.\\nPMLR.</p><p>Schulman,      J.,      F.      Wolski,      P.      Dhariwal,      A.      Radford,      and      O.      Klimov      (2017).\\nProximal      policy      optimiza-  \\n \\n  tion      algorithms.\\narXiv      preprint      arXiv:      1707.06347.</p><p>Shi,      Y.,      V.      De      Bortoli,      A.      Campbell,      and      A.      Doucet      (2024).\\nDiffusion      schrédinger      bridge      matching.\\n \\n \\n  Advances      in      Neural      Information      Processing      Systems      36.</p><p> \\n \\n  Sohl-Dickstein,      J.,      E.      Weiss,      N.      Maheswaranathan,      and      S.      Ganguli      (2015).\\nDeep      unsupervised  \\n \\n  learning      using      nonequilibrium      thermodynamics.\\nIn      /nternational      conference      on      machine      learning,  \\n \\n  pp.\\n2256-2265.\\nPMLR.</p><p>Song,      J.,      C.      Meng,      and      S.      Ermon      (2020).\\nDenoising      diffusion      implicit      models.\\narXiv      preprint  \\n \\n  arXiv:2010.02502.</p><p>Song,      Y.,      C.      Durkan,      I.      Murray,      and      S.      Ermon      (2021).\\nMaximum      likelihood      training      of      score-based  \\n \\n  diffusion      models.\\nAdvances      in      neural      information      processing      systems      34,      1415-1428.</p><p>Stark,      H.,      B.      Jing,      C.      Wang,      G.      Corso,      B.      Berger,      R.      Barzilay,      and      T.      Jaakkola      (2024).\\nDirichlet  \\n \\n  flow      matching      with      applications      to      dna      sequence      design.\\narXiv      preprint      arXiv:2402.05841.</p><p>Sutton,      R.      S.      and      A.      G.      Barto      (2018).\\nReinforcement      learning:      An      introduction.\\nMIT      press.</p><p>Tang,      W.      (2024).\\nFine-tuning      of      diffusion      models      via      stochastic      control:      entropy      regularization      and  \\n \\n  beyond.\\narXiv      preprint      arXiv:2403.06279.</p><p>Tang,      W.      and      H.      Zhao      (2024).\\nScore-based      diffusion      models      via      stochastic      differential      equations—a  \\n \\n  technical      tutorial.\\narXiv      preprint      arXiv:2402.07487.</p><p>Theodorou,      E.,      J.      Buchli,      and      S.      Schaal      (2010).\\n \\n \\nA      generalized      path      integral      control      approach      to  \\n \\n  reinforcement      learning.\\nThe      Journal      of      Machine      Learning      Research      11,      3137-3181.</p><p>Tiapkin,      D.,      N.      Morozov,      A.      Naumov,      and      D.      Vetrov      (2023).\\nGenerative      flow      networks      as      entropy-  \\n \\n  regularized      rl.\\narXiv      preprint      arXiv:2310.12934.</p><p> \\n \\n  Tong,      A.,      N.      Malkin,      G.      Huguet,      Y.      Zhang,      J.      Rector-Brooks,      K.      Fatras,      G.      Wolf,      and      Y.      Bengio  \\n \\n  (2023).\\nConditional      flow      matching:      Simulation-free      dynamic      optimal      transport.\\narXiv      preprint  \\n \\n  arXiv:2302.00482.</p><p>Uehara,      M.      and      W.      Sun      (2021).\\nPessimistic      model-based      offline      reinforcement      learning      under  \\n \\n  partial      coverage.\\narXiv      preprint      arXiv:2107.06226.</p><p>Uehara,      M.,      Y.      Zhao,      K.      Black,      E.      Hajiramezanali,      G.      Scalia,      N.      L.      Diamant,      A.      M.      Tseng, T.      Biancalani,      and      S S.      Levine      (2024).\\nFine-tuning      of      continuous-time      diffusion      models      as      entropy-  \\n \\n  regularized      control.\\narXiv      preprint      arXiv:2402.15194.</p><p> \\n \\n  Uehara,      M.,      Y.      Zhao,      K.      Black,      E.      Hajiramezanali,      G.      Scalia,      N.      L.      Diamant,      A.      M.      Tseng,      S.      Levine,  \\n \\n  and      T.      Biancalani      (2024).\\nFeedback      efficient      online      fine-tuning      of      diffusion      models.\\narXiv      preprint  \\n \\n  arXiv:2402.\\n16359.</p><p> \\n \\n  Uehara,      M.,      Y.      Zhao,      E.      Hajiramezanali,      G.      Scalia,      G.      Eraslan,      A.      Lal,      S.      Levine,      and      T.      Biancalani  \\n \\n  (2024).\\nBridging      model-based      optimization      and      generative      modeling      via      conservative      fine-tuning  \\n \\n  of      diffusion      models.\\narXiv      preprint      arXiv:2405.19673.</p><p>Vargas,      F.,      W.      Grathwohl,      and      A.      Doucet      (2023).\\nDenoising      diffusion      samplers.\\narXiv      preprint  \\n \\n  arXiv:2302.\\n13834.</p><p> \\n \\n  Wallace,      B.,      M.      Dang,      R.      Rafailov,      L.      Zhou,      A.      Lou,      S.      Purushwalkam,      S.      Ermon,      C.      Xiong,      S.      Joty,  \\n \\n  and      N.      Naik      (2023).\\nDiffusion      model      alignment      using      direct      preference      optimization.\\narXiv  \\n \\n  preprint      arXiv:2311.12908.</p><p>Wang,      Z.,      J.      J.      Hunt,      and      M.      Zhou      (2022).\\nDiffusion      policies      as      an      expressive      policy      class      for      offline  \\n \\n  reinforcement      learning.\\narXiv      preprint      arXiv:2208.06193.</p><p>Widatalla,      T.,      R.      Rafailov,      and      B.      Hie      (2024).\\nAligning      protein      generative      models      with      experimental  \\n \\n  fitness      via      direct      preference      optimization.\\nbioRxiv,      2024-05.</p><p>Williams,      G.,      A.      Aldrich,      and      E.      A.      Theodorou      (2017).\\nModel      predictive      path      integral      control:      From  \\n \\n  theory      to      parallel      computation.\\nJournal      of      Guidance,      Control,      and      Dynamics      40(2),      344-357.</p><p>Wu,      X.,      K.      Sun,      F.      Zhu,      R.      Zhao,      and      H.      Li      (2023).\\nBetter      aligning      text-to-image      models      with  \\n \\n  human      preference.\\narXiv      preprint      arXiv:2303.14420.</p><p>Wu,      Y.,      G.      Tucker,      and      O.      Nachum      (2019).\\nBehavior      regularized      offline      reinforcement      learning.\\n \\n \\n  arXiv      preprint      arXiv:      1911.11361.</p><p>Wulfmeier,      M.,      P.      Ondruska,      and      I.      Posner      (2015).\\nMaximum      entropy      deep      inverse      reinforcement  \\n \\n  learning.\\narXiv      preprint      arXiv:      1507.04888.</p><p>Xie,      T.,      C.-A.\\nCheng,      N.      Jiang,      P.      Mineiro,      and      A.      Agarwal      (2021).\\nBellman-consistent      pessimism  \\n \\n  for      offline      reinforcement      learning.\\nAdvances      in      neural      information      processing      systems      34.</p><p>Xu,      M.,      L.      Yu,      Y.      Song,      C.      Shi,      S.      Ermon,      and      J.      Tang      (2022).\\nGeodiff:   \\n \\nA      geometric      diffusion  \\n \\n  model      for      molecular      conformation      generation.\\narXiv      preprint      arXiv:2203.02923.</p><p> \\n \\n  Yang,      L.,      Z.      Zhang,      Y.      Song,      S.      Hong,      R.      Xu,      Y.      Zhao,      W.      Zhang,      B.      Cui,      and      M.-H.\\nYang      (2023).\\n \\n \\n  Diffusion      models:   \\n \\nA      comprehensive      survey      of      methods      and      applications.\\nACM      Computing  \\n \\n  Surveys      56(4),      1-39.</p><p> \\n \\n  Yu,      T.,      G.      Thomas,      L.      Yu,      S.      Ermon,      J.      Y.      Zou,      S.      Levine,      C.      Finn,      and      T.      Ma      (2020).\\nMopo:  \\n \\n  Model-based      offline      policy      optimization.\\nAdvances      in      Neural      Information      Processing      Systems      33,  \\n \\n  14129-14142.</p><p> \\n \\n  Yuan,      H.,      K.      Huang,      C.      Ni,      M.      Chen,      and      M.      Wang      (2023).\\nReward-directed      conditional      diffusion:  \\n \\n  Provable      distribution      estimation      and      reward      improvement.\\nIn      Thirty-seventh      Conference      on  \\n \\n  Neural      Information      Processing      Systems.</p><p>Zhang,      H.      and      T.      Xu      (2023).\\nTowards      controllable      diffusion      models      via      reward-guided      exploration.\\n \\n \\n  arXiv      preprint      arXiv:2304.07132.</p><p>Zhang,      Q.      and      Y.      Chen      (2021).\\nPath      integral      sampler:   \\n \\na      stochastic      control      approach      for      sampling.\\n \\n \\n  arXiv      preprint      arXiv:2111,15141.</p><p> \\n \\n  Zhao,      Y.,      M.      Uehara,      G.      Scalia,      T.      Biancalani,      S.      Levine,      and      E.      Hajiramezanali      (2024).\\n \\n \\n  Adding      conditional      control      to      diffusion      models      with      reinforcement      learning.\\narXiv      preprint  \\n \\n  arXiv:2406.\\n12120.</p><p>Zhou,      Z.,      S.      Kearnes,      L.      Li,      R.      N.      Zare,      and      P.      Riley      (2019).\\nOptimization      of      molecules      via      deep  \\n \\n  reinforcement      learning.\\nScientific      reports      9(1),      10752.</p><p>Zhu,      Z.,      H.      Zhao,      H.      He,      Y.      Zhong,      S.      Zhang,      Y.      Yu,      and      W.      Zhang      (2023).\\nDiffusion      models      for  \\n \\n  reinforcement      learning:   \\n \\nA      survey.\\narXiv      preprint      arXiv:2311.01223.</p><p>Ziebart,      B.      D.,      A.      L.      Maas,      J.      A.      Bagnell,      A.      K.      Dey,      et      al.\\n(2008).\\nMaximum      entropy      inverse  \\n \\n  reinforcement      learning.\\nIn      Aaai,      Volume      8,      pp.\\n1433-1438.\\nChicago,      IL,      USA.</p><h2>6.1.1      Relation      with      Loss      Functions      for      the      Original      Training      Objective</h2><p>tive In      this      subsection,      we      explore      the      connection      with      the      original      loss      function      of      pre-trained      diffusion  \\n \\n  models.\\nTo      see      that,      as      we      see      in      Section      1.1.1,      recall ol)   \\n \\n=      al   \\n \\n4      (0.50,   \\n \\n—      1/o%eg,,,      (a1,   \\n \\nT   \\n \\n—      t)](5t)      +e?\\n02,      €)   \\n \\n~      N(0,1).</p><p> \\n \\n \\n/       \\\\\\\\\\\\\\\\-  \\n \\n  (al?\\nt;Opre) Then,      the      loss      function      (24)      in      reward-weighted      MLE      reduces      to</p><table><th><td colSpan=1>      =      r(x”)       A,      —      Vo      S>      Ss\"      exp      |      ~~      —       t=T+1      i=1</td><td colSpan=1>      .      _      ;      2       (i,t)      (ay,      Tt;      Opre)      _      eal,      T      -t;      0)       ey      _       {or}?</td><td colSpan=1>|o=0</td></th><tr><td colSpan=1></td><td colSpan=1>      2</td><td colSpan=1>      (26)</td></tr></table><p>This      objective      function      closely      resembles      the      reward-weighted      version      of      the      loss      function      (5)      used  \\n \\n  for      training      pre-trained      diffusion      models.</p><h3>6.2      Value-Weighted      Sampling</h3><p>ling  \\n \\n  Thus      far,      we      have      discussed      methods      for      fine-tuning      pre-trained      diffusion      models.\\nNow,      let’s      delve  \\n \\n  into      an      alternative      approach      during      inference      that      aims      to      sample      from      the      target      distribution      p,.\\n \\n \\n  without      explicitly      fine-tuning      the      diffusion      models.\\nIn      essence,      this      approach      involves      incorporating  \\n \\n  gradients      of      value      functions      during      inference      alongside      the      denoising      process      in      pre-trained      diffusion  \\n \\n  models.\\nHence,      we      refer      to      this      approach      as      value-weighted      sampling.\\nWhile      it      seems      that      this  \\n \\n  method      has      not      been      explicitly      formalized      in      previous      literature,      the      value-weighted      sampling  \\n \\n  closely      connects      with      classifier      guidance,      as      discussed      in      Section      8.1.  \\n \\n  Before      delving      into      the      algorithmic      details,      we      outline      the      motivation.\\nSubsequently,      we      present  \\n \\n  the      concrete      algorithm      and      discuss      its      advantages—specifically,      its      capability      to      operate      without      the  \\n \\n  necessity      of      fine-tuning      diffusion      models—and      its      disadvantage,      which      involves      the      need      to      obtain  \\n \\n  differentiable      value      functions.</p><p>Algorithm   \\n \\n4      Value-weighted      sampling 1:      Require:      Pre-trained      model      {p?\\n\"*(2,_1|r1)      }:   \\n \\n=      {N(0(      22,      t;      Opre),      07(t))      be.\\n \\n \\n  2:      Estimate      v   \\n \\n:   \\n \\n¥   \\n \\nx      [0,7]   \\n \\n—      R      and      denote      it      by      i(.,      -) ¢\\n \\n   (1)      Monte-Carlo      approach      in      (28) e      (2)      Value      iteration      approach      (Soft      Q-learning)      in      (29)      in      Section      6.2.1 ¢\\n \\n   (3)      Approximation      using      Tweedie’s      formula      in      Section      6.2.2 3:      fort   \\n \\n€      [7      +1,---   \\n \\n,      1}      do  \\n \\n  4:      Set.\\na?\\n \\n \\nt      Ve0   \\n \\nx      xL=Lt  \\n \\n  P(t,      t)   \\n \\n=   \\n \\n(   \\n \\n)      \\\\\\\\\\\\\\\\   \\n \\nu   \\n \\n+      p(xz,t      Are)</p><p>5:      end      for  \\n \\n  6:      Output:      {N(p      O(      Le,      t),o7(t))      ery</p><p> \\n \\n  Motivation.\\nConsidering   \\n \\na      Gaussian      policy      7:1   \\n \\n~      N(/(2:,t),07(t)),      we      aim      to      determine  \\n \\n  P(x4,t;      0)      such      that      N(A(2x;,      t;      0),      0?(t))      closely      approximates      p*(-|x,).\\nHere,      typically,      we      have  \\n \\n  p(xz,t;0)   \\n \\n=      a,   \\n \\n+      (t)g(a;,t)      and      o?(t)   \\n \\n=      g(t)(ot)      for      certain      function   \\n \\ng   \\n \\n:   \\n \\nY   \\n \\nx      [0,7]   \\n \\n>      R¢  \\n \\n  and   \\n \\ng   \\n \\n:      [0,7]   \\n \\n—      R,      as      we      have      explained      in      Section      1.1.1.\\nThen,      using   \\n \\nx      as      equality      up      to      the  \\n \\n  normalizing      constant, Piles      |)      exp(eralar-s)/a)      exp      (BEE      OI)</p><p>2\\n \\n   exp(vy(am)   \\n \\n+      Ver(      ee)»      {a1   \\n \\n—      3)      exp      (—      “2      Clr      uF)</p><p>_\\n \\n   lve1   \\n \\n—      ae   \\n \\n—      (58)      9(t)      Vora)      /o   \\n \\n—      (6t)9(ae,      t))      |?\\n \\n \\n  esp      (-      0.59(#)      (6t)      )</p><h4>Hence,      we      can      approximate:</h4><p> \\n \\n  o°(t)      Vu;      (a4)  \\n \\n  a   \\n \\n+      p(      xz,      ti      Ore):      (27)  \\n \\n  P(x,      t)      od</p><p>Remark      5.\\nNote      while      the      above      derivation      is      heuristic,      it      is      formalized      in      Uehara      et      al.\\n(2024,  \\n \\n  Lemma      2)      in      the      continuous-time      formulation.</p><p> \\n \\n  Algorithm.\\nUtilizing      (27),      the      entire      algorithm      of      value-weighted      sampling      is      outlined      in      Algo-  \\n \\n  rithm      4.\\nThis      method      doesn’t      involve      updating      parameters      in   \\n \\na      diffusion      model;      hence,      it’s      not   \\n \\na       fine-tuning      method.</p><p> \\n \\n  Note      that      in      this      algorithm,      we      require   \\n \\na      form      of      v;(-)      to      compute      gradients.\\nThis      value      function  \\n \\n  can      be      estimated      through      regression      using      the      characterization      described      in      Lemma      2.\\nHere,      inspired  \\n \\n  by      Lemma      2,      we      use      the      following      loss      function:</p><p>1\\n \\n   om      (i,t)      (i,t)   \\n \\n°       o(2)   \\n \\n=      argmin      S-      S-      {es      (me)   \\n \\n—      exp   \\n \\n|   \\n \\n,      (28)</p><p>h:[R4,{0,T]]-R      t=T+1      i=1</p><p>where      the      data      is      collected      in      an      off-policy      way.\\nLater,      we      explain      two      alternative      approaches      in  \\n \\n  Section      6.2.1      and      Section      6.2.2.</p><p> \\n \\n  Advantages      and      potential      disadvantages.\\nThe      value-weighted      sampling      is      straightforward      and  \\n \\n  less      memory-intensive      since      it      avoids      fine-tuning.\\nTherefore,      it      presents      an      appealing      simple      option  \\n \\n  if      it      performs      well.\\nIndeed,      in      various      inverse      problems,      such      as      inpainting,      super-resolution,      and  \\n \\n  colorization,      setting      r()      as   \\n \\na      likelihood      of      the      measurement      model   \\n \\ny   \\n \\n=      g(a)      +e      (where   \\n \\ny      represents  \\n \\n  actual      measurements,   \\n \\n€      denotes      noise,      and   \\n \\ng      defines      the      measurement      function)      has      proven      highly  \\n \\n  successful      (Chung      et      al.,      2022;      Bansal      et      al.,      2023;      Chung      et      al.,      2022).</p><p> \\n \\n  The      potential      drawback      compared      to      fine-tuning      algorithms      so      far      (i.e.,      PPO      and      reward-  \\n \\n  weighted      MLE)      is      the      necessity      to      learn      differetiable      soft      value      functions      like      direct      reward      back-  \\n \\n  propagation.\\nThis      learning      process      is      often      not      straightforward      as      explained      in      Section      5.2.\\nThe  \\n \\n  previously      discussed      fine-tuning      algorithms      (i.e.,      PPO,      reward-weighted      MLE)      circumvent      this  \\n \\n  requirement      by      obtaining      rewards      to      go      via      Monte      Carlo      approaches,      thereby      avoiding      the      need      for  \\n \\n  soft      value      functions.</p><h3>6.2      Value-Weighted      Sampling</h3><p>ling  \\n \\n  Thus      far,      we      have      discussed      methods      for      fine-tuning      pre-trained      diffusion      models.\\nNow,      let’s      delve  \\n \\n  into      an      alternative      approach      during      inference      that      aims      to      sample      from      the      target      distribution      p,.\\n \\n \\n  without      explicitly      fine-tuning      the      diffusion      models.\\nIn      essence,      this      approach      involves      incorporating  \\n \\n  gradients      of      value      functions      during      inference      alongside      the      denoising      process      in      pre-trained      diffusion  \\n \\n  models.\\nHence,      we      refer      to      this      approach      as      value-weighted      sampling.\\nWhile      it      seems      that      this  \\n \\n  method      has      not      been      explicitly      formalized      in      previous      literature,      the      value-weighted      sampling  \\n \\n  closely      connects      with      classifier      guidance,      as      discussed      in      Section      8.1.  \\n \\n  Before      delving      into      the      algorithmic      details,      we      outline      the      motivation.\\nSubsequently,      we      present  \\n \\n  the      concrete      algorithm      and      discuss      its      advantages—specifically,      its      capability      to      operate      without      the  \\n \\n  necessity      of      fine-tuning      diffusion      models—and      its      disadvantage,      which      involves      the      need      to      obtain  \\n \\n  differentiable      value      functions.</p><p>Algorithm   \\n \\n4      Value-weighted      sampling 1:      Require:      Pre-trained      model      {p?\\n\"*(2,_1|r1)      }:   \\n \\n=      {N(0(      22,      t;      Opre),      07(t))      be.\\n \\n \\n  2:      Estimate      v   \\n \\n:   \\n \\n¥   \\n \\nx      [0,7]   \\n \\n—      R      and      denote      it      by      i(.,      -) ¢\\n \\n   (1)      Monte-Carlo      approach      in      (28) e      (2)      Value      iteration      approach      (Soft      Q-learning)      in      (29)      in      Section      6.2.1 ¢\\n \\n   (3)      Approximation      using      Tweedie’s      formula      in      Section      6.2.2 3:      fort   \\n \\n€      [7      +1,---   \\n \\n,      1}      do  \\n \\n  4:      Set.\\na?\\n \\n \\nt      Ve0   \\n \\nx      xL=Lt  \\n \\n  P(t,      t)   \\n \\n=   \\n \\n(   \\n \\n)      \\\\\\\\\\\\\\\\   \\n \\nu   \\n \\n+      p(xz,t      Are)</p><p>5:      end      for  \\n \\n  6:      Output:      {N(p      O(      Le,      t),o7(t))      ery</p><p> \\n \\n  Motivation.\\nConsidering   \\n \\na      Gaussian      policy      7:1   \\n \\n~      N(/(2:,t),07(t)),      we      aim      to      determine  \\n \\n  P(x4,t;      0)      such      that      N(A(2x;,      t;      0),      0?(t))      closely      approximates      p*(-|x,).\\nHere,      typically,      we      have  \\n \\n  p(xz,t;0)   \\n \\n=      a,   \\n \\n+      (t)g(a;,t)      and      o?(t)   \\n \\n=      g(t)(ot)      for      certain      function   \\n \\ng   \\n \\n:   \\n \\nY   \\n \\nx      [0,7]   \\n \\n>      R¢  \\n \\n  and   \\n \\ng   \\n \\n:      [0,7]   \\n \\n—      R,      as      we      have      explained      in      Section      1.1.1.\\nThen,      using   \\n \\nx      as      equality      up      to      the  \\n \\n  normalizing      constant, Piles      |)      exp(eralar-s)/a)      exp      (BEE      OI)</p><p>2\\n \\n   exp(vy(am)   \\n \\n+      Ver(      ee)»      {a1   \\n \\n—      3)      exp      (—      “2      Clr      uF)</p><p>_\\n \\n   lve1   \\n \\n—      ae   \\n \\n—      (58)      9(t)      Vora)      /o   \\n \\n—      (6t)9(ae,      t))      |?\\n \\n \\n  esp      (-      0.59(#)      (6t)      )</p><h4>Hence,      we      can      approximate:</h4><p> \\n \\n  o°(t)      Vu;      (a4)  \\n \\n  a   \\n \\n+      p(      xz,      ti      Ore):      (27)  \\n \\n  P(x,      t)      od</p><p>Remark      5.\\nNote      while      the      above      derivation      is      heuristic,      it      is      formalized      in      Uehara      et      al.\\n(2024,  \\n \\n  Lemma      2)      in      the      continuous-time      formulation.</p><p> \\n \\n  Algorithm.\\nUtilizing      (27),      the      entire      algorithm      of      value-weighted      sampling      is      outlined      in      Algo-  \\n \\n  rithm      4.\\nThis      method      doesn’t      involve      updating      parameters      in   \\n \\na      diffusion      model;      hence,      it’s      not   \\n \\na       fine-tuning      method.</p><p> \\n \\n  Note      that      in      this      algorithm,      we      require   \\n \\na      form      of      v;(-)      to      compute      gradients.\\nThis      value      function  \\n \\n  can      be      estimated      through      regression      using      the      characterization      described      in      Lemma      2.\\nHere,      inspired  \\n \\n  by      Lemma      2,      we      use      the      following      loss      function:</p><p>1\\n \\n   om      (i,t)      (i,t)   \\n \\n°       o(2)   \\n \\n=      argmin      S-      S-      {es      (me)   \\n \\n—      exp   \\n \\n|   \\n \\n,      (28)</p><p>h:[R4,{0,T]]-R      t=T+1      i=1</p><p>where      the      data      is      collected      in      an      off-policy      way.\\nLater,      we      explain      two      alternative      approaches      in  \\n \\n  Section      6.2.1      and      Section      6.2.2.</p><p> \\n \\n  Advantages      and      potential      disadvantages.\\nThe      value-weighted      sampling      is      straightforward      and  \\n \\n  less      memory-intensive      since      it      avoids      fine-tuning.\\nTherefore,      it      presents      an      appealing      simple      option  \\n \\n  if      it      performs      well.\\nIndeed,      in      various      inverse      problems,      such      as      inpainting,      super-resolution,      and  \\n \\n  colorization,      setting      r()      as   \\n \\na      likelihood      of      the      measurement      model   \\n \\ny   \\n \\n=      g(a)      +e      (where   \\n \\ny      represents  \\n \\n  actual      measurements,   \\n \\n€      denotes      noise,      and   \\n \\ng      defines      the      measurement      function)      has      proven      highly  \\n \\n  successful      (Chung      et      al.,      2022;      Bansal      et      al.,      2023;      Chung      et      al.,      2022).</p><p> \\n \\n  The      potential      drawback      compared      to      fine-tuning      algorithms      so      far      (i.e.,      PPO      and      reward-  \\n \\n  weighted      MLE)      is      the      necessity      to      learn      differetiable      soft      value      functions      like      direct      reward      back-  \\n \\n  propagation.\\nThis      learning      process      is      often      not      straightforward      as      explained      in      Section      5.2.\\nThe  \\n \\n  previously      discussed      fine-tuning      algorithms      (i.e.,      PPO,      reward-weighted      MLE)      circumvent      this  \\n \\n  requirement      by      obtaining      rewards      to      go      via      Monte      Carlo      approaches,      thereby      avoiding      the      need      for  \\n \\n  soft      value      functions.</p><h4>Hence,      we      can      approximate:</h4><p> \\n \\n  o°(t)      Vu;      (a4)  \\n \\n  a   \\n \\n+      p(      xz,      ti      Ore):      (27)  \\n \\n  P(x,      t)      od</p><p>Remark      5.\\nNote      while      the      above      derivation      is      heuristic,      it      is      formalized      in      Uehara      et      al.\\n(2024,  \\n \\n  Lemma      2)      in      the      continuous-time      formulation.</p><p> \\n \\n  Algorithm.\\nUtilizing      (27),      the      entire      algorithm      of      value-weighted      sampling      is      outlined      in      Algo-  \\n \\n  rithm      4.\\nThis      method      doesn’t      involve      updating      parameters      in   \\n \\na      diffusion      model;      hence,      it’s      not   \\n \\na       fine-tuning      method.</p><p> \\n \\n  Note      that      in      this      algorithm,      we      require   \\n \\na      form      of      v;(-)      to      compute      gradients.\\nThis      value      function  \\n \\n  can      be      estimated      through      regression      using      the      characterization      described      in      Lemma      2.\\nHere,      inspired  \\n \\n  by      Lemma      2,      we      use      the      following      loss      function:</p><p>1\\n \\n   om      (i,t)      (i,t)   \\n \\n°       o(2)   \\n \\n=      argmin      S-      S-      {es      (me)   \\n \\n—      exp   \\n \\n|   \\n \\n,      (28)</p><p>h:[R4,{0,T]]-R      t=T+1      i=1</p><p>where      the      data      is      collected      in      an      off-policy      way.\\nLater,      we      explain      two      alternative      approaches      in  \\n \\n  Section      6.2.1      and      Section      6.2.2.</p><p> \\n \\n  Advantages      and      potential      disadvantages.\\nThe      value-weighted      sampling      is      straightforward      and  \\n \\n  less      memory-intensive      since      it      avoids      fine-tuning.\\nTherefore,      it      presents      an      appealing      simple      option  \\n \\n  if      it      performs      well.\\nIndeed,      in      various      inverse      problems,      such      as      inpainting,      super-resolution,      and  \\n \\n  colorization,      setting      r()      as   \\n \\na      likelihood      of      the      measurement      model   \\n \\ny   \\n \\n=      g(a)      +e      (where   \\n \\ny      represents  \\n \\n  actual      measurements,   \\n \\n€      denotes      noise,      and   \\n \\ng      defines      the      measurement      function)      has      proven      highly  \\n \\n  successful      (Chung      et      al.,      2022;      Bansal      et      al.,      2023;      Chung      et      al.,      2022).</p><p> \\n \\n  The      potential      drawback      compared      to      fine-tuning      algorithms      so      far      (i.e.,      PPO      and      reward-  \\n \\n  weighted      MLE)      is      the      necessity      to      learn      differetiable      soft      value      functions      like      direct      reward      back-  \\n \\n  propagation.\\nThis      learning      process      is      often      not      straightforward      as      explained      in      Section      5.2.\\nThe  \\n \\n  previously      discussed      fine-tuning      algorithms      (i.e.,      PPO,      reward-weighted      MLE)      circumvent      this  \\n \\n  requirement      by      obtaining      rewards      to      go      via      Monte      Carlo      approaches,      thereby      avoiding      the      need      for  \\n \\n  soft      value      functions.</p><h2>6.2.1      Soft      Q-learning</h2><p>ning  \\n \\n  We      have      elucidated      that      leveraging      Lemma      2,      we      can      estimate      soft      value      functions      v;(-)      based  \\n \\n  on      Equation      (28)      in   \\n \\na      Monte      Carlo      way.\\nSubsequently,      these      soft      value      functions      are      used      in  \\n \\n  value-weighted      sampling      to      sample      from      the      target      distribution      p,.\\nAlternatively,      there      is      another  \\n \\n  method      that      involves      using      soft      Bellman      equations      to      estimate      soft      value      functions      v;(-).\\nThis  \\n \\n  technique      is      commonly      called      soft      Q-learning      in      the      context      of      standard      RL.</p><p> \\n \\n  First,      recalling      the      soft      Bellman      equations      in      (16),      we      have en      (22)   \\n \\n=      fey      (=)      pa      nd</p><p>Taking      the      logarithm,      we      obtain u(ay)   \\n \\n=      alog      f      exp      (eaen)      De      (@4-1   \\n \\n|      @4)dxy-1.</p><p>a</p><h3>Hence,</h3><p> \\n \\n \\n_   \\n \\n:      (xt)      Up-1(Lt-1)   \\n \\n°       vu   \\n \\n=      argmin      Ey,.u,   \\n \\n|   \\n \\n4      ——   \\n \\n—      log   \\n \\n|      exp   \\n \\n|      ————   \\n \\n]      Dpre(@e-1]      21)      da4_1  \\n \\n  a   \\n \\na h:X->R  \\n \\n  where      u,   \\n \\n€      A(%)      is      any      roll-in      distribution      that      covers      the      entire      space      V.      Using      this      relation      and  \\n \\n  replacing      the      expectation      E,,,.,,,      with      empirical      approximation,      we      are      able      to      estimate      soft      value  \\n \\n  functions      v;      in   \\n \\na      recursive      manner:</p><p> \\n \\n \\n1   \\n \\nm      ~(j—1)   \\n \\n°       aj   \\n \\n;   \\n \\ni      Ur      Ut   \\n \\ny       {6      (x)},   \\n \\n—       argmin   \\n \\ny      S      {ht   \\n \\n)   \\n \\n—      log   \\n \\n/      exp      (eee)      Ppre(@1—1|}      yan] h:      [R¢,[0,7]]|-R      t=T+1      i=1</p><p>where      the      data      is      collected      in      an      off-policy      way.</p><p> \\n \\n  Remark      6.\\nAlthough      soft      Q-learning      is      widely      used      in      standard      RL      (Schulman      et      al.,      2017),      it  \\n \\n  cannot      be      directly      applied      to      our      fine-tuning      context      without      resorting      to      value-weighted      sampling  \\n \\n  or      value-weighted      MLE.\\nThis      is      because,      even      if      we      estimate      soft-value      functions      as      %;,      substituting  \\n \\n  Uz      with      0,      in      the      soft-optimal      policy      results      in      an      unnormalized      policy.<ul><li>6.2.2.\\nApproximation      using      Tweedie’s      formula</li></ul></p><p> \\n \\n  So      far,      we      have      explained      two      approaches:   \\n \\na      Monte      Carlo      approach      and   \\n \\na      value      iteration      approach      to  \\n \\n  estimate      soft      value      functions.\\nHowever,      learning      value      functions      in      (28)      can      still      be      often      challenging  \\n \\n  in      practice.\\nTherefore,      we      can      employ      approximation      strategies      inspired      by      recent      literature      on  \\n \\n  classifier      guidance      (e.g.,      reconstruction      guidance      (Ho      et      al.,      2022),      manifold      constrained      gradients  \\n \\n  (Chung      et      al.,      2022),      universal      guidance      (Bansal      et      al.,      2023),      and      diffusion      posterior      sampling  \\n \\n  (Chung      et      al.,      2022)).</p><p> \\n \\n  Specifically,      we      adopt      the      following      approximation:</p><p>ur(xz)   \\n \\n=      alog      Egpry      lex      (“)      oy   \\n \\n=      alog      (/      exp      (om)      p      (colar      (30)</p><p>~\\n \\n   alog      (exw      (ceo)   \\n \\n)   \\n \\n»       £o(t1)   \\n \\n=      Egprey|xo   \\n \\n|      xe],      (31) a\\n \\n    =\\n \\n   1r(Xo(2t)).</p><p> \\n \\n  Here,      we      replace      the      integration      in      (30)      with   \\n \\na      Dirac      delta      distribution      with      the      posterior      mean.\\n \\n \\n  Importantly,      we      can      calculate      Zo      (x;)   \\n \\n=      E,»*-[xo   \\n \\n|      x4]      using      the      pre-trained      (score-based)      diffusion  \\n \\n  model      based      on      Tweedie’s      formula:</p><table><tr><td colSpan=1>      O12</td><td colSpan=1><p>]\\n \\n \\n  E,pre      [xo   \\n \\n|      x1   \\n \\n—      Ut   \\n \\n+      {or}      V      0g      Ge(&e)  \\n \\n  Le</p></td></tr></table><p>Recall      that      the      notation      p/?,      0?,      q,      are      defined      in      (3).\\nFinally,      by      recalling      V      log   \\n \\n@   \\n \\n=      Si      ore      (x;,t)      in  \\n \\n  score-based      diffusion      models,      we      can      approximate      Vv;(a)      with</p><h4>Le</h4><p> \\n \\n  and      plug      it      into      Algorithm   \\n \\n4      (i.e.,      value-weighted      sampling).\\n \\n \\n  As      mentioned      earlier,      this      approach      has      been      practically      widely      used      in      the      context      of      classifier  \\n \\n  guidance.\\nDespite      its      simplicity,      the      approximation      error      can      be      significant,      as      it      can      not      be  \\n \\n  diminished      even      with   \\n \\na      large      sample      size      because      the      discrepancy      from      (30)      to      (31)      is      not      merely   \\n \\na       statistical      error      that      diminishes      with      increasing      sample      size.</p><li>6.2.3.\\nZeroth-Order      Guidance      using      Path      Integral      Control</li><p> \\n \\n  In      the      previous      subsection      (Section      6.2.2),      we      discussed      an      approximation      technique      to      bypass      the  \\n \\n  necessity      of      learning      explicit      value      functions.\\nAnother      approach      to      bypass      this      requirement      is      to      use  \\n \\n  control-based      techniques      for      obtaining      soft-optimal      policies,      a.k.a.,      path      integral      control      (Theodorou  \\n \\n  et      al.,      2010;      Williams      et      al.,      2017;      Kazim      et      al.,      2024).</p><p> \\n \\n  Recall      that      we      have      p(x;,t;      6)   \\n \\n=      2;   \\n \\n+      (dt)g(x+,      t)      and      o°(t)   \\n \\n=      g(t)(dt),      the      motivation      behind this      approach      is      as      follows.\\nInitially,      we      have:</p><h5>Vo,      exp(v(#1)/@)</h5><p>=\\n \\n   Egppreyexp(vr_1      (22-1)      /@)   \\n \\n|      24]      (Soft      Bellman      equation) _\\n \\n   (1   \\n \\n—   \\n \\nx   \\n \\n—      (Ot)      G(a2,      t))?\\n=\\n \\n   Vz»,   \\n \\n{   \\n \\n[      explatena/ay)      exp      (-      0-5a(t)      (ot)   \\n \\n)      in,-}</p><p>=\\n \\n   Byes      [ese      a(era/a))      {ea      ~~      (61)g(nn,)}      EE      Cah</p><p>~\\n \\n   arp      Buty      lexr(era(ea/a))      Gera   \\n \\n—      21   \\n \\n~      (8t)ale      plod</p><p>_t   \\n \\n1       Ht)      ot)      Eg,prey      [E      ppp}      [exp(r(2o)/a)   \\n \\n|      T-1]      e|24|   \\n \\n=      Woon)      [exp(r(xo/a))e|x¢]      ;</p><h5>Now,      we      obtain:</h5><p>o7(t)Vnv(ae)   \\n \\n1      Eqppry      lexp(r(wo)/a)      eel    \\n \\nQ   \\n \\n~   \\n \\na   \\n \\n*      Egyprey      lexp(r(2o)/a))      |x]      (32)</p><p> \\n \\n  Importantly,      this      approach      does      not      require      any      differentiation.\\nTherefore,      by      running      policies  \\n \\n  from      pre-trained      models      and      simply      using      Monte      Carlo      approximation      in      both      the      denominator  \\n \\n  and      numerator,      we      can      estimate      the      above      quantity      (the      right-hand      side      of      (32))      without      making  \\n \\n  any      models      (classifiers),      unlike      classifier      guidance.\\nNote      while      this      approach      is      widely      used      in      the  \\n \\n  control      community,      it      may      not      be      feasible      for      diffusion      models      due      to      the      high      dimensionality      of  \\n \\n  the      input      space.</p><h3>6.3      Path      Consistency      Learning      (Losses      Often      Used      in      Gflownets)</h3><p>ets)  \\n \\n  Now,      we      explain      how      to      apply      path      consistency      learning      (PCL)      (Nachum      et      al.,      2017)      to      fine-  \\n \\n  tune      diffusion      models.\\nIn      the      Gflownets      literature      (Bengio      et      al.,      2023),      it      seems      that      this      variant      is  \\n \\n  utilized      as      either   \\n \\na      detailed      balance      or   \\n \\na      trajectory      balance      loss,      as      discussed      in      Mohammadpour  \\n \\n  et      al.\\n(2023);      Tiapkin      et      al.\\n(2023);      Deleu      et      al.\\n(2024).\\nHowever,      to      the      best      of      our      knowledge,  \\n \\n  the      precise      formulation      of      path      consistency      learning      in      the      context      of      fine-tuning      diffusion      models  \\n \\n  has      not      been      established.\\nTherefore,      we      start      by      elucidating      the      rationale      of      PCL.\\nSubsequently,  \\n \\n  we      provide   \\n \\na      comprehensive      explanation      of      the      PCL.\\nFinally,      we      discuss      its      connection      with      the  \\n \\n  literature      on      Gflownets.</p><p>Motivation.\\nHere,      we      present      the      fundamental      principles      of      the      PCL.\\nTo      start      with,      we      prove      the  \\n \\n  following      lemma,      which      characterizes      soft-value      functions      and      soft-optimal      policies      recursively.</p><p>Algorithm   \\n \\n5      Path      Consistency      Learning      (Training      with      detailed      balance      loss)  \\n \\n  1:      Require:      Diffusion-model      {N(p(2,t;),07(t))}i-74,,  \\n \\n  -pre-trained   \\n \\n=      model  \\n \\n  {N      (p(t,      t;      Opre),      07      (t))      }i_744,      batch      size      m,      parameter   \\n \\na   \\n \\n€      Rt,      learning      rate   \\n \\n7       2:      Set   \\n \\na      model      {v;(-;@)}      to      learn      optimal      soft      value      function,      and   \\n \\na      model      {p;(-|-;@)}      to      learn  \\n \\n  optimal      polices.\\n \\n \\n  3:      Initialize:      0;   \\n \\n=      Opre  \\n \\n  4:      for      s   \\n \\n=      {1,---      ,S}do  \\n \\n  5:      Collect   \\n \\nm      samples      {a      (0)      }?_7,,      from   \\n \\na      current      diffusion      model      (i.e.,      generating      by      sequen- tially      running      polices      {N(p(2z,      t;      8),      07(t))      }i_-74,      from   \\n \\nt   \\n \\n=   \\n \\nT      to   \\n \\nt   \\n \\n=      0)  \\n \\n  6:      Setuo=r</p><li>(i)   \\n \\n?       a!\\n \\n \\n3   \\n \\na   \\n \\n4      Up—-1(@      13      ds)      re  \\n \\n  bs41   \\n \\n—      bs   \\n \\n—      Vo   \\n \\n3   \\n \\na      9)   \\n \\n+      log      p:(a\\\\\\\\\\\\\\\\,      |a\\\\\\\\\\\\\\\\;0,)   \\n \\n-      EP   \\n \\n—      Jog      rl      >}      Ibe:</li><p>a\\n \\n \\n  t=T+1      i=1  \\n \\n  (c}      ite)      ()      via)      43s)   \\n \\n;       Os41   \\n \\n<      Os   \\n \\n—      Vo   \\n \\n3      TE      velee      ids)      —S<   \\n \\n+      log      pe(ay      2s      |p      0)      —§      log      pee      (ar?\\nifort?\\nJe      lass</p><p>t=T+1      i=1 7:      end      for  \\n \\n  8:      Output:      {p,(x1-1      [215      Os)      fe</p><h3>Lemma      4      (1-step      Consistency      Equation).</h3><li>(2)   \\n \\n+      log      pi      (aa      |a1)   \\n \\n=      (eaeen)   \\n \\n+      log      pp      (x+-1|2+)      (33)</li><p> \\n \\n  Proof.\\nWe      consider      the      marginal      distribution      with      respect      to      x,      and      2;_,      induced      by      the      soft-  \\n \\n  optimal      policy,      and      denote      it      [(z;,      2:1)   \\n \\n€      A(X   \\n \\nx      X).\\nThen,      it      is      clearly      I(x,)l(a,_1   \\n \\n|      x)   \\n \\n=       I(x4-1)l      (xz   \\n \\n|      a1).\\nNow,      from      Theorem   \\n \\n2      that      characterizes      marginal      distributions,      and      Theorem   \\n \\n3       that      characterizes      posterior      distributions,      this      results      in      :</p><p>1\\n \\n   UE      L      re   \\n \\nx   \\n \\n1      Otte      me  \\n \\n  exp   \\n \\n(   \\n \\ni      2)      Pe      (xt)   \\n \\nx      pe      (®e-1|@e)   \\n \\n=   \\n \\nB      exp      (faeev)      Pra      (@1-1)   \\n \\nX      Pe      (1/01) a\\n \\n   —_-_——”’   \\n \\nC   \\n \\na      ~~  \\n \\n  C  \\n \\n  we  \\n \\n  Optimal      policy  \\n \\n  “~   \\n \\n“      Posterior      distribution  \\n \\n  Marginal      distribution      at   \\n \\nt      Marginal      distribution      at      t-1 Rearranging      yields:</p><p>1\\n \\n   UL(&   \\n \\n1      Up—1      (Le      re  \\n \\n  G      exp   \\n \\n(      ))   \\n \\nx      Di      (@-1|%4)   \\n \\n=   \\n \\nG      exp      (eaeen)   \\n \\nx      pe      (@4-1|24)</p><p>Taking      the      logarithm,      the      statement      is      concluded.\\nO</p><p>Algorithm.\\nBeing      motivated      by      the      relation      in      (33),      after      initializing      v9   \\n \\n=      r,      we      obtain      the  \\n \\n  recursive      equation:</p><p>2 Olen      U_-1      (Lt  \\n \\n  (pp)   \\n \\n=      argmin   \\n \\n—      Egany   \\n \\n|      oO   \\n \\n4      tog   \\n \\ng      (arp      ng)   \\n \\n—      PEED)   \\n \\n—      dog      pP\"      (ya      er) g):%3R,g2):¥      A(X)   \\n \\na   \\n \\no U   \\n \\n_   \\n \\nx   \\n \\n_      re      re  \\n \\n  =      (MEY)      tog      peal      altiansa)      +++      Flog      pe      (esl),  \\n \\n  which      is      an      extension      of      (33).\\nThe      loss      function      based      on      the      above      k-step      consistency      equation  \\n \\n  could      make      training      faster      without      learning      value      functions      at      every      time      point,      as      noted      in      the  \\n \\n  literature      in      PCL.\\nIn      the      extreme      case      (i.e.,      when      we      recursively      apply      it      with   \\n \\nt   \\n \\n=      7’),      we      obtain      the  \\n \\n  following.</p><li>(34)  \\n \\n  where      u,   \\n \\n€      A(X)      is      any      exploratory      roll-in      distribution.\\nBased      on      this      algorithm,      we      outline      the  \\n \\n  entire      algorithm      in      Algorithm      5.  \\n \\n  We      make      several      important      remarks      regarding      Algorithm      5.\\nFirstly,      while      we      use      on-policy      data  \\n \\n  collection,      technically,      any      policy      can      be      used      in      this      off-policy      algorithm,      like      reward-weighted  \\n \\n  MLE.\\nSecondly,      in      practice,      it      might      be      preferable      to      utilize   \\n \\na      sub-trajectory      from      x;      to      x;_,      based  \\n \\n  on      the      following      expression:</li><p>UFZ  \\n \\n  log      pi_      pa      (Gee      |Vt—ng1)   \\n \\n+      +++   \\n \\n+      log      pi      (4-1      |a4)   \\n \\n+      (2)</p><h3>Corollary      1      (7-step      consistency).</h3><p>log      pf      (xo|21)   \\n \\n+      +++   \\n \\n+      log      pp(wr-a|er)   \\n \\n+      log      pp      (x7)      (35) r(x      re      re      re  \\n \\n  —      (=)   \\n \\n+      log      p?\\n*(xo|v1)      +--+   \\n \\n+      log      pr\"      (ar-i|er)   \\n \\n+      log      pe      (xr).</p><p>Proof.\\nWe      consider      the      marginal      distribution      with      respect      to      7,      +--+   \\n \\n,   \\n \\n%      induced      by      the      soft-optimal  \\n \\n  policy,      and      denote      it      I(vp,---      ,--+      ,%o)   \\n \\n€   \\n \\n¥      X--+   \\n \\nx      X.      We      have Uar)l(ar—1   \\n \\n|      Lr)      tee      I(xo|21)   \\n \\n=      U(x      )l(a4   \\n \\n|      Xo)      te      (ar   \\n \\n|      LT-1)      (36) From      Theorem   \\n \\n2      that      characterized      marginal      distributions,      and      Theorem   \\n \\n3      that      characterize      posterior  \\n \\n  distributions,      the      left      hand      side      of      (36)      is      equal      to *\\n \\n   *\\n \\n   *</p><p>Pr      (xr)   \\n \\nx      pp      (r|er-1)      X-++   \\n \\nx      pi      (Xo|@1) Marginal      distribution      atT      Optimal      policy      at   \\n \\nT   \\n \\n—   \\n \\n1      Optimal      policy      at   \\n \\n1 and      the      right-hand      side      in      (36)      is      equal      to exp(r      (xo)      /a)      pre      pre      pre  \\n \\n  aq      Po      (xo)   \\n \\nX      po      (a1   \\n \\n|      Zo)      X+++   \\n \\nX      prey      (@r   \\n \\n|      er-1)-.\\n \\n \\n|      ~~  \\n \\n  Marginal      distribution      at   \\n \\n0      Posterior      distribution      Posterior      distribution By      rearranging      the      term,      we      obtain      (35).\\nO</p><p> \\n \\n  Comparison      with      Gflownets.\\nIn      the      Gflownets      literature,      similar      losses      are      used.\\nFor      instance,  \\n \\n  the      loss      derived      from      (33)      or      (35)      is      commonly      known      as   \\n \\na      detailed      balance      loss      (Bengio      et      al.,  \\n \\n  2023)      or   \\n \\na      trajectory      loss      (Malkin      et      al.,      2022),      respectively.</p><p> \\n \\n  Note      in      general,      the      literature      in      Gflownets      primarily      focuses      on      sampling      from      unnormalized  \\n \\n  models      (distributions      proportional      to      exp(r(a))).\\nHence,      reference      policies      (i.e,      {p?\"\"}\\n \\n \\n)      or      latent  \\n \\n  states      (1.e.,      %7.;      before      x9)      are      introduced      without      relying      on      pre-trained      diffusion      models.\\nIn  \\n \\n  contrast,      in      our      context,      we      use      policies      derived      from      pre-trained      diffusion      models      as      reference  \\n \\n  policies,      leveraging      them      as      our      prior      knowledge.</p><p>7\\n \\n   Fine-Tuning      Settings      Taxonomy</p><p> \\n \\n  So      far,      we      implicitly      assume      we      have      access      to      reward      functions.\\nHowever,      these      functions      are      often  \\n \\n  unknown      and      need      to      be      learned      from      data.\\nWe      classify      several      settings      in      terms      of      whether      reward  \\n \\n  functions      are      available      or,      if      not,      how      they      could      be      learned.\\nThis      section      is      summarized      in      Figure      3.</p><h4>Direct      backpropagation      (Sec.      5.2)       _       Differentiable</h4><p>_\\n \\n   ——_      Non-differentiable PP      5.1</p><h4>Accurate      rewards      O      (Sec.      5.1)</h4><p> \\n \\n  feedback   \\n \\n;    \\n \\n—      Reward-weighted      MLE      (Sec.      6.1  \\n \\n  Xo      Unknown      reward      Memory      effi-      oe      eward-welgnte      (Sec.      6.1) unctions      —_\\\\\\\\\\\\\\\\_—      ciency</p><h4>No      fine-tuning      Computational</h4><p>efficiency —\\n \\n    NN      Direct      backpropagation      (Sec.      5.2)</p><h5>Value-weighted      sampling      (Sec.      6.2)</h5><p>Figure      3:      Practical      recommendation      of      RL-based      algorithms      to      optimize      downstream      reward  \\n \\n  functions.</p><h6>7.1      Fine-Tuning      with      Known,      Differentiable      Reward      Functions</h6><p>ions  \\n \\n  When      accurate      differentiable      reward      functions      are      available      (e.g.,      standard      in      computer      vision      tasks),  \\n \\n  our      primary      focus      is      typically      on      computational      and      memory      efficiency.\\nIn      general,      we      advocate      for  \\n \\n  employing      reward      backpropagation      (c.f.      Algorithm      2)      for      its      computational      efficiency.\\nAlternatively,  \\n \\n  if      memory      efficiency      is   \\n \\na      significant      concern,      we      suggest      using      PPO      due      to      its      stability.</p><h6>7.2      Fine-Tuning      with      Black-Box      Reward      Feedback</h6><p>back  \\n \\n  Here,      we      explore      scenarios      where      we      have      access      to      accurate      but      non-differentiable      (black-box)  \\n \\n  reward      feedback,      often      found      in      scientific      simulations.\\nThe      emphasis      still      remains      primarily      on  \\n \\n  computational      and      memory      efficiency.\\nIn      such      scenarios,      due      to      the      non-differentiability      of      reward  \\n \\n  feedback,      we      recommend      using      PPO      or      reward-weighted      MLE      for      the      following      reasons.</p><p> \\n \\n  In      the      preceding      section,      we      advocated      for      the      use      of      reward      backpropagation      due      to      its      com-  \\n \\n  putational      advantages.\\nHowever,      when      dealing      with      non-differentiable      feedback,      the      advantage  \\n \\n  of      reward      backpropagation      may      diminish.\\nThis      is      because      learning      from      such      feedback      requires  \\n \\n  learning      differentiable      reward      functions      to      make      the      algorithm      work      as      we      discuss      in      Section      5.2.  \\n \\n  However,      obtaining   \\n \\na      differentiable      reward      function      can      be      challenging      in      many      domains.\\nFor  \\n \\n  example,      in      molecular      property      prediction,      molecular      fingerprints      are      informative      features,      and  \\n \\n  accurate      reward      functions      can      be      derived      by      training      simple      neural      networks      on      these      features  \\n \\n  (Pattanaik      and      Coley,      2020).\\nYet,      these      mappings,      grounded      in      prior      scientific      understanding,      are  \\n \\n  non-differentiable,      thereby      restricting      their      applicability.</p><p> \\n \\n  In      contrast,      PPO      and      reward-weighted      MLE      allow      for      policy      updates      without      the      explicit  \\n \\n  requirement      to      learn   \\n \\na      differentiable      reward      function,      offering   \\n \\na      significant      advantage      over      reward  \\n \\n  backpropagation.</p><h6>7.3      Fine-Tuning      with      Unknown      Rewards      Functions</h6><p>ions  \\n \\n  When      reward      functions      (or      computational      proxies      in      Section      7.2)      are      unavailable,      we      must      learn  \\n \\n  from      data      with      reward      feedback.\\nIn      such      cases,      compared      to      the      previous      scenarios,      two      important  \\n \\n  considerations      arise:</p><p>¢\\n \\n   Not      only      computational      or      memory      efficiency      but      also      feedback      efficiency      (i.e.,      sample  \\n \\n  efficiency      regarding      reward      feedback)      is      crucial.</p><p> \\n \\n \\n¢      Given      that      learned      reward      feedback      may      not      generalize      well      outside      the      training      data      distri-  \\n \\n  butions,      it      is      essential      to      constrain      fine-tuned      models      to      prevent      significant      divergence      from  \\n \\n  diffusion      models.\\nIn      these      situations,      not      only      soft      PPO      and      direct      backpropagation      but      also  \\n \\n  methods      discussed      in      Section      6,      such      as      reward-weighted      MLE,      can      be      particularly      effective.</p><p>Further,      the      scenario      involving      unknown      reward      functions      can      be      broadly      categorized      into      two      cases,  \\n \\n  which      we      will      discuss      in      more      detail.</p><p>Offline      scenario      (Figure      4).\\nIn      many      scenarios,      even  \\n \\n  in      cases      where      reward      functions      are      unknown,      we      of-  \\n \\n.     .</p><p>Offline      data      with ten      have      access      to      offline      data      with      reward      feedback      reward      feedback</p><li>(i)      (i)   \\n \\nO   \\n \\n;   \\n \\ni       a”      ra  \\n \\n.      ne      straightforward      approach      is      to</li><p> \\n \\n \\n{   \\n \\n,   \\n \\n(   \\n \\n)    \\n.\\n    .\\n \\n   g\\n \\n   PP  \\n \\n.      <=>      Learning      reward  \\n \\n  perform      regression      with      neural      networks      and      build   \\n \\na      models  \\n \\n  learned      regressor      *.\\nHowever,      this      learned      reward      func-      SS  \\n \\n  tion   \\n \\n*      might      not      generalize      well      beyond      the      distribution      x)  \\n \\n  of      the      offline      data.\\nIn      regions      outside      this      distribution,   \\n \\n7       could      assign   \\n \\na      high      value      even      when      the      actual      reward  \\n \\n  r      is      low      due      to      high      uncertainty.\\nConsequently,      we  \\n \\n  could      be      easily      misled      by      out-of-distribution      samples  \\n \\n  if      we      solely      optimize      7.\\n|Fine-tuning      diffusion      models   \\n \\n|       One      approach      to      alleviate      this      issue      is      to      adopt   \\n \\na       pessimistic      (i.e.,      conservative)      strategy,      as      proposed      in      Figure      4:      Offline      scenario  \\n \\n  Uehara      et      al.\\n(2024).\\nThis      technique      has      been      widely      Collecting      data]   \\n \\nY      Learning      reward  \\n \\n  applied      in      the      field      of      offline      RL      (Levine,      2018).\\nThe      aa  \\n \\n  main      idea      is      to      penalize   \\n \\n7      outside      the      distributions      us-  \\n \\n  ing      techniques      such      as      adding      an      explicit      penalty      term  \\n \\n  (Yu      et      al.,      2020;      Chang      et      al.,      2021),      bootstrapping,  \\n \\n  or      more      sophisticated      methods      (Kumar      et      al.,      2020;  \\n \\n  Xie      et      al.,      2021;      Rigter      et      al.,      2022;      Uehara      and      Sun,  \\n \\n  2021).\\nBy      doing      so,      we      can      avoid      being      fooled      by  \\n \\n  out-to-distribution      regions      while      still      benefiting      from  \\n \\n  the      extrapolation      capabilities      of      reward      models.</p><p>S—      diffusion      models      |</p><p>Figure      5:      Online      scenario</p><p> \\n \\n  Online      scenario      (Figure      5).\\nConsider      situations  \\n \\n  where      reward      functions      are      unknown,      but      we      can      gather      data      online.\\nThese      scenarios      are      often  \\n \\n  referred      to      as   \\n \\na      lab-in-the-loop      setting.</p><p> \\n \\n  In      such      scenarios,      Uehara      et      al.\\n(2024)      proposes      an      iterative      procedure      comprising      three      steps:</p><li>(1)      collecting      feedback      data      by      exploring      new      areas      (i.e.,      obtaining   \\n \\nx      from      current      diffusion      models  \\n \\n  and      corresponding      r(x)),      (2)      learning      the      reward      function      from      the      collected      feedback      data,      and</li><li>(3)      fine-tuning      current      diffusion      models      by      maximizing      the      learned      reward      function      enhanced      by  \\n \\n  an      optimistic      bonus      term,      using      algorithms      discussed      in      Section      4.2.\\nIn      contrast      to      standard      online  \\n \\n  PPO,      where      feedback      data      is      directly      used      in      the      fine-tuning      process,      actual      reward      feedback      is  \\n \\n  only      used      in      step      (2)      and      not      in      step      (3).\\nAn      additional      key      aspect      is      the      use      of      an      optimistic  \\n \\n  reward      function      that      encourages      exploration      beyond      the      distributions      of      the      current      diffusion      model.\\n \\n \\n  This      deliberate      separation      between      reward      learning      and      fine-tuning      steps,      coupled      with      the      use      of  \\n \\n  optimism,      significantly      enhances      feedback      efficiency.</li><p>8\\n \\n   Connection      with      Classifier      Guidance</p><p> \\n \\n  Classifier      guidance      (Dhariwal      and      Nichol,      2021)      is      commonly      used      for      conditional      generation      in  \\n \\n  diffusion      models.\\nInterestingly,      RL-based      methods      share   \\n \\na      close      connection      with      classifier      guidance.\\n \\n \\n  More      specifically,      in      this      section,      following      Zhao      et      al.\\n(2024),      we      elucidate      how      RL-based      methods  \\n \\n  can      be      applied      to      conditional      generation.\\nFurthermore,      from      this      unified      viewpoint,      we      highlight  \\n \\n  that      classifier      guidance      is      seen      as   \\n \\na      value-weighted      sampling      in      Section      4.2.</p><h6>8.1      Classfier      Guidance</h6><p>ance  \\n \\n  In      this      section,      we      first      introduce      classifier      guidance.\\nClassifier      guidance      utilizes      an      unconditional  \\n \\n  pre-trained      model      for      conditional      generation.\\nMore      specifically,      we      consider   \\n \\na      scenario      where      we  \\n \\n  have   \\n \\na      pre-trained      diffusion      model,      which      enables      us      to      sample      from      p?\\n\"*(x),      and      data      with      feedback  \\n \\n  {x,y},      where   \\n \\ny      represents      the      new      label      we      want      to      condition      on.\\nOur      objective      here      is      to      sample  \\n \\n  from      p(-|y)   \\n \\n€      [Y   \\n \\n>      ACR), P(-   \\n \\n|      y)   \\n \\nx      py(yl-)pP*(-).\\n(37)</p><p>where      p,   \\n \\n:   \\n \\nX      —>      A(Y)      denotes      the      conditional      distribution      of   \\n \\ny      given      x.</p><p>Motivation.\\nIn      classifier      guidance,      we      consider      the      following      policy:</p><p>pe      (-lare,      y)   \\n \\n=   \\n \\nN      (p(22,      t;      Opre)   \\n \\n+      a(t)      Ve      log      Vy      (Xt;      t),      a*(t))   \\n \\n’ dy(x,t)   \\n \\n=      Egy.\\nyap,      (-leo)      L(Y   \\n \\n=      y)|a,   \\n \\n=      2].</p><p>Under      the      continuous-time      formulation      of      diffusion      models,      applying      Doob’s      h-transform      (Rogers  \\n \\n  and      Williams,      2000),      it      is      shown      that      we      can      sample      from      p(x|y)      by      sequentially      running      {p7\"\\\\\\\\\\\\\\'(-|7+,      y)      }i-r41-</p><p>Algorithm.\\nSpecifically,      the      algorithm      comprises      two      steps.\\nIt’s      important      to      note      that      this      is      not  \\n \\n  a      fine-tuning      method      but      rather      an      inference-time      technique      as      we      freeze      the      pre-trained      model.</p><li>1. Learning      q,(x,t)      through      regression      by      gathering      data      containing      {z;,      y}.\\nHere,      for      each  \\n \\n  (Xo,      y),      using   \\n \\na      forward      policy      for      the      pre-training      (from   \\n \\n0      to      T),      we      obtain      2;      starting      from</li><p>Lo.</p><li>2. During      inference,      sequentially      execute      {p2\"(-|a,,      y)      }1_>   \\n \\n1      by      leverage      the      fact      that      compared</li><p>gui to      the      policy      in      pre-trained      diffusion      models,      the      mean      of      {p?\\n\"\\\\\\\\\\\\\\'(-|71,      y)      }i-74,      is      just      shifted  \\n \\n  with      an      additional      term      V,,      log      q,(2,      t).</p><p> \\n \\n  Notably,      the      method      described      above      can      be      reduced      to      value-weighted      sampling      (cf.      Sec-  \\n \\n  tion      6.2),      when      r(x,      y)   \\n \\n=      log      p(y|x).\\nInspired      by      such      an      observation,      we      apply      an      RL-based  \\n \\n  fine-tuning      method      for      conditional      generation      as      below.</p><h7>8.2.      RL-Based      Fine-Tuning      for      Conditional      Generation</h7><p>tion  \\n \\n  In      this      section,      following      Zhao      et      al.\\n(2024),      we      explain      an      RL-based      fine-tuning      approach      for  \\n \\n  conditional      generation.\\nThe      core      idea      is      that      setting      r(x,      y)   \\n \\n=      log      p(y|x)      allows      us      to      sample      from  \\n \\n  the      target      conditional      distribution      (37),      drawing      on      the      insights      discussed      in      Section      4.2.  \\n \\n  More      precisely,      introducing   \\n \\na      distribution      over   \\n \\ny      as      q(-),      we      formulate      the      following      RL      problem:</p><p>{Di      ft   \\n \\n=      argmax      Egy,      (-\\\\\\\\\\\\\\\\-y)}.yott      log      p(y|xo)   \\n \\n—      NeopyaKL(pe(-|e,      y)      ||P      (late).\\n \\n \\n  {prE[(V,.X)PA(A)]      Heyy (38)</p><p> \\n \\n  Compared      to      the      objective      function      (18)      in      Section      4.2,      the      main      differences      are:      (1)      policy      space  \\n \\n  is      expanded      to      accommodate      additional      control      (i.e.,      y):      pz(-|-,-)   \\n \\nC      [R¢   \\n \\nx   \\n \\nY   \\n \\n+      A(R®)],      (2)      the  \\n \\n  fine-tuning      objectives      are      set      as      log-likelihoods:      log      p(y|z).\\nThen,      following      Theorem      1,      we      can  \\n \\n  establish      the      following      result.</p><p>Corollary      2.\\nThe      distribution      induced      by      {p*}1_74      (ue.   \\n \\n[      {Teri      Ph      (ae-1   \\n \\n|      te,      y)}deur      is      the target      conditional      distribution      (37).</p><p> \\n \\n  The      above      corollary      suggests      that      for      conditional      generation,      we      can      utilize      various      off-the-shelf  \\n \\n  algorithms      discussed      in      Section      4.2      and      6,      such      as      PPO,      reward      backpropagation,      and      reward-  \\n \\n  weighted      MLE,      as      well      as      value-weighted      sampling.</p><p>9\\n \\n   Connection      with      Flow-Based      Diffusion      Models</p><p> \\n \\n  We      elucidate      the      close      relationship      between      bridge      matching      and      fine-tuning,      as      summarized      in  \\n \\n  Table      2.\\nBridge      (flow)      matching      (Liu      et      al.,      2022;      Tong      et      al.,      2023;      Lipman      et      al.,      2023;      Liu      et      al.,  \\n \\n  2022;      Shi      et      al.,      2024;      Albergo      and      Vanden-Eijnden,      2022),      has      recently      gained      popularity      as   \\n \\na       distinct      type      of      diffusion      model      from      score-matching-based      ones.\\nThese      works      suggest      that      training  \\n \\n  and      inference      speed      will      be      accelerated      due      to      the      ability      to      define   \\n \\na      more      direct      trajectory      from  \\n \\n  noise      to      data      distribution      than      score-matching-based      diffusion      models.</p><p> \\n \\n  The      connection      between      bridge      matching      and      fine-tuning      becomes      evident      in      the      continuous-  \\n \\n  time      formulation.\\nTo      begin,      we      first      provide   \\n \\na      brief      overview      of      the      continuous-time      formulation      of  \\n \\n  score-matching-based      diffusion      models.</p><p> \\n \\n  Table      2:      Comparison      between      training      score/flow-based      diffusion      models      and      fine-tuning.\\nWe  \\n \\n  optimize      the      parameter   \\n \\n6      in      P’,      which      represents      the      induced      distribution      over      trajectories      associated  \\n \\n  with      6.</p><h8>Loss      Note      of      Q</h8><p>Score-based      KL(Q\\\\\\\\\\\\\\'™¢|P?)      Qtime:      Time-reversal      of      reference      SDE      (reference training      SDE      is      defined      from   \\n \\n0      to      T’      where      the      initial      dis-  \\n \\n  tribute      at   \\n \\n0      is      data      distribution)</p><p> \\n \\n  Flow-based      KL(Q*°>      ||P?)      @Q*°°h:      Coupling      between      the      reference      SDE      from  \\n \\n  training   \\n \\nT      to   \\n \\n0      conditioning      at   \\n \\n0      (dist.      over      x.;|7o)      and  \\n \\n  a      data      distribution      at      time   \\n \\n0      (dis.      over      x).\\nNote  \\n \\n  that      reference      SDE      is      defined      from      7’      to      0.</p><p> \\n \\n  Fine-tuning      KL(P?||Q*°)      @*:      Coupling      between      the      pre-trained      SDE  \\n \\n  from   \\n \\nT      to   \\n \\n0      conditioned      on   \\n \\n0      (dist.      over      x7.1|X0)  \\n \\n  and   \\n \\na      data      distribution      at      time   \\n \\n0      (dis.      over      Xo) 9.1      Continuous      Time      Formulation      of      Score-Based      diffusion      models</p><p> \\n \\n  We      provide   \\n \\na      brief      overview      of      the      continuous-time      formulation      of      diffusion      models      (Song      et      al.,  \\n \\n  2021),      further      expanding      Section      1.1.1.\\nIn      this      formulation,      we      initially      define      the      forward      reference  \\n \\n  SDE,      spanning      from      time   \\n \\n0      to      7’      (e.g.,      variance      exploding      (VE)      process      or      variance      preserving      (VP)  \\n \\n  process),      by      setting      an      initial      distribution      at   \\n \\n0      as      the      data      distribution.\\nAdditionally,      the      reference  \\n \\n  SDE      is      tailored      to      converge      to      an      easy      distribution      at      time      7’,      such      as      the      normal      distribution.</p><p> \\n \\n  Subsequently,      we      consider      the      time-reversal      SDE      (Anderson,      1982),      progressing      from      T\\\\\\\\\\\\\\'      to      0.\\nIf  \\n \\n  we      could      learn      this      time-reversal      SDE,      it      would      enable      us      to      sample      from      the      data      distribution      by  \\n \\n  starting      with      the      easy      initial      distribution      (at      7’)      and      following      the      time-reversal      SDE      from   \\n \\nT      to   \\n \\n0      at  \\n \\n  inference      time.</p><p> \\n \\n  Training.\\nNow,      the      remaining      question      is      how      to      learn      this      time-reversal      SDE.\\nHere,      by      repre-  \\n \\n  senting      the      induced      distribution      over      trajectories      from   \\n \\nT      to   \\n \\n0      with      this      time-reversal      SDE      as      Q\\\\\\\\\\\\\\'™®,  \\n \\n  our      goal      is      to      train   \\n \\na      new      SDE,      parameterized      by   \\n \\n@      in      the      neural      network,      such      that      the      induced  \\n \\n  distribution      P®      over      trajectories      associated      with   \\n \\n6      aligns      with      Q\"™*.\\nSpecifically,      the      objective      is      to minimize      the      following      losses:</p><p> \\n \\n  argmin      KL(Q\"™||P*).\\n(39)  \\n \\n  0  \\n \\n  With      some      algebra,      leveraging      the      observation      that      the      time-reversal      SDE      comprises      the      original  \\n \\n  forward      SDE      and      the      scoring      term      of      the      marginal      distribution      (Anderson,      1982)      as      in      (2),      Song  \\n \\n  et      al.\\n(2021)      demonstrates      that      this      KL      loss      (39)      is      approximated      as Dore   \\n \\n=      argmin      Eeuni((0,7))01~qyo0(weleo)co~pP\"e      Fe      Fl      Vey      log      jo      (Xt   \\n \\n|      Xo)   \\n \\n_      so(Xt,      t)|I?\\n],      (40)  \\n \\n  which      is      equal      to      (4)      where      the      weight      A(t)      is      {o?\\n}?\\n(for      notation,      refer      to      Section      1.1.1).</p><h9>9.2      Flow-Based      (Bridge-Based)      Diffusion      Models</h9><p>dels We      adopt      the      explanation      outlined      in      Liu      et      al.\\n(2022).\\nIn      this      framework,      we      begin      by      considering  \\n \\n  a      reference      SDE      (from      T\\\\\\\\\\\\\\'      to      0),      x;   \\n \\n=      zr_;      where      z;      is      defined      as      Brownian      motion:</p><p>dz,   \\n \\n=      dwt,      te      (0,      7).\\n(41)</p><p> \\n \\n  Subsequently,      we      consider      the      coupling      of      the      reference      SDE      conditioned      on      state      at      time   \\n \\n0      (i.e.,  \\n \\n  distribution      of      xo.7      conditional      on      x)      and      the      data      distribution      at      time      0,      denoting      the      resulting  \\n \\n  distribution      as      Q*°?,      Informally,      this      Q*°°?\\nis      characterized      as      follows.</p><p>Informal      characterization      of      Q*”.\\nWhen      discretizing      the      time      step,      by      denoting      the      distribution  \\n \\n  induced      by      the      reference      SDE      as      p\"(x7,--+   \\n \\n,      29),      we      define QF      (a7,      tae   \\n \\n,      20)   \\n \\n=      p(x,      ay   \\n \\n|      r9)pP\"°      (x9).</p><p>The      distribution      Q*”      above      is   \\n \\na      limiting      distribution      obtained      by      making      the      discretization      step small.</p><p>Training.\\nIn      bridge-based      diffusion      models,      our      objective      is      to      train   \\n \\na      new      SDE,      parametrized      by  \\n \\n  6      (e.g.,   \\n \\na      neural      network),      to      align      the      induced      distribution      P®      with      Q*°?.\\nSpecifically,      bridge-based training      aims      to      minimize aremin      KL(Q*>||P’).\\n(42)  \\n \\n  0  \\n \\n  While      the      method      to      minimize      the      above      is      currently      unclear,      we      can      derive   \\n \\na      trainable      objective  \\n \\n  through      algebraic      manipulation.\\nInitially,      we      observe      that      the      reference      SDE      (41),      conditioned      on  \\n \\n  state      xo      at      time   \\n \\n0      (zr   \\n \\n=      D),      is      expressed      as:</p><p>b\\n \\n \\n  b—      2;</p><p>t\\n \\n   €\\n \\n   [0,T];dz?\\n \\n \\n=      g?(z?,t)dt      +du,,  \\n \\n  g(z?,t)   \\n \\n=   \\n \\na       (43)</p><p>using      the      seminal      “Doob’s      h-transform”      (Rogers      and      Williams,      2000).\\nThis      SDE      is      commonly  \\n \\n  known      as      the      Brownian      bridge.\\nNow,      we      consider   \\n \\na      new      SDE      parameterized      by      0:</p><p>t\\n \\n   €\\n \\n   [0,7];      dz   \\n \\n=      s(z,t;      @)dt   \\n \\n+      dw.\\n(44)</p><p>Then,      we      can      demonstrate      that      minimizing      the      KL      loss      in      (42)      is      equivalent      to argmin      E,W      unil0,7],1~4\"°      cop?\\n[Ilg”      (2,      t)   \\n \\n_      s(21,      t;      0)      M3]      (45)</p><p>where      q;°      denotes      the      induced      distribution      at      time   \\n \\nt      following      SDE      (43)      conditioned      on      xo,      and  \\n \\n  s:      Xx      |0,T]   \\n \\n>   \\n \\n&      is      the      model      we      aim      to      optimize.</p><p> \\n \\n  Flow-based      diffusion      models.\\nThe      aforementioned      formulation      has      been      originally      proposed      as   \\n \\na       bridge-based      SDE      (Liu      et      al.,      2022)      because      the      conditional      SDE      is      often      referred      to      as   \\n \\na      Brownian  \\n \\n  bridge.\\nFlow-based      diffusion      models      proposed      in      Lipman      et      al.\\n(2023);      Tong      et      al.\\n(2023)      share   \\n \\na       close      relationship      with      bridge-based      diffusion      models.\\nThese      flow-based      models      use   \\n \\na      target      SDE (43)      and      an      SDE      with   \\n \\na      parameter      (44)      without      stochastic      terms      (1.e.,      no      dw,)      and      aim      to      minimize  \\n \\n  the      loss      function      (45).</p><p> \\n \\n  Comparison      with      score-based      diffusion      models.\\nBoth      score-based      diffusion      models      and      bridge-  \\n \\n  based      diffusion      models      aim      to      minimize      KL      divergences.\\nDespite      its      similarity,      in      contrast      to  \\n \\n  score-based      diffusion      models,      bridge-based      diffusion      models      target      Q*°>      rather      than      the      time-  \\n \\n  reversal      SDE      Q\"™®.\\nThe      distribution      Q*”      is      often      considered      preferable      because      it      can      impose      an  \\n \\n  SDE      that      navigates      efficiently      from   \\n \\n0      to      7’      (e.g.,   \\n \\na      Brownian      bridge      with      minimal      noise).\\nThis      can  \\n \\n  expedite      the      learning      process      since      we      lack      direct      control      over      the      time-reversal      SDE      in      score-based  \\n \\n  diffusion      models.</p><h10>9.3.      Connection      with      RL-Based      Fine-Tuning</h10><p>ning  \\n \\n  RL-based      fine-tuning      shares   \\n \\na      close      relationship      with      flow-based      training.\\nTo      illustrate      this,      consider  \\n \\n  the      reference      SDE      (from      7’      to      0)      following      the      pre-trained      diffusion      model.\\nThen,      akin      to      flow-  \\n \\n  based      training,      we      consider      the      coupling      of      the      reference      SDE      conditioned      on      state      at      time   \\n \\n0       (i.e.,      distribution      over      xo-r      conditioned      on      x9)      and      the      target      distribution      at      time   \\n \\n0      (i.e.,      p,(-)   \\n \\n«       exp(r(-)/a)p?\\n\"°(-)),      denoting      the      induced      distribution      as      Q*™.</p><p>Informal      characterization      of      Q™°*.\\nInformally,      Q*\"°      is      introduced      as      follows.\\nWhen      discretizing  \\n \\n  the      time      step,      by      denoting      the      distribution      induced      by      the      reference      SDE      as      p™\\\\\\\\\\\\\\'(xp,-+-   \\n \\n,      9),      we define Qi\"      (xr,   \\n \\na   \\n \\n,      0)   \\n \\n=      p(x,      nr      Ly   \\n \\n|      L0)Pr      (Xo).</p><p>The      distribution      Q*°      above      is   \\n \\na      limiting      distribution      obtained      by      making      the      discretization      step small.</p><p>Training.\\nNow,      we      introduce   \\n \\na      new      SDE      parameterized      by   \\n \\n@      such      as      an      neural      network      to      align  \\n \\n  the      induced      distribution      P’      with      Q®\"*.\\nActually,      in      the      continuous      formulation,      the      RL-problem      (18) in      Section      4.2      is      equal      to      solving argmin      KL(P?||Q\"™).\\n \\n \\n  0 This      formalization      has      been      presented      in      Uehara      et      al.\\n(2024).\\nIntuitively,      as      we      see      in      Theorem      3,  \\n \\n  this      is      expected      because      we      see      that      the      posterior      distribution      induced      by      pre-trained      models:</p><p>ref   \\n \\n(      PD      (ar,-++      21   \\n \\n|      to)      (=      vey      (er|er_-1)p      Pp      (ar-1   \\n \\n|      er_2)      +++      pp\"      (x1   \\n \\n|      20), remains      preserved      after      fine-tuning      based      on      the      RL-formulation      (18).</p><p> \\n \\n  Comparison      with      flow-based      training.\\nIn      contrast      to      flow-based      training,      RL-based      fine-tuning  \\n \\n  minimizes      the      inverse      KL      divergence      rather      than      the      KL      divergence.\\nIntuitively,      this      is      because,  \\n \\n  unlike      Qt™*      or      Q*>   \\n \\n,      we      cannot      sample      from      Q*”      (recall      that   \\n \\na      marginal      distribution      at      time   \\n \\n0      in  \\n \\n  Qi,      i.e.,      p,,      is      unnormalized);      on      the      other      hand,      the      marginal      distributions      at      time   \\n \\n0      in      both      Q\\\\\\\\\\\\\\'™*  \\n \\n  and      Q*°°>      are      data      distributions.</p><p> \\n \\n  Remark   \\n \\n7      (Extension      to      f-divergence).\\nEach      of      these      methods      can      be      technically      extended      by  \\n \\n  incorporating      more      general      divergences      beyond      KL      divergence.\\nFor      instance,      in      the      context      of  \\n \\n  fine-tuning,      please      see      Tang      (2024).</p><p>10      Connection      with      Sampling      from      Unnormalized      Distributions</p><p> \\n \\n  Numerous      studies      delve      into      sampling      from      an      unnormalized      distribution      proportional      to      exp(r()),  \\n \\n  commonly      known      as      the      Gibbs      distribution,      extensively      discussed      in      the      computational      statistics  \\n \\n  and      statistical      physics      community      (Robert,      2014).\\nThis      issue      is      pertinent      to      our      work,      as      during  \\n \\n  fine-tuning,      the      target      distribution      is      also      formulated      in      the      Gibbs      distribution      form      in      (10),      which      is  \\n \\n  proportional      to      exp(r(a))p?\\n\"*      (x).</p><p> \\n \\n  The      literature      employs      two      main      approaches      to      tackle      this      challenge:      Markov      Chain      Monte  \\n \\n  Carlo      (MCMC)      and      RL-based      methods.\\nIn      particular,      the      RL-based      diffusion      model      fine-tuning  \\n \\n  we      have      discussed      so      far      is      closely      related      to      the      latter      approach.\\nIn      this      section,      we      explain      and  \\n \\n  compare      these      two      approaches.</p><h10>10.1      Markov      Chain      Monte      Carlo      (MCMC)</h10><p>CMC)  \\n \\n  In      MCMC,   \\n \\na      Markov      chain      is      constructed      to      approximate   \\n \\na      target      distribution      such      that      the      equilib-  \\n \\n  rium      distribution      of      the      Markov      chain      converges      to      the      target      distribution.\\nHowever,      dealing      with  \\n \\n  high-dimensional      domain      spaces      is      challenging      for      MCMC.\\n \\n \\nA      common      strategy      involves      leveraging  \\n \\n  gradient      information,      such      as      the      Metropolis-adjusted      Langevin      algorithm      (MALA)      (Besag,      1994)  \\n \\n  or      Hamiltonian      Monte      Carlo      (HMC)      (Neal      et      al.,      2011;      Girolami      and      Calderhead,      2011).\\nMALA  \\n \\n  shares      similarities      with      classifier      guidance      and      value-weighted      sampling,      as      both      methods      depend  \\n \\n  on      the      first-order      information      from      reward      (or      value)      functions.</p><p> \\n \\n  Can      we      use      MCMC      for      fine-tuning      diffusion      models?\\nIn      the      context      of      fine-tuning      dif-  \\n \\n  fusion      models,      while      it      may      seem      intuitive      to      sample      from      the      target      distribution      (10)      (i.e.,  \\n \\n  Dy   \\n \\n&      exp(r(x))pP\"*(x))      using      MCMC,      it’s      not      straightforward      for      the      following      reasons:</p><p>¢\\n \\n   We      lack      of      an      analytical      form      of      p?\\n\"*(-).<ul><li>*\\n \\n   Additionally,      even      if      we      can      estimate      p?\\n’°(-)      in      an      unbiased      manner,      as      in      Chen      et      al.\\n(2018),  \\n \\n  the      mixing      time      for      obtaining   \\n \\na      single      sample      in      MCMC      might      be      lengthy.</li></ul></p><p> \\n \\n  Therefore,      we      explore      creating   \\n \\na      generative      model      (i.e.,      diffusion      model)      for      this      task      using      RL-  \\n \\n  based      fine-tuning      so      that      we      can      easily      sample      from      p,(-)      during      inference      (i.e.,      simulating      the  \\n \\n  time-reversal      SDE      (2)      with      policies      from   \\n \\n¢   \\n \\n=      T\\\\\\\\\\\\\\'+   \\n \\n1      to   \\n \\nt   \\n \\n=      1),      without      relying      on      MCMC.\\nThis  \\n \\n  addresses      the      concern      of      MCMC      by      avoiding      the      need      to      estimate      p?\\n\"*(-)      and      minimizing      inference  \\n \\n  time,      albeit      at      the      expense      of      increased      training      time.</p><p>10.2}      RL-Based      Approaches  \\n \\n  While      MCMC      has      historically      been      widely      used      for      this      purpose,      recent      works      have      proposed      an  \\n \\n  RL-based      approach      or      its      variant      (e.g.,      Bernton      et      al.      (2019);      Heng      et      al.\\n(2020);      Huang      et      al.\\n(2023);</p><p> \\n \\n  Vargas      et      al.\\n(2023)).\\nFor      example,   \\n \\na      seminal      study      (Zhang      and      Chen,      2021)      introduced   \\n \\na      method  \\n \\n  very      similar      to      reward      backpropagation.\\nIn      their      work,      they      initially      define      Brownian      motion      as   \\n \\na       reference      SDE      (from      7’      to      0),      similar      to      the      pre-trained      diffusion      models      in      our      context.\\nThen,      by  \\n \\n  appropriately      setting      rewards,      Zhang      and      Chen      (2021)      aims      to      learn   \\n \\na      new      SDE      (from      T\\\\\\\\\\\\\\'      to      0)      such  \\n \\n  that      we      can      sample      from      the      target      distribution      at      the      endpoint      0.</p><p> \\n \\n  These      approaches      offer      advantages      over      MCMC      due      to      their      ability      to      minimize      inference      time  \\n \\n  (amortization)      by      constructing   \\n \\na      generative      model      comprising      policies      in      the      training      time      and      solely  \\n \\n  executing      learned      policies      during      inference.\\nUnlike      MCMC,      these      methods      significantly      reduce      the  \\n \\n  inference      cost      as      we      don’t      need      to      be      concerned      about      the      mixing      time.\\nAdditionally,      RL-based  \\n \\n  approaches      have      the      potential      to      effectively      handle      high-dimensional,      complex      (i.e.,      multi-modal)  \\n \\n  distributions      by      harnessing      the      considerable      expressive      power      arising      from      latent      states.</p><p> \\n \\n  Despite      these      advantages,      it      is      often      unclear      how      to      select   \\n \\na      reference      SDE      in      these      works.\\nIn  \\n \\n  contrast,      in      RL-based      fine-tuning      of      diffusion      models,      we      directly      leverage      pre-trained      diffusion  \\n \\n  models      as      the      reference      SDEs,      i.e.,      as      our      prior      knowledge.</p><p> \\n \\n  Key      message.\\nFinally,      it      is      worth      noting      that      we      can      technically      utilize      any      off-the-shelf      RL      algo-  \\n \\n  rithms      for      training.\\nFor      example,      Zhang      and      Chen      (2021)      employs      reward      backpropagation,      while  \\n \\n  works      in      Gflownets      typically      leverage      PCL,      as      we      mention      in      Section      6.3.\\nHowever,      additionally,      it  \\n \\n  is      possible      to      use      PPO      or      reward-weighted      MLE      mentioned      in      Section      4.2.</p><p>11      Closely      Related      Directions</p><p>Lastly,      we      highlight      several      closely      related      directions      we      have      not      yet      discussed.</p><p> \\n \\n  Aligning      text-to-image      models.\\nWe      recognize      that      current      efforts      in      fine-tuning      diffusion      models  \\n \\n  primarily      revolve      around      aligning      text-to-image      models      using      human      feedback      (Lee      et      al.,      2023;  \\n \\n  Wallace      et      al.,      2023;      Wu      et      al.,      2023;      Yang      et      al.,      2023).\\nIn      this      paper,      our      goal      is      to      consider   \\n \\na      more  \\n \\n  general      and      fundamental      formulation      that      is      not      tailored      to      any      particular      task.</p><p> \\n \\n  Diffusion      models      for      RL.\\nDiffusion      models      are      well-suited      for      specific      reinforcement      learning  \\n \\n  applications      due      to      their      ability      to      model      complex      and      multimodal      distributions      as      polices      (Janner  \\n \\n  et      al.,      2022;      Ajay      et      al.,      2023;      Wang      et      al.,      2022;      Hansen-Estruch      et      al.,      2023;      Du      et      al.,      2024;      Zhu  \\n \\n  et      al.,      2023).\\nFor      example,      Wang      et      al.\\n(2022)      use      conditional      diffusion      models      as      policies      and  \\n \\n  demonstrate      their      effectiveness      in      typical      offline      RL      benchmarks.\\nWhile      RL-based      fine-tuning      also  \\n \\n  aims      to      maximize      certain      reward      functions,      the      primary      focus      of      this      tutorial      covers      methods      that  \\n \\n  leverage      pre-trained      diffusion      models      for      this      purpose.</p><p> \\n \\n  RL      from      human      feedback      (RLHF)      for      language      models.\\nRLHF      is      widely      discussed      in      the  \\n \\n  context      of      language      models,      where      many      algorithms      share      similarities      with      those      used      in      diffusion  \\n \\n  models      (Ouyang      et      al.,      2022;      Bai      et      al.,      2022;      Casper      et      al.,      2023).\\nFor      instance,      both      domains  \\n \\n  commonly      employ      PPO      as   \\n \\na      standard      approach,      and      reward-weighted      training      (or      reward-weighted  \\n \\n  MLE)      often      serves      as   \\n \\na      basic      baseline      approach.\\nDespite      these      similarities,      nuanced      differences  \\n \\n  exist      mainly      because      diffusion      models      typically      operate      in   \\n \\na      continuous      input      space      rather      than  \\n \\n  a      discrete      one.\\nMoreover,      techniques      like      value-weighted      sampling      emerge      uniquely      within      the  \\n \\n  context      of      diffusion      models.</p><p>12      Summary</p><p> \\n \\n  In      this      article,      we      comprehensively      explain      how      fine-tuning      diffusion      models      to      maximize      down-  \\n \\n  stream      reward      functions      can      be      formalized      as   \\n \\na      reinforcement      learning      (RL)      problem      in      entropy-  \\n \\n  regularized      Markov      decision      processes      (MDPs).\\nBased      on      this      viewpoint,      we      elaborate      on      the  \\n \\n  application      of      various      RL      algorithms,      such      as      PPO      and      reward-weighted      MLE,      specifically      tailored  \\n \\n  for      fine-tuning      diffusion      models.\\nAdditionally,      we      categorize      different      scenarios      based      on      how  \\n \\n  reward      feedback      is      obtained.\\nWhile      this      categorization      is      not      always      explicitly      mentioned      in      many  \\n \\n  existing      works,      it      is      crucial      when      selecting      appropriate      algorithms.\\nFinally,      we      discuss      the      rela-  \\n \\n  tionships      with      several      related      topics,      including      classifier      guidance,      Gflownets,      path      integral      control  \\n \\n  theory,      and      sampling      from      unnormalized      distributions.</p><h4>References</h4><p>Agarwal,      A.,      N.      Jiang,      S.      M.      Kakade,      and      W.      Sun      (2019).\\nReinforcement      learning:      Theory      and  \\n \\n  algorithms.\\nCS      Dept.,      UW      Seattle,      Seattle,      WA,      USA,      Tech.\\nRep,      10-4.</p><p>Agarwal,      V.      and      D.      R.      Kelley      (2022).\\nThe      genetic      and      biochemical      determinants      of      mrna      degradation  \\n \\n  rates      in      mammals.\\nGenome      biology      23(1),      245.</p><p> \\n \\n  Ajay,      A.,      Y.      Du,      A.      Gupta,      J.      B.      Tenenbaum,      T.      S.      Jaakkola,      and      P.      Agrawal      (2023).\\nIs      conditional  \\n \\n  generative      modeling      all      you      need      for      decision      making?\\nIn      The      Eleventh      International      Conference  \\n \\n  on      Learning      Representations.</p><p>Albergo,      M.      S.      and      E.      Vanden-Eijnden      (2022).\\nBuilding      normalizing      flows      with      stochastic      inter-  \\n \\n  polants.\\narXiv      preprint      arXiv:2209.15571.</p><p>Anderson,      B.      D.      (1982).\\nReverse-time      diffusion      equation      models.\\nStochastic      Processes      and      their  \\n \\n  Applications      12(3),      313-326.</p><p> \\n \\n  Austin,      J.,      D.      D.      Johnson,      J.      Ho,      D.      Tarlow,      and      R.      Van      Den      Berg      (2021).\\nStructured      denoising  \\n \\n  diffusion      models      in      discrete      state-spaces.\\nAdvances      in      Neural      Information      Processing      Systems      34,  \\n \\n  17981-17993.</p><p>Avdeyey,      P.,      C.      Shi,      Y.      Tan,      K.      Dudnyk,      and      J.      Zhou      (2023).\\nDirichlet      diffusion      score      model      for  \\n \\n  biological      sequence      generation.\\narXiv      preprint      arXiv:2305.\\n10699.</p><p>Bai,      Y.,      S.      Kadavath,      S.      Kundu,      A.      Askell,      J.      Kernion,      A.      Jones,      A.      Chen,      A.      Goldie,      A.      Mirhoseini, C.      McKinnon,      et      al.\\n(2022).\\nConstitutional      ai:      Harmlessness      from      ai      feedback.\\narXiv      preprint  \\n \\n  arXiv:2212.08073.</p><p> \\n \\n  Bansal,      A.,      H.-M.\\nChu,      A.      Schwarzschild,      S.      Sengupta,      M.      Goldblum,      J.      Geiping,      and      T.      Goldstein  \\n \\n  (2023).\\nUniversal      guidance      for      diffusion      models.\\nIn      Proceedings      of      the      IEEE/CVF      Conference  \\n \\n  on      Computer      Vision      and      Pattern      Recognition,      pp.\\n843-852.</p><p>Bengio,      Y.,      S.      Lahlou,      T.      Deleu,      E.      J.      Hu,      M.      Tiwari,      and      E.      Bengio      (2023).\\nGflownet      foundations.\\n \\n \\n  Journal      of      Machine      Learning      Research      24(210),      1-55.</p><p> \\n \\n  Benton,      J.,      Y.      Shi,      V.      De      Bortoli,      G.      Deligiannidis,      and      A.      Doucet      (2024).\\nFrom      denoising      diffusions  \\n \\n  to      denoising      markov      models.\\nJournal      of      the      Royal      Statistical      Society      Series      B:      Statistical  \\n \\n  Methodology      86(2),      286-301.</p><p>Bernton,      E.,      J.      Heng,      A.      Doucet,      and      P.      E.      Jacob      (2019).\\nSchr\\\\\\\\\\\\\\\\””      odinger      bridge      samplers.\\narXiv  \\n \\n  preprint      arXiv:      1912.13170.</p><p>Besag,      J.\\n(1994).\\nComments      on      “representations      of      knowledge      in      complex      systems”      by      u.      grenander  \\n \\n  and      mi      miller.\\nJ.      Roy.\\nStatist.\\nSoc.      Ser.\\n \\n \\nB      56(591-592),      4.</p><p>Black,      K.,      M.      Janner,      Y.      Du,      I.      Kostrikov,      and      S.      Levine      (2023).\\nTraining      diffusion      models      with  \\n \\n  reinforcement      learning.\\narXiv      preprint      arXiv:2305.13301.</p><p> \\n \\n  Campbell,      A.,      J.      Benton,      V.      De      Bortoli,      T.      Rainforth,      G.      Deligiannidis,      and      A.      Doucet      (2022).\\n \\n \\nA       continuous      time      framework      for      discrete      denoising      models.\\nAdvances      in      Neural      Information  \\n \\n  Processing      Systems      35,      28266-28279.</p><p> \\n \\n  Campbell,      A.,      J.      Yim,      R.      Barzilay,      T.      Rainforth,      and      T.      Jaakkola      (2024).\\nGenerative      flows      on  \\n \\n  discrete      state-spaces:      Enabling      multimodal      flows      with      applications      to      protein      co-design.\\narXiv  \\n \\n  preprint      arXiv:2402.04997.</p><p>Cao,      H.,      C.      Tan,      Z.      Gao,      Y.      Xu,      G.      Chen,      P.-A.\\nHeng,      and      S.      Z.      Li      (2024).\\n \\n \\nA      survey      on      generative  \\n \\n  diffusion      models.\\n[EEE      Transactions      on      Knowledge      and      Data      Engineering.</p><p>Casper,      S.,      X.      Davies,      C.      Shi,      T.      K.      Gilbert,      J.      Scheurer,      J.      Rando,      R.      Freedman,      T.      Korbak, D.      Lindner,   \\n \\nP P.      Freire,      et      al.\\n(2023).\\nOpen      problems      and      fundamental      limitations      of      reinforcement  \\n \\n  learning      from      human      feedback.\\narXiv      preprint      arXiv:2307.15217.</p><p>Castillo-Hair,      S.      M.      and      G.      Seelig      (2021).\\nMachine      learning      for      designing      next-generation      mrna  \\n \\n  therapeutics.\\nAccounts      of      Chemical      Research      55(1),      24-34.</p><p>Chang,      J.      D.,      M.      Uehara,      D.      Sreenivas,      R.      Kidambi,      and      W.      Sun      (2021).\\nMitigating      covariate      shift  \\n \\n  in      imitation      learning      via      offline      data      without      great      coverage.\\narXiv      preprint      arXiv:2106.03207.</p><p>Chen,      M.,      S.      Mei,      J.      Fan,      and      M.      Wang      (2024).\\nAn      overview      of      diffusion      models:      Applications,  \\n \\n  guided      generation,      statistical      rates      and      optimization.\\narXiv      preprint      arXiv:2404.07771.</p><p>Chen,      R.      T.,      Y.      Rubanova,      J.      Bettencourt,      and      D.      K.      Duvenaud      (2018).\\nNeural      ordinary      differential  \\n \\n  equations.\\nAdvances      in      neural      information      processing      systems      31.</p><p>Chung,      H.,      J.      Kim,      M.      T.      Mccann,      M.      L.      Klasky,      and      J.      C.      Ye      (2022).\\nDiffusion      posterior      sampling  \\n \\n  for      general      noisy      inverse      problems.\\narXiv      preprint      arXiv:2209.\\n14687.</p><p> \\n \\n  Chung,      H.,      B.      Sim,      D.      Ryu,      and      J.      C.      Ye      (2022).\\nImproving      diffusion      models      for      inverse      problems  \\n \\n  using      manifold      constraints.\\nAdvances      in      Neural      Information      Processing      Systems      35,      25683-—  \\n \\n  25696.</p><p>Clark,      K.,      P.      Vicol,      K.      Swersky,      and      D.      J.      Fleet      (2023).\\nDirectly      fine-tuning      diffusion      models      on  \\n \\n  differentiable      rewards.\\narXiv      preprint      arXiv:2309.\\n17400.</p><p> \\n \\n  De      Bortoli,      V.,      E.      Mathieu,      M.      Hutchinson,      J.      Thornton,      Y.      W.      Teh,      and      A.      Doucet      (2022).\\nRieman-  \\n \\n  nian      score-based      generative      modelling.\\nAdvances      in      Neural      Information      Processing      Systems      35,  \\n \\n  2406-2422.</p><p>Deleu,      T.,      P.      Nouri,      N.      Malkin,      D.      Precup,      and      Y.      Bengio      (2024).\\nDiscrete      probabilistic      inference      as  \\n \\n  control      in      multi-path      environments.\\narXiv      preprint      arXiv:2402.10309.</p><p>Dhariwal,      P.      and      A.      Nichol      (2021).\\nDiffusion      models      beat      gans      on      image      synthesis.\\nAdvances      in  \\n \\n  neural      information      processing      systems      34,      8780-8794.</p><p> \\n \\n  Du,      Y.,      S.      Yang,      B.      Dai,      H.      Dai,      O.      Nachum,      J.      Tenenbaum,      D.      Schuurmans,      and      P.      Abbeel      (2024).\\n \\n \\n  Learning      universal      policies      via      text-guided      video      generation.\\nAdvances      in      Neural      Information  \\n \\n  Processing      Systems      36.</p><p>Fan,      Y.,      O.      Watkins,      Y.      Du,      H.      Liu,      M.      Ryu,      C.      Boutilier,      P.      Abbeel,      M.      Ghavamzadeh,      K.      Lee,      and K.      Lee      (2023) 023).\\nDPOK:      Reinforcement      learning      for      fine-tuning      text-to-image      diffusion      models.\\n \\n \\n  arXiv      preprint      arXiv:2305.16381.</p><p>Finn,      C.,      S.      Levine,      and      P.      Abbeel      (2016).\\nGuided      cost      learning:      Deep      inverse      optimal      control      via  \\n \\n  policy      optimization.\\nIn      International      conference      on      machine      learning,      pp.\\n49-58.\\nPMLR.</p><p>Fox,      R.,      A.      Pakman,      and      N.      Tishby      (2015).\\nTaming      the      noise      in      reinforcement      learning      via      soft  \\n \\n  updates.\\narXiv      preprint      arXiv:      1512.08562.</p><p>Frey,      N.      C.,      D.      Berenberg,      K.      Zadorozhny,      J.      Kleinhenz,      J.      Lafrance-Vanasse,      I.      Hotzel,      Y.      Wu, S.      Ra,   \\n \\nR R.      Bonneau,      K.      Cho,      et      al.\\n(2023).\\nProtein      discovery      with      discrete      walk-jump      sampling.\\n \\n \\n  arXiv      preprint      arXiv:2306.\\n12360.</p><p>Geist,      M.,      B.      Scherrer,      and      O.      Pietquin      (2019).\\n \\n \\nA      theory      of      regularized      markov      decision      processes.\\n \\n \\n  In      International      Conference      on      Machine      Learning,      pp.\\n2160-2169.\\nPMLR.</p><p> \\n \\n  Girolami,      M.      and      B.      Calderhead      (2011).\\nRiemann      manifold      langevin      and      hamiltonian      monte  \\n \\n  carlo      methods.\\nJournal      of      the      Royal      Statistical      Society      Series      B:      Statistical      Methodology      73(2),  \\n \\n  123-214.</p><p>Gosai,      S.      J.,      R.      I.      Castro,      N.      Fuentes,      J.      C.      Butts,      S.      Kales,      R.      R.      Noche,      K.      Mouri,      P.      C.      Sabeti, S.   \\n \\nK K.      Reilly,      and      R.      Tewhey      (2023).\\nMachine-guided      design      of      synthetic      cell      type-specific  \\n \\n  cis-regulatory      elements.\\nbioRxiv.</p><p>Haarnoja,      T.,      H.      Tang,      P.      Abbeel,      and      S.      Levine      (2017).\\nReinforcement      learning      with      deep      energy-  \\n \\n  based      policies.\\nIn      International      conference      on      machine      learning,      pp.\\n1352-1361.\\nPMLR.</p><p>Hansen-Estruch,      P.,      I.      Kostrikov,      M.      Janner,      J.      G.      Kuba,      and      S.      Levine      (2023).\\nIdql:      Implicit  \\n \\n  q-learning      as      an      actor-critic      method      with      diffusion      policies.\\narXiv      preprint      arXiv:2304.10573.</p><p>Heng,      J.,      A.      N.      Bishop,      G.      Deligiannidis,      and      A.      Doucet      (2020).\\nControlled      sequential      monte      carlo.</p><p>Ho,      J.,      A.      Jain,      and      P.      Abbeel      (2020).\\nDenoising      diffusion      probabilistic      models.\\nAdvances      in      neural  \\n \\n  information      processing      systems      33,      6840-6851.</p><p>Ho,      J.      and      T.      Salimans      (2022).\\nClassifier-free      diffusion      guidance.\\narXiv      preprint      arXiv:2207.12598.</p><p>Ho,      J.,      T.      Salimans,      A.      Gritsenko,      W.      Chan,      M.      Norouzi,      and      D.      J.      Fleet      (2022).\\nVideo      diffusion  \\n \\n  models.\\nAdvances      in      Neural      Information      Processing      Systems      35,      8633-8646.</p><p> \\n \\n  Hoogeboom,      E.,      V.      G.      Satorras,      C.      Vignac,      and      M.      Welling      (2022).\\nEquivariant      diffusion      for  \\n \\n  molecule      generation      in      3d.\\nIn      International      conference      on      machine      learning,      pp.\\n8867-8887.</p><h4>      PMLR.</h4><p>Huang,      X.,      H.      Dong,      H.      Yifan,      Y.      Ma,      and      T.      Zhang      (2023).\\nReverse      diffusion      monte      carlo.\\nIn      The  \\n \\n  Twelfth      International      Conference      on      Learning      Representations.</p><p>Janner,      M.,      Y.      Du,      J.      B.      Tenenbaum,      and      S.      Levine      (2022).\\nPlanning      with      diffusion      for      flexible  \\n \\n  behavior      synthesis.\\narXiv      preprint      arXiv:2205.09991.</p><p> \\n \\n  Jo,      J.,      S.      Lee,      and      S.      J.      Hwang      (2022).\\nScore-based      generative      modeling      of      graphs      via      the      system  \\n \\n  of      stochastic      differential      equations.\\nIn      International      Conference      on      Machine      Learning,      pp.\\n \\n \\n  10362-10383.\\nPMLR.</p><p> \\n \\n  Kazim,      M.,      J.      Hong,      M.-G.\\nKim,      and      K.-K.\\nK.      Kim      (2024).\\nRecent      advances      in      path      integral      control  \\n \\n  for      trajectory      optimization:      An      overview      in      theoretical      and      algorithmic      perspectives.\\nAnnual  \\n \\n  Reviews      in      Control      57,      100931.</p><p>Krishnamoorthy,      S.,      S.      M.      Mashkaria,      and      A.      Grover      (2023).\\nDiffusion      models      for      black-box  \\n \\n  optimization.\\narXiv      preprint      arXiv:2306.07180.</p><p>Kumar,      A.,      A.      Zhou,      G.      Tucker,      and      S.      Levine      (2020).\\nConservative      q-learning      for      offline      reinforce-  \\n \\n  ment      learning.\\nAdvances      in      Neural      Information      Processing      Systems      33,      1179-1191.</p><p>Lal,      A.,      D.      Garfield,      T.      Biancalani,      and      G.      Eraslan      (2024).\\nreglm:      Designing      realistic      regulatory  \\n \\n  dna      with      autoregressive      language      models.\\nbioRxiv,      2024-02.</p><p>Lee,      K.,      H.      Liu,      M.      Ryu,      O.      Watkins,      Y.      Du,      C.      Boutilier,      P.      Abbeel,      M.      Ghavamzadeh,      and      S.      S.      Gu  \\n \\n  (2023).\\nAligning      text-to-image      models      using      human      feedback.\\narXiv      preprint      arXiv:2302.12192.</p><p>Levine,      S.      (2018).\\nReinforcement      learning      and      control      as      probabilistic      inference:      Tutorial      and  \\n \\n  review.\\narXiv      preprint      arXiv:      1805.00909.</p><p>Li,      Z.,      Y.      Ni,      T.      A.      B.      Huygelen,      A.      Das,      G.      Xia,      G.-B.\\nStan,      and      Y.      Zhao      (2023).\\nLatent      diffusion  \\n \\n  model      for      dna      sequence      generation.\\narXiv      preprint      arXiv:2310.06150.</p><p>Lipman,      Y.,      R.      T.      Chen,      H.      Ben-Hamu,      M.      Nickel,      and      M.      Le      (2023).\\nFlow      matching      for      generative  \\n \\n  modeling.\\nJCLR      2023.</p><p>Liu,      X.,      C.      Gong,      and      Q.      Liu      (2022).\\nFlow      straight      and      fast:      Learning      to      generate      and      transfer      data  \\n \\n  with      rectified      flow.\\narXiv      preprint      arXiv:2209.03003.</p><p>Liu,      X.,      L.      Wu,      M.      Ye,      and      Q.      Liu      (2022).\\nLet      us      build      bridges:      Understanding      and      extending  \\n \\n  diffusion      generative      models.\\narXiv      preprint      arXiv:2208.14699.</p><p>Lou,      A.,      C.      Meng,      and      S.      Ermon      (2023).\\nDiscrete      diffusion      language      modeling      by      estimating      the  \\n \\n  ratios      of      the      data      distribution.\\narXiv      preprint      arXiv:2310.16834.</p><p>Malkin,      N.,      M.      Jain,      E.      Bengio,      C.      Sun,      and      Y.      Bengio      (2022).\\nTrajectory      balance:      Improved      credit  \\n \\n  assignment      in      gflownets.\\nAdvances      in      Neural      Information      Processing      Systems      35,      5955-5967.</p><p>Mohammadpour,      S.,      E.      Bengio,      E.      Frejinger,      and      P.-L.\\nBacon      (2023).\\nMaximum      entropy      gflownets  \\n \\n  with      soft      q-learning.\\narXiv      preprint      arXiv:2312.14331.</p><p>Nachum,      O.,      M.      Norouzi,      K.      Xu,      and      D.      Schuurmans      (2017).\\nBridging      the      gap      between      value      and  \\n \\n  policy      based      reinforcement      learning.\\nAdvances      in      neural      information      processing      systems      30.</p><p>Neal,      R.      M.      et      al.\\n(2011).\\nMcmce      using      hamiltonian      dynamics.\\nHandbook      of      markov      chain      monte  \\n \\n  carlo      2(11),      2.</p><p>Neu,      G.,      A.      Jonsson,      and      V.      Gémez      (2017).\\n \\n \\nA      unified      view      of      entropy-regularized      markov      decision  \\n \\n  processes.\\narXiv      preprint      arXiv:      1705.07798.</p><p>Ouyang,      L.,      J.      Wu,      X.      Jiang,      D.      Almeida,      C.      Wainwright,      P.      Mishkin,      C.      Zhang,      S.      Agarwal, K.      Slama,      A A.      Ray,      et      al.\\n(2022).\\nTraining      language      models      to      follow      instructions      with      human  \\n \\n  feedback.\\nAdvances      in      Neural      Information      Processing      Systems      35,      27730-27744.</p><p>Pattanaik,      L.      and      C.      W.      Coley      (2020).\\nMolecular      representation:      going      long      on      fingerprints.\\n \\n \\n  Chem      6(6),      1204-1207.</p><p>Peng,      X.      B.,      A.      Kumar,      G.      Zhang,      and      S.      Levine      (2019).\\nAdvantage-weighted      regression:      Simple  \\n \\n  and      scalable      off-policy      reinforcement      learning.\\narXiv      preprint      arXiv:1910.00177.</p><p>Peters,      J.,      K.      Mulling,      and      Y.      Altun      (2010).\\nRelative      entropy      policy      search.\\nIn      Proceedings      of      the  \\n \\n  AAAI      Conference      on      Artificial      Intelligence,      Volume      24,      pp.\\n1607-1612.</p><p> \\n \\n  Podell,      D.,      Z.      English,      K.      Lacey,      A.      Blattmann,      T.      Dockhorn,      J.      Miiller,      J.      Penna,      and      R.      Rombach  \\n \\n  (2023).\\nSdxl:      Improving      latent      diffusion      models      for      high-resolution      image      synthesis.\\narXiv  \\n \\n  preprint      arXiv:2307.01952.</p><p>Prabhudesai,      M.,      A.      Goyal,      D.      Pathak,      and      K.      Fragkiadaki      (2023).\\nAligning      text-to-image      diffusion  \\n \\n  models      with      reward      backpropagation.\\narXiv      preprint      arXiv:2310.03739.</p><p>Ramesh,      A.,      P.      Dhariwal,      A.      Nichol,      C.      Chu,      and      M.      Chen      (2022).\\nHierarchical      text-conditional  \\n \\n  image      generation      with      clip      latents.\\narXiv      preprint      arXiv:2204.06125      1(2),      3.</p><p>Rigter,      M.,      B.      Lacerda,      and      N.      Hawes      (2022).\\nRambo-rl:      Robust      adversarial      model-based      offline  \\n \\n  reinforcement      learning.\\nAdvances      in      neural      information      processing      systems      35,      16082-16097.</p><p>Robert,      C.      (2014).\\nStatistical      modeling      and      computation.</p><p>Rogers,      L.      C.      G.      and      D.      Williams      (2000).\\nDiffusions,      Markov      processes      and      martingales:      Volume      2,  \\n \\n  It6é      calculus,      Volume      2.\\nCambridge      university      press.</p><p> \\n \\n  Rombach,      R.,      A.      Blattmann,      D.      Lorenz,      P.      Esser,      and      B.      Ommer      (2022,      June).\\nHigh-resolution  \\n \\n  image      synthesis      with      latent      diffusion      models.\\nIn      Proceedings      of      the      IEEE/CVF      Conference      on  \\n \\n  Computer      Vision      and      Pattern      Recognition      (CVPR),      pp.\\n10684-10695.</p><p>Sarkar,      A.,      Z.      Tang,      C.      Zhao,      and      P.      Koo      (2024).\\nDesigning      dna      with      tunable      regulatory      activity  \\n \\n  using      discrete      diffusion.\\nbioRxiv,      2024-05.</p><p>Schulman,      J.,      X.      Chen,      and      P.      Abbeel      (2017).\\nEquivalence      between      policy      gradients      and      soft  \\n \\n  q-learning.\\narXiv      preprint      arXiv:      1704.06440.</p><p>Schulman,      J.,      S.      Levine,      P.      Abbeel,      M.      Jordan,      and      P.      Moritz      (2015).\\nTrust      region      policy      optimization.\\n \\n \\n  In      International      conference      on      machine      learning,      pp.\\n1889-1897.\\nPMLR.</p><p>Schulman,      J.,      F.      Wolski,      P.      Dhariwal,      A.      Radford,      and      O.      Klimov      (2017).\\nProximal      policy      optimiza-  \\n \\n  tion      algorithms.\\narXiv      preprint      arXiv:      1707.06347.</p><p>Shi,      Y.,      V.      De      Bortoli,      A.      Campbell,      and      A.      Doucet      (2024).\\nDiffusion      schrédinger      bridge      matching.\\n \\n \\n  Advances      in      Neural      Information      Processing      Systems      36.</p><p> \\n \\n  Sohl-Dickstein,      J.,      E.      Weiss,      N.      Maheswaranathan,      and      S.      Ganguli      (2015).\\nDeep      unsupervised  \\n \\n  learning      using      nonequilibrium      thermodynamics.\\nIn      /nternational      conference      on      machine      learning,  \\n \\n  pp.\\n2256-2265.\\nPMLR.</p><p>Song,      J.,      C.      Meng,      and      S.      Ermon      (2020).\\nDenoising      diffusion      implicit      models.\\narXiv      preprint  \\n \\n  arXiv:2010.02502.</p><p>Song,      Y.,      C.      Durkan,      I.      Murray,      and      S.      Ermon      (2021).\\nMaximum      likelihood      training      of      score-based  \\n \\n  diffusion      models.\\nAdvances      in      neural      information      processing      systems      34,      1415-1428.</p><p>Stark,      H.,      B.      Jing,      C.      Wang,      G.      Corso,      B.      Berger,      R.      Barzilay,      and      T.      Jaakkola      (2024).\\nDirichlet  \\n \\n  flow      matching      with      applications      to      dna      sequence      design.\\narXiv      preprint      arXiv:2402.05841.</p><p>Sutton,      R.      S.      and      A.      G.      Barto      (2018).\\nReinforcement      learning:      An      introduction.\\nMIT      press.</p><p>Tang,      W.      (2024).\\nFine-tuning      of      diffusion      models      via      stochastic      control:      entropy      regularization      and  \\n \\n  beyond.\\narXiv      preprint      arXiv:2403.06279.</p><p>Tang,      W.      and      H.      Zhao      (2024).\\nScore-based      diffusion      models      via      stochastic      differential      equations—a  \\n \\n  technical      tutorial.\\narXiv      preprint      arXiv:2402.07487.</p><p>Theodorou,      E.,      J.      Buchli,      and      S.      Schaal      (2010).\\n \\n \\nA      generalized      path      integral      control      approach      to  \\n \\n  reinforcement      learning.\\nThe      Journal      of      Machine      Learning      Research      11,      3137-3181.</p><p>Tiapkin,      D.,      N.      Morozov,      A.      Naumov,      and      D.      Vetrov      (2023).\\nGenerative      flow      networks      as      entropy-  \\n \\n  regularized      rl.\\narXiv      preprint      arXiv:2310.12934.</p><p> \\n \\n  Tong,      A.,      N.      Malkin,      G.      Huguet,      Y.      Zhang,      J.      Rector-Brooks,      K.      Fatras,      G.      Wolf,      and      Y.      Bengio  \\n \\n  (2023).\\nConditional      flow      matching:      Simulation-free      dynamic      optimal      transport.\\narXiv      preprint  \\n \\n  arXiv:2302.00482.</p><p>Uehara,      M.      and      W.      Sun      (2021).\\nPessimistic      model-based      offline      reinforcement      learning      under  \\n \\n  partial      coverage.\\narXiv      preprint      arXiv:2107.06226.</p><p>Uehara,      M.,      Y.      Zhao,      K.      Black,      E.      Hajiramezanali,      G.      Scalia,      N.      L.      Diamant,      A.      M.      Tseng, T.      Biancalani,      and      S S.      Levine      (2024).\\nFine-tuning      of      continuous-time      diffusion      models      as      entropy-  \\n \\n  regularized      control.\\narXiv      preprint      arXiv:2402.15194.</p><p> \\n \\n  Uehara,      M.,      Y.      Zhao,      K.      Black,      E.      Hajiramezanali,      G.      Scalia,      N.      L.      Diamant,      A.      M.      Tseng,      S.      Levine,  \\n \\n  and      T.      Biancalani      (2024).\\nFeedback      efficient      online      fine-tuning      of      diffusion      models.\\narXiv      preprint  \\n \\n  arXiv:2402.\\n16359.</p><p> \\n \\n  Uehara,      M.,      Y.      Zhao,      E.      Hajiramezanali,      G.      Scalia,      G.      Eraslan,      A.      Lal,      S.      Levine,      and      T.      Biancalani  \\n \\n  (2024).\\nBridging      model-based      optimization      and      generative      modeling      via      conservative      fine-tuning  \\n \\n  of      diffusion      models.\\narXiv      preprint      arXiv:2405.19673.</p><p>Vargas,      F.,      W.      Grathwohl,      and      A.      Doucet      (2023).\\nDenoising      diffusion      samplers.\\narXiv      preprint  \\n \\n  arXiv:2302.\\n13834.</p><p> \\n \\n  Wallace,      B.,      M.      Dang,      R.      Rafailov,      L.      Zhou,      A.      Lou,      S.      Purushwalkam,      S.      Ermon,      C.      Xiong,      S.      Joty,  \\n \\n  and      N.      Naik      (2023).\\nDiffusion      model      alignment      using      direct      preference      optimization.\\narXiv  \\n \\n  preprint      arXiv:2311.12908.</p><p>Wang,      Z.,      J.      J.      Hunt,      and      M.      Zhou      (2022).\\nDiffusion      policies      as      an      expressive      policy      class      for      offline  \\n \\n  reinforcement      learning.\\narXiv      preprint      arXiv:2208.06193.</p><p>Widatalla,      T.,      R.      Rafailov,      and      B.      Hie      (2024).\\nAligning      protein      generative      models      with      experimental  \\n \\n  fitness      via      direct      preference      optimization.\\nbioRxiv,      2024-05.</p><p>Williams,      G.,      A.      Aldrich,      and      E.      A.      Theodorou      (2017).\\nModel      predictive      path      integral      control:      From  \\n \\n  theory      to      parallel      computation.\\nJournal      of      Guidance,      Control,      and      Dynamics      40(2),      344-357.</p><p>Wu,      X.,      K.      Sun,      F.      Zhu,      R.      Zhao,      and      H.      Li      (2023).\\nBetter      aligning      text-to-image      models      with  \\n \\n  human      preference.\\narXiv      preprint      arXiv:2303.14420.</p><p>Wu,      Y.,      G.      Tucker,      and      O.      Nachum      (2019).\\nBehavior      regularized      offline      reinforcement      learning.\\n \\n \\n  arXiv      preprint      arXiv:      1911.11361.</p><p>Wulfmeier,      M.,      P.      Ondruska,      and      I.      Posner      (2015).\\nMaximum      entropy      deep      inverse      reinforcement  \\n \\n  learning.\\narXiv      preprint      arXiv:      1507.04888.</p><p>Xie,      T.,      C.-A.\\nCheng,      N.      Jiang,      P.      Mineiro,      and      A.      Agarwal      (2021).\\nBellman-consistent      pessimism  \\n \\n  for      offline      reinforcement      learning.\\nAdvances      in      neural      information      processing      systems      34.</p><p>Xu,      M.,      L.      Yu,      Y.      Song,      C.      Shi,      S.      Ermon,      and      J.      Tang      (2022).\\nGeodiff:   \\n \\nA      geometric      diffusion  \\n \\n  model      for      molecular      conformation      generation.\\narXiv      preprint      arXiv:2203.02923.</p><p> \\n \\n  Yang,      L.,      Z.      Zhang,      Y.      Song,      S.      Hong,      R.      Xu,      Y.      Zhao,      W.      Zhang,      B.      Cui,      and      M.-H.\\nYang      (2023).\\n \\n \\n  Diffusion      models:   \\n \\nA      comprehensive      survey      of      methods      and      applications.\\nACM      Computing  \\n \\n  Surveys      56(4),      1-39.</p><p> \\n \\n  Yu,      T.,      G.      Thomas,      L.      Yu,      S.      Ermon,      J.      Y.      Zou,      S.      Levine,      C.      Finn,      and      T.      Ma      (2020).\\nMopo:  \\n \\n  Model-based      offline      policy      optimization.\\nAdvances      in      Neural      Information      Processing      Systems      33,  \\n \\n  14129-14142.</p><p> \\n \\n  Yuan,      H.,      K.      Huang,      C.      Ni,      M.      Chen,      and      M.      Wang      (2023).\\nReward-directed      conditional      diffusion:  \\n \\n  Provable      distribution      estimation      and      reward      improvement.\\nIn      Thirty-seventh      Conference      on  \\n \\n  Neural      Information      Processing      Systems.</p><p>Zhang,      H.      and      T.      Xu      (2023).\\nTowards      controllable      diffusion      models      via      reward-guided      exploration.\\n \\n \\n  arXiv      preprint      arXiv:2304.07132.</p><p>Zhang,      Q.      and      Y.      Chen      (2021).\\nPath      integral      sampler:   \\n \\na      stochastic      control      approach      for      sampling.\\n \\n \\n  arXiv      preprint      arXiv:2111,15141.</p><p> \\n \\n  Zhao,      Y.,      M.      Uehara,      G.      Scalia,      T.      Biancalani,      S.      Levine,      and      E.      Hajiramezanali      (2024).\\n \\n \\n  Adding      conditional      control      to      diffusion      models      with      reinforcement      learning.\\narXiv      preprint  \\n \\n  arXiv:2406.\\n12120.</p><p>Zhou,      Z.,      S.      Kearnes,      L.      Li,      R.      N.      Zare,      and      P.      Riley      (2019).\\nOptimization      of      molecules      via      deep  \\n \\n  reinforcement      learning.\\nScientific      reports      9(1),      10752.</p><p>Zhu,      Z.,      H.      Zhao,      H.      He,      Y.      Zhong,      S.      Zhang,      Y.      Yu,      and      W.      Zhang      (2023).\\nDiffusion      models      for  \\n \\n  reinforcement      learning:   \\n \\nA      survey.\\narXiv      preprint      arXiv:2311.01223.</p><p>Ziebart,      B.      D.,      A.      L.      Maas,      J.      A.      Bagnell,      A.      K.      Dey,      et      al.\\n(2008).\\nMaximum      entropy      inverse  \\n \\n  reinforcement      learning.\\nIn      Aaai,      Volume      8,      pp.\\n1433-1438.\\nChicago,      IL,      USA.</p><h3>Hence,</h3><p> \\n \\n \\n_   \\n \\n:      (xt)      Up-1(Lt-1)   \\n \\n°       vu   \\n \\n=      argmin      Ey,.u,   \\n \\n|   \\n \\n4      ——   \\n \\n—      log   \\n \\n|      exp   \\n \\n|      ————   \\n \\n]      Dpre(@e-1]      21)      da4_1  \\n \\n  a   \\n \\na h:X->R  \\n \\n  where      u,   \\n \\n€      A(%)      is      any      roll-in      distribution      that      covers      the      entire      space      V.      Using      this      relation      and  \\n \\n  replacing      the      expectation      E,,,.,,,      with      empirical      approximation,      we      are      able      to      estimate      soft      value  \\n \\n  functions      v;      in   \\n \\na      recursive      manner:</p><p> \\n \\n \\n1   \\n \\nm      ~(j—1)   \\n \\n°       aj   \\n \\n;   \\n \\ni      Ur      Ut   \\n \\ny       {6      (x)},   \\n \\n—       argmin   \\n \\ny      S      {ht   \\n \\n)   \\n \\n—      log   \\n \\n/      exp      (eee)      Ppre(@1—1|}      yan] h:      [R¢,[0,7]]|-R      t=T+1      i=1</p><p>where      the      data      is      collected      in      an      off-policy      way.</p><p> \\n \\n  Remark      6.\\nAlthough      soft      Q-learning      is      widely      used      in      standard      RL      (Schulman      et      al.,      2017),      it  \\n \\n  cannot      be      directly      applied      to      our      fine-tuning      context      without      resorting      to      value-weighted      sampling  \\n \\n  or      value-weighted      MLE.\\nThis      is      because,      even      if      we      estimate      soft-value      functions      as      %;,      substituting  \\n \\n  Uz      with      0,      in      the      soft-optimal      policy      results      in      an      unnormalized      policy.<ul><li>6.2.2.\\nApproximation      using      Tweedie’s      formula</li></ul></p><p> \\n \\n  So      far,      we      have      explained      two      approaches:   \\n \\na      Monte      Carlo      approach      and   \\n \\na      value      iteration      approach      to  \\n \\n  estimate      soft      value      functions.\\nHowever,      learning      value      functions      in      (28)      can      still      be      often      challenging  \\n \\n  in      practice.\\nTherefore,      we      can      employ      approximation      strategies      inspired      by      recent      literature      on  \\n \\n  classifier      guidance      (e.g.,      reconstruction      guidance      (Ho      et      al.,      2022),      manifold      constrained      gradients  \\n \\n  (Chung      et      al.,      2022),      universal      guidance      (Bansal      et      al.,      2023),      and      diffusion      posterior      sampling  \\n \\n  (Chung      et      al.,      2022)).</p><p> \\n \\n  Specifically,      we      adopt      the      following      approximation:</p><p>ur(xz)   \\n \\n=      alog      Egpry      lex      (“)      oy   \\n \\n=      alog      (/      exp      (om)      p      (colar      (30)</p><p>~\\n \\n   alog      (exw      (ceo)   \\n \\n)   \\n \\n»       £o(t1)   \\n \\n=      Egprey|xo   \\n \\n|      xe],      (31) a\\n \\n    =\\n \\n   1r(Xo(2t)).</p><p> \\n \\n  Here,      we      replace      the      integration      in      (30)      with   \\n \\na      Dirac      delta      distribution      with      the      posterior      mean.\\n \\n \\n  Importantly,      we      can      calculate      Zo      (x;)   \\n \\n=      E,»*-[xo   \\n \\n|      x4]      using      the      pre-trained      (score-based)      diffusion  \\n \\n  model      based      on      Tweedie’s      formula:</p><table><tr><td colSpan=1>      O12</td><td colSpan=1><p>]\\n \\n \\n  E,pre      [xo   \\n \\n|      x1   \\n \\n—      Ut   \\n \\n+      {or}      V      0g      Ge(&e)  \\n \\n  Le</p></td></tr></table><p>Recall      that      the      notation      p/?,      0?,      q,      are      defined      in      (3).\\nFinally,      by      recalling      V      log   \\n \\n@   \\n \\n=      Si      ore      (x;,t)      in  \\n \\n  score-based      diffusion      models,      we      can      approximate      Vv;(a)      with</p><h4>Le</h4><p> \\n \\n  and      plug      it      into      Algorithm   \\n \\n4      (i.e.,      value-weighted      sampling).\\n \\n \\n  As      mentioned      earlier,      this      approach      has      been      practically      widely      used      in      the      context      of      classifier  \\n \\n  guidance.\\nDespite      its      simplicity,      the      approximation      error      can      be      significant,      as      it      can      not      be  \\n \\n  diminished      even      with   \\n \\na      large      sample      size      because      the      discrepancy      from      (30)      to      (31)      is      not      merely   \\n \\na       statistical      error      that      diminishes      with      increasing      sample      size.</p><li>6.2.3.\\nZeroth-Order      Guidance      using      Path      Integral      Control</li><p> \\n \\n  In      the      previous      subsection      (Section      6.2.2),      we      discussed      an      approximation      technique      to      bypass      the  \\n \\n  necessity      of      learning      explicit      value      functions.\\nAnother      approach      to      bypass      this      requirement      is      to      use  \\n \\n  control-based      techniques      for      obtaining      soft-optimal      policies,      a.k.a.,      path      integral      control      (Theodorou  \\n \\n  et      al.,      2010;      Williams      et      al.,      2017;      Kazim      et      al.,      2024).</p><p> \\n \\n  Recall      that      we      have      p(x;,t;      6)   \\n \\n=      2;   \\n \\n+      (dt)g(x+,      t)      and      o°(t)   \\n \\n=      g(t)(dt),      the      motivation      behind this      approach      is      as      follows.\\nInitially,      we      have:</p><h5>Vo,      exp(v(#1)/@)</h5><p>=\\n \\n   Egppreyexp(vr_1      (22-1)      /@)   \\n \\n|      24]      (Soft      Bellman      equation) _\\n \\n   (1   \\n \\n—   \\n \\nx   \\n \\n—      (Ot)      G(a2,      t))?\\n=\\n \\n   Vz»,   \\n \\n{   \\n \\n[      explatena/ay)      exp      (-      0-5a(t)      (ot)   \\n \\n)      in,-}</p><p>=\\n \\n   Byes      [ese      a(era/a))      {ea      ~~      (61)g(nn,)}      EE      Cah</p><p>~\\n \\n   arp      Buty      lexr(era(ea/a))      Gera   \\n \\n—      21   \\n \\n~      (8t)ale      plod</p><p>_t   \\n \\n1       Ht)      ot)      Eg,prey      [E      ppp}      [exp(r(2o)/a)   \\n \\n|      T-1]      e|24|   \\n \\n=      Woon)      [exp(r(xo/a))e|x¢]      ;</p><h5>Now,      we      obtain:</h5><p>o7(t)Vnv(ae)   \\n \\n1      Eqppry      lexp(r(wo)/a)      eel    \\n \\nQ   \\n \\n~   \\n \\na   \\n \\n*      Egyprey      lexp(r(2o)/a))      |x]      (32)</p><p> \\n \\n  Importantly,      this      approach      does      not      require      any      differentiation.\\nTherefore,      by      running      policies  \\n \\n  from      pre-trained      models      and      simply      using      Monte      Carlo      approximation      in      both      the      denominator  \\n \\n  and      numerator,      we      can      estimate      the      above      quantity      (the      right-hand      side      of      (32))      without      making  \\n \\n  any      models      (classifiers),      unlike      classifier      guidance.\\nNote      while      this      approach      is      widely      used      in      the  \\n \\n  control      community,      it      may      not      be      feasible      for      diffusion      models      due      to      the      high      dimensionality      of  \\n \\n  the      input      space.</p><h4>Le</h4><p> \\n \\n  and      plug      it      into      Algorithm   \\n \\n4      (i.e.,      value-weighted      sampling).\\n \\n \\n  As      mentioned      earlier,      this      approach      has      been      practically      widely      used      in      the      context      of      classifier  \\n \\n  guidance.\\nDespite      its      simplicity,      the      approximation      error      can      be      significant,      as      it      can      not      be  \\n \\n  diminished      even      with   \\n \\na      large      sample      size      because      the      discrepancy      from      (30)      to      (31)      is      not      merely   \\n \\na       statistical      error      that      diminishes      with      increasing      sample      size.</p><li>6.2.3.\\nZeroth-Order      Guidance      using      Path      Integral      Control</li><p> \\n \\n  In      the      previous      subsection      (Section      6.2.2),      we      discussed      an      approximation      technique      to      bypass      the  \\n \\n  necessity      of      learning      explicit      value      functions.\\nAnother      approach      to      bypass      this      requirement      is      to      use  \\n \\n  control-based      techniques      for      obtaining      soft-optimal      policies,      a.k.a.,      path      integral      control      (Theodorou  \\n \\n  et      al.,      2010;      Williams      et      al.,      2017;      Kazim      et      al.,      2024).</p><p> \\n \\n  Recall      that      we      have      p(x;,t;      6)   \\n \\n=      2;   \\n \\n+      (dt)g(x+,      t)      and      o°(t)   \\n \\n=      g(t)(dt),      the      motivation      behind this      approach      is      as      follows.\\nInitially,      we      have:</p><h5>Vo,      exp(v(#1)/@)</h5><p>=\\n \\n   Egppreyexp(vr_1      (22-1)      /@)   \\n \\n|      24]      (Soft      Bellman      equation) _\\n \\n   (1   \\n \\n—   \\n \\nx   \\n \\n—      (Ot)      G(a2,      t))?\\n=\\n \\n   Vz»,   \\n \\n{   \\n \\n[      explatena/ay)      exp      (-      0-5a(t)      (ot)   \\n \\n)      in,-}</p><p>=\\n \\n   Byes      [ese      a(era/a))      {ea      ~~      (61)g(nn,)}      EE      Cah</p><p>~\\n \\n   arp      Buty      lexr(era(ea/a))      Gera   \\n \\n—      21   \\n \\n~      (8t)ale      plod</p><p>_t   \\n \\n1       Ht)      ot)      Eg,prey      [E      ppp}      [exp(r(2o)/a)   \\n \\n|      T-1]      e|24|   \\n \\n=      Woon)      [exp(r(xo/a))e|x¢]      ;</p><h5>Now,      we      obtain:</h5><p>o7(t)Vnv(ae)   \\n \\n1      Eqppry      lexp(r(wo)/a)      eel    \\n \\nQ   \\n \\n~   \\n \\na   \\n \\n*      Egyprey      lexp(r(2o)/a))      |x]      (32)</p><p> \\n \\n  Importantly,      this      approach      does      not      require      any      differentiation.\\nTherefore,      by      running      policies  \\n \\n  from      pre-trained      models      and      simply      using      Monte      Carlo      approximation      in      both      the      denominator  \\n \\n  and      numerator,      we      can      estimate      the      above      quantity      (the      right-hand      side      of      (32))      without      making  \\n \\n  any      models      (classifiers),      unlike      classifier      guidance.\\nNote      while      this      approach      is      widely      used      in      the  \\n \\n  control      community,      it      may      not      be      feasible      for      diffusion      models      due      to      the      high      dimensionality      of  \\n \\n  the      input      space.</p><h5>Vo,      exp(v(#1)/@)</h5><p>=\\n \\n   Egppreyexp(vr_1      (22-1)      /@)   \\n \\n|      24]      (Soft      Bellman      equation) _\\n \\n   (1   \\n \\n—   \\n \\nx   \\n \\n—      (Ot)      G(a2,      t))?\\n=\\n \\n   Vz»,   \\n \\n{   \\n \\n[      explatena/ay)      exp      (-      0-5a(t)      (ot)   \\n \\n)      in,-}</p><p>=\\n \\n   Byes      [ese      a(era/a))      {ea      ~~      (61)g(nn,)}      EE      Cah</p><p>~\\n \\n   arp      Buty      lexr(era(ea/a))      Gera   \\n \\n—      21   \\n \\n~      (8t)ale      plod</p><p>_t   \\n \\n1       Ht)      ot)      Eg,prey      [E      ppp}      [exp(r(2o)/a)   \\n \\n|      T-1]      e|24|   \\n \\n=      Woon)      [exp(r(xo/a))e|x¢]      ;</p><h5>Now,      we      obtain:</h5><p>o7(t)Vnv(ae)   \\n \\n1      Eqppry      lexp(r(wo)/a)      eel    \\n \\nQ   \\n \\n~   \\n \\na   \\n \\n*      Egyprey      lexp(r(2o)/a))      |x]      (32)</p><p> \\n \\n  Importantly,      this      approach      does      not      require      any      differentiation.\\nTherefore,      by      running      policies  \\n \\n  from      pre-trained      models      and      simply      using      Monte      Carlo      approximation      in      both      the      denominator  \\n \\n  and      numerator,      we      can      estimate      the      above      quantity      (the      right-hand      side      of      (32))      without      making  \\n \\n  any      models      (classifiers),      unlike      classifier      guidance.\\nNote      while      this      approach      is      widely      used      in      the  \\n \\n  control      community,      it      may      not      be      feasible      for      diffusion      models      due      to      the      high      dimensionality      of  \\n \\n  the      input      space.</p><h3>6.3      Path      Consistency      Learning      (Losses      Often      Used      in      Gflownets)</h3><p>ets)  \\n \\n  Now,      we      explain      how      to      apply      path      consistency      learning      (PCL)      (Nachum      et      al.,      2017)      to      fine-  \\n \\n  tune      diffusion      models.\\nIn      the      Gflownets      literature      (Bengio      et      al.,      2023),      it      seems      that      this      variant      is  \\n \\n  utilized      as      either   \\n \\na      detailed      balance      or   \\n \\na      trajectory      balance      loss,      as      discussed      in      Mohammadpour  \\n \\n  et      al.\\n(2023);      Tiapkin      et      al.\\n(2023);      Deleu      et      al.\\n(2024).\\nHowever,      to      the      best      of      our      knowledge,  \\n \\n  the      precise      formulation      of      path      consistency      learning      in      the      context      of      fine-tuning      diffusion      models  \\n \\n  has      not      been      established.\\nTherefore,      we      start      by      elucidating      the      rationale      of      PCL.\\nSubsequently,  \\n \\n  we      provide   \\n \\na      comprehensive      explanation      of      the      PCL.\\nFinally,      we      discuss      its      connection      with      the  \\n \\n  literature      on      Gflownets.</p><p>Motivation.\\nHere,      we      present      the      fundamental      principles      of      the      PCL.\\nTo      start      with,      we      prove      the  \\n \\n  following      lemma,      which      characterizes      soft-value      functions      and      soft-optimal      policies      recursively.</p><p>Algorithm   \\n \\n5      Path      Consistency      Learning      (Training      with      detailed      balance      loss)  \\n \\n  1:      Require:      Diffusion-model      {N(p(2,t;),07(t))}i-74,,  \\n \\n  -pre-trained   \\n \\n=      model  \\n \\n  {N      (p(t,      t;      Opre),      07      (t))      }i_744,      batch      size      m,      parameter   \\n \\na   \\n \\n€      Rt,      learning      rate   \\n \\n7       2:      Set   \\n \\na      model      {v;(-;@)}      to      learn      optimal      soft      value      function,      and   \\n \\na      model      {p;(-|-;@)}      to      learn  \\n \\n  optimal      polices.\\n \\n \\n  3:      Initialize:      0;   \\n \\n=      Opre  \\n \\n  4:      for      s   \\n \\n=      {1,---      ,S}do  \\n \\n  5:      Collect   \\n \\nm      samples      {a      (0)      }?_7,,      from   \\n \\na      current      diffusion      model      (i.e.,      generating      by      sequen- tially      running      polices      {N(p(2z,      t;      8),      07(t))      }i_-74,      from   \\n \\nt   \\n \\n=   \\n \\nT      to   \\n \\nt   \\n \\n=      0)  \\n \\n  6:      Setuo=r</p><li>(i)   \\n \\n?       a!\\n \\n \\n3   \\n \\na   \\n \\n4      Up—-1(@      13      ds)      re  \\n \\n  bs41   \\n \\n—      bs   \\n \\n—      Vo   \\n \\n3   \\n \\na      9)   \\n \\n+      log      p:(a\\\\\\\\\\\\\\\\,      |a\\\\\\\\\\\\\\\\;0,)   \\n \\n-      EP   \\n \\n—      Jog      rl      >}      Ibe:</li><p>a\\n \\n \\n  t=T+1      i=1  \\n \\n  (c}      ite)      ()      via)      43s)   \\n \\n;       Os41   \\n \\n<      Os   \\n \\n—      Vo   \\n \\n3      TE      velee      ids)      —S<   \\n \\n+      log      pe(ay      2s      |p      0)      —§      log      pee      (ar?\\nifort?\\nJe      lass</p><p>t=T+1      i=1 7:      end      for  \\n \\n  8:      Output:      {p,(x1-1      [215      Os)      fe</p><h3>Lemma      4      (1-step      Consistency      Equation).</h3><li>(2)   \\n \\n+      log      pi      (aa      |a1)   \\n \\n=      (eaeen)   \\n \\n+      log      pp      (x+-1|2+)      (33)</li><p> \\n \\n  Proof.\\nWe      consider      the      marginal      distribution      with      respect      to      x,      and      2;_,      induced      by      the      soft-  \\n \\n  optimal      policy,      and      denote      it      [(z;,      2:1)   \\n \\n€      A(X   \\n \\nx      X).\\nThen,      it      is      clearly      I(x,)l(a,_1   \\n \\n|      x)   \\n \\n=       I(x4-1)l      (xz   \\n \\n|      a1).\\nNow,      from      Theorem   \\n \\n2      that      characterizes      marginal      distributions,      and      Theorem   \\n \\n3       that      characterizes      posterior      distributions,      this      results      in      :</p><p>1\\n \\n   UE      L      re   \\n \\nx   \\n \\n1      Otte      me  \\n \\n  exp   \\n \\n(   \\n \\ni      2)      Pe      (xt)   \\n \\nx      pe      (®e-1|@e)   \\n \\n=   \\n \\nB      exp      (faeev)      Pra      (@1-1)   \\n \\nX      Pe      (1/01) a\\n \\n   —_-_——”’   \\n \\nC   \\n \\na      ~~  \\n \\n  C  \\n \\n  we  \\n \\n  Optimal      policy  \\n \\n  “~   \\n \\n“      Posterior      distribution  \\n \\n  Marginal      distribution      at   \\n \\nt      Marginal      distribution      at      t-1 Rearranging      yields:</p><p>1\\n \\n   UL(&   \\n \\n1      Up—1      (Le      re  \\n \\n  G      exp   \\n \\n(      ))   \\n \\nx      Di      (@-1|%4)   \\n \\n=   \\n \\nG      exp      (eaeen)   \\n \\nx      pe      (@4-1|24)</p><p>Taking      the      logarithm,      the      statement      is      concluded.\\nO</p><p>Algorithm.\\nBeing      motivated      by      the      relation      in      (33),      after      initializing      v9   \\n \\n=      r,      we      obtain      the  \\n \\n  recursive      equation:</p><p>2 Olen      U_-1      (Lt  \\n \\n  (pp)   \\n \\n=      argmin   \\n \\n—      Egany   \\n \\n|      oO   \\n \\n4      tog   \\n \\ng      (arp      ng)   \\n \\n—      PEED)   \\n \\n—      dog      pP\"      (ya      er) g):%3R,g2):¥      A(X)   \\n \\na   \\n \\no U   \\n \\n_   \\n \\nx   \\n \\n_      re      re  \\n \\n  =      (MEY)      tog      peal      altiansa)      +++      Flog      pe      (esl),  \\n \\n  which      is      an      extension      of      (33).\\nThe      loss      function      based      on      the      above      k-step      consistency      equation  \\n \\n  could      make      training      faster      without      learning      value      functions      at      every      time      point,      as      noted      in      the  \\n \\n  literature      in      PCL.\\nIn      the      extreme      case      (i.e.,      when      we      recursively      apply      it      with   \\n \\nt   \\n \\n=      7’),      we      obtain      the  \\n \\n  following.</p><li>(34)  \\n \\n  where      u,   \\n \\n€      A(X)      is      any      exploratory      roll-in      distribution.\\nBased      on      this      algorithm,      we      outline      the  \\n \\n  entire      algorithm      in      Algorithm      5.  \\n \\n  We      make      several      important      remarks      regarding      Algorithm      5.\\nFirstly,      while      we      use      on-policy      data  \\n \\n  collection,      technically,      any      policy      can      be      used      in      this      off-policy      algorithm,      like      reward-weighted  \\n \\n  MLE.\\nSecondly,      in      practice,      it      might      be      preferable      to      utilize   \\n \\na      sub-trajectory      from      x;      to      x;_,      based  \\n \\n  on      the      following      expression:</li><p>UFZ  \\n \\n  log      pi_      pa      (Gee      |Vt—ng1)   \\n \\n+      +++   \\n \\n+      log      pi      (4-1      |a4)   \\n \\n+      (2)</p><h3>Corollary      1      (7-step      consistency).</h3><p>log      pf      (xo|21)   \\n \\n+      +++   \\n \\n+      log      pp(wr-a|er)   \\n \\n+      log      pp      (x7)      (35) r(x      re      re      re  \\n \\n  —      (=)   \\n \\n+      log      p?\\n*(xo|v1)      +--+   \\n \\n+      log      pr\"      (ar-i|er)   \\n \\n+      log      pe      (xr).</p><p>Proof.\\nWe      consider      the      marginal      distribution      with      respect      to      7,      +--+   \\n \\n,   \\n \\n%      induced      by      the      soft-optimal  \\n \\n  policy,      and      denote      it      I(vp,---      ,--+      ,%o)   \\n \\n€   \\n \\n¥      X--+   \\n \\nx      X.      We      have Uar)l(ar—1   \\n \\n|      Lr)      tee      I(xo|21)   \\n \\n=      U(x      )l(a4   \\n \\n|      Xo)      te      (ar   \\n \\n|      LT-1)      (36) From      Theorem   \\n \\n2      that      characterized      marginal      distributions,      and      Theorem   \\n \\n3      that      characterize      posterior  \\n \\n  distributions,      the      left      hand      side      of      (36)      is      equal      to *\\n \\n   *\\n \\n   *</p><p>Pr      (xr)   \\n \\nx      pp      (r|er-1)      X-++   \\n \\nx      pi      (Xo|@1) Marginal      distribution      atT      Optimal      policy      at   \\n \\nT   \\n \\n—   \\n \\n1      Optimal      policy      at   \\n \\n1 and      the      right-hand      side      in      (36)      is      equal      to exp(r      (xo)      /a)      pre      pre      pre  \\n \\n  aq      Po      (xo)   \\n \\nX      po      (a1   \\n \\n|      Zo)      X+++   \\n \\nX      prey      (@r   \\n \\n|      er-1)-.\\n \\n \\n|      ~~  \\n \\n  Marginal      distribution      at   \\n \\n0      Posterior      distribution      Posterior      distribution By      rearranging      the      term,      we      obtain      (35).\\nO</p><p> \\n \\n  Comparison      with      Gflownets.\\nIn      the      Gflownets      literature,      similar      losses      are      used.\\nFor      instance,  \\n \\n  the      loss      derived      from      (33)      or      (35)      is      commonly      known      as   \\n \\na      detailed      balance      loss      (Bengio      et      al.,  \\n \\n  2023)      or   \\n \\na      trajectory      loss      (Malkin      et      al.,      2022),      respectively.</p><p> \\n \\n  Note      in      general,      the      literature      in      Gflownets      primarily      focuses      on      sampling      from      unnormalized  \\n \\n  models      (distributions      proportional      to      exp(r(a))).\\nHence,      reference      policies      (i.e,      {p?\"\"}\\n \\n \\n)      or      latent  \\n \\n  states      (1.e.,      %7.;      before      x9)      are      introduced      without      relying      on      pre-trained      diffusion      models.\\nIn  \\n \\n  contrast,      in      our      context,      we      use      policies      derived      from      pre-trained      diffusion      models      as      reference  \\n \\n  policies,      leveraging      them      as      our      prior      knowledge.</p><p>7\\n \\n   Fine-Tuning      Settings      Taxonomy</p><p> \\n \\n  So      far,      we      implicitly      assume      we      have      access      to      reward      functions.\\nHowever,      these      functions      are      often  \\n \\n  unknown      and      need      to      be      learned      from      data.\\nWe      classify      several      settings      in      terms      of      whether      reward  \\n \\n  functions      are      available      or,      if      not,      how      they      could      be      learned.\\nThis      section      is      summarized      in      Figure      3.</p><h4>Direct      backpropagation      (Sec.      5.2)       _       Differentiable</h4><p>_\\n \\n   ——_      Non-differentiable PP      5.1</p><h4>Accurate      rewards      O      (Sec.      5.1)</h4><p> \\n \\n  feedback   \\n \\n;    \\n \\n—      Reward-weighted      MLE      (Sec.      6.1  \\n \\n  Xo      Unknown      reward      Memory      effi-      oe      eward-welgnte      (Sec.      6.1) unctions      —_\\\\\\\\\\\\\\\\_—      ciency</p><h4>No      fine-tuning      Computational</h4><p>efficiency —\\n \\n    NN      Direct      backpropagation      (Sec.      5.2)</p><h5>Value-weighted      sampling      (Sec.      6.2)</h5><p>Figure      3:      Practical      recommendation      of      RL-based      algorithms      to      optimize      downstream      reward  \\n \\n  functions.</p><h6>7.1      Fine-Tuning      with      Known,      Differentiable      Reward      Functions</h6><p>ions  \\n \\n  When      accurate      differentiable      reward      functions      are      available      (e.g.,      standard      in      computer      vision      tasks),  \\n \\n  our      primary      focus      is      typically      on      computational      and      memory      efficiency.\\nIn      general,      we      advocate      for  \\n \\n  employing      reward      backpropagation      (c.f.      Algorithm      2)      for      its      computational      efficiency.\\nAlternatively,  \\n \\n  if      memory      efficiency      is   \\n \\na      significant      concern,      we      suggest      using      PPO      due      to      its      stability.</p><h6>7.2      Fine-Tuning      with      Black-Box      Reward      Feedback</h6><p>back  \\n \\n  Here,      we      explore      scenarios      where      we      have      access      to      accurate      but      non-differentiable      (black-box)  \\n \\n  reward      feedback,      often      found      in      scientific      simulations.\\nThe      emphasis      still      remains      primarily      on  \\n \\n  computational      and      memory      efficiency.\\nIn      such      scenarios,      due      to      the      non-differentiability      of      reward  \\n \\n  feedback,      we      recommend      using      PPO      or      reward-weighted      MLE      for      the      following      reasons.</p><p> \\n \\n  In      the      preceding      section,      we      advocated      for      the      use      of      reward      backpropagation      due      to      its      com-  \\n \\n  putational      advantages.\\nHowever,      when      dealing      with      non-differentiable      feedback,      the      advantage  \\n \\n  of      reward      backpropagation      may      diminish.\\nThis      is      because      learning      from      such      feedback      requires  \\n \\n  learning      differentiable      reward      functions      to      make      the      algorithm      work      as      we      discuss      in      Section      5.2.  \\n \\n  However,      obtaining   \\n \\na      differentiable      reward      function      can      be      challenging      in      many      domains.\\nFor  \\n \\n  example,      in      molecular      property      prediction,      molecular      fingerprints      are      informative      features,      and  \\n \\n  accurate      reward      functions      can      be      derived      by      training      simple      neural      networks      on      these      features  \\n \\n  (Pattanaik      and      Coley,      2020).\\nYet,      these      mappings,      grounded      in      prior      scientific      understanding,      are  \\n \\n  non-differentiable,      thereby      restricting      their      applicability.</p><p> \\n \\n  In      contrast,      PPO      and      reward-weighted      MLE      allow      for      policy      updates      without      the      explicit  \\n \\n  requirement      to      learn   \\n \\na      differentiable      reward      function,      offering   \\n \\na      significant      advantage      over      reward  \\n \\n  backpropagation.</p><h6>7.3      Fine-Tuning      with      Unknown      Rewards      Functions</h6><p>ions  \\n \\n  When      reward      functions      (or      computational      proxies      in      Section      7.2)      are      unavailable,      we      must      learn  \\n \\n  from      data      with      reward      feedback.\\nIn      such      cases,      compared      to      the      previous      scenarios,      two      important  \\n \\n  considerations      arise:</p><p>¢\\n \\n   Not      only      computational      or      memory      efficiency      but      also      feedback      efficiency      (i.e.,      sample  \\n \\n  efficiency      regarding      reward      feedback)      is      crucial.</p><p> \\n \\n \\n¢      Given      that      learned      reward      feedback      may      not      generalize      well      outside      the      training      data      distri-  \\n \\n  butions,      it      is      essential      to      constrain      fine-tuned      models      to      prevent      significant      divergence      from  \\n \\n  diffusion      models.\\nIn      these      situations,      not      only      soft      PPO      and      direct      backpropagation      but      also  \\n \\n  methods      discussed      in      Section      6,      such      as      reward-weighted      MLE,      can      be      particularly      effective.</p><p>Further,      the      scenario      involving      unknown      reward      functions      can      be      broadly      categorized      into      two      cases,  \\n \\n  which      we      will      discuss      in      more      detail.</p><p>Offline      scenario      (Figure      4).\\nIn      many      scenarios,      even  \\n \\n  in      cases      where      reward      functions      are      unknown,      we      of-  \\n \\n.     .</p><p>Offline      data      with ten      have      access      to      offline      data      with      reward      feedback      reward      feedback</p><li>(i)      (i)   \\n \\nO   \\n \\n;   \\n \\ni       a”      ra  \\n \\n.      ne      straightforward      approach      is      to</li><p> \\n \\n \\n{   \\n \\n,   \\n \\n(   \\n \\n)    \\n.\\n    .\\n \\n   g\\n \\n   PP  \\n \\n.      <=>      Learning      reward  \\n \\n  perform      regression      with      neural      networks      and      build   \\n \\na      models  \\n \\n  learned      regressor      *.\\nHowever,      this      learned      reward      func-      SS  \\n \\n  tion   \\n \\n*      might      not      generalize      well      beyond      the      distribution      x)  \\n \\n  of      the      offline      data.\\nIn      regions      outside      this      distribution,   \\n \\n7       could      assign   \\n \\na      high      value      even      when      the      actual      reward  \\n \\n  r      is      low      due      to      high      uncertainty.\\nConsequently,      we  \\n \\n  could      be      easily      misled      by      out-of-distribution      samples  \\n \\n  if      we      solely      optimize      7.\\n|Fine-tuning      diffusion      models   \\n \\n|       One      approach      to      alleviate      this      issue      is      to      adopt   \\n \\na       pessimistic      (i.e.,      conservative)      strategy,      as      proposed      in      Figure      4:      Offline      scenario  \\n \\n  Uehara      et      al.\\n(2024).\\nThis      technique      has      been      widely      Collecting      data]   \\n \\nY      Learning      reward  \\n \\n  applied      in      the      field      of      offline      RL      (Levine,      2018).\\nThe      aa  \\n \\n  main      idea      is      to      penalize   \\n \\n7      outside      the      distributions      us-  \\n \\n  ing      techniques      such      as      adding      an      explicit      penalty      term  \\n \\n  (Yu      et      al.,      2020;      Chang      et      al.,      2021),      bootstrapping,  \\n \\n  or      more      sophisticated      methods      (Kumar      et      al.,      2020;  \\n \\n  Xie      et      al.,      2021;      Rigter      et      al.,      2022;      Uehara      and      Sun,  \\n \\n  2021).\\nBy      doing      so,      we      can      avoid      being      fooled      by  \\n \\n  out-to-distribution      regions      while      still      benefiting      from  \\n \\n  the      extrapolation      capabilities      of      reward      models.</p><p>S—      diffusion      models      |</p><p>Figure      5:      Online      scenario</p><p> \\n \\n  Online      scenario      (Figure      5).\\nConsider      situations  \\n \\n  where      reward      functions      are      unknown,      but      we      can      gather      data      online.\\nThese      scenarios      are      often  \\n \\n  referred      to      as   \\n \\na      lab-in-the-loop      setting.</p><p> \\n \\n  In      such      scenarios,      Uehara      et      al.\\n(2024)      proposes      an      iterative      procedure      comprising      three      steps:</p><li>(1)      collecting      feedback      data      by      exploring      new      areas      (i.e.,      obtaining   \\n \\nx      from      current      diffusion      models  \\n \\n  and      corresponding      r(x)),      (2)      learning      the      reward      function      from      the      collected      feedback      data,      and</li><li>(3)      fine-tuning      current      diffusion      models      by      maximizing      the      learned      reward      function      enhanced      by  \\n \\n  an      optimistic      bonus      term,      using      algorithms      discussed      in      Section      4.2.\\nIn      contrast      to      standard      online  \\n \\n  PPO,      where      feedback      data      is      directly      used      in      the      fine-tuning      process,      actual      reward      feedback      is  \\n \\n  only      used      in      step      (2)      and      not      in      step      (3).\\nAn      additional      key      aspect      is      the      use      of      an      optimistic  \\n \\n  reward      function      that      encourages      exploration      beyond      the      distributions      of      the      current      diffusion      model.\\n \\n \\n  This      deliberate      separation      between      reward      learning      and      fine-tuning      steps,      coupled      with      the      use      of  \\n \\n  optimism,      significantly      enhances      feedback      efficiency.</li><p>8\\n \\n   Connection      with      Classifier      Guidance</p><p> \\n \\n  Classifier      guidance      (Dhariwal      and      Nichol,      2021)      is      commonly      used      for      conditional      generation      in  \\n \\n  diffusion      models.\\nInterestingly,      RL-based      methods      share   \\n \\na      close      connection      with      classifier      guidance.\\n \\n \\n  More      specifically,      in      this      section,      following      Zhao      et      al.\\n(2024),      we      elucidate      how      RL-based      methods  \\n \\n  can      be      applied      to      conditional      generation.\\nFurthermore,      from      this      unified      viewpoint,      we      highlight  \\n \\n  that      classifier      guidance      is      seen      as   \\n \\na      value-weighted      sampling      in      Section      4.2.</p><h6>8.1      Classfier      Guidance</h6><p>ance  \\n \\n  In      this      section,      we      first      introduce      classifier      guidance.\\nClassifier      guidance      utilizes      an      unconditional  \\n \\n  pre-trained      model      for      conditional      generation.\\nMore      specifically,      we      consider   \\n \\na      scenario      where      we  \\n \\n  have   \\n \\na      pre-trained      diffusion      model,      which      enables      us      to      sample      from      p?\\n\"*(x),      and      data      with      feedback  \\n \\n  {x,y},      where   \\n \\ny      represents      the      new      label      we      want      to      condition      on.\\nOur      objective      here      is      to      sample  \\n \\n  from      p(-|y)   \\n \\n€      [Y   \\n \\n>      ACR), P(-   \\n \\n|      y)   \\n \\nx      py(yl-)pP*(-).\\n(37)</p><p>where      p,   \\n \\n:   \\n \\nX      —>      A(Y)      denotes      the      conditional      distribution      of   \\n \\ny      given      x.</p><p>Motivation.\\nIn      classifier      guidance,      we      consider      the      following      policy:</p><p>pe      (-lare,      y)   \\n \\n=   \\n \\nN      (p(22,      t;      Opre)   \\n \\n+      a(t)      Ve      log      Vy      (Xt;      t),      a*(t))   \\n \\n’ dy(x,t)   \\n \\n=      Egy.\\nyap,      (-leo)      L(Y   \\n \\n=      y)|a,   \\n \\n=      2].</p><p>Under      the      continuous-time      formulation      of      diffusion      models,      applying      Doob’s      h-transform      (Rogers  \\n \\n  and      Williams,      2000),      it      is      shown      that      we      can      sample      from      p(x|y)      by      sequentially      running      {p7\"\\\\\\\\\\\\\\'(-|7+,      y)      }i-r41-</p><p>Algorithm.\\nSpecifically,      the      algorithm      comprises      two      steps.\\nIt’s      important      to      note      that      this      is      not  \\n \\n  a      fine-tuning      method      but      rather      an      inference-time      technique      as      we      freeze      the      pre-trained      model.</p><li>1. Learning      q,(x,t)      through      regression      by      gathering      data      containing      {z;,      y}.\\nHere,      for      each  \\n \\n  (Xo,      y),      using   \\n \\na      forward      policy      for      the      pre-training      (from   \\n \\n0      to      T),      we      obtain      2;      starting      from</li><p>Lo.</p><li>2. During      inference,      sequentially      execute      {p2\"(-|a,,      y)      }1_>   \\n \\n1      by      leverage      the      fact      that      compared</li><p>gui to      the      policy      in      pre-trained      diffusion      models,      the      mean      of      {p?\\n\"\\\\\\\\\\\\\\'(-|71,      y)      }i-74,      is      just      shifted  \\n \\n  with      an      additional      term      V,,      log      q,(2,      t).</p><p> \\n \\n  Notably,      the      method      described      above      can      be      reduced      to      value-weighted      sampling      (cf.      Sec-  \\n \\n  tion      6.2),      when      r(x,      y)   \\n \\n=      log      p(y|x).\\nInspired      by      such      an      observation,      we      apply      an      RL-based  \\n \\n  fine-tuning      method      for      conditional      generation      as      below.</p><h7>8.2.      RL-Based      Fine-Tuning      for      Conditional      Generation</h7><p>tion  \\n \\n  In      this      section,      following      Zhao      et      al.\\n(2024),      we      explain      an      RL-based      fine-tuning      approach      for  \\n \\n  conditional      generation.\\nThe      core      idea      is      that      setting      r(x,      y)   \\n \\n=      log      p(y|x)      allows      us      to      sample      from  \\n \\n  the      target      conditional      distribution      (37),      drawing      on      the      insights      discussed      in      Section      4.2.  \\n \\n  More      precisely,      introducing   \\n \\na      distribution      over   \\n \\ny      as      q(-),      we      formulate      the      following      RL      problem:</p><p>{Di      ft   \\n \\n=      argmax      Egy,      (-\\\\\\\\\\\\\\\\-y)}.yott      log      p(y|xo)   \\n \\n—      NeopyaKL(pe(-|e,      y)      ||P      (late).\\n \\n \\n  {prE[(V,.X)PA(A)]      Heyy (38)</p><p> \\n \\n  Compared      to      the      objective      function      (18)      in      Section      4.2,      the      main      differences      are:      (1)      policy      space  \\n \\n  is      expanded      to      accommodate      additional      control      (i.e.,      y):      pz(-|-,-)   \\n \\nC      [R¢   \\n \\nx   \\n \\nY   \\n \\n+      A(R®)],      (2)      the  \\n \\n  fine-tuning      objectives      are      set      as      log-likelihoods:      log      p(y|z).\\nThen,      following      Theorem      1,      we      can  \\n \\n  establish      the      following      result.</p><p>Corollary      2.\\nThe      distribution      induced      by      {p*}1_74      (ue.   \\n \\n[      {Teri      Ph      (ae-1   \\n \\n|      te,      y)}deur      is      the target      conditional      distribution      (37).</p><p> \\n \\n  The      above      corollary      suggests      that      for      conditional      generation,      we      can      utilize      various      off-the-shelf  \\n \\n  algorithms      discussed      in      Section      4.2      and      6,      such      as      PPO,      reward      backpropagation,      and      reward-  \\n \\n  weighted      MLE,      as      well      as      value-weighted      sampling.</p><p>9\\n \\n   Connection      with      Flow-Based      Diffusion      Models</p><p> \\n \\n  We      elucidate      the      close      relationship      between      bridge      matching      and      fine-tuning,      as      summarized      in  \\n \\n  Table      2.\\nBridge      (flow)      matching      (Liu      et      al.,      2022;      Tong      et      al.,      2023;      Lipman      et      al.,      2023;      Liu      et      al.,  \\n \\n  2022;      Shi      et      al.,      2024;      Albergo      and      Vanden-Eijnden,      2022),      has      recently      gained      popularity      as   \\n \\na       distinct      type      of      diffusion      model      from      score-matching-based      ones.\\nThese      works      suggest      that      training  \\n \\n  and      inference      speed      will      be      accelerated      due      to      the      ability      to      define   \\n \\na      more      direct      trajectory      from  \\n \\n  noise      to      data      distribution      than      score-matching-based      diffusion      models.</p><p> \\n \\n  The      connection      between      bridge      matching      and      fine-tuning      becomes      evident      in      the      continuous-  \\n \\n  time      formulation.\\nTo      begin,      we      first      provide   \\n \\na      brief      overview      of      the      continuous-time      formulation      of  \\n \\n  score-matching-based      diffusion      models.</p><p> \\n \\n  Table      2:      Comparison      between      training      score/flow-based      diffusion      models      and      fine-tuning.\\nWe  \\n \\n  optimize      the      parameter   \\n \\n6      in      P’,      which      represents      the      induced      distribution      over      trajectories      associated  \\n \\n  with      6.</p><h8>Loss      Note      of      Q</h8><p>Score-based      KL(Q\\\\\\\\\\\\\\'™¢|P?)      Qtime:      Time-reversal      of      reference      SDE      (reference training      SDE      is      defined      from   \\n \\n0      to      T’      where      the      initial      dis-  \\n \\n  tribute      at   \\n \\n0      is      data      distribution)</p><p> \\n \\n  Flow-based      KL(Q*°>      ||P?)      @Q*°°h:      Coupling      between      the      reference      SDE      from  \\n \\n  training   \\n \\nT      to   \\n \\n0      conditioning      at   \\n \\n0      (dist.      over      x.;|7o)      and  \\n \\n  a      data      distribution      at      time   \\n \\n0      (dis.      over      x).\\nNote  \\n \\n  that      reference      SDE      is      defined      from      7’      to      0.</p><p> \\n \\n  Fine-tuning      KL(P?||Q*°)      @*:      Coupling      between      the      pre-trained      SDE  \\n \\n  from   \\n \\nT      to   \\n \\n0      conditioned      on   \\n \\n0      (dist.      over      x7.1|X0)  \\n \\n  and   \\n \\na      data      distribution      at      time   \\n \\n0      (dis.      over      Xo) 9.1      Continuous      Time      Formulation      of      Score-Based      diffusion      models</p><p> \\n \\n  We      provide   \\n \\na      brief      overview      of      the      continuous-time      formulation      of      diffusion      models      (Song      et      al.,  \\n \\n  2021),      further      expanding      Section      1.1.1.\\nIn      this      formulation,      we      initially      define      the      forward      reference  \\n \\n  SDE,      spanning      from      time   \\n \\n0      to      7’      (e.g.,      variance      exploding      (VE)      process      or      variance      preserving      (VP)  \\n \\n  process),      by      setting      an      initial      distribution      at   \\n \\n0      as      the      data      distribution.\\nAdditionally,      the      reference  \\n \\n  SDE      is      tailored      to      converge      to      an      easy      distribution      at      time      7’,      such      as      the      normal      distribution.</p><p> \\n \\n  Subsequently,      we      consider      the      time-reversal      SDE      (Anderson,      1982),      progressing      from      T\\\\\\\\\\\\\\'      to      0.\\nIf  \\n \\n  we      could      learn      this      time-reversal      SDE,      it      would      enable      us      to      sample      from      the      data      distribution      by  \\n \\n  starting      with      the      easy      initial      distribution      (at      7’)      and      following      the      time-reversal      SDE      from   \\n \\nT      to   \\n \\n0      at  \\n \\n  inference      time.</p><p> \\n \\n  Training.\\nNow,      the      remaining      question      is      how      to      learn      this      time-reversal      SDE.\\nHere,      by      repre-  \\n \\n  senting      the      induced      distribution      over      trajectories      from   \\n \\nT      to   \\n \\n0      with      this      time-reversal      SDE      as      Q\\\\\\\\\\\\\\'™®,  \\n \\n  our      goal      is      to      train   \\n \\na      new      SDE,      parameterized      by   \\n \\n@      in      the      neural      network,      such      that      the      induced  \\n \\n  distribution      P®      over      trajectories      associated      with   \\n \\n6      aligns      with      Q\"™*.\\nSpecifically,      the      objective      is      to minimize      the      following      losses:</p><p> \\n \\n  argmin      KL(Q\"™||P*).\\n(39)  \\n \\n  0  \\n \\n  With      some      algebra,      leveraging      the      observation      that      the      time-reversal      SDE      comprises      the      original  \\n \\n  forward      SDE      and      the      scoring      term      of      the      marginal      distribution      (Anderson,      1982)      as      in      (2),      Song  \\n \\n  et      al.\\n(2021)      demonstrates      that      this      KL      loss      (39)      is      approximated      as Dore   \\n \\n=      argmin      Eeuni((0,7))01~qyo0(weleo)co~pP\"e      Fe      Fl      Vey      log      jo      (Xt   \\n \\n|      Xo)   \\n \\n_      so(Xt,      t)|I?\\n],      (40)  \\n \\n  which      is      equal      to      (4)      where      the      weight      A(t)      is      {o?\\n}?\\n(for      notation,      refer      to      Section      1.1.1).</p><h9>9.2      Flow-Based      (Bridge-Based)      Diffusion      Models</h9><p>dels We      adopt      the      explanation      outlined      in      Liu      et      al.\\n(2022).\\nIn      this      framework,      we      begin      by      considering  \\n \\n  a      reference      SDE      (from      T\\\\\\\\\\\\\\'      to      0),      x;   \\n \\n=      zr_;      where      z;      is      defined      as      Brownian      motion:</p><p>dz,   \\n \\n=      dwt,      te      (0,      7).\\n(41)</p><p> \\n \\n  Subsequently,      we      consider      the      coupling      of      the      reference      SDE      conditioned      on      state      at      time   \\n \\n0      (i.e.,  \\n \\n  distribution      of      xo.7      conditional      on      x)      and      the      data      distribution      at      time      0,      denoting      the      resulting  \\n \\n  distribution      as      Q*°?,      Informally,      this      Q*°°?\\nis      characterized      as      follows.</p><p>Informal      characterization      of      Q*”.\\nWhen      discretizing      the      time      step,      by      denoting      the      distribution  \\n \\n  induced      by      the      reference      SDE      as      p\"(x7,--+   \\n \\n,      29),      we      define QF      (a7,      tae   \\n \\n,      20)   \\n \\n=      p(x,      ay   \\n \\n|      r9)pP\"°      (x9).</p><p>The      distribution      Q*”      above      is   \\n \\na      limiting      distribution      obtained      by      making      the      discretization      step small.</p><p>Training.\\nIn      bridge-based      diffusion      models,      our      objective      is      to      train   \\n \\na      new      SDE,      parametrized      by  \\n \\n  6      (e.g.,   \\n \\na      neural      network),      to      align      the      induced      distribution      P®      with      Q*°?.\\nSpecifically,      bridge-based training      aims      to      minimize aremin      KL(Q*>||P’).\\n(42)  \\n \\n  0  \\n \\n  While      the      method      to      minimize      the      above      is      currently      unclear,      we      can      derive   \\n \\na      trainable      objective  \\n \\n  through      algebraic      manipulation.\\nInitially,      we      observe      that      the      reference      SDE      (41),      conditioned      on  \\n \\n  state      xo      at      time   \\n \\n0      (zr   \\n \\n=      D),      is      expressed      as:</p><p>b\\n \\n \\n  b—      2;</p><p>t\\n \\n   €\\n \\n   [0,T];dz?\\n \\n \\n=      g?(z?,t)dt      +du,,  \\n \\n  g(z?,t)   \\n \\n=   \\n \\na       (43)</p><p>using      the      seminal      “Doob’s      h-transform”      (Rogers      and      Williams,      2000).\\nThis      SDE      is      commonly  \\n \\n  known      as      the      Brownian      bridge.\\nNow,      we      consider   \\n \\na      new      SDE      parameterized      by      0:</p><p>t\\n \\n   €\\n \\n   [0,7];      dz   \\n \\n=      s(z,t;      @)dt   \\n \\n+      dw.\\n(44)</p><p>Then,      we      can      demonstrate      that      minimizing      the      KL      loss      in      (42)      is      equivalent      to argmin      E,W      unil0,7],1~4\"°      cop?\\n[Ilg”      (2,      t)   \\n \\n_      s(21,      t;      0)      M3]      (45)</p><p>where      q;°      denotes      the      induced      distribution      at      time   \\n \\nt      following      SDE      (43)      conditioned      on      xo,      and  \\n \\n  s:      Xx      |0,T]   \\n \\n>   \\n \\n&      is      the      model      we      aim      to      optimize.</p><p> \\n \\n  Flow-based      diffusion      models.\\nThe      aforementioned      formulation      has      been      originally      proposed      as   \\n \\na       bridge-based      SDE      (Liu      et      al.,      2022)      because      the      conditional      SDE      is      often      referred      to      as   \\n \\na      Brownian  \\n \\n  bridge.\\nFlow-based      diffusion      models      proposed      in      Lipman      et      al.\\n(2023);      Tong      et      al.\\n(2023)      share   \\n \\na       close      relationship      with      bridge-based      diffusion      models.\\nThese      flow-based      models      use   \\n \\na      target      SDE (43)      and      an      SDE      with   \\n \\na      parameter      (44)      without      stochastic      terms      (1.e.,      no      dw,)      and      aim      to      minimize  \\n \\n  the      loss      function      (45).</p><p> \\n \\n  Comparison      with      score-based      diffusion      models.\\nBoth      score-based      diffusion      models      and      bridge-  \\n \\n  based      diffusion      models      aim      to      minimize      KL      divergences.\\nDespite      its      similarity,      in      contrast      to  \\n \\n  score-based      diffusion      models,      bridge-based      diffusion      models      target      Q*°>      rather      than      the      time-  \\n \\n  reversal      SDE      Q\"™®.\\nThe      distribution      Q*”      is      often      considered      preferable      because      it      can      impose      an  \\n \\n  SDE      that      navigates      efficiently      from   \\n \\n0      to      7’      (e.g.,   \\n \\na      Brownian      bridge      with      minimal      noise).\\nThis      can  \\n \\n  expedite      the      learning      process      since      we      lack      direct      control      over      the      time-reversal      SDE      in      score-based  \\n \\n  diffusion      models.</p><h10>9.3.      Connection      with      RL-Based      Fine-Tuning</h10><p>ning  \\n \\n  RL-based      fine-tuning      shares   \\n \\na      close      relationship      with      flow-based      training.\\nTo      illustrate      this,      consider  \\n \\n  the      reference      SDE      (from      7’      to      0)      following      the      pre-trained      diffusion      model.\\nThen,      akin      to      flow-  \\n \\n  based      training,      we      consider      the      coupling      of      the      reference      SDE      conditioned      on      state      at      time   \\n \\n0       (i.e.,      distribution      over      xo-r      conditioned      on      x9)      and      the      target      distribution      at      time   \\n \\n0      (i.e.,      p,(-)   \\n \\n«       exp(r(-)/a)p?\\n\"°(-)),      denoting      the      induced      distribution      as      Q*™.</p><p>Informal      characterization      of      Q™°*.\\nInformally,      Q*\"°      is      introduced      as      follows.\\nWhen      discretizing  \\n \\n  the      time      step,      by      denoting      the      distribution      induced      by      the      reference      SDE      as      p™\\\\\\\\\\\\\\'(xp,-+-   \\n \\n,      9),      we define Qi\"      (xr,   \\n \\na   \\n \\n,      0)   \\n \\n=      p(x,      nr      Ly   \\n \\n|      L0)Pr      (Xo).</p><p>The      distribution      Q*°      above      is   \\n \\na      limiting      distribution      obtained      by      making      the      discretization      step small.</p><p>Training.\\nNow,      we      introduce   \\n \\na      new      SDE      parameterized      by   \\n \\n@      such      as      an      neural      network      to      align  \\n \\n  the      induced      distribution      P’      with      Q®\"*.\\nActually,      in      the      continuous      formulation,      the      RL-problem      (18) in      Section      4.2      is      equal      to      solving argmin      KL(P?||Q\"™).\\n \\n \\n  0 This      formalization      has      been      presented      in      Uehara      et      al.\\n(2024).\\nIntuitively,      as      we      see      in      Theorem      3,  \\n \\n  this      is      expected      because      we      see      that      the      posterior      distribution      induced      by      pre-trained      models:</p><p>ref   \\n \\n(      PD      (ar,-++      21   \\n \\n|      to)      (=      vey      (er|er_-1)p      Pp      (ar-1   \\n \\n|      er_2)      +++      pp\"      (x1   \\n \\n|      20), remains      preserved      after      fine-tuning      based      on      the      RL-formulation      (18).</p><p> \\n \\n  Comparison      with      flow-based      training.\\nIn      contrast      to      flow-based      training,      RL-based      fine-tuning  \\n \\n  minimizes      the      inverse      KL      divergence      rather      than      the      KL      divergence.\\nIntuitively,      this      is      because,  \\n \\n  unlike      Qt™*      or      Q*>   \\n \\n,      we      cannot      sample      from      Q*”      (recall      that   \\n \\na      marginal      distribution      at      time   \\n \\n0      in  \\n \\n  Qi,      i.e.,      p,,      is      unnormalized);      on      the      other      hand,      the      marginal      distributions      at      time   \\n \\n0      in      both      Q\\\\\\\\\\\\\\'™*  \\n \\n  and      Q*°°>      are      data      distributions.</p><p> \\n \\n  Remark   \\n \\n7      (Extension      to      f-divergence).\\nEach      of      these      methods      can      be      technically      extended      by  \\n \\n  incorporating      more      general      divergences      beyond      KL      divergence.\\nFor      instance,      in      the      context      of  \\n \\n  fine-tuning,      please      see      Tang      (2024).</p><p>10      Connection      with      Sampling      from      Unnormalized      Distributions</p><p> \\n \\n  Numerous      studies      delve      into      sampling      from      an      unnormalized      distribution      proportional      to      exp(r()),  \\n \\n  commonly      known      as      the      Gibbs      distribution,      extensively      discussed      in      the      computational      statistics  \\n \\n  and      statistical      physics      community      (Robert,      2014).\\nThis      issue      is      pertinent      to      our      work,      as      during  \\n \\n  fine-tuning,      the      target      distribution      is      also      formulated      in      the      Gibbs      distribution      form      in      (10),      which      is  \\n \\n  proportional      to      exp(r(a))p?\\n\"*      (x).</p><p> \\n \\n  The      literature      employs      two      main      approaches      to      tackle      this      challenge:      Markov      Chain      Monte  \\n \\n  Carlo      (MCMC)      and      RL-based      methods.\\nIn      particular,      the      RL-based      diffusion      model      fine-tuning  \\n \\n  we      have      discussed      so      far      is      closely      related      to      the      latter      approach.\\nIn      this      section,      we      explain      and  \\n \\n  compare      these      two      approaches.</p><h10>10.1      Markov      Chain      Monte      Carlo      (MCMC)</h10><p>CMC)  \\n \\n  In      MCMC,   \\n \\na      Markov      chain      is      constructed      to      approximate   \\n \\na      target      distribution      such      that      the      equilib-  \\n \\n  rium      distribution      of      the      Markov      chain      converges      to      the      target      distribution.\\nHowever,      dealing      with  \\n \\n  high-dimensional      domain      spaces      is      challenging      for      MCMC.\\n \\n \\nA      common      strategy      involves      leveraging  \\n \\n  gradient      information,      such      as      the      Metropolis-adjusted      Langevin      algorithm      (MALA)      (Besag,      1994)  \\n \\n  or      Hamiltonian      Monte      Carlo      (HMC)      (Neal      et      al.,      2011;      Girolami      and      Calderhead,      2011).\\nMALA  \\n \\n  shares      similarities      with      classifier      guidance      and      value-weighted      sampling,      as      both      methods      depend  \\n \\n  on      the      first-order      information      from      reward      (or      value)      functions.</p><p> \\n \\n  Can      we      use      MCMC      for      fine-tuning      diffusion      models?\\nIn      the      context      of      fine-tuning      dif-  \\n \\n  fusion      models,      while      it      may      seem      intuitive      to      sample      from      the      target      distribution      (10)      (i.e.,  \\n \\n  Dy   \\n \\n&      exp(r(x))pP\"*(x))      using      MCMC,      it’s      not      straightforward      for      the      following      reasons:</p><p>¢\\n \\n   We      lack      of      an      analytical      form      of      p?\\n\"*(-).<ul><li>*\\n \\n   Additionally,      even      if      we      can      estimate      p?\\n’°(-)      in      an      unbiased      manner,      as      in      Chen      et      al.\\n(2018),  \\n \\n  the      mixing      time      for      obtaining   \\n \\na      single      sample      in      MCMC      might      be      lengthy.</li></ul></p><p> \\n \\n  Therefore,      we      explore      creating   \\n \\na      generative      model      (i.e.,      diffusion      model)      for      this      task      using      RL-  \\n \\n  based      fine-tuning      so      that      we      can      easily      sample      from      p,(-)      during      inference      (i.e.,      simulating      the  \\n \\n  time-reversal      SDE      (2)      with      policies      from   \\n \\n¢   \\n \\n=      T\\\\\\\\\\\\\\'+   \\n \\n1      to   \\n \\nt   \\n \\n=      1),      without      relying      on      MCMC.\\nThis  \\n \\n  addresses      the      concern      of      MCMC      by      avoiding      the      need      to      estimate      p?\\n\"*(-)      and      minimizing      inference  \\n \\n  time,      albeit      at      the      expense      of      increased      training      time.</p><p>10.2}      RL-Based      Approaches  \\n \\n  While      MCMC      has      historically      been      widely      used      for      this      purpose,      recent      works      have      proposed      an  \\n \\n  RL-based      approach      or      its      variant      (e.g.,      Bernton      et      al.      (2019);      Heng      et      al.\\n(2020);      Huang      et      al.\\n(2023);</p><p> \\n \\n  Vargas      et      al.\\n(2023)).\\nFor      example,   \\n \\na      seminal      study      (Zhang      and      Chen,      2021)      introduced   \\n \\na      method  \\n \\n  very      similar      to      reward      backpropagation.\\nIn      their      work,      they      initially      define      Brownian      motion      as   \\n \\na       reference      SDE      (from      7’      to      0),      similar      to      the      pre-trained      diffusion      models      in      our      context.\\nThen,      by  \\n \\n  appropriately      setting      rewards,      Zhang      and      Chen      (2021)      aims      to      learn   \\n \\na      new      SDE      (from      T\\\\\\\\\\\\\\'      to      0)      such  \\n \\n  that      we      can      sample      from      the      target      distribution      at      the      endpoint      0.</p><p> \\n \\n  These      approaches      offer      advantages      over      MCMC      due      to      their      ability      to      minimize      inference      time  \\n \\n  (amortization)      by      constructing   \\n \\na      generative      model      comprising      policies      in      the      training      time      and      solely  \\n \\n  executing      learned      policies      during      inference.\\nUnlike      MCMC,      these      methods      significantly      reduce      the  \\n \\n  inference      cost      as      we      don’t      need      to      be      concerned      about      the      mixing      time.\\nAdditionally,      RL-based  \\n \\n  approaches      have      the      potential      to      effectively      handle      high-dimensional,      complex      (i.e.,      multi-modal)  \\n \\n  distributions      by      harnessing      the      considerable      expressive      power      arising      from      latent      states.</p><p> \\n \\n  Despite      these      advantages,      it      is      often      unclear      how      to      select   \\n \\na      reference      SDE      in      these      works.\\nIn  \\n \\n  contrast,      in      RL-based      fine-tuning      of      diffusion      models,      we      directly      leverage      pre-trained      diffusion  \\n \\n  models      as      the      reference      SDEs,      i.e.,      as      our      prior      knowledge.</p><p> \\n \\n  Key      message.\\nFinally,      it      is      worth      noting      that      we      can      technically      utilize      any      off-the-shelf      RL      algo-  \\n \\n  rithms      for      training.\\nFor      example,      Zhang      and      Chen      (2021)      employs      reward      backpropagation,      while  \\n \\n  works      in      Gflownets      typically      leverage      PCL,      as      we      mention      in      Section      6.3.\\nHowever,      additionally,      it  \\n \\n  is      possible      to      use      PPO      or      reward-weighted      MLE      mentioned      in      Section      4.2.</p><p>11      Closely      Related      Directions</p><p>Lastly,      we      highlight      several      closely      related      directions      we      have      not      yet      discussed.</p><p> \\n \\n  Aligning      text-to-image      models.\\nWe      recognize      that      current      efforts      in      fine-tuning      diffusion      models  \\n \\n  primarily      revolve      around      aligning      text-to-image      models      using      human      feedback      (Lee      et      al.,      2023;  \\n \\n  Wallace      et      al.,      2023;      Wu      et      al.,      2023;      Yang      et      al.,      2023).\\nIn      this      paper,      our      goal      is      to      consider   \\n \\na      more  \\n \\n  general      and      fundamental      formulation      that      is      not      tailored      to      any      particular      task.</p><p> \\n \\n  Diffusion      models      for      RL.\\nDiffusion      models      are      well-suited      for      specific      reinforcement      learning  \\n \\n  applications      due      to      their      ability      to      model      complex      and      multimodal      distributions      as      polices      (Janner  \\n \\n  et      al.,      2022;      Ajay      et      al.,      2023;      Wang      et      al.,      2022;      Hansen-Estruch      et      al.,      2023;      Du      et      al.,      2024;      Zhu  \\n \\n  et      al.,      2023).\\nFor      example,      Wang      et      al.\\n(2022)      use      conditional      diffusion      models      as      policies      and  \\n \\n  demonstrate      their      effectiveness      in      typical      offline      RL      benchmarks.\\nWhile      RL-based      fine-tuning      also  \\n \\n  aims      to      maximize      certain      reward      functions,      the      primary      focus      of      this      tutorial      covers      methods      that  \\n \\n  leverage      pre-trained      diffusion      models      for      this      purpose.</p><p> \\n \\n  RL      from      human      feedback      (RLHF)      for      language      models.\\nRLHF      is      widely      discussed      in      the  \\n \\n  context      of      language      models,      where      many      algorithms      share      similarities      with      those      used      in      diffusion  \\n \\n  models      (Ouyang      et      al.,      2022;      Bai      et      al.,      2022;      Casper      et      al.,      2023).\\nFor      instance,      both      domains  \\n \\n  commonly      employ      PPO      as   \\n \\na      standard      approach,      and      reward-weighted      training      (or      reward-weighted  \\n \\n  MLE)      often      serves      as   \\n \\na      basic      baseline      approach.\\nDespite      these      similarities,      nuanced      differences  \\n \\n  exist      mainly      because      diffusion      models      typically      operate      in   \\n \\na      continuous      input      space      rather      than  \\n \\n  a      discrete      one.\\nMoreover,      techniques      like      value-weighted      sampling      emerge      uniquely      within      the  \\n \\n  context      of      diffusion      models.</p><p>12      Summary</p><p> \\n \\n  In      this      article,      we      comprehensively      explain      how      fine-tuning      diffusion      models      to      maximize      down-  \\n \\n  stream      reward      functions      can      be      formalized      as   \\n \\na      reinforcement      learning      (RL)      problem      in      entropy-  \\n \\n  regularized      Markov      decision      processes      (MDPs).\\nBased      on      this      viewpoint,      we      elaborate      on      the  \\n \\n  application      of      various      RL      algorithms,      such      as      PPO      and      reward-weighted      MLE,      specifically      tailored  \\n \\n  for      fine-tuning      diffusion      models.\\nAdditionally,      we      categorize      different      scenarios      based      on      how  \\n \\n  reward      feedback      is      obtained.\\nWhile      this      categorization      is      not      always      explicitly      mentioned      in      many  \\n \\n  existing      works,      it      is      crucial      when      selecting      appropriate      algorithms.\\nFinally,      we      discuss      the      rela-  \\n \\n  tionships      with      several      related      topics,      including      classifier      guidance,      Gflownets,      path      integral      control  \\n \\n  theory,      and      sampling      from      unnormalized      distributions.</p><h4>References</h4><p>Agarwal,      A.,      N.      Jiang,      S.      M.      Kakade,      and      W.      Sun      (2019).\\nReinforcement      learning:      Theory      and  \\n \\n  algorithms.\\nCS      Dept.,      UW      Seattle,      Seattle,      WA,      USA,      Tech.\\nRep,      10-4.</p><p>Agarwal,      V.      and      D.      R.      Kelley      (2022).\\nThe      genetic      and      biochemical      determinants      of      mrna      degradation  \\n \\n  rates      in      mammals.\\nGenome      biology      23(1),      245.</p><p> \\n \\n  Ajay,      A.,      Y.      Du,      A.      Gupta,      J.      B.      Tenenbaum,      T.      S.      Jaakkola,      and      P.      Agrawal      (2023).\\nIs      conditional  \\n \\n  generative      modeling      all      you      need      for      decision      making?\\nIn      The      Eleventh      International      Conference  \\n \\n  on      Learning      Representations.</p><p>Albergo,      M.      S.      and      E.      Vanden-Eijnden      (2022).\\nBuilding      normalizing      flows      with      stochastic      inter-  \\n \\n  polants.\\narXiv      preprint      arXiv:2209.15571.</p><p>Anderson,      B.      D.      (1982).\\nReverse-time      diffusion      equation      models.\\nStochastic      Processes      and      their  \\n \\n  Applications      12(3),      313-326.</p><p> \\n \\n  Austin,      J.,      D.      D.      Johnson,      J.      Ho,      D.      Tarlow,      and      R.      Van      Den      Berg      (2021).\\nStructured      denoising  \\n \\n  diffusion      models      in      discrete      state-spaces.\\nAdvances      in      Neural      Information      Processing      Systems      34,  \\n \\n  17981-17993.</p><p>Avdeyey,      P.,      C.      Shi,      Y.      Tan,      K.      Dudnyk,      and      J.      Zhou      (2023).\\nDirichlet      diffusion      score      model      for  \\n \\n  biological      sequence      generation.\\narXiv      preprint      arXiv:2305.\\n10699.</p><p>Bai,      Y.,      S.      Kadavath,      S.      Kundu,      A.      Askell,      J.      Kernion,      A.      Jones,      A.      Chen,      A.      Goldie,      A.      Mirhoseini, C.      McKinnon,      et      al.\\n(2022).\\nConstitutional      ai:      Harmlessness      from      ai      feedback.\\narXiv      preprint  \\n \\n  arXiv:2212.08073.</p><p> \\n \\n  Bansal,      A.,      H.-M.\\nChu,      A.      Schwarzschild,      S.      Sengupta,      M.      Goldblum,      J.      Geiping,      and      T.      Goldstein  \\n \\n  (2023).\\nUniversal      guidance      for      diffusion      models.\\nIn      Proceedings      of      the      IEEE/CVF      Conference  \\n \\n  on      Computer      Vision      and      Pattern      Recognition,      pp.\\n843-852.</p><p>Bengio,      Y.,      S.      Lahlou,      T.      Deleu,      E.      J.      Hu,      M.      Tiwari,      and      E.      Bengio      (2023).\\nGflownet      foundations.\\n \\n \\n  Journal      of      Machine      Learning      Research      24(210),      1-55.</p><p> \\n \\n  Benton,      J.,      Y.      Shi,      V.      De      Bortoli,      G.      Deligiannidis,      and      A.      Doucet      (2024).\\nFrom      denoising      diffusions  \\n \\n  to      denoising      markov      models.\\nJournal      of      the      Royal      Statistical      Society      Series      B:      Statistical  \\n \\n  Methodology      86(2),      286-301.</p><p>Bernton,      E.,      J.      Heng,      A.      Doucet,      and      P.      E.      Jacob      (2019).\\nSchr\\\\\\\\\\\\\\\\””      odinger      bridge      samplers.\\narXiv  \\n \\n  preprint      arXiv:      1912.13170.</p><p>Besag,      J.\\n(1994).\\nComments      on      “representations      of      knowledge      in      complex      systems”      by      u.      grenander  \\n \\n  and      mi      miller.\\nJ.      Roy.\\nStatist.\\nSoc.      Ser.\\n \\n \\nB      56(591-592),      4.</p><p>Black,      K.,      M.      Janner,      Y.      Du,      I.      Kostrikov,      and      S.      Levine      (2023).\\nTraining      diffusion      models      with  \\n \\n  reinforcement      learning.\\narXiv      preprint      arXiv:2305.13301.</p><p> \\n \\n  Campbell,      A.,      J.      Benton,      V.      De      Bortoli,      T.      Rainforth,      G.      Deligiannidis,      and      A.      Doucet      (2022).\\n \\n \\nA       continuous      time      framework      for      discrete      denoising      models.\\nAdvances      in      Neural      Information  \\n \\n  Processing      Systems      35,      28266-28279.</p><p> \\n \\n  Campbell,      A.,      J.      Yim,      R.      Barzilay,      T.      Rainforth,      and      T.      Jaakkola      (2024).\\nGenerative      flows      on  \\n \\n  discrete      state-spaces:      Enabling      multimodal      flows      with      applications      to      protein      co-design.\\narXiv  \\n \\n  preprint      arXiv:2402.04997.</p><p>Cao,      H.,      C.      Tan,      Z.      Gao,      Y.      Xu,      G.      Chen,      P.-A.\\nHeng,      and      S.      Z.      Li      (2024).\\n \\n \\nA      survey      on      generative  \\n \\n  diffusion      models.\\n[EEE      Transactions      on      Knowledge      and      Data      Engineering.</p><p>Casper,      S.,      X.      Davies,      C.      Shi,      T.      K.      Gilbert,      J.      Scheurer,      J.      Rando,      R.      Freedman,      T.      Korbak, D.      Lindner,   \\n \\nP P.      Freire,      et      al.\\n(2023).\\nOpen      problems      and      fundamental      limitations      of      reinforcement  \\n \\n  learning      from      human      feedback.\\narXiv      preprint      arXiv:2307.15217.</p><p>Castillo-Hair,      S.      M.      and      G.      Seelig      (2021).\\nMachine      learning      for      designing      next-generation      mrna  \\n \\n  therapeutics.\\nAccounts      of      Chemical      Research      55(1),      24-34.</p><p>Chang,      J.      D.,      M.      Uehara,      D.      Sreenivas,      R.      Kidambi,      and      W.      Sun      (2021).\\nMitigating      covariate      shift  \\n \\n  in      imitation      learning      via      offline      data      without      great      coverage.\\narXiv      preprint      arXiv:2106.03207.</p><p>Chen,      M.,      S.      Mei,      J.      Fan,      and      M.      Wang      (2024).\\nAn      overview      of      diffusion      models:      Applications,  \\n \\n  guided      generation,      statistical      rates      and      optimization.\\narXiv      preprint      arXiv:2404.07771.</p><p>Chen,      R.      T.,      Y.      Rubanova,      J.      Bettencourt,      and      D.      K.      Duvenaud      (2018).\\nNeural      ordinary      differential  \\n \\n  equations.\\nAdvances      in      neural      information      processing      systems      31.</p><p>Chung,      H.,      J.      Kim,      M.      T.      Mccann,      M.      L.      Klasky,      and      J.      C.      Ye      (2022).\\nDiffusion      posterior      sampling  \\n \\n  for      general      noisy      inverse      problems.\\narXiv      preprint      arXiv:2209.\\n14687.</p><p> \\n \\n  Chung,      H.,      B.      Sim,      D.      Ryu,      and      J.      C.      Ye      (2022).\\nImproving      diffusion      models      for      inverse      problems  \\n \\n  using      manifold      constraints.\\nAdvances      in      Neural      Information      Processing      Systems      35,      25683-—  \\n \\n  25696.</p><p>Clark,      K.,      P.      Vicol,      K.      Swersky,      and      D.      J.      Fleet      (2023).\\nDirectly      fine-tuning      diffusion      models      on  \\n \\n  differentiable      rewards.\\narXiv      preprint      arXiv:2309.\\n17400.</p><p> \\n \\n  De      Bortoli,      V.,      E.      Mathieu,      M.      Hutchinson,      J.      Thornton,      Y.      W.      Teh,      and      A.      Doucet      (2022).\\nRieman-  \\n \\n  nian      score-based      generative      modelling.\\nAdvances      in      Neural      Information      Processing      Systems      35,  \\n \\n  2406-2422.</p><p>Deleu,      T.,      P.      Nouri,      N.      Malkin,      D.      Precup,      and      Y.      Bengio      (2024).\\nDiscrete      probabilistic      inference      as  \\n \\n  control      in      multi-path      environments.\\narXiv      preprint      arXiv:2402.10309.</p><p>Dhariwal,      P.      and      A.      Nichol      (2021).\\nDiffusion      models      beat      gans      on      image      synthesis.\\nAdvances      in  \\n \\n  neural      information      processing      systems      34,      8780-8794.</p><p> \\n \\n  Du,      Y.,      S.      Yang,      B.      Dai,      H.      Dai,      O.      Nachum,      J.      Tenenbaum,      D.      Schuurmans,      and      P.      Abbeel      (2024).\\n \\n \\n  Learning      universal      policies      via      text-guided      video      generation.\\nAdvances      in      Neural      Information  \\n \\n  Processing      Systems      36.</p><p>Fan,      Y.,      O.      Watkins,      Y.      Du,      H.      Liu,      M.      Ryu,      C.      Boutilier,      P.      Abbeel,      M.      Ghavamzadeh,      K.      Lee,      and K.      Lee      (2023) 023).\\nDPOK:      Reinforcement      learning      for      fine-tuning      text-to-image      diffusion      models.\\n \\n \\n  arXiv      preprint      arXiv:2305.16381.</p><p>Finn,      C.,      S.      Levine,      and      P.      Abbeel      (2016).\\nGuided      cost      learning:      Deep      inverse      optimal      control      via  \\n \\n  policy      optimization.\\nIn      International      conference      on      machine      learning,      pp.\\n49-58.\\nPMLR.</p><p>Fox,      R.,      A.      Pakman,      and      N.      Tishby      (2015).\\nTaming      the      noise      in      reinforcement      learning      via      soft  \\n \\n  updates.\\narXiv      preprint      arXiv:      1512.08562.</p><p>Frey,      N.      C.,      D.      Berenberg,      K.      Zadorozhny,      J.      Kleinhenz,      J.      Lafrance-Vanasse,      I.      Hotzel,      Y.      Wu, S.      Ra,   \\n \\nR R.      Bonneau,      K.      Cho,      et      al.\\n(2023).\\nProtein      discovery      with      discrete      walk-jump      sampling.\\n \\n \\n  arXiv      preprint      arXiv:2306.\\n12360.</p><p>Geist,      M.,      B.      Scherrer,      and      O.      Pietquin      (2019).\\n \\n \\nA      theory      of      regularized      markov      decision      processes.\\n \\n \\n  In      International      Conference      on      Machine      Learning,      pp.\\n2160-2169.\\nPMLR.</p><p> \\n \\n  Girolami,      M.      and      B.      Calderhead      (2011).\\nRiemann      manifold      langevin      and      hamiltonian      monte  \\n \\n  carlo      methods.\\nJournal      of      the      Royal      Statistical      Society      Series      B:      Statistical      Methodology      73(2),  \\n \\n  123-214.</p><p>Gosai,      S.      J.,      R.      I.      Castro,      N.      Fuentes,      J.      C.      Butts,      S.      Kales,      R.      R.      Noche,      K.      Mouri,      P.      C.      Sabeti, S.   \\n \\nK K.      Reilly,      and      R.      Tewhey      (2023).\\nMachine-guided      design      of      synthetic      cell      type-specific  \\n \\n  cis-regulatory      elements.\\nbioRxiv.</p><p>Haarnoja,      T.,      H.      Tang,      P.      Abbeel,      and      S.      Levine      (2017).\\nReinforcement      learning      with      deep      energy-  \\n \\n  based      policies.\\nIn      International      conference      on      machine      learning,      pp.\\n1352-1361.\\nPMLR.</p><p>Hansen-Estruch,      P.,      I.      Kostrikov,      M.      Janner,      J.      G.      Kuba,      and      S.      Levine      (2023).\\nIdql:      Implicit  \\n \\n  q-learning      as      an      actor-critic      method      with      diffusion      policies.\\narXiv      preprint      arXiv:2304.10573.</p><p>Heng,      J.,      A.      N.      Bishop,      G.      Deligiannidis,      and      A.      Doucet      (2020).\\nControlled      sequential      monte      carlo.</p><p>Ho,      J.,      A.      Jain,      and      P.      Abbeel      (2020).\\nDenoising      diffusion      probabilistic      models.\\nAdvances      in      neural  \\n \\n  information      processing      systems      33,      6840-6851.</p><p>Ho,      J.      and      T.      Salimans      (2022).\\nClassifier-free      diffusion      guidance.\\narXiv      preprint      arXiv:2207.12598.</p><p>Ho,      J.,      T.      Salimans,      A.      Gritsenko,      W.      Chan,      M.      Norouzi,      and      D.      J.      Fleet      (2022).\\nVideo      diffusion  \\n \\n  models.\\nAdvances      in      Neural      Information      Processing      Systems      35,      8633-8646.</p><p> \\n \\n  Hoogeboom,      E.,      V.      G.      Satorras,      C.      Vignac,      and      M.      Welling      (2022).\\nEquivariant      diffusion      for  \\n \\n  molecule      generation      in      3d.\\nIn      International      conference      on      machine      learning,      pp.\\n8867-8887.</p><h4>      PMLR.</h4><p>Huang,      X.,      H.      Dong,      H.      Yifan,      Y.      Ma,      and      T.      Zhang      (2023).\\nReverse      diffusion      monte      carlo.\\nIn      The  \\n \\n  Twelfth      International      Conference      on      Learning      Representations.</p><p>Janner,      M.,      Y.      Du,      J.      B.      Tenenbaum,      and      S.      Levine      (2022).\\nPlanning      with      diffusion      for      flexible  \\n \\n  behavior      synthesis.\\narXiv      preprint      arXiv:2205.09991.</p><p> \\n \\n  Jo,      J.,      S.      Lee,      and      S.      J.      Hwang      (2022).\\nScore-based      generative      modeling      of      graphs      via      the      system  \\n \\n  of      stochastic      differential      equations.\\nIn      International      Conference      on      Machine      Learning,      pp.\\n \\n \\n  10362-10383.\\nPMLR.</p><p> \\n \\n  Kazim,      M.,      J.      Hong,      M.-G.\\nKim,      and      K.-K.\\nK.      Kim      (2024).\\nRecent      advances      in      path      integral      control  \\n \\n  for      trajectory      optimization:      An      overview      in      theoretical      and      algorithmic      perspectives.\\nAnnual  \\n \\n  Reviews      in      Control      57,      100931.</p><p>Krishnamoorthy,      S.,      S.      M.      Mashkaria,      and      A.      Grover      (2023).\\nDiffusion      models      for      black-box  \\n \\n  optimization.\\narXiv      preprint      arXiv:2306.07180.</p><p>Kumar,      A.,      A.      Zhou,      G.      Tucker,      and      S.      Levine      (2020).\\nConservative      q-learning      for      offline      reinforce-  \\n \\n  ment      learning.\\nAdvances      in      Neural      Information      Processing      Systems      33,      1179-1191.</p><p>Lal,      A.,      D.      Garfield,      T.      Biancalani,      and      G.      Eraslan      (2024).\\nreglm:      Designing      realistic      regulatory  \\n \\n  dna      with      autoregressive      language      models.\\nbioRxiv,      2024-02.</p><p>Lee,      K.,      H.      Liu,      M.      Ryu,      O.      Watkins,      Y.      Du,      C.      Boutilier,      P.      Abbeel,      M.      Ghavamzadeh,      and      S.      S.      Gu  \\n \\n  (2023).\\nAligning      text-to-image      models      using      human      feedback.\\narXiv      preprint      arXiv:2302.12192.</p><p>Levine,      S.      (2018).\\nReinforcement      learning      and      control      as      probabilistic      inference:      Tutorial      and  \\n \\n  review.\\narXiv      preprint      arXiv:      1805.00909.</p><p>Li,      Z.,      Y.      Ni,      T.      A.      B.      Huygelen,      A.      Das,      G.      Xia,      G.-B.\\nStan,      and      Y.      Zhao      (2023).\\nLatent      diffusion  \\n \\n  model      for      dna      sequence      generation.\\narXiv      preprint      arXiv:2310.06150.</p><p>Lipman,      Y.,      R.      T.      Chen,      H.      Ben-Hamu,      M.      Nickel,      and      M.      Le      (2023).\\nFlow      matching      for      generative  \\n \\n  modeling.\\nJCLR      2023.</p><p>Liu,      X.,      C.      Gong,      and      Q.      Liu      (2022).\\nFlow      straight      and      fast:      Learning      to      generate      and      transfer      data  \\n \\n  with      rectified      flow.\\narXiv      preprint      arXiv:2209.03003.</p><p>Liu,      X.,      L.      Wu,      M.      Ye,      and      Q.      Liu      (2022).\\nLet      us      build      bridges:      Understanding      and      extending  \\n \\n  diffusion      generative      models.\\narXiv      preprint      arXiv:2208.14699.</p><p>Lou,      A.,      C.      Meng,      and      S.      Ermon      (2023).\\nDiscrete      diffusion      language      modeling      by      estimating      the  \\n \\n  ratios      of      the      data      distribution.\\narXiv      preprint      arXiv:2310.16834.</p><p>Malkin,      N.,      M.      Jain,      E.      Bengio,      C.      Sun,      and      Y.      Bengio      (2022).\\nTrajectory      balance:      Improved      credit  \\n \\n  assignment      in      gflownets.\\nAdvances      in      Neural      Information      Processing      Systems      35,      5955-5967.</p><p>Mohammadpour,      S.,      E.      Bengio,      E.      Frejinger,      and      P.-L.\\nBacon      (2023).\\nMaximum      entropy      gflownets  \\n \\n  with      soft      q-learning.\\narXiv      preprint      arXiv:2312.14331.</p><p>Nachum,      O.,      M.      Norouzi,      K.      Xu,      and      D.      Schuurmans      (2017).\\nBridging      the      gap      between      value      and  \\n \\n  policy      based      reinforcement      learning.\\nAdvances      in      neural      information      processing      systems      30.</p><p>Neal,      R.      M.      et      al.\\n(2011).\\nMcmce      using      hamiltonian      dynamics.\\nHandbook      of      markov      chain      monte  \\n \\n  carlo      2(11),      2.</p><p>Neu,      G.,      A.      Jonsson,      and      V.      Gémez      (2017).\\n \\n \\nA      unified      view      of      entropy-regularized      markov      decision  \\n \\n  processes.\\narXiv      preprint      arXiv:      1705.07798.</p><p>Ouyang,      L.,      J.      Wu,      X.      Jiang,      D.      Almeida,      C.      Wainwright,      P.      Mishkin,      C.      Zhang,      S.      Agarwal, K.      Slama,      A A.      Ray,      et      al.\\n(2022).\\nTraining      language      models      to      follow      instructions      with      human  \\n \\n  feedback.\\nAdvances      in      Neural      Information      Processing      Systems      35,      27730-27744.</p><p>Pattanaik,      L.      and      C.      W.      Coley      (2020).\\nMolecular      representation:      going      long      on      fingerprints.\\n \\n \\n  Chem      6(6),      1204-1207.</p><p>Peng,      X.      B.,      A.      Kumar,      G.      Zhang,      and      S.      Levine      (2019).\\nAdvantage-weighted      regression:      Simple  \\n \\n  and      scalable      off-policy      reinforcement      learning.\\narXiv      preprint      arXiv:1910.00177.</p><p>Peters,      J.,      K.      Mulling,      and      Y.      Altun      (2010).\\nRelative      entropy      policy      search.\\nIn      Proceedings      of      the  \\n \\n  AAAI      Conference      on      Artificial      Intelligence,      Volume      24,      pp.\\n1607-1612.</p><p> \\n \\n  Podell,      D.,      Z.      English,      K.      Lacey,      A.      Blattmann,      T.      Dockhorn,      J.      Miiller,      J.      Penna,      and      R.      Rombach  \\n \\n  (2023).\\nSdxl:      Improving      latent      diffusion      models      for      high-resolution      image      synthesis.\\narXiv  \\n \\n  preprint      arXiv:2307.01952.</p><p>Prabhudesai,      M.,      A.      Goyal,      D.      Pathak,      and      K.      Fragkiadaki      (2023).\\nAligning      text-to-image      diffusion  \\n \\n  models      with      reward      backpropagation.\\narXiv      preprint      arXiv:2310.03739.</p><p>Ramesh,      A.,      P.      Dhariwal,      A.      Nichol,      C.      Chu,      and      M.      Chen      (2022).\\nHierarchical      text-conditional  \\n \\n  image      generation      with      clip      latents.\\narXiv      preprint      arXiv:2204.06125      1(2),      3.</p><p>Rigter,      M.,      B.      Lacerda,      and      N.      Hawes      (2022).\\nRambo-rl:      Robust      adversarial      model-based      offline  \\n \\n  reinforcement      learning.\\nAdvances      in      neural      information      processing      systems      35,      16082-16097.</p><p>Robert,      C.      (2014).\\nStatistical      modeling      and      computation.</p><p>Rogers,      L.      C.      G.      and      D.      Williams      (2000).\\nDiffusions,      Markov      processes      and      martingales:      Volume      2,  \\n \\n  It6é      calculus,      Volume      2.\\nCambridge      university      press.</p><p> \\n \\n  Rombach,      R.,      A.      Blattmann,      D.      Lorenz,      P.      Esser,      and      B.      Ommer      (2022,      June).\\nHigh-resolution  \\n \\n  image      synthesis      with      latent      diffusion      models.\\nIn      Proceedings      of      the      IEEE/CVF      Conference      on  \\n \\n  Computer      Vision      and      Pattern      Recognition      (CVPR),      pp.\\n10684-10695.</p><p>Sarkar,      A.,      Z.      Tang,      C.      Zhao,      and      P.      Koo      (2024).\\nDesigning      dna      with      tunable      regulatory      activity  \\n \\n  using      discrete      diffusion.\\nbioRxiv,      2024-05.</p><p>Schulman,      J.,      X.      Chen,      and      P.      Abbeel      (2017).\\nEquivalence      between      policy      gradients      and      soft  \\n \\n  q-learning.\\narXiv      preprint      arXiv:      1704.06440.</p><p>Schulman,      J.,      S.      Levine,      P.      Abbeel,      M.      Jordan,      and      P.      Moritz      (2015).\\nTrust      region      policy      optimization.\\n \\n \\n  In      International      conference      on      machine      learning,      pp.\\n1889-1897.\\nPMLR.</p><p>Schulman,      J.,      F.      Wolski,      P.      Dhariwal,      A.      Radford,      and      O.      Klimov      (2017).\\nProximal      policy      optimiza-  \\n \\n  tion      algorithms.\\narXiv      preprint      arXiv:      1707.06347.</p><p>Shi,      Y.,      V.      De      Bortoli,      A.      Campbell,      and      A.      Doucet      (2024).\\nDiffusion      schrédinger      bridge      matching.\\n \\n \\n  Advances      in      Neural      Information      Processing      Systems      36.</p><p> \\n \\n  Sohl-Dickstein,      J.,      E.      Weiss,      N.      Maheswaranathan,      and      S.      Ganguli      (2015).\\nDeep      unsupervised  \\n \\n  learning      using      nonequilibrium      thermodynamics.\\nIn      /nternational      conference      on      machine      learning,  \\n \\n  pp.\\n2256-2265.\\nPMLR.</p><p>Song,      J.,      C.      Meng,      and      S.      Ermon      (2020).\\nDenoising      diffusion      implicit      models.\\narXiv      preprint  \\n \\n  arXiv:2010.02502.</p><p>Song,      Y.,      C.      Durkan,      I.      Murray,      and      S.      Ermon      (2021).\\nMaximum      likelihood      training      of      score-based  \\n \\n  diffusion      models.\\nAdvances      in      neural      information      processing      systems      34,      1415-1428.</p><p>Stark,      H.,      B.      Jing,      C.      Wang,      G.      Corso,      B.      Berger,      R.      Barzilay,      and      T.      Jaakkola      (2024).\\nDirichlet  \\n \\n  flow      matching      with      applications      to      dna      sequence      design.\\narXiv      preprint      arXiv:2402.05841.</p><p>Sutton,      R.      S.      and      A.      G.      Barto      (2018).\\nReinforcement      learning:      An      introduction.\\nMIT      press.</p><p>Tang,      W.      (2024).\\nFine-tuning      of      diffusion      models      via      stochastic      control:      entropy      regularization      and  \\n \\n  beyond.\\narXiv      preprint      arXiv:2403.06279.</p><p>Tang,      W.      and      H.      Zhao      (2024).\\nScore-based      diffusion      models      via      stochastic      differential      equations—a  \\n \\n  technical      tutorial.\\narXiv      preprint      arXiv:2402.07487.</p><p>Theodorou,      E.,      J.      Buchli,      and      S.      Schaal      (2010).\\n \\n \\nA      generalized      path      integral      control      approach      to  \\n \\n  reinforcement      learning.\\nThe      Journal      of      Machine      Learning      Research      11,      3137-3181.</p><p>Tiapkin,      D.,      N.      Morozov,      A.      Naumov,      and      D.      Vetrov      (2023).\\nGenerative      flow      networks      as      entropy-  \\n \\n  regularized      rl.\\narXiv      preprint      arXiv:2310.12934.</p><p> \\n \\n  Tong,      A.,      N.      Malkin,      G.      Huguet,      Y.      Zhang,      J.      Rector-Brooks,      K.      Fatras,      G.      Wolf,      and      Y.      Bengio  \\n \\n  (2023).\\nConditional      flow      matching:      Simulation-free      dynamic      optimal      transport.\\narXiv      preprint  \\n \\n  arXiv:2302.00482.</p><p>Uehara,      M.      and      W.      Sun      (2021).\\nPessimistic      model-based      offline      reinforcement      learning      under  \\n \\n  partial      coverage.\\narXiv      preprint      arXiv:2107.06226.</p><p>Uehara,      M.,      Y.      Zhao,      K.      Black,      E.      Hajiramezanali,      G.      Scalia,      N.      L.      Diamant,      A.      M.      Tseng, T.      Biancalani,      and      S S.      Levine      (2024).\\nFine-tuning      of      continuous-time      diffusion      models      as      entropy-  \\n \\n  regularized      control.\\narXiv      preprint      arXiv:2402.15194.</p><p> \\n \\n  Uehara,      M.,      Y.      Zhao,      K.      Black,      E.      Hajiramezanali,      G.      Scalia,      N.      L.      Diamant,      A.      M.      Tseng,      S.      Levine,  \\n \\n  and      T.      Biancalani      (2024).\\nFeedback      efficient      online      fine-tuning      of      diffusion      models.\\narXiv      preprint  \\n \\n  arXiv:2402.\\n16359.</p><p> \\n \\n  Uehara,      M.,      Y.      Zhao,      E.      Hajiramezanali,      G.      Scalia,      G.      Eraslan,      A.      Lal,      S.      Levine,      and      T.      Biancalani  \\n \\n  (2024).\\nBridging      model-based      optimization      and      generative      modeling      via      conservative      fine-tuning  \\n \\n  of      diffusion      models.\\narXiv      preprint      arXiv:2405.19673.</p><p>Vargas,      F.,      W.      Grathwohl,      and      A.      Doucet      (2023).\\nDenoising      diffusion      samplers.\\narXiv      preprint  \\n \\n  arXiv:2302.\\n13834.</p><p> \\n \\n  Wallace,      B.,      M.      Dang,      R.      Rafailov,      L.      Zhou,      A.      Lou,      S.      Purushwalkam,      S.      Ermon,      C.      Xiong,      S.      Joty,  \\n \\n  and      N.      Naik      (2023).\\nDiffusion      model      alignment      using      direct      preference      optimization.\\narXiv  \\n \\n  preprint      arXiv:2311.12908.</p><p>Wang,      Z.,      J.      J.      Hunt,      and      M.      Zhou      (2022).\\nDiffusion      policies      as      an      expressive      policy      class      for      offline  \\n \\n  reinforcement      learning.\\narXiv      preprint      arXiv:2208.06193.</p><p>Widatalla,      T.,      R.      Rafailov,      and      B.      Hie      (2024).\\nAligning      protein      generative      models      with      experimental  \\n \\n  fitness      via      direct      preference      optimization.\\nbioRxiv,      2024-05.</p><p>Williams,      G.,      A.      Aldrich,      and      E.      A.      Theodorou      (2017).\\nModel      predictive      path      integral      control:      From  \\n \\n  theory      to      parallel      computation.\\nJournal      of      Guidance,      Control,      and      Dynamics      40(2),      344-357.</p><p>Wu,      X.,      K.      Sun,      F.      Zhu,      R.      Zhao,      and      H.      Li      (2023).\\nBetter      aligning      text-to-image      models      with  \\n \\n  human      preference.\\narXiv      preprint      arXiv:2303.14420.</p><p>Wu,      Y.,      G.      Tucker,      and      O.      Nachum      (2019).\\nBehavior      regularized      offline      reinforcement      learning.\\n \\n \\n  arXiv      preprint      arXiv:      1911.11361.</p><p>Wulfmeier,      M.,      P.      Ondruska,      and      I.      Posner      (2015).\\nMaximum      entropy      deep      inverse      reinforcement  \\n \\n  learning.\\narXiv      preprint      arXiv:      1507.04888.</p><p>Xie,      T.,      C.-A.\\nCheng,      N.      Jiang,      P.      Mineiro,      and      A.      Agarwal      (2021).\\nBellman-consistent      pessimism  \\n \\n  for      offline      reinforcement      learning.\\nAdvances      in      neural      information      processing      systems      34.</p><p>Xu,      M.,      L.      Yu,      Y.      Song,      C.      Shi,      S.      Ermon,      and      J.      Tang      (2022).\\nGeodiff:   \\n \\nA      geometric      diffusion  \\n \\n  model      for      molecular      conformation      generation.\\narXiv      preprint      arXiv:2203.02923.</p><p> \\n \\n  Yang,      L.,      Z.      Zhang,      Y.      Song,      S.      Hong,      R.      Xu,      Y.      Zhao,      W.      Zhang,      B.      Cui,      and      M.-H.\\nYang      (2023).\\n \\n \\n  Diffusion      models:   \\n \\nA      comprehensive      survey      of      methods      and      applications.\\nACM      Computing  \\n \\n  Surveys      56(4),      1-39.</p><p> \\n \\n  Yu,      T.,      G.      Thomas,      L.      Yu,      S.      Ermon,      J.      Y.      Zou,      S.      Levine,      C.      Finn,      and      T.      Ma      (2020).\\nMopo:  \\n \\n  Model-based      offline      policy      optimization.\\nAdvances      in      Neural      Information      Processing      Systems      33,  \\n \\n  14129-14142.</p><p> \\n \\n  Yuan,      H.,      K.      Huang,      C.      Ni,      M.      Chen,      and      M.      Wang      (2023).\\nReward-directed      conditional      diffusion:  \\n \\n  Provable      distribution      estimation      and      reward      improvement.\\nIn      Thirty-seventh      Conference      on  \\n \\n  Neural      Information      Processing      Systems.</p><p>Zhang,      H.      and      T.      Xu      (2023).\\nTowards      controllable      diffusion      models      via      reward-guided      exploration.\\n \\n \\n  arXiv      preprint      arXiv:2304.07132.</p><p>Zhang,      Q.      and      Y.      Chen      (2021).\\nPath      integral      sampler:   \\n \\na      stochastic      control      approach      for      sampling.\\n \\n \\n  arXiv      preprint      arXiv:2111,15141.</p><p> \\n \\n  Zhao,      Y.,      M.      Uehara,      G.      Scalia,      T.      Biancalani,      S.      Levine,      and      E.      Hajiramezanali      (2024).\\n \\n \\n  Adding      conditional      control      to      diffusion      models      with      reinforcement      learning.\\narXiv      preprint  \\n \\n  arXiv:2406.\\n12120.</p><p>Zhou,      Z.,      S.      Kearnes,      L.      Li,      R.      N.      Zare,      and      P.      Riley      (2019).\\nOptimization      of      molecules      via      deep  \\n \\n  reinforcement      learning.\\nScientific      reports      9(1),      10752.</p><p>Zhu,      Z.,      H.      Zhao,      H.      He,      Y.      Zhong,      S.      Zhang,      Y.      Yu,      and      W.      Zhang      (2023).\\nDiffusion      models      for  \\n \\n  reinforcement      learning:   \\n \\nA      survey.\\narXiv      preprint      arXiv:2311.01223.</p><p>Ziebart,      B.      D.,      A.      L.      Maas,      J.      A.      Bagnell,      A.      K.      Dey,      et      al.\\n(2008).\\nMaximum      entropy      inverse  \\n \\n  reinforcement      learning.\\nIn      Aaai,      Volume      8,      pp.\\n1433-1438.\\nChicago,      IL,      USA.</p><h4>Direct      backpropagation      (Sec.      5.2)       _       Differentiable</h4><p>_\\n \\n   ——_      Non-differentiable PP      5.1</p><h4>Accurate      rewards      O      (Sec.      5.1)</h4><p> \\n \\n  feedback   \\n \\n;    \\n \\n—      Reward-weighted      MLE      (Sec.      6.1  \\n \\n  Xo      Unknown      reward      Memory      effi-      oe      eward-welgnte      (Sec.      6.1) unctions      —_\\\\\\\\\\\\\\\\_—      ciency</p><h4>No      fine-tuning      Computational</h4><p>efficiency —\\n \\n    NN      Direct      backpropagation      (Sec.      5.2)</p><h5>Value-weighted      sampling      (Sec.      6.2)</h5><p>Figure      3:      Practical      recommendation      of      RL-based      algorithms      to      optimize      downstream      reward  \\n \\n  functions.</p><h6>7.1      Fine-Tuning      with      Known,      Differentiable      Reward      Functions</h6><p>ions  \\n \\n  When      accurate      differentiable      reward      functions      are      available      (e.g.,      standard      in      computer      vision      tasks),  \\n \\n  our      primary      focus      is      typically      on      computational      and      memory      efficiency.\\nIn      general,      we      advocate      for  \\n \\n  employing      reward      backpropagation      (c.f.      Algorithm      2)      for      its      computational      efficiency.\\nAlternatively,  \\n \\n  if      memory      efficiency      is   \\n \\na      significant      concern,      we      suggest      using      PPO      due      to      its      stability.</p><h6>7.2      Fine-Tuning      with      Black-Box      Reward      Feedback</h6><p>back  \\n \\n  Here,      we      explore      scenarios      where      we      have      access      to      accurate      but      non-differentiable      (black-box)  \\n \\n  reward      feedback,      often      found      in      scientific      simulations.\\nThe      emphasis      still      remains      primarily      on  \\n \\n  computational      and      memory      efficiency.\\nIn      such      scenarios,      due      to      the      non-differentiability      of      reward  \\n \\n  feedback,      we      recommend      using      PPO      or      reward-weighted      MLE      for      the      following      reasons.</p><p> \\n \\n  In      the      preceding      section,      we      advocated      for      the      use      of      reward      backpropagation      due      to      its      com-  \\n \\n  putational      advantages.\\nHowever,      when      dealing      with      non-differentiable      feedback,      the      advantage  \\n \\n  of      reward      backpropagation      may      diminish.\\nThis      is      because      learning      from      such      feedback      requires  \\n \\n  learning      differentiable      reward      functions      to      make      the      algorithm      work      as      we      discuss      in      Section      5.2.  \\n \\n  However,      obtaining   \\n \\na      differentiable      reward      function      can      be      challenging      in      many      domains.\\nFor  \\n \\n  example,      in      molecular      property      prediction,      molecular      fingerprints      are      informative      features,      and  \\n \\n  accurate      reward      functions      can      be      derived      by      training      simple      neural      networks      on      these      features  \\n \\n  (Pattanaik      and      Coley,      2020).\\nYet,      these      mappings,      grounded      in      prior      scientific      understanding,      are  \\n \\n  non-differentiable,      thereby      restricting      their      applicability.</p><p> \\n \\n  In      contrast,      PPO      and      reward-weighted      MLE      allow      for      policy      updates      without      the      explicit  \\n \\n  requirement      to      learn   \\n \\na      differentiable      reward      function,      offering   \\n \\na      significant      advantage      over      reward  \\n \\n  backpropagation.</p><h6>7.3      Fine-Tuning      with      Unknown      Rewards      Functions</h6><p>ions  \\n \\n  When      reward      functions      (or      computational      proxies      in      Section      7.2)      are      unavailable,      we      must      learn  \\n \\n  from      data      with      reward      feedback.\\nIn      such      cases,      compared      to      the      previous      scenarios,      two      important  \\n \\n  considerations      arise:</p><p>¢\\n \\n   Not      only      computational      or      memory      efficiency      but      also      feedback      efficiency      (i.e.,      sample  \\n \\n  efficiency      regarding      reward      feedback)      is      crucial.</p><p> \\n \\n \\n¢      Given      that      learned      reward      feedback      may      not      generalize      well      outside      the      training      data      distri-  \\n \\n  butions,      it      is      essential      to      constrain      fine-tuned      models      to      prevent      significant      divergence      from  \\n \\n  diffusion      models.\\nIn      these      situations,      not      only      soft      PPO      and      direct      backpropagation      but      also  \\n \\n  methods      discussed      in      Section      6,      such      as      reward-weighted      MLE,      can      be      particularly      effective.</p><p>Further,      the      scenario      involving      unknown      reward      functions      can      be      broadly      categorized      into      two      cases,  \\n \\n  which      we      will      discuss      in      more      detail.</p><p>Offline      scenario      (Figure      4).\\nIn      many      scenarios,      even  \\n \\n  in      cases      where      reward      functions      are      unknown,      we      of-  \\n \\n.     .</p><p>Offline      data      with ten      have      access      to      offline      data      with      reward      feedback      reward      feedback</p><li>(i)      (i)   \\n \\nO   \\n \\n;   \\n \\ni       a”      ra  \\n \\n.      ne      straightforward      approach      is      to</li><p> \\n \\n \\n{   \\n \\n,   \\n \\n(   \\n \\n)    \\n.\\n    .\\n \\n   g\\n \\n   PP  \\n \\n.      <=>      Learning      reward  \\n \\n  perform      regression      with      neural      networks      and      build   \\n \\na      models  \\n \\n  learned      regressor      *.\\nHowever,      this      learned      reward      func-      SS  \\n \\n  tion   \\n \\n*      might      not      generalize      well      beyond      the      distribution      x)  \\n \\n  of      the      offline      data.\\nIn      regions      outside      this      distribution,   \\n \\n7       could      assign   \\n \\na      high      value      even      when      the      actual      reward  \\n \\n  r      is      low      due      to      high      uncertainty.\\nConsequently,      we  \\n \\n  could      be      easily      misled      by      out-of-distribution      samples  \\n \\n  if      we      solely      optimize      7.\\n|Fine-tuning      diffusion      models   \\n \\n|       One      approach      to      alleviate      this      issue      is      to      adopt   \\n \\na       pessimistic      (i.e.,      conservative)      strategy,      as      proposed      in      Figure      4:      Offline      scenario  \\n \\n  Uehara      et      al.\\n(2024).\\nThis      technique      has      been      widely      Collecting      data]   \\n \\nY      Learning      reward  \\n \\n  applied      in      the      field      of      offline      RL      (Levine,      2018).\\nThe      aa  \\n \\n  main      idea      is      to      penalize   \\n \\n7      outside      the      distributions      us-  \\n \\n  ing      techniques      such      as      adding      an      explicit      penalty      term  \\n \\n  (Yu      et      al.,      2020;      Chang      et      al.,      2021),      bootstrapping,  \\n \\n  or      more      sophisticated      methods      (Kumar      et      al.,      2020;  \\n \\n  Xie      et      al.,      2021;      Rigter      et      al.,      2022;      Uehara      and      Sun,  \\n \\n  2021).\\nBy      doing      so,      we      can      avoid      being      fooled      by  \\n \\n  out-to-distribution      regions      while      still      benefiting      from  \\n \\n  the      extrapolation      capabilities      of      reward      models.</p><p>S—      diffusion      models      |</p><p>Figure      5:      Online      scenario</p><p> \\n \\n  Online      scenario      (Figure      5).\\nConsider      situations  \\n \\n  where      reward      functions      are      unknown,      but      we      can      gather      data      online.\\nThese      scenarios      are      often  \\n \\n  referred      to      as   \\n \\na      lab-in-the-loop      setting.</p><p> \\n \\n  In      such      scenarios,      Uehara      et      al.\\n(2024)      proposes      an      iterative      procedure      comprising      three      steps:</p><li>(1)      collecting      feedback      data      by      exploring      new      areas      (i.e.,      obtaining   \\n \\nx      from      current      diffusion      models  \\n \\n  and      corresponding      r(x)),      (2)      learning      the      reward      function      from      the      collected      feedback      data,      and</li><li>(3)      fine-tuning      current      diffusion      models      by      maximizing      the      learned      reward      function      enhanced      by  \\n \\n  an      optimistic      bonus      term,      using      algorithms      discussed      in      Section      4.2.\\nIn      contrast      to      standard      online  \\n \\n  PPO,      where      feedback      data      is      directly      used      in      the      fine-tuning      process,      actual      reward      feedback      is  \\n \\n  only      used      in      step      (2)      and      not      in      step      (3).\\nAn      additional      key      aspect      is      the      use      of      an      optimistic  \\n \\n  reward      function      that      encourages      exploration      beyond      the      distributions      of      the      current      diffusion      model.\\n \\n \\n  This      deliberate      separation      between      reward      learning      and      fine-tuning      steps,      coupled      with      the      use      of  \\n \\n  optimism,      significantly      enhances      feedback      efficiency.</li><p>8\\n \\n   Connection      with      Classifier      Guidance</p><p> \\n \\n  Classifier      guidance      (Dhariwal      and      Nichol,      2021)      is      commonly      used      for      conditional      generation      in  \\n \\n  diffusion      models.\\nInterestingly,      RL-based      methods      share   \\n \\na      close      connection      with      classifier      guidance.\\n \\n \\n  More      specifically,      in      this      section,      following      Zhao      et      al.\\n(2024),      we      elucidate      how      RL-based      methods  \\n \\n  can      be      applied      to      conditional      generation.\\nFurthermore,      from      this      unified      viewpoint,      we      highlight  \\n \\n  that      classifier      guidance      is      seen      as   \\n \\na      value-weighted      sampling      in      Section      4.2.</p><h6>8.1      Classfier      Guidance</h6><p>ance  \\n \\n  In      this      section,      we      first      introduce      classifier      guidance.\\nClassifier      guidance      utilizes      an      unconditional  \\n \\n  pre-trained      model      for      conditional      generation.\\nMore      specifically,      we      consider   \\n \\na      scenario      where      we  \\n \\n  have   \\n \\na      pre-trained      diffusion      model,      which      enables      us      to      sample      from      p?\\n\"*(x),      and      data      with      feedback  \\n \\n  {x,y},      where   \\n \\ny      represents      the      new      label      we      want      to      condition      on.\\nOur      objective      here      is      to      sample  \\n \\n  from      p(-|y)   \\n \\n€      [Y   \\n \\n>      ACR), P(-   \\n \\n|      y)   \\n \\nx      py(yl-)pP*(-).\\n(37)</p><p>where      p,   \\n \\n:   \\n \\nX      —>      A(Y)      denotes      the      conditional      distribution      of   \\n \\ny      given      x.</p><p>Motivation.\\nIn      classifier      guidance,      we      consider      the      following      policy:</p><p>pe      (-lare,      y)   \\n \\n=   \\n \\nN      (p(22,      t;      Opre)   \\n \\n+      a(t)      Ve      log      Vy      (Xt;      t),      a*(t))   \\n \\n’ dy(x,t)   \\n \\n=      Egy.\\nyap,      (-leo)      L(Y   \\n \\n=      y)|a,   \\n \\n=      2].</p><p>Under      the      continuous-time      formulation      of      diffusion      models,      applying      Doob’s      h-transform      (Rogers  \\n \\n  and      Williams,      2000),      it      is      shown      that      we      can      sample      from      p(x|y)      by      sequentially      running      {p7\"\\\\\\\\\\\\\\'(-|7+,      y)      }i-r41-</p><p>Algorithm.\\nSpecifically,      the      algorithm      comprises      two      steps.\\nIt’s      important      to      note      that      this      is      not  \\n \\n  a      fine-tuning      method      but      rather      an      inference-time      technique      as      we      freeze      the      pre-trained      model.</p><li>1. Learning      q,(x,t)      through      regression      by      gathering      data      containing      {z;,      y}.\\nHere,      for      each  \\n \\n  (Xo,      y),      using   \\n \\na      forward      policy      for      the      pre-training      (from   \\n \\n0      to      T),      we      obtain      2;      starting      from</li><p>Lo.</p><li>2. During      inference,      sequentially      execute      {p2\"(-|a,,      y)      }1_>   \\n \\n1      by      leverage      the      fact      that      compared</li><p>gui to      the      policy      in      pre-trained      diffusion      models,      the      mean      of      {p?\\n\"\\\\\\\\\\\\\\'(-|71,      y)      }i-74,      is      just      shifted  \\n \\n  with      an      additional      term      V,,      log      q,(2,      t).</p><p> \\n \\n  Notably,      the      method      described      above      can      be      reduced      to      value-weighted      sampling      (cf.      Sec-  \\n \\n  tion      6.2),      when      r(x,      y)   \\n \\n=      log      p(y|x).\\nInspired      by      such      an      observation,      we      apply      an      RL-based  \\n \\n  fine-tuning      method      for      conditional      generation      as      below.</p><h7>8.2.      RL-Based      Fine-Tuning      for      Conditional      Generation</h7><p>tion  \\n \\n  In      this      section,      following      Zhao      et      al.\\n(2024),      we      explain      an      RL-based      fine-tuning      approach      for  \\n \\n  conditional      generation.\\nThe      core      idea      is      that      setting      r(x,      y)   \\n \\n=      log      p(y|x)      allows      us      to      sample      from  \\n \\n  the      target      conditional      distribution      (37),      drawing      on      the      insights      discussed      in      Section      4.2.  \\n \\n  More      precisely,      introducing   \\n \\na      distribution      over   \\n \\ny      as      q(-),      we      formulate      the      following      RL      problem:</p><p>{Di      ft   \\n \\n=      argmax      Egy,      (-\\\\\\\\\\\\\\\\-y)}.yott      log      p(y|xo)   \\n \\n—      NeopyaKL(pe(-|e,      y)      ||P      (late).\\n \\n \\n  {prE[(V,.X)PA(A)]      Heyy (38)</p><p> \\n \\n  Compared      to      the      objective      function      (18)      in      Section      4.2,      the      main      differences      are:      (1)      policy      space  \\n \\n  is      expanded      to      accommodate      additional      control      (i.e.,      y):      pz(-|-,-)   \\n \\nC      [R¢   \\n \\nx   \\n \\nY   \\n \\n+      A(R®)],      (2)      the  \\n \\n  fine-tuning      objectives      are      set      as      log-likelihoods:      log      p(y|z).\\nThen,      following      Theorem      1,      we      can  \\n \\n  establish      the      following      result.</p><p>Corollary      2.\\nThe      distribution      induced      by      {p*}1_74      (ue.   \\n \\n[      {Teri      Ph      (ae-1   \\n \\n|      te,      y)}deur      is      the target      conditional      distribution      (37).</p><p> \\n \\n  The      above      corollary      suggests      that      for      conditional      generation,      we      can      utilize      various      off-the-shelf  \\n \\n  algorithms      discussed      in      Section      4.2      and      6,      such      as      PPO,      reward      backpropagation,      and      reward-  \\n \\n  weighted      MLE,      as      well      as      value-weighted      sampling.</p><p>9\\n \\n   Connection      with      Flow-Based      Diffusion      Models</p><p> \\n \\n  We      elucidate      the      close      relationship      between      bridge      matching      and      fine-tuning,      as      summarized      in  \\n \\n  Table      2.\\nBridge      (flow)      matching      (Liu      et      al.,      2022;      Tong      et      al.,      2023;      Lipman      et      al.,      2023;      Liu      et      al.,  \\n \\n  2022;      Shi      et      al.,      2024;      Albergo      and      Vanden-Eijnden,      2022),      has      recently      gained      popularity      as   \\n \\na       distinct      type      of      diffusion      model      from      score-matching-based      ones.\\nThese      works      suggest      that      training  \\n \\n  and      inference      speed      will      be      accelerated      due      to      the      ability      to      define   \\n \\na      more      direct      trajectory      from  \\n \\n  noise      to      data      distribution      than      score-matching-based      diffusion      models.</p><p> \\n \\n  The      connection      between      bridge      matching      and      fine-tuning      becomes      evident      in      the      continuous-  \\n \\n  time      formulation.\\nTo      begin,      we      first      provide   \\n \\na      brief      overview      of      the      continuous-time      formulation      of  \\n \\n  score-matching-based      diffusion      models.</p><p> \\n \\n  Table      2:      Comparison      between      training      score/flow-based      diffusion      models      and      fine-tuning.\\nWe  \\n \\n  optimize      the      parameter   \\n \\n6      in      P’,      which      represents      the      induced      distribution      over      trajectories      associated  \\n \\n  with      6.</p><h8>Loss      Note      of      Q</h8><p>Score-based      KL(Q\\\\\\\\\\\\\\'™¢|P?)      Qtime:      Time-reversal      of      reference      SDE      (reference training      SDE      is      defined      from   \\n \\n0      to      T’      where      the      initial      dis-  \\n \\n  tribute      at   \\n \\n0      is      data      distribution)</p><p> \\n \\n  Flow-based      KL(Q*°>      ||P?)      @Q*°°h:      Coupling      between      the      reference      SDE      from  \\n \\n  training   \\n \\nT      to   \\n \\n0      conditioning      at   \\n \\n0      (dist.      over      x.;|7o)      and  \\n \\n  a      data      distribution      at      time   \\n \\n0      (dis.      over      x).\\nNote  \\n \\n  that      reference      SDE      is      defined      from      7’      to      0.</p><p> \\n \\n  Fine-tuning      KL(P?||Q*°)      @*:      Coupling      between      the      pre-trained      SDE  \\n \\n  from   \\n \\nT      to   \\n \\n0      conditioned      on   \\n \\n0      (dist.      over      x7.1|X0)  \\n \\n  and   \\n \\na      data      distribution      at      time   \\n \\n0      (dis.      over      Xo) 9.1      Continuous      Time      Formulation      of      Score-Based      diffusion      models</p><p> \\n \\n  We      provide   \\n \\na      brief      overview      of      the      continuous-time      formulation      of      diffusion      models      (Song      et      al.,  \\n \\n  2021),      further      expanding      Section      1.1.1.\\nIn      this      formulation,      we      initially      define      the      forward      reference  \\n \\n  SDE,      spanning      from      time   \\n \\n0      to      7’      (e.g.,      variance      exploding      (VE)      process      or      variance      preserving      (VP)  \\n \\n  process),      by      setting      an      initial      distribution      at   \\n \\n0      as      the      data      distribution.\\nAdditionally,      the      reference  \\n \\n  SDE      is      tailored      to      converge      to      an      easy      distribution      at      time      7’,      such      as      the      normal      distribution.</p><p> \\n \\n  Subsequently,      we      consider      the      time-reversal      SDE      (Anderson,      1982),      progressing      from      T\\\\\\\\\\\\\\'      to      0.\\nIf  \\n \\n  we      could      learn      this      time-reversal      SDE,      it      would      enable      us      to      sample      from      the      data      distribution      by  \\n \\n  starting      with      the      easy      initial      distribution      (at      7’)      and      following      the      time-reversal      SDE      from   \\n \\nT      to   \\n \\n0      at  \\n \\n  inference      time.</p><p> \\n \\n  Training.\\nNow,      the      remaining      question      is      how      to      learn      this      time-reversal      SDE.\\nHere,      by      repre-  \\n \\n  senting      the      induced      distribution      over      trajectories      from   \\n \\nT      to   \\n \\n0      with      this      time-reversal      SDE      as      Q\\\\\\\\\\\\\\'™®,  \\n \\n  our      goal      is      to      train   \\n \\na      new      SDE,      parameterized      by   \\n \\n@      in      the      neural      network,      such      that      the      induced  \\n \\n  distribution      P®      over      trajectories      associated      with   \\n \\n6      aligns      with      Q\"™*.\\nSpecifically,      the      objective      is      to minimize      the      following      losses:</p><p> \\n \\n  argmin      KL(Q\"™||P*).\\n(39)  \\n \\n  0  \\n \\n  With      some      algebra,      leveraging      the      observation      that      the      time-reversal      SDE      comprises      the      original  \\n \\n  forward      SDE      and      the      scoring      term      of      the      marginal      distribution      (Anderson,      1982)      as      in      (2),      Song  \\n \\n  et      al.\\n(2021)      demonstrates      that      this      KL      loss      (39)      is      approximated      as Dore   \\n \\n=      argmin      Eeuni((0,7))01~qyo0(weleo)co~pP\"e      Fe      Fl      Vey      log      jo      (Xt   \\n \\n|      Xo)   \\n \\n_      so(Xt,      t)|I?\\n],      (40)  \\n \\n  which      is      equal      to      (4)      where      the      weight      A(t)      is      {o?\\n}?\\n(for      notation,      refer      to      Section      1.1.1).</p><h9>9.2      Flow-Based      (Bridge-Based)      Diffusion      Models</h9><p>dels We      adopt      the      explanation      outlined      in      Liu      et      al.\\n(2022).\\nIn      this      framework,      we      begin      by      considering  \\n \\n  a      reference      SDE      (from      T\\\\\\\\\\\\\\'      to      0),      x;   \\n \\n=      zr_;      where      z;      is      defined      as      Brownian      motion:</p><p>dz,   \\n \\n=      dwt,      te      (0,      7).\\n(41)</p><p> \\n \\n  Subsequently,      we      consider      the      coupling      of      the      reference      SDE      conditioned      on      state      at      time   \\n \\n0      (i.e.,  \\n \\n  distribution      of      xo.7      conditional      on      x)      and      the      data      distribution      at      time      0,      denoting      the      resulting  \\n \\n  distribution      as      Q*°?,      Informally,      this      Q*°°?\\nis      characterized      as      follows.</p><p>Informal      characterization      of      Q*”.\\nWhen      discretizing      the      time      step,      by      denoting      the      distribution  \\n \\n  induced      by      the      reference      SDE      as      p\"(x7,--+   \\n \\n,      29),      we      define QF      (a7,      tae   \\n \\n,      20)   \\n \\n=      p(x,      ay   \\n \\n|      r9)pP\"°      (x9).</p><p>The      distribution      Q*”      above      is   \\n \\na      limiting      distribution      obtained      by      making      the      discretization      step small.</p><p>Training.\\nIn      bridge-based      diffusion      models,      our      objective      is      to      train   \\n \\na      new      SDE,      parametrized      by  \\n \\n  6      (e.g.,   \\n \\na      neural      network),      to      align      the      induced      distribution      P®      with      Q*°?.\\nSpecifically,      bridge-based training      aims      to      minimize aremin      KL(Q*>||P’).\\n(42)  \\n \\n  0  \\n \\n  While      the      method      to      minimize      the      above      is      currently      unclear,      we      can      derive   \\n \\na      trainable      objective  \\n \\n  through      algebraic      manipulation.\\nInitially,      we      observe      that      the      reference      SDE      (41),      conditioned      on  \\n \\n  state      xo      at      time   \\n \\n0      (zr   \\n \\n=      D),      is      expressed      as:</p><p>b\\n \\n \\n  b—      2;</p><p>t\\n \\n   €\\n \\n   [0,T];dz?\\n \\n \\n=      g?(z?,t)dt      +du,,  \\n \\n  g(z?,t)   \\n \\n=   \\n \\na       (43)</p><p>using      the      seminal      “Doob’s      h-transform”      (Rogers      and      Williams,      2000).\\nThis      SDE      is      commonly  \\n \\n  known      as      the      Brownian      bridge.\\nNow,      we      consider   \\n \\na      new      SDE      parameterized      by      0:</p><p>t\\n \\n   €\\n \\n   [0,7];      dz   \\n \\n=      s(z,t;      @)dt   \\n \\n+      dw.\\n(44)</p><p>Then,      we      can      demonstrate      that      minimizing      the      KL      loss      in      (42)      is      equivalent      to argmin      E,W      unil0,7],1~4\"°      cop?\\n[Ilg”      (2,      t)   \\n \\n_      s(21,      t;      0)      M3]      (45)</p><p>where      q;°      denotes      the      induced      distribution      at      time   \\n \\nt      following      SDE      (43)      conditioned      on      xo,      and  \\n \\n  s:      Xx      |0,T]   \\n \\n>   \\n \\n&      is      the      model      we      aim      to      optimize.</p><p> \\n \\n  Flow-based      diffusion      models.\\nThe      aforementioned      formulation      has      been      originally      proposed      as   \\n \\na       bridge-based      SDE      (Liu      et      al.,      2022)      because      the      conditional      SDE      is      often      referred      to      as   \\n \\na      Brownian  \\n \\n  bridge.\\nFlow-based      diffusion      models      proposed      in      Lipman      et      al.\\n(2023);      Tong      et      al.\\n(2023)      share   \\n \\na       close      relationship      with      bridge-based      diffusion      models.\\nThese      flow-based      models      use   \\n \\na      target      SDE (43)      and      an      SDE      with   \\n \\na      parameter      (44)      without      stochastic      terms      (1.e.,      no      dw,)      and      aim      to      minimize  \\n \\n  the      loss      function      (45).</p><p> \\n \\n  Comparison      with      score-based      diffusion      models.\\nBoth      score-based      diffusion      models      and      bridge-  \\n \\n  based      diffusion      models      aim      to      minimize      KL      divergences.\\nDespite      its      similarity,      in      contrast      to  \\n \\n  score-based      diffusion      models,      bridge-based      diffusion      models      target      Q*°>      rather      than      the      time-  \\n \\n  reversal      SDE      Q\"™®.\\nThe      distribution      Q*”      is      often      considered      preferable      because      it      can      impose      an  \\n \\n  SDE      that      navigates      efficiently      from   \\n \\n0      to      7’      (e.g.,   \\n \\na      Brownian      bridge      with      minimal      noise).\\nThis      can  \\n \\n  expedite      the      learning      process      since      we      lack      direct      control      over      the      time-reversal      SDE      in      score-based  \\n \\n  diffusion      models.</p><h10>9.3.      Connection      with      RL-Based      Fine-Tuning</h10><p>ning  \\n \\n  RL-based      fine-tuning      shares   \\n \\na      close      relationship      with      flow-based      training.\\nTo      illustrate      this,      consider  \\n \\n  the      reference      SDE      (from      7’      to      0)      following      the      pre-trained      diffusion      model.\\nThen,      akin      to      flow-  \\n \\n  based      training,      we      consider      the      coupling      of      the      reference      SDE      conditioned      on      state      at      time   \\n \\n0       (i.e.,      distribution      over      xo-r      conditioned      on      x9)      and      the      target      distribution      at      time   \\n \\n0      (i.e.,      p,(-)   \\n \\n«       exp(r(-)/a)p?\\n\"°(-)),      denoting      the      induced      distribution      as      Q*™.</p><p>Informal      characterization      of      Q™°*.\\nInformally,      Q*\"°      is      introduced      as      follows.\\nWhen      discretizing  \\n \\n  the      time      step,      by      denoting      the      distribution      induced      by      the      reference      SDE      as      p™\\\\\\\\\\\\\\'(xp,-+-   \\n \\n,      9),      we define Qi\"      (xr,   \\n \\na   \\n \\n,      0)   \\n \\n=      p(x,      nr      Ly   \\n \\n|      L0)Pr      (Xo).</p><p>The      distribution      Q*°      above      is   \\n \\na      limiting      distribution      obtained      by      making      the      discretization      step small.</p><p>Training.\\nNow,      we      introduce   \\n \\na      new      SDE      parameterized      by   \\n \\n@      such      as      an      neural      network      to      align  \\n \\n  the      induced      distribution      P’      with      Q®\"*.\\nActually,      in      the      continuous      formulation,      the      RL-problem      (18) in      Section      4.2      is      equal      to      solving argmin      KL(P?||Q\"™).\\n \\n \\n  0 This      formalization      has      been      presented      in      Uehara      et      al.\\n(2024).\\nIntuitively,      as      we      see      in      Theorem      3,  \\n \\n  this      is      expected      because      we      see      that      the      posterior      distribution      induced      by      pre-trained      models:</p><p>ref   \\n \\n(      PD      (ar,-++      21   \\n \\n|      to)      (=      vey      (er|er_-1)p      Pp      (ar-1   \\n \\n|      er_2)      +++      pp\"      (x1   \\n \\n|      20), remains      preserved      after      fine-tuning      based      on      the      RL-formulation      (18).</p><p> \\n \\n  Comparison      with      flow-based      training.\\nIn      contrast      to      flow-based      training,      RL-based      fine-tuning  \\n \\n  minimizes      the      inverse      KL      divergence      rather      than      the      KL      divergence.\\nIntuitively,      this      is      because,  \\n \\n  unlike      Qt™*      or      Q*>   \\n \\n,      we      cannot      sample      from      Q*”      (recall      that   \\n \\na      marginal      distribution      at      time   \\n \\n0      in  \\n \\n  Qi,      i.e.,      p,,      is      unnormalized);      on      the      other      hand,      the      marginal      distributions      at      time   \\n \\n0      in      both      Q\\\\\\\\\\\\\\'™*  \\n \\n  and      Q*°°>      are      data      distributions.</p><p> \\n \\n  Remark   \\n \\n7      (Extension      to      f-divergence).\\nEach      of      these      methods      can      be      technically      extended      by  \\n \\n  incorporating      more      general      divergences      beyond      KL      divergence.\\nFor      instance,      in      the      context      of  \\n \\n  fine-tuning,      please      see      Tang      (2024).</p><p>10      Connection      with      Sampling      from      Unnormalized      Distributions</p><p> \\n \\n  Numerous      studies      delve      into      sampling      from      an      unnormalized      distribution      proportional      to      exp(r()),  \\n \\n  commonly      known      as      the      Gibbs      distribution,      extensively      discussed      in      the      computational      statistics  \\n \\n  and      statistical      physics      community      (Robert,      2014).\\nThis      issue      is      pertinent      to      our      work,      as      during  \\n \\n  fine-tuning,      the      target      distribution      is      also      formulated      in      the      Gibbs      distribution      form      in      (10),      which      is  \\n \\n  proportional      to      exp(r(a))p?\\n\"*      (x).</p><p> \\n \\n  The      literature      employs      two      main      approaches      to      tackle      this      challenge:      Markov      Chain      Monte  \\n \\n  Carlo      (MCMC)      and      RL-based      methods.\\nIn      particular,      the      RL-based      diffusion      model      fine-tuning  \\n \\n  we      have      discussed      so      far      is      closely      related      to      the      latter      approach.\\nIn      this      section,      we      explain      and  \\n \\n  compare      these      two      approaches.</p><h10>10.1      Markov      Chain      Monte      Carlo      (MCMC)</h10><p>CMC)  \\n \\n  In      MCMC,   \\n \\na      Markov      chain      is      constructed      to      approximate   \\n \\na      target      distribution      such      that      the      equilib-  \\n \\n  rium      distribution      of      the      Markov      chain      converges      to      the      target      distribution.\\nHowever,      dealing      with  \\n \\n  high-dimensional      domain      spaces      is      challenging      for      MCMC.\\n \\n \\nA      common      strategy      involves      leveraging  \\n \\n  gradient      information,      such      as      the      Metropolis-adjusted      Langevin      algorithm      (MALA)      (Besag,      1994)  \\n \\n  or      Hamiltonian      Monte      Carlo      (HMC)      (Neal      et      al.,      2011;      Girolami      and      Calderhead,      2011).\\nMALA  \\n \\n  shares      similarities      with      classifier      guidance      and      value-weighted      sampling,      as      both      methods      depend  \\n \\n  on      the      first-order      information      from      reward      (or      value)      functions.</p><p> \\n \\n  Can      we      use      MCMC      for      fine-tuning      diffusion      models?\\nIn      the      context      of      fine-tuning      dif-  \\n \\n  fusion      models,      while      it      may      seem      intuitive      to      sample      from      the      target      distribution      (10)      (i.e.,  \\n \\n  Dy   \\n \\n&      exp(r(x))pP\"*(x))      using      MCMC,      it’s      not      straightforward      for      the      following      reasons:</p><p>¢\\n \\n   We      lack      of      an      analytical      form      of      p?\\n\"*(-).<ul><li>*\\n \\n   Additionally,      even      if      we      can      estimate      p?\\n’°(-)      in      an      unbiased      manner,      as      in      Chen      et      al.\\n(2018),  \\n \\n  the      mixing      time      for      obtaining   \\n \\na      single      sample      in      MCMC      might      be      lengthy.</li></ul></p><p> \\n \\n  Therefore,      we      explore      creating   \\n \\na      generative      model      (i.e.,      diffusion      model)      for      this      task      using      RL-  \\n \\n  based      fine-tuning      so      that      we      can      easily      sample      from      p,(-)      during      inference      (i.e.,      simulating      the  \\n \\n  time-reversal      SDE      (2)      with      policies      from   \\n \\n¢   \\n \\n=      T\\\\\\\\\\\\\\'+   \\n \\n1      to   \\n \\nt   \\n \\n=      1),      without      relying      on      MCMC.\\nThis  \\n \\n  addresses      the      concern      of      MCMC      by      avoiding      the      need      to      estimate      p?\\n\"*(-)      and      minimizing      inference  \\n \\n  time,      albeit      at      the      expense      of      increased      training      time.</p><p>10.2}      RL-Based      Approaches  \\n \\n  While      MCMC      has      historically      been      widely      used      for      this      purpose,      recent      works      have      proposed      an  \\n \\n  RL-based      approach      or      its      variant      (e.g.,      Bernton      et      al.      (2019);      Heng      et      al.\\n(2020);      Huang      et      al.\\n(2023);</p><p> \\n \\n  Vargas      et      al.\\n(2023)).\\nFor      example,   \\n \\na      seminal      study      (Zhang      and      Chen,      2021)      introduced   \\n \\na      method  \\n \\n  very      similar      to      reward      backpropagation.\\nIn      their      work,      they      initially      define      Brownian      motion      as   \\n \\na       reference      SDE      (from      7’      to      0),      similar      to      the      pre-trained      diffusion      models      in      our      context.\\nThen,      by  \\n \\n  appropriately      setting      rewards,      Zhang      and      Chen      (2021)      aims      to      learn   \\n \\na      new      SDE      (from      T\\\\\\\\\\\\\\'      to      0)      such  \\n \\n  that      we      can      sample      from      the      target      distribution      at      the      endpoint      0.</p><p> \\n \\n  These      approaches      offer      advantages      over      MCMC      due      to      their      ability      to      minimize      inference      time  \\n \\n  (amortization)      by      constructing   \\n \\na      generative      model      comprising      policies      in      the      training      time      and      solely  \\n \\n  executing      learned      policies      during      inference.\\nUnlike      MCMC,      these      methods      significantly      reduce      the  \\n \\n  inference      cost      as      we      don’t      need      to      be      concerned      about      the      mixing      time.\\nAdditionally,      RL-based  \\n \\n  approaches      have      the      potential      to      effectively      handle      high-dimensional,      complex      (i.e.,      multi-modal)  \\n \\n  distributions      by      harnessing      the      considerable      expressive      power      arising      from      latent      states.</p><p> \\n \\n  Despite      these      advantages,      it      is      often      unclear      how      to      select   \\n \\na      reference      SDE      in      these      works.\\nIn  \\n \\n  contrast,      in      RL-based      fine-tuning      of      diffusion      models,      we      directly      leverage      pre-trained      diffusion  \\n \\n  models      as      the      reference      SDEs,      i.e.,      as      our      prior      knowledge.</p><p> \\n \\n  Key      message.\\nFinally,      it      is      worth      noting      that      we      can      technically      utilize      any      off-the-shelf      RL      algo-  \\n \\n  rithms      for      training.\\nFor      example,      Zhang      and      Chen      (2021)      employs      reward      backpropagation,      while  \\n \\n  works      in      Gflownets      typically      leverage      PCL,      as      we      mention      in      Section      6.3.\\nHowever,      additionally,      it  \\n \\n  is      possible      to      use      PPO      or      reward-weighted      MLE      mentioned      in      Section      4.2.</p><p>11      Closely      Related      Directions</p><p>Lastly,      we      highlight      several      closely      related      directions      we      have      not      yet      discussed.</p><p> \\n \\n  Aligning      text-to-image      models.\\nWe      recognize      that      current      efforts      in      fine-tuning      diffusion      models  \\n \\n  primarily      revolve      around      aligning      text-to-image      models      using      human      feedback      (Lee      et      al.,      2023;  \\n \\n  Wallace      et      al.,      2023;      Wu      et      al.,      2023;      Yang      et      al.,      2023).\\nIn      this      paper,      our      goal      is      to      consider   \\n \\na      more  \\n \\n  general      and      fundamental      formulation      that      is      not      tailored      to      any      particular      task.</p><p> \\n \\n  Diffusion      models      for      RL.\\nDiffusion      models      are      well-suited      for      specific      reinforcement      learning  \\n \\n  applications      due      to      their      ability      to      model      complex      and      multimodal      distributions      as      polices      (Janner  \\n \\n  et      al.,      2022;      Ajay      et      al.,      2023;      Wang      et      al.,      2022;      Hansen-Estruch      et      al.,      2023;      Du      et      al.,      2024;      Zhu  \\n \\n  et      al.,      2023).\\nFor      example,      Wang      et      al.\\n(2022)      use      conditional      diffusion      models      as      policies      and  \\n \\n  demonstrate      their      effectiveness      in      typical      offline      RL      benchmarks.\\nWhile      RL-based      fine-tuning      also  \\n \\n  aims      to      maximize      certain      reward      functions,      the      primary      focus      of      this      tutorial      covers      methods      that  \\n \\n  leverage      pre-trained      diffusion      models      for      this      purpose.</p><p> \\n \\n  RL      from      human      feedback      (RLHF)      for      language      models.\\nRLHF      is      widely      discussed      in      the  \\n \\n  context      of      language      models,      where      many      algorithms      share      similarities      with      those      used      in      diffusion  \\n \\n  models      (Ouyang      et      al.,      2022;      Bai      et      al.,      2022;      Casper      et      al.,      2023).\\nFor      instance,      both      domains  \\n \\n  commonly      employ      PPO      as   \\n \\na      standard      approach,      and      reward-weighted      training      (or      reward-weighted  \\n \\n  MLE)      often      serves      as   \\n \\na      basic      baseline      approach.\\nDespite      these      similarities,      nuanced      differences  \\n \\n  exist      mainly      because      diffusion      models      typically      operate      in   \\n \\na      continuous      input      space      rather      than  \\n \\n  a      discrete      one.\\nMoreover,      techniques      like      value-weighted      sampling      emerge      uniquely      within      the  \\n \\n  context      of      diffusion      models.</p><p>12      Summary</p><p> \\n \\n  In      this      article,      we      comprehensively      explain      how      fine-tuning      diffusion      models      to      maximize      down-  \\n \\n  stream      reward      functions      can      be      formalized      as   \\n \\na      reinforcement      learning      (RL)      problem      in      entropy-  \\n \\n  regularized      Markov      decision      processes      (MDPs).\\nBased      on      this      viewpoint,      we      elaborate      on      the  \\n \\n  application      of      various      RL      algorithms,      such      as      PPO      and      reward-weighted      MLE,      specifically      tailored  \\n \\n  for      fine-tuning      diffusion      models.\\nAdditionally,      we      categorize      different      scenarios      based      on      how  \\n \\n  reward      feedback      is      obtained.\\nWhile      this      categorization      is      not      always      explicitly      mentioned      in      many  \\n \\n  existing      works,      it      is      crucial      when      selecting      appropriate      algorithms.\\nFinally,      we      discuss      the      rela-  \\n \\n  tionships      with      several      related      topics,      including      classifier      guidance,      Gflownets,      path      integral      control  \\n \\n  theory,      and      sampling      from      unnormalized      distributions.</p><h5>Value-weighted      sampling      (Sec.      6.2)</h5><p>Figure      3:      Practical      recommendation      of      RL-based      algorithms      to      optimize      downstream      reward  \\n \\n  functions.</p><h6>7.1      Fine-Tuning      with      Known,      Differentiable      Reward      Functions</h6><p>ions  \\n \\n  When      accurate      differentiable      reward      functions      are      available      (e.g.,      standard      in      computer      vision      tasks),  \\n \\n  our      primary      focus      is      typically      on      computational      and      memory      efficiency.\\nIn      general,      we      advocate      for  \\n \\n  employing      reward      backpropagation      (c.f.      Algorithm      2)      for      its      computational      efficiency.\\nAlternatively,  \\n \\n  if      memory      efficiency      is   \\n \\na      significant      concern,      we      suggest      using      PPO      due      to      its      stability.</p><h6>7.2      Fine-Tuning      with      Black-Box      Reward      Feedback</h6><p>back  \\n \\n  Here,      we      explore      scenarios      where      we      have      access      to      accurate      but      non-differentiable      (black-box)  \\n \\n  reward      feedback,      often      found      in      scientific      simulations.\\nThe      emphasis      still      remains      primarily      on  \\n \\n  computational      and      memory      efficiency.\\nIn      such      scenarios,      due      to      the      non-differentiability      of      reward  \\n \\n  feedback,      we      recommend      using      PPO      or      reward-weighted      MLE      for      the      following      reasons.</p><p> \\n \\n  In      the      preceding      section,      we      advocated      for      the      use      of      reward      backpropagation      due      to      its      com-  \\n \\n  putational      advantages.\\nHowever,      when      dealing      with      non-differentiable      feedback,      the      advantage  \\n \\n  of      reward      backpropagation      may      diminish.\\nThis      is      because      learning      from      such      feedback      requires  \\n \\n  learning      differentiable      reward      functions      to      make      the      algorithm      work      as      we      discuss      in      Section      5.2.  \\n \\n  However,      obtaining   \\n \\na      differentiable      reward      function      can      be      challenging      in      many      domains.\\nFor  \\n \\n  example,      in      molecular      property      prediction,      molecular      fingerprints      are      informative      features,      and  \\n \\n  accurate      reward      functions      can      be      derived      by      training      simple      neural      networks      on      these      features  \\n \\n  (Pattanaik      and      Coley,      2020).\\nYet,      these      mappings,      grounded      in      prior      scientific      understanding,      are  \\n \\n  non-differentiable,      thereby      restricting      their      applicability.</p><p> \\n \\n  In      contrast,      PPO      and      reward-weighted      MLE      allow      for      policy      updates      without      the      explicit  \\n \\n  requirement      to      learn   \\n \\na      differentiable      reward      function,      offering   \\n \\na      significant      advantage      over      reward  \\n \\n  backpropagation.</p><h6>7.3      Fine-Tuning      with      Unknown      Rewards      Functions</h6><p>ions  \\n \\n  When      reward      functions      (or      computational      proxies      in      Section      7.2)      are      unavailable,      we      must      learn  \\n \\n  from      data      with      reward      feedback.\\nIn      such      cases,      compared      to      the      previous      scenarios,      two      important  \\n \\n  considerations      arise:</p><p>¢\\n \\n   Not      only      computational      or      memory      efficiency      but      also      feedback      efficiency      (i.e.,      sample  \\n \\n  efficiency      regarding      reward      feedback)      is      crucial.</p><p> \\n \\n \\n¢      Given      that      learned      reward      feedback      may      not      generalize      well      outside      the      training      data      distri-  \\n \\n  butions,      it      is      essential      to      constrain      fine-tuned      models      to      prevent      significant      divergence      from  \\n \\n  diffusion      models.\\nIn      these      situations,      not      only      soft      PPO      and      direct      backpropagation      but      also  \\n \\n  methods      discussed      in      Section      6,      such      as      reward-weighted      MLE,      can      be      particularly      effective.</p><p>Further,      the      scenario      involving      unknown      reward      functions      can      be      broadly      categorized      into      two      cases,  \\n \\n  which      we      will      discuss      in      more      detail.</p><p>Offline      scenario      (Figure      4).\\nIn      many      scenarios,      even  \\n \\n  in      cases      where      reward      functions      are      unknown,      we      of-  \\n \\n.     .</p><p>Offline      data      with ten      have      access      to      offline      data      with      reward      feedback      reward      feedback</p><li>(i)      (i)   \\n \\nO   \\n \\n;   \\n \\ni       a”      ra  \\n \\n.      ne      straightforward      approach      is      to</li><p> \\n \\n \\n{   \\n \\n,   \\n \\n(   \\n \\n)    \\n.\\n    .\\n \\n   g\\n \\n   PP  \\n \\n.      <=>      Learning      reward  \\n \\n  perform      regression      with      neural      networks      and      build   \\n \\na      models  \\n \\n  learned      regressor      *.\\nHowever,      this      learned      reward      func-      SS  \\n \\n  tion   \\n \\n*      might      not      generalize      well      beyond      the      distribution      x)  \\n \\n  of      the      offline      data.\\nIn      regions      outside      this      distribution,   \\n \\n7       could      assign   \\n \\na      high      value      even      when      the      actual      reward  \\n \\n  r      is      low      due      to      high      uncertainty.\\nConsequently,      we  \\n \\n  could      be      easily      misled      by      out-of-distribution      samples  \\n \\n  if      we      solely      optimize      7.\\n|Fine-tuning      diffusion      models   \\n \\n|       One      approach      to      alleviate      this      issue      is      to      adopt   \\n \\na       pessimistic      (i.e.,      conservative)      strategy,      as      proposed      in      Figure      4:      Offline      scenario  \\n \\n  Uehara      et      al.\\n(2024).\\nThis      technique      has      been      widely      Collecting      data]   \\n \\nY      Learning      reward  \\n \\n  applied      in      the      field      of      offline      RL      (Levine,      2018).\\nThe      aa  \\n \\n  main      idea      is      to      penalize   \\n \\n7      outside      the      distributions      us-  \\n \\n  ing      techniques      such      as      adding      an      explicit      penalty      term  \\n \\n  (Yu      et      al.,      2020;      Chang      et      al.,      2021),      bootstrapping,  \\n \\n  or      more      sophisticated      methods      (Kumar      et      al.,      2020;  \\n \\n  Xie      et      al.,      2021;      Rigter      et      al.,      2022;      Uehara      and      Sun,  \\n \\n  2021).\\nBy      doing      so,      we      can      avoid      being      fooled      by  \\n \\n  out-to-distribution      regions      while      still      benefiting      from  \\n \\n  the      extrapolation      capabilities      of      reward      models.</p><p>S—      diffusion      models      |</p><p>Figure      5:      Online      scenario</p><p> \\n \\n  Online      scenario      (Figure      5).\\nConsider      situations  \\n \\n  where      reward      functions      are      unknown,      but      we      can      gather      data      online.\\nThese      scenarios      are      often  \\n \\n  referred      to      as   \\n \\na      lab-in-the-loop      setting.</p><p> \\n \\n  In      such      scenarios,      Uehara      et      al.\\n(2024)      proposes      an      iterative      procedure      comprising      three      steps:</p><li>(1)      collecting      feedback      data      by      exploring      new      areas      (i.e.,      obtaining   \\n \\nx      from      current      diffusion      models  \\n \\n  and      corresponding      r(x)),      (2)      learning      the      reward      function      from      the      collected      feedback      data,      and</li><li>(3)      fine-tuning      current      diffusion      models      by      maximizing      the      learned      reward      function      enhanced      by  \\n \\n  an      optimistic      bonus      term,      using      algorithms      discussed      in      Section      4.2.\\nIn      contrast      to      standard      online  \\n \\n  PPO,      where      feedback      data      is      directly      used      in      the      fine-tuning      process,      actual      reward      feedback      is  \\n \\n  only      used      in      step      (2)      and      not      in      step      (3).\\nAn      additional      key      aspect      is      the      use      of      an      optimistic  \\n \\n  reward      function      that      encourages      exploration      beyond      the      distributions      of      the      current      diffusion      model.\\n \\n \\n  This      deliberate      separation      between      reward      learning      and      fine-tuning      steps,      coupled      with      the      use      of  \\n \\n  optimism,      significantly      enhances      feedback      efficiency.</li><p>8\\n \\n   Connection      with      Classifier      Guidance</p><p> \\n \\n  Classifier      guidance      (Dhariwal      and      Nichol,      2021)      is      commonly      used      for      conditional      generation      in  \\n \\n  diffusion      models.\\nInterestingly,      RL-based      methods      share   \\n \\na      close      connection      with      classifier      guidance.\\n \\n \\n  More      specifically,      in      this      section,      following      Zhao      et      al.\\n(2024),      we      elucidate      how      RL-based      methods  \\n \\n  can      be      applied      to      conditional      generation.\\nFurthermore,      from      this      unified      viewpoint,      we      highlight  \\n \\n  that      classifier      guidance      is      seen      as   \\n \\na      value-weighted      sampling      in      Section      4.2.</p><h6>8.1      Classfier      Guidance</h6><p>ance  \\n \\n  In      this      section,      we      first      introduce      classifier      guidance.\\nClassifier      guidance      utilizes      an      unconditional  \\n \\n  pre-trained      model      for      conditional      generation.\\nMore      specifically,      we      consider   \\n \\na      scenario      where      we  \\n \\n  have   \\n \\na      pre-trained      diffusion      model,      which      enables      us      to      sample      from      p?\\n\"*(x),      and      data      with      feedback  \\n \\n  {x,y},      where   \\n \\ny      represents      the      new      label      we      want      to      condition      on.\\nOur      objective      here      is      to      sample  \\n \\n  from      p(-|y)   \\n \\n€      [Y   \\n \\n>      ACR), P(-   \\n \\n|      y)   \\n \\nx      py(yl-)pP*(-).\\n(37)</p><p>where      p,   \\n \\n:   \\n \\nX      —>      A(Y)      denotes      the      conditional      distribution      of   \\n \\ny      given      x.</p><p>Motivation.\\nIn      classifier      guidance,      we      consider      the      following      policy:</p><p>pe      (-lare,      y)   \\n \\n=   \\n \\nN      (p(22,      t;      Opre)   \\n \\n+      a(t)      Ve      log      Vy      (Xt;      t),      a*(t))   \\n \\n’ dy(x,t)   \\n \\n=      Egy.\\nyap,      (-leo)      L(Y   \\n \\n=      y)|a,   \\n \\n=      2].</p><p>Under      the      continuous-time      formulation      of      diffusion      models,      applying      Doob’s      h-transform      (Rogers  \\n \\n  and      Williams,      2000),      it      is      shown      that      we      can      sample      from      p(x|y)      by      sequentially      running      {p7\"\\\\\\\\\\\\\\'(-|7+,      y)      }i-r41-</p><p>Algorithm.\\nSpecifically,      the      algorithm      comprises      two      steps.\\nIt’s      important      to      note      that      this      is      not  \\n \\n  a      fine-tuning      method      but      rather      an      inference-time      technique      as      we      freeze      the      pre-trained      model.</p><li>1. Learning      q,(x,t)      through      regression      by      gathering      data      containing      {z;,      y}.\\nHere,      for      each  \\n \\n  (Xo,      y),      using   \\n \\na      forward      policy      for      the      pre-training      (from   \\n \\n0      to      T),      we      obtain      2;      starting      from</li><p>Lo.</p><li>2. During      inference,      sequentially      execute      {p2\"(-|a,,      y)      }1_>   \\n \\n1      by      leverage      the      fact      that      compared</li><p>gui to      the      policy      in      pre-trained      diffusion      models,      the      mean      of      {p?\\n\"\\\\\\\\\\\\\\'(-|71,      y)      }i-74,      is      just      shifted  \\n \\n  with      an      additional      term      V,,      log      q,(2,      t).</p><p> \\n \\n  Notably,      the      method      described      above      can      be      reduced      to      value-weighted      sampling      (cf.      Sec-  \\n \\n  tion      6.2),      when      r(x,      y)   \\n \\n=      log      p(y|x).\\nInspired      by      such      an      observation,      we      apply      an      RL-based  \\n \\n  fine-tuning      method      for      conditional      generation      as      below.</p><h7>8.2.      RL-Based      Fine-Tuning      for      Conditional      Generation</h7><p>tion  \\n \\n  In      this      section,      following      Zhao      et      al.\\n(2024),      we      explain      an      RL-based      fine-tuning      approach      for  \\n \\n  conditional      generation.\\nThe      core      idea      is      that      setting      r(x,      y)   \\n \\n=      log      p(y|x)      allows      us      to      sample      from  \\n \\n  the      target      conditional      distribution      (37),      drawing      on      the      insights      discussed      in      Section      4.2.  \\n \\n  More      precisely,      introducing   \\n \\na      distribution      over   \\n \\ny      as      q(-),      we      formulate      the      following      RL      problem:</p><p>{Di      ft   \\n \\n=      argmax      Egy,      (-\\\\\\\\\\\\\\\\-y)}.yott      log      p(y|xo)   \\n \\n—      NeopyaKL(pe(-|e,      y)      ||P      (late).\\n \\n \\n  {prE[(V,.X)PA(A)]      Heyy (38)</p><p> \\n \\n  Compared      to      the      objective      function      (18)      in      Section      4.2,      the      main      differences      are:      (1)      policy      space  \\n \\n  is      expanded      to      accommodate      additional      control      (i.e.,      y):      pz(-|-,-)   \\n \\nC      [R¢   \\n \\nx   \\n \\nY   \\n \\n+      A(R®)],      (2)      the  \\n \\n  fine-tuning      objectives      are      set      as      log-likelihoods:      log      p(y|z).\\nThen,      following      Theorem      1,      we      can  \\n \\n  establish      the      following      result.</p><p>Corollary      2.\\nThe      distribution      induced      by      {p*}1_74      (ue.   \\n \\n[      {Teri      Ph      (ae-1   \\n \\n|      te,      y)}deur      is      the target      conditional      distribution      (37).</p><p> \\n \\n  The      above      corollary      suggests      that      for      conditional      generation,      we      can      utilize      various      off-the-shelf  \\n \\n  algorithms      discussed      in      Section      4.2      and      6,      such      as      PPO,      reward      backpropagation,      and      reward-  \\n \\n  weighted      MLE,      as      well      as      value-weighted      sampling.</p><p>9\\n \\n   Connection      with      Flow-Based      Diffusion      Models</p><p> \\n \\n  We      elucidate      the      close      relationship      between      bridge      matching      and      fine-tuning,      as      summarized      in  \\n \\n  Table      2.\\nBridge      (flow)      matching      (Liu      et      al.,      2022;      Tong      et      al.,      2023;      Lipman      et      al.,      2023;      Liu      et      al.,  \\n \\n  2022;      Shi      et      al.,      2024;      Albergo      and      Vanden-Eijnden,      2022),      has      recently      gained      popularity      as   \\n \\na       distinct      type      of      diffusion      model      from      score-matching-based      ones.\\nThese      works      suggest      that      training  \\n \\n  and      inference      speed      will      be      accelerated      due      to      the      ability      to      define   \\n \\na      more      direct      trajectory      from  \\n \\n  noise      to      data      distribution      than      score-matching-based      diffusion      models.</p><p> \\n \\n  The      connection      between      bridge      matching      and      fine-tuning      becomes      evident      in      the      continuous-  \\n \\n  time      formulation.\\nTo      begin,      we      first      provide   \\n \\na      brief      overview      of      the      continuous-time      formulation      of  \\n \\n  score-matching-based      diffusion      models.</p><p> \\n \\n  Table      2:      Comparison      between      training      score/flow-based      diffusion      models      and      fine-tuning.\\nWe  \\n \\n  optimize      the      parameter   \\n \\n6      in      P’,      which      represents      the      induced      distribution      over      trajectories      associated  \\n \\n  with      6.</p><h8>Loss      Note      of      Q</h8><p>Score-based      KL(Q\\\\\\\\\\\\\\'™¢|P?)      Qtime:      Time-reversal      of      reference      SDE      (reference training      SDE      is      defined      from   \\n \\n0      to      T’      where      the      initial      dis-  \\n \\n  tribute      at   \\n \\n0      is      data      distribution)</p><p> \\n \\n  Flow-based      KL(Q*°>      ||P?)      @Q*°°h:      Coupling      between      the      reference      SDE      from  \\n \\n  training   \\n \\nT      to   \\n \\n0      conditioning      at   \\n \\n0      (dist.      over      x.;|7o)      and  \\n \\n  a      data      distribution      at      time   \\n \\n0      (dis.      over      x).\\nNote  \\n \\n  that      reference      SDE      is      defined      from      7’      to      0.</p><p> \\n \\n  Fine-tuning      KL(P?||Q*°)      @*:      Coupling      between      the      pre-trained      SDE  \\n \\n  from   \\n \\nT      to   \\n \\n0      conditioned      on   \\n \\n0      (dist.      over      x7.1|X0)  \\n \\n  and   \\n \\na      data      distribution      at      time   \\n \\n0      (dis.      over      Xo) 9.1      Continuous      Time      Formulation      of      Score-Based      diffusion      models</p><p> \\n \\n  We      provide   \\n \\na      brief      overview      of      the      continuous-time      formulation      of      diffusion      models      (Song      et      al.,  \\n \\n  2021),      further      expanding      Section      1.1.1.\\nIn      this      formulation,      we      initially      define      the      forward      reference  \\n \\n  SDE,      spanning      from      time   \\n \\n0      to      7’      (e.g.,      variance      exploding      (VE)      process      or      variance      preserving      (VP)  \\n \\n  process),      by      setting      an      initial      distribution      at   \\n \\n0      as      the      data      distribution.\\nAdditionally,      the      reference  \\n \\n  SDE      is      tailored      to      converge      to      an      easy      distribution      at      time      7’,      such      as      the      normal      distribution.</p><p> \\n \\n  Subsequently,      we      consider      the      time-reversal      SDE      (Anderson,      1982),      progressing      from      T\\\\\\\\\\\\\\'      to      0.\\nIf  \\n \\n  we      could      learn      this      time-reversal      SDE,      it      would      enable      us      to      sample      from      the      data      distribution      by  \\n \\n  starting      with      the      easy      initial      distribution      (at      7’)      and      following      the      time-reversal      SDE      from   \\n \\nT      to   \\n \\n0      at  \\n \\n  inference      time.</p><p> \\n \\n  Training.\\nNow,      the      remaining      question      is      how      to      learn      this      time-reversal      SDE.\\nHere,      by      repre-  \\n \\n  senting      the      induced      distribution      over      trajectories      from   \\n \\nT      to   \\n \\n0      with      this      time-reversal      SDE      as      Q\\\\\\\\\\\\\\'™®,  \\n \\n  our      goal      is      to      train   \\n \\na      new      SDE,      parameterized      by   \\n \\n@      in      the      neural      network,      such      that      the      induced  \\n \\n  distribution      P®      over      trajectories      associated      with   \\n \\n6      aligns      with      Q\"™*.\\nSpecifically,      the      objective      is      to minimize      the      following      losses:</p><p> \\n \\n  argmin      KL(Q\"™||P*).\\n(39)  \\n \\n  0  \\n \\n  With      some      algebra,      leveraging      the      observation      that      the      time-reversal      SDE      comprises      the      original  \\n \\n  forward      SDE      and      the      scoring      term      of      the      marginal      distribution      (Anderson,      1982)      as      in      (2),      Song  \\n \\n  et      al.\\n(2021)      demonstrates      that      this      KL      loss      (39)      is      approximated      as Dore   \\n \\n=      argmin      Eeuni((0,7))01~qyo0(weleo)co~pP\"e      Fe      Fl      Vey      log      jo      (Xt   \\n \\n|      Xo)   \\n \\n_      so(Xt,      t)|I?\\n],      (40)  \\n \\n  which      is      equal      to      (4)      where      the      weight      A(t)      is      {o?\\n}?\\n(for      notation,      refer      to      Section      1.1.1).</p><h9>9.2      Flow-Based      (Bridge-Based)      Diffusion      Models</h9><p>dels We      adopt      the      explanation      outlined      in      Liu      et      al.\\n(2022).\\nIn      this      framework,      we      begin      by      considering  \\n \\n  a      reference      SDE      (from      T\\\\\\\\\\\\\\'      to      0),      x;   \\n \\n=      zr_;      where      z;      is      defined      as      Brownian      motion:</p><p>dz,   \\n \\n=      dwt,      te      (0,      7).\\n(41)</p><p> \\n \\n  Subsequently,      we      consider      the      coupling      of      the      reference      SDE      conditioned      on      state      at      time   \\n \\n0      (i.e.,  \\n \\n  distribution      of      xo.7      conditional      on      x)      and      the      data      distribution      at      time      0,      denoting      the      resulting  \\n \\n  distribution      as      Q*°?,      Informally,      this      Q*°°?\\nis      characterized      as      follows.</p><p>Informal      characterization      of      Q*”.\\nWhen      discretizing      the      time      step,      by      denoting      the      distribution  \\n \\n  induced      by      the      reference      SDE      as      p\"(x7,--+   \\n \\n,      29),      we      define QF      (a7,      tae   \\n \\n,      20)   \\n \\n=      p(x,      ay   \\n \\n|      r9)pP\"°      (x9).</p><p>The      distribution      Q*”      above      is   \\n \\na      limiting      distribution      obtained      by      making      the      discretization      step small.</p><p>Training.\\nIn      bridge-based      diffusion      models,      our      objective      is      to      train   \\n \\na      new      SDE,      parametrized      by  \\n \\n  6      (e.g.,   \\n \\na      neural      network),      to      align      the      induced      distribution      P®      with      Q*°?.\\nSpecifically,      bridge-based training      aims      to      minimize aremin      KL(Q*>||P’).\\n(42)  \\n \\n  0  \\n \\n  While      the      method      to      minimize      the      above      is      currently      unclear,      we      can      derive   \\n \\na      trainable      objective  \\n \\n  through      algebraic      manipulation.\\nInitially,      we      observe      that      the      reference      SDE      (41),      conditioned      on  \\n \\n  state      xo      at      time   \\n \\n0      (zr   \\n \\n=      D),      is      expressed      as:</p><p>b\\n \\n \\n  b—      2;</p><p>t\\n \\n   €\\n \\n   [0,T];dz?\\n \\n \\n=      g?(z?,t)dt      +du,,  \\n \\n  g(z?,t)   \\n \\n=   \\n \\na       (43)</p><p>using      the      seminal      “Doob’s      h-transform”      (Rogers      and      Williams,      2000).\\nThis      SDE      is      commonly  \\n \\n  known      as      the      Brownian      bridge.\\nNow,      we      consider   \\n \\na      new      SDE      parameterized      by      0:</p><p>t\\n \\n   €\\n \\n   [0,7];      dz   \\n \\n=      s(z,t;      @)dt   \\n \\n+      dw.\\n(44)</p><p>Then,      we      can      demonstrate      that      minimizing      the      KL      loss      in      (42)      is      equivalent      to argmin      E,W      unil0,7],1~4\"°      cop?\\n[Ilg”      (2,      t)   \\n \\n_      s(21,      t;      0)      M3]      (45)</p><p>where      q;°      denotes      the      induced      distribution      at      time   \\n \\nt      following      SDE      (43)      conditioned      on      xo,      and  \\n \\n  s:      Xx      |0,T]   \\n \\n>   \\n \\n&      is      the      model      we      aim      to      optimize.</p><p> \\n \\n  Flow-based      diffusion      models.\\nThe      aforementioned      formulation      has      been      originally      proposed      as   \\n \\na       bridge-based      SDE      (Liu      et      al.,      2022)      because      the      conditional      SDE      is      often      referred      to      as   \\n \\na      Brownian  \\n \\n  bridge.\\nFlow-based      diffusion      models      proposed      in      Lipman      et      al.\\n(2023);      Tong      et      al.\\n(2023)      share   \\n \\na       close      relationship      with      bridge-based      diffusion      models.\\nThese      flow-based      models      use   \\n \\na      target      SDE (43)      and      an      SDE      with   \\n \\na      parameter      (44)      without      stochastic      terms      (1.e.,      no      dw,)      and      aim      to      minimize  \\n \\n  the      loss      function      (45).</p><p> \\n \\n  Comparison      with      score-based      diffusion      models.\\nBoth      score-based      diffusion      models      and      bridge-  \\n \\n  based      diffusion      models      aim      to      minimize      KL      divergences.\\nDespite      its      similarity,      in      contrast      to  \\n \\n  score-based      diffusion      models,      bridge-based      diffusion      models      target      Q*°>      rather      than      the      time-  \\n \\n  reversal      SDE      Q\"™®.\\nThe      distribution      Q*”      is      often      considered      preferable      because      it      can      impose      an  \\n \\n  SDE      that      navigates      efficiently      from   \\n \\n0      to      7’      (e.g.,   \\n \\na      Brownian      bridge      with      minimal      noise).\\nThis      can  \\n \\n  expedite      the      learning      process      since      we      lack      direct      control      over      the      time-reversal      SDE      in      score-based  \\n \\n  diffusion      models.</p><h10>9.3.      Connection      with      RL-Based      Fine-Tuning</h10><p>ning  \\n \\n  RL-based      fine-tuning      shares   \\n \\na      close      relationship      with      flow-based      training.\\nTo      illustrate      this,      consider  \\n \\n  the      reference      SDE      (from      7’      to      0)      following      the      pre-trained      diffusion      model.\\nThen,      akin      to      flow-  \\n \\n  based      training,      we      consider      the      coupling      of      the      reference      SDE      conditioned      on      state      at      time   \\n \\n0       (i.e.,      distribution      over      xo-r      conditioned      on      x9)      and      the      target      distribution      at      time   \\n \\n0      (i.e.,      p,(-)   \\n \\n«       exp(r(-)/a)p?\\n\"°(-)),      denoting      the      induced      distribution      as      Q*™.</p><p>Informal      characterization      of      Q™°*.\\nInformally,      Q*\"°      is      introduced      as      follows.\\nWhen      discretizing  \\n \\n  the      time      step,      by      denoting      the      distribution      induced      by      the      reference      SDE      as      p™\\\\\\\\\\\\\\'(xp,-+-   \\n \\n,      9),      we define Qi\"      (xr,   \\n \\na   \\n \\n,      0)   \\n \\n=      p(x,      nr      Ly   \\n \\n|      L0)Pr      (Xo).</p><p>The      distribution      Q*°      above      is   \\n \\na      limiting      distribution      obtained      by      making      the      discretization      step small.</p><p>Training.\\nNow,      we      introduce   \\n \\na      new      SDE      parameterized      by   \\n \\n@      such      as      an      neural      network      to      align  \\n \\n  the      induced      distribution      P’      with      Q®\"*.\\nActually,      in      the      continuous      formulation,      the      RL-problem      (18) in      Section      4.2      is      equal      to      solving argmin      KL(P?||Q\"™).\\n \\n \\n  0 This      formalization      has      been      presented      in      Uehara      et      al.\\n(2024).\\nIntuitively,      as      we      see      in      Theorem      3,  \\n \\n  this      is      expected      because      we      see      that      the      posterior      distribution      induced      by      pre-trained      models:</p><p>ref   \\n \\n(      PD      (ar,-++      21   \\n \\n|      to)      (=      vey      (er|er_-1)p      Pp      (ar-1   \\n \\n|      er_2)      +++      pp\"      (x1   \\n \\n|      20), remains      preserved      after      fine-tuning      based      on      the      RL-formulation      (18).</p><p> \\n \\n  Comparison      with      flow-based      training.\\nIn      contrast      to      flow-based      training,      RL-based      fine-tuning  \\n \\n  minimizes      the      inverse      KL      divergence      rather      than      the      KL      divergence.\\nIntuitively,      this      is      because,  \\n \\n  unlike      Qt™*      or      Q*>   \\n \\n,      we      cannot      sample      from      Q*”      (recall      that   \\n \\na      marginal      distribution      at      time   \\n \\n0      in  \\n \\n  Qi,      i.e.,      p,,      is      unnormalized);      on      the      other      hand,      the      marginal      distributions      at      time   \\n \\n0      in      both      Q\\\\\\\\\\\\\\'™*  \\n \\n  and      Q*°°>      are      data      distributions.</p><p> \\n \\n  Remark   \\n \\n7      (Extension      to      f-divergence).\\nEach      of      these      methods      can      be      technically      extended      by  \\n \\n  incorporating      more      general      divergences      beyond      KL      divergence.\\nFor      instance,      in      the      context      of  \\n \\n  fine-tuning,      please      see      Tang      (2024).</p><p>10      Connection      with      Sampling      from      Unnormalized      Distributions</p><p> \\n \\n  Numerous      studies      delve      into      sampling      from      an      unnormalized      distribution      proportional      to      exp(r()),  \\n \\n  commonly      known      as      the      Gibbs      distribution,      extensively      discussed      in      the      computational      statistics  \\n \\n  and      statistical      physics      community      (Robert,      2014).\\nThis      issue      is      pertinent      to      our      work,      as      during  \\n \\n  fine-tuning,      the      target      distribution      is      also      formulated      in      the      Gibbs      distribution      form      in      (10),      which      is  \\n \\n  proportional      to      exp(r(a))p?\\n\"*      (x).</p><p> \\n \\n  The      literature      employs      two      main      approaches      to      tackle      this      challenge:      Markov      Chain      Monte  \\n \\n  Carlo      (MCMC)      and      RL-based      methods.\\nIn      particular,      the      RL-based      diffusion      model      fine-tuning  \\n \\n  we      have      discussed      so      far      is      closely      related      to      the      latter      approach.\\nIn      this      section,      we      explain      and  \\n \\n  compare      these      two      approaches.</p><h10>10.1      Markov      Chain      Monte      Carlo      (MCMC)</h10><p>CMC)  \\n \\n  In      MCMC,   \\n \\na      Markov      chain      is      constructed      to      approximate   \\n \\na      target      distribution      such      that      the      equilib-  \\n \\n  rium      distribution      of      the      Markov      chain      converges      to      the      target      distribution.\\nHowever,      dealing      with  \\n \\n  high-dimensional      domain      spaces      is      challenging      for      MCMC.\\n \\n \\nA      common      strategy      involves      leveraging  \\n \\n  gradient      information,      such      as      the      Metropolis-adjusted      Langevin      algorithm      (MALA)      (Besag,      1994)  \\n \\n  or      Hamiltonian      Monte      Carlo      (HMC)      (Neal      et      al.,      2011;      Girolami      and      Calderhead,      2011).\\nMALA  \\n \\n  shares      similarities      with      classifier      guidance      and      value-weighted      sampling,      as      both      methods      depend  \\n \\n  on      the      first-order      information      from      reward      (or      value)      functions.</p><p> \\n \\n  Can      we      use      MCMC      for      fine-tuning      diffusion      models?\\nIn      the      context      of      fine-tuning      dif-  \\n \\n  fusion      models,      while      it      may      seem      intuitive      to      sample      from      the      target      distribution      (10)      (i.e.,  \\n \\n  Dy   \\n \\n&      exp(r(x))pP\"*(x))      using      MCMC,      it’s      not      straightforward      for      the      following      reasons:</p><p>¢\\n \\n   We      lack      of      an      analytical      form      of      p?\\n\"*(-).<ul><li>*\\n \\n   Additionally,      even      if      we      can      estimate      p?\\n’°(-)      in      an      unbiased      manner,      as      in      Chen      et      al.\\n(2018),  \\n \\n  the      mixing      time      for      obtaining   \\n \\na      single      sample      in      MCMC      might      be      lengthy.</li></ul></p><p> \\n \\n  Therefore,      we      explore      creating   \\n \\na      generative      model      (i.e.,      diffusion      model)      for      this      task      using      RL-  \\n \\n  based      fine-tuning      so      that      we      can      easily      sample      from      p,(-)      during      inference      (i.e.,      simulating      the  \\n \\n  time-reversal      SDE      (2)      with      policies      from   \\n \\n¢   \\n \\n=      T\\\\\\\\\\\\\\'+   \\n \\n1      to   \\n \\nt   \\n \\n=      1),      without      relying      on      MCMC.\\nThis  \\n \\n  addresses      the      concern      of      MCMC      by      avoiding      the      need      to      estimate      p?\\n\"*(-)      and      minimizing      inference  \\n \\n  time,      albeit      at      the      expense      of      increased      training      time.</p><p>10.2}      RL-Based      Approaches  \\n \\n  While      MCMC      has      historically      been      widely      used      for      this      purpose,      recent      works      have      proposed      an  \\n \\n  RL-based      approach      or      its      variant      (e.g.,      Bernton      et      al.      (2019);      Heng      et      al.\\n(2020);      Huang      et      al.\\n(2023);</p><p> \\n \\n  Vargas      et      al.\\n(2023)).\\nFor      example,   \\n \\na      seminal      study      (Zhang      and      Chen,      2021)      introduced   \\n \\na      method  \\n \\n  very      similar      to      reward      backpropagation.\\nIn      their      work,      they      initially      define      Brownian      motion      as   \\n \\na       reference      SDE      (from      7’      to      0),      similar      to      the      pre-trained      diffusion      models      in      our      context.\\nThen,      by  \\n \\n  appropriately      setting      rewards,      Zhang      and      Chen      (2021)      aims      to      learn   \\n \\na      new      SDE      (from      T\\\\\\\\\\\\\\'      to      0)      such  \\n \\n  that      we      can      sample      from      the      target      distribution      at      the      endpoint      0.</p><p> \\n \\n  These      approaches      offer      advantages      over      MCMC      due      to      their      ability      to      minimize      inference      time  \\n \\n  (amortization)      by      constructing   \\n \\na      generative      model      comprising      policies      in      the      training      time      and      solely  \\n \\n  executing      learned      policies      during      inference.\\nUnlike      MCMC,      these      methods      significantly      reduce      the  \\n \\n  inference      cost      as      we      don’t      need      to      be      concerned      about      the      mixing      time.\\nAdditionally,      RL-based  \\n \\n  approaches      have      the      potential      to      effectively      handle      high-dimensional,      complex      (i.e.,      multi-modal)  \\n \\n  distributions      by      harnessing      the      considerable      expressive      power      arising      from      latent      states.</p><p> \\n \\n  Despite      these      advantages,      it      is      often      unclear      how      to      select   \\n \\na      reference      SDE      in      these      works.\\nIn  \\n \\n  contrast,      in      RL-based      fine-tuning      of      diffusion      models,      we      directly      leverage      pre-trained      diffusion  \\n \\n  models      as      the      reference      SDEs,      i.e.,      as      our      prior      knowledge.</p><p> \\n \\n  Key      message.\\nFinally,      it      is      worth      noting      that      we      can      technically      utilize      any      off-the-shelf      RL      algo-  \\n \\n  rithms      for      training.\\nFor      example,      Zhang      and      Chen      (2021)      employs      reward      backpropagation,      while  \\n \\n  works      in      Gflownets      typically      leverage      PCL,      as      we      mention      in      Section      6.3.\\nHowever,      additionally,      it  \\n \\n  is      possible      to      use      PPO      or      reward-weighted      MLE      mentioned      in      Section      4.2.</p><p>11      Closely      Related      Directions</p><p>Lastly,      we      highlight      several      closely      related      directions      we      have      not      yet      discussed.</p><p> \\n \\n  Aligning      text-to-image      models.\\nWe      recognize      that      current      efforts      in      fine-tuning      diffusion      models  \\n \\n  primarily      revolve      around      aligning      text-to-image      models      using      human      feedback      (Lee      et      al.,      2023;  \\n \\n  Wallace      et      al.,      2023;      Wu      et      al.,      2023;      Yang      et      al.,      2023).\\nIn      this      paper,      our      goal      is      to      consider   \\n \\na      more  \\n \\n  general      and      fundamental      formulation      that      is      not      tailored      to      any      particular      task.</p><p> \\n \\n  Diffusion      models      for      RL.\\nDiffusion      models      are      well-suited      for      specific      reinforcement      learning  \\n \\n  applications      due      to      their      ability      to      model      complex      and      multimodal      distributions      as      polices      (Janner  \\n \\n  et      al.,      2022;      Ajay      et      al.,      2023;      Wang      et      al.,      2022;      Hansen-Estruch      et      al.,      2023;      Du      et      al.,      2024;      Zhu  \\n \\n  et      al.,      2023).\\nFor      example,      Wang      et      al.\\n(2022)      use      conditional      diffusion      models      as      policies      and  \\n \\n  demonstrate      their      effectiveness      in      typical      offline      RL      benchmarks.\\nWhile      RL-based      fine-tuning      also  \\n \\n  aims      to      maximize      certain      reward      functions,      the      primary      focus      of      this      tutorial      covers      methods      that  \\n \\n  leverage      pre-trained      diffusion      models      for      this      purpose.</p><p> \\n \\n  RL      from      human      feedback      (RLHF)      for      language      models.\\nRLHF      is      widely      discussed      in      the  \\n \\n  context      of      language      models,      where      many      algorithms      share      similarities      with      those      used      in      diffusion  \\n \\n  models      (Ouyang      et      al.,      2022;      Bai      et      al.,      2022;      Casper      et      al.,      2023).\\nFor      instance,      both      domains  \\n \\n  commonly      employ      PPO      as   \\n \\na      standard      approach,      and      reward-weighted      training      (or      reward-weighted  \\n \\n  MLE)      often      serves      as   \\n \\na      basic      baseline      approach.\\nDespite      these      similarities,      nuanced      differences  \\n \\n  exist      mainly      because      diffusion      models      typically      operate      in   \\n \\na      continuous      input      space      rather      than  \\n \\n  a      discrete      one.\\nMoreover,      techniques      like      value-weighted      sampling      emerge      uniquely      within      the  \\n \\n  context      of      diffusion      models.</p><p>12      Summary</p><p> \\n \\n  In      this      article,      we      comprehensively      explain      how      fine-tuning      diffusion      models      to      maximize      down-  \\n \\n  stream      reward      functions      can      be      formalized      as   \\n \\na      reinforcement      learning      (RL)      problem      in      entropy-  \\n \\n  regularized      Markov      decision      processes      (MDPs).\\nBased      on      this      viewpoint,      we      elaborate      on      the  \\n \\n  application      of      various      RL      algorithms,      such      as      PPO      and      reward-weighted      MLE,      specifically      tailored  \\n \\n  for      fine-tuning      diffusion      models.\\nAdditionally,      we      categorize      different      scenarios      based      on      how  \\n \\n  reward      feedback      is      obtained.\\nWhile      this      categorization      is      not      always      explicitly      mentioned      in      many  \\n \\n  existing      works,      it      is      crucial      when      selecting      appropriate      algorithms.\\nFinally,      we      discuss      the      rela-  \\n \\n  tionships      with      several      related      topics,      including      classifier      guidance,      Gflownets,      path      integral      control  \\n \\n  theory,      and      sampling      from      unnormalized      distributions.</p><h6>7.1      Fine-Tuning      with      Known,      Differentiable      Reward      Functions</h6><p>ions  \\n \\n  When      accurate      differentiable      reward      functions      are      available      (e.g.,      standard      in      computer      vision      tasks),  \\n \\n  our      primary      focus      is      typically      on      computational      and      memory      efficiency.\\nIn      general,      we      advocate      for  \\n \\n  employing      reward      backpropagation      (c.f.      Algorithm      2)      for      its      computational      efficiency.\\nAlternatively,  \\n \\n  if      memory      efficiency      is   \\n \\na      significant      concern,      we      suggest      using      PPO      due      to      its      stability.</p><h6>7.2      Fine-Tuning      with      Black-Box      Reward      Feedback</h6><p>back  \\n \\n  Here,      we      explore      scenarios      where      we      have      access      to      accurate      but      non-differentiable      (black-box)  \\n \\n  reward      feedback,      often      found      in      scientific      simulations.\\nThe      emphasis      still      remains      primarily      on  \\n \\n  computational      and      memory      efficiency.\\nIn      such      scenarios,      due      to      the      non-differentiability      of      reward  \\n \\n  feedback,      we      recommend      using      PPO      or      reward-weighted      MLE      for      the      following      reasons.</p><p> \\n \\n  In      the      preceding      section,      we      advocated      for      the      use      of      reward      backpropagation      due      to      its      com-  \\n \\n  putational      advantages.\\nHowever,      when      dealing      with      non-differentiable      feedback,      the      advantage  \\n \\n  of      reward      backpropagation      may      diminish.\\nThis      is      because      learning      from      such      feedback      requires  \\n \\n  learning      differentiable      reward      functions      to      make      the      algorithm      work      as      we      discuss      in      Section      5.2.  \\n \\n  However,      obtaining   \\n \\na      differentiable      reward      function      can      be      challenging      in      many      domains.\\nFor  \\n \\n  example,      in      molecular      property      prediction,      molecular      fingerprints      are      informative      features,      and  \\n \\n  accurate      reward      functions      can      be      derived      by      training      simple      neural      networks      on      these      features  \\n \\n  (Pattanaik      and      Coley,      2020).\\nYet,      these      mappings,      grounded      in      prior      scientific      understanding,      are  \\n \\n  non-differentiable,      thereby      restricting      their      applicability.</p><p> \\n \\n  In      contrast,      PPO      and      reward-weighted      MLE      allow      for      policy      updates      without      the      explicit  \\n \\n  requirement      to      learn   \\n \\na      differentiable      reward      function,      offering   \\n \\na      significant      advantage      over      reward  \\n \\n  backpropagation.</p><h6>7.3      Fine-Tuning      with      Unknown      Rewards      Functions</h6><p>ions  \\n \\n  When      reward      functions      (or      computational      proxies      in      Section      7.2)      are      unavailable,      we      must      learn  \\n \\n  from      data      with      reward      feedback.\\nIn      such      cases,      compared      to      the      previous      scenarios,      two      important  \\n \\n  considerations      arise:</p><p>¢\\n \\n   Not      only      computational      or      memory      efficiency      but      also      feedback      efficiency      (i.e.,      sample  \\n \\n  efficiency      regarding      reward      feedback)      is      crucial.</p><p> \\n \\n \\n¢      Given      that      learned      reward      feedback      may      not      generalize      well      outside      the      training      data      distri-  \\n \\n  butions,      it      is      essential      to      constrain      fine-tuned      models      to      prevent      significant      divergence      from  \\n \\n  diffusion      models.\\nIn      these      situations,      not      only      soft      PPO      and      direct      backpropagation      but      also  \\n \\n  methods      discussed      in      Section      6,      such      as      reward-weighted      MLE,      can      be      particularly      effective.</p><p>Further,      the      scenario      involving      unknown      reward      functions      can      be      broadly      categorized      into      two      cases,  \\n \\n  which      we      will      discuss      in      more      detail.</p><p>Offline      scenario      (Figure      4).\\nIn      many      scenarios,      even  \\n \\n  in      cases      where      reward      functions      are      unknown,      we      of-  \\n \\n.     .</p><p>Offline      data      with ten      have      access      to      offline      data      with      reward      feedback      reward      feedback</p><li>(i)      (i)   \\n \\nO   \\n \\n;   \\n \\ni       a”      ra  \\n \\n.      ne      straightforward      approach      is      to</li><p> \\n \\n \\n{   \\n \\n,   \\n \\n(   \\n \\n)    \\n.\\n    .\\n \\n   g\\n \\n   PP  \\n \\n.      <=>      Learning      reward  \\n \\n  perform      regression      with      neural      networks      and      build   \\n \\na      models  \\n \\n  learned      regressor      *.\\nHowever,      this      learned      reward      func-      SS  \\n \\n  tion   \\n \\n*      might      not      generalize      well      beyond      the      distribution      x)  \\n \\n  of      the      offline      data.\\nIn      regions      outside      this      distribution,   \\n \\n7       could      assign   \\n \\na      high      value      even      when      the      actual      reward  \\n \\n  r      is      low      due      to      high      uncertainty.\\nConsequently,      we  \\n \\n  could      be      easily      misled      by      out-of-distribution      samples  \\n \\n  if      we      solely      optimize      7.\\n|Fine-tuning      diffusion      models   \\n \\n|       One      approach      to      alleviate      this      issue      is      to      adopt   \\n \\na       pessimistic      (i.e.,      conservative)      strategy,      as      proposed      in      Figure      4:      Offline      scenario  \\n \\n  Uehara      et      al.\\n(2024).\\nThis      technique      has      been      widely      Collecting      data]   \\n \\nY      Learning      reward  \\n \\n  applied      in      the      field      of      offline      RL      (Levine,      2018).\\nThe      aa  \\n \\n  main      idea      is      to      penalize   \\n \\n7      outside      the      distributions      us-  \\n \\n  ing      techniques      such      as      adding      an      explicit      penalty      term  \\n \\n  (Yu      et      al.,      2020;      Chang      et      al.,      2021),      bootstrapping,  \\n \\n  or      more      sophisticated      methods      (Kumar      et      al.,      2020;  \\n \\n  Xie      et      al.,      2021;      Rigter      et      al.,      2022;      Uehara      and      Sun,  \\n \\n  2021).\\nBy      doing      so,      we      can      avoid      being      fooled      by  \\n \\n  out-to-distribution      regions      while      still      benefiting      from  \\n \\n  the      extrapolation      capabilities      of      reward      models.</p><p>S—      diffusion      models      |</p><p>Figure      5:      Online      scenario</p><p> \\n \\n  Online      scenario      (Figure      5).\\nConsider      situations  \\n \\n  where      reward      functions      are      unknown,      but      we      can      gather      data      online.\\nThese      scenarios      are      often  \\n \\n  referred      to      as   \\n \\na      lab-in-the-loop      setting.</p><p> \\n \\n  In      such      scenarios,      Uehara      et      al.\\n(2024)      proposes      an      iterative      procedure      comprising      three      steps:</p><li>(1)      collecting      feedback      data      by      exploring      new      areas      (i.e.,      obtaining   \\n \\nx      from      current      diffusion      models  \\n \\n  and      corresponding      r(x)),      (2)      learning      the      reward      function      from      the      collected      feedback      data,      and</li><li>(3)      fine-tuning      current      diffusion      models      by      maximizing      the      learned      reward      function      enhanced      by  \\n \\n  an      optimistic      bonus      term,      using      algorithms      discussed      in      Section      4.2.\\nIn      contrast      to      standard      online  \\n \\n  PPO,      where      feedback      data      is      directly      used      in      the      fine-tuning      process,      actual      reward      feedback      is  \\n \\n  only      used      in      step      (2)      and      not      in      step      (3).\\nAn      additional      key      aspect      is      the      use      of      an      optimistic  \\n \\n  reward      function      that      encourages      exploration      beyond      the      distributions      of      the      current      diffusion      model.\\n \\n \\n  This      deliberate      separation      between      reward      learning      and      fine-tuning      steps,      coupled      with      the      use      of  \\n \\n  optimism,      significantly      enhances      feedback      efficiency.</li><p>8\\n \\n   Connection      with      Classifier      Guidance</p><p> \\n \\n  Classifier      guidance      (Dhariwal      and      Nichol,      2021)      is      commonly      used      for      conditional      generation      in  \\n \\n  diffusion      models.\\nInterestingly,      RL-based      methods      share   \\n \\na      close      connection      with      classifier      guidance.\\n \\n \\n  More      specifically,      in      this      section,      following      Zhao      et      al.\\n(2024),      we      elucidate      how      RL-based      methods  \\n \\n  can      be      applied      to      conditional      generation.\\nFurthermore,      from      this      unified      viewpoint,      we      highlight  \\n \\n  that      classifier      guidance      is      seen      as   \\n \\na      value-weighted      sampling      in      Section      4.2.</p><h6>8.1      Classfier      Guidance</h6><p>ance  \\n \\n  In      this      section,      we      first      introduce      classifier      guidance.\\nClassifier      guidance      utilizes      an      unconditional  \\n \\n  pre-trained      model      for      conditional      generation.\\nMore      specifically,      we      consider   \\n \\na      scenario      where      we  \\n \\n  have   \\n \\na      pre-trained      diffusion      model,      which      enables      us      to      sample      from      p?\\n\"*(x),      and      data      with      feedback  \\n \\n  {x,y},      where   \\n \\ny      represents      the      new      label      we      want      to      condition      on.\\nOur      objective      here      is      to      sample  \\n \\n  from      p(-|y)   \\n \\n€      [Y   \\n \\n>      ACR), P(-   \\n \\n|      y)   \\n \\nx      py(yl-)pP*(-).\\n(37)</p><p>where      p,   \\n \\n:   \\n \\nX      —>      A(Y)      denotes      the      conditional      distribution      of   \\n \\ny      given      x.</p><p>Motivation.\\nIn      classifier      guidance,      we      consider      the      following      policy:</p><p>pe      (-lare,      y)   \\n \\n=   \\n \\nN      (p(22,      t;      Opre)   \\n \\n+      a(t)      Ve      log      Vy      (Xt;      t),      a*(t))   \\n \\n’ dy(x,t)   \\n \\n=      Egy.\\nyap,      (-leo)      L(Y   \\n \\n=      y)|a,   \\n \\n=      2].</p><p>Under      the      continuous-time      formulation      of      diffusion      models,      applying      Doob’s      h-transform      (Rogers  \\n \\n  and      Williams,      2000),      it      is      shown      that      we      can      sample      from      p(x|y)      by      sequentially      running      {p7\"\\\\\\\\\\\\\\'(-|7+,      y)      }i-r41-</p><p>Algorithm.\\nSpecifically,      the      algorithm      comprises      two      steps.\\nIt’s      important      to      note      that      this      is      not  \\n \\n  a      fine-tuning      method      but      rather      an      inference-time      technique      as      we      freeze      the      pre-trained      model.</p><li>1. Learning      q,(x,t)      through      regression      by      gathering      data      containing      {z;,      y}.\\nHere,      for      each  \\n \\n  (Xo,      y),      using   \\n \\na      forward      policy      for      the      pre-training      (from   \\n \\n0      to      T),      we      obtain      2;      starting      from</li><p>Lo.</p><li>2. During      inference,      sequentially      execute      {p2\"(-|a,,      y)      }1_>   \\n \\n1      by      leverage      the      fact      that      compared</li><p>gui to      the      policy      in      pre-trained      diffusion      models,      the      mean      of      {p?\\n\"\\\\\\\\\\\\\\'(-|71,      y)      }i-74,      is      just      shifted  \\n \\n  with      an      additional      term      V,,      log      q,(2,      t).</p><p> \\n \\n  Notably,      the      method      described      above      can      be      reduced      to      value-weighted      sampling      (cf.      Sec-  \\n \\n  tion      6.2),      when      r(x,      y)   \\n \\n=      log      p(y|x).\\nInspired      by      such      an      observation,      we      apply      an      RL-based  \\n \\n  fine-tuning      method      for      conditional      generation      as      below.</p><h7>8.2.      RL-Based      Fine-Tuning      for      Conditional      Generation</h7><p>tion  \\n \\n  In      this      section,      following      Zhao      et      al.\\n(2024),      we      explain      an      RL-based      fine-tuning      approach      for  \\n \\n  conditional      generation.\\nThe      core      idea      is      that      setting      r(x,      y)   \\n \\n=      log      p(y|x)      allows      us      to      sample      from  \\n \\n  the      target      conditional      distribution      (37),      drawing      on      the      insights      discussed      in      Section      4.2.  \\n \\n  More      precisely,      introducing   \\n \\na      distribution      over   \\n \\ny      as      q(-),      we      formulate      the      following      RL      problem:</p><p>{Di      ft   \\n \\n=      argmax      Egy,      (-\\\\\\\\\\\\\\\\-y)}.yott      log      p(y|xo)   \\n \\n—      NeopyaKL(pe(-|e,      y)      ||P      (late).\\n \\n \\n  {prE[(V,.X)PA(A)]      Heyy (38)</p><p> \\n \\n  Compared      to      the      objective      function      (18)      in      Section      4.2,      the      main      differences      are:      (1)      policy      space  \\n \\n  is      expanded      to      accommodate      additional      control      (i.e.,      y):      pz(-|-,-)   \\n \\nC      [R¢   \\n \\nx   \\n \\nY   \\n \\n+      A(R®)],      (2)      the  \\n \\n  fine-tuning      objectives      are      set      as      log-likelihoods:      log      p(y|z).\\nThen,      following      Theorem      1,      we      can  \\n \\n  establish      the      following      result.</p><p>Corollary      2.\\nThe      distribution      induced      by      {p*}1_74      (ue.   \\n \\n[      {Teri      Ph      (ae-1   \\n \\n|      te,      y)}deur      is      the target      conditional      distribution      (37).</p><p> \\n \\n  The      above      corollary      suggests      that      for      conditional      generation,      we      can      utilize      various      off-the-shelf  \\n \\n  algorithms      discussed      in      Section      4.2      and      6,      such      as      PPO,      reward      backpropagation,      and      reward-  \\n \\n  weighted      MLE,      as      well      as      value-weighted      sampling.</p><p>9\\n \\n   Connection      with      Flow-Based      Diffusion      Models</p><p> \\n \\n  We      elucidate      the      close      relationship      between      bridge      matching      and      fine-tuning,      as      summarized      in  \\n \\n  Table      2.\\nBridge      (flow)      matching      (Liu      et      al.,      2022;      Tong      et      al.,      2023;      Lipman      et      al.,      2023;      Liu      et      al.,  \\n \\n  2022;      Shi      et      al.,      2024;      Albergo      and      Vanden-Eijnden,      2022),      has      recently      gained      popularity      as   \\n \\na       distinct      type      of      diffusion      model      from      score-matching-based      ones.\\nThese      works      suggest      that      training  \\n \\n  and      inference      speed      will      be      accelerated      due      to      the      ability      to      define   \\n \\na      more      direct      trajectory      from  \\n \\n  noise      to      data      distribution      than      score-matching-based      diffusion      models.</p><p> \\n \\n  The      connection      between      bridge      matching      and      fine-tuning      becomes      evident      in      the      continuous-  \\n \\n  time      formulation.\\nTo      begin,      we      first      provide   \\n \\na      brief      overview      of      the      continuous-time      formulation      of  \\n \\n  score-matching-based      diffusion      models.</p><p> \\n \\n  Table      2:      Comparison      between      training      score/flow-based      diffusion      models      and      fine-tuning.\\nWe  \\n \\n  optimize      the      parameter   \\n \\n6      in      P’,      which      represents      the      induced      distribution      over      trajectories      associated  \\n \\n  with      6.</p><h8>Loss      Note      of      Q</h8><p>Score-based      KL(Q\\\\\\\\\\\\\\'™¢|P?)      Qtime:      Time-reversal      of      reference      SDE      (reference training      SDE      is      defined      from   \\n \\n0      to      T’      where      the      initial      dis-  \\n \\n  tribute      at   \\n \\n0      is      data      distribution)</p><p> \\n \\n  Flow-based      KL(Q*°>      ||P?)      @Q*°°h:      Coupling      between      the      reference      SDE      from  \\n \\n  training   \\n \\nT      to   \\n \\n0      conditioning      at   \\n \\n0      (dist.      over      x.;|7o)      and  \\n \\n  a      data      distribution      at      time   \\n \\n0      (dis.      over      x).\\nNote  \\n \\n  that      reference      SDE      is      defined      from      7’      to      0.</p><p> \\n \\n  Fine-tuning      KL(P?||Q*°)      @*:      Coupling      between      the      pre-trained      SDE  \\n \\n  from   \\n \\nT      to   \\n \\n0      conditioned      on   \\n \\n0      (dist.      over      x7.1|X0)  \\n \\n  and   \\n \\na      data      distribution      at      time   \\n \\n0      (dis.      over      Xo) 9.1      Continuous      Time      Formulation      of      Score-Based      diffusion      models</p><p> \\n \\n  We      provide   \\n \\na      brief      overview      of      the      continuous-time      formulation      of      diffusion      models      (Song      et      al.,  \\n \\n  2021),      further      expanding      Section      1.1.1.\\nIn      this      formulation,      we      initially      define      the      forward      reference  \\n \\n  SDE,      spanning      from      time   \\n \\n0      to      7’      (e.g.,      variance      exploding      (VE)      process      or      variance      preserving      (VP)  \\n \\n  process),      by      setting      an      initial      distribution      at   \\n \\n0      as      the      data      distribution.\\nAdditionally,      the      reference  \\n \\n  SDE      is      tailored      to      converge      to      an      easy      distribution      at      time      7’,      such      as      the      normal      distribution.</p><p> \\n \\n  Subsequently,      we      consider      the      time-reversal      SDE      (Anderson,      1982),      progressing      from      T\\\\\\\\\\\\\\'      to      0.\\nIf  \\n \\n  we      could      learn      this      time-reversal      SDE,      it      would      enable      us      to      sample      from      the      data      distribution      by  \\n \\n  starting      with      the      easy      initial      distribution      (at      7’)      and      following      the      time-reversal      SDE      from   \\n \\nT      to   \\n \\n0      at  \\n \\n  inference      time.</p><p> \\n \\n  Training.\\nNow,      the      remaining      question      is      how      to      learn      this      time-reversal      SDE.\\nHere,      by      repre-  \\n \\n  senting      the      induced      distribution      over      trajectories      from   \\n \\nT      to   \\n \\n0      with      this      time-reversal      SDE      as      Q\\\\\\\\\\\\\\'™®,  \\n \\n  our      goal      is      to      train   \\n \\na      new      SDE,      parameterized      by   \\n \\n@      in      the      neural      network,      such      that      the      induced  \\n \\n  distribution      P®      over      trajectories      associated      with   \\n \\n6      aligns      with      Q\"™*.\\nSpecifically,      the      objective      is      to minimize      the      following      losses:</p><p> \\n \\n  argmin      KL(Q\"™||P*).\\n(39)  \\n \\n  0  \\n \\n  With      some      algebra,      leveraging      the      observation      that      the      time-reversal      SDE      comprises      the      original  \\n \\n  forward      SDE      and      the      scoring      term      of      the      marginal      distribution      (Anderson,      1982)      as      in      (2),      Song  \\n \\n  et      al.\\n(2021)      demonstrates      that      this      KL      loss      (39)      is      approximated      as Dore   \\n \\n=      argmin      Eeuni((0,7))01~qyo0(weleo)co~pP\"e      Fe      Fl      Vey      log      jo      (Xt   \\n \\n|      Xo)   \\n \\n_      so(Xt,      t)|I?\\n],      (40)  \\n \\n  which      is      equal      to      (4)      where      the      weight      A(t)      is      {o?\\n}?\\n(for      notation,      refer      to      Section      1.1.1).</p><h9>9.2      Flow-Based      (Bridge-Based)      Diffusion      Models</h9><p>dels We      adopt      the      explanation      outlined      in      Liu      et      al.\\n(2022).\\nIn      this      framework,      we      begin      by      considering  \\n \\n  a      reference      SDE      (from      T\\\\\\\\\\\\\\'      to      0),      x;   \\n \\n=      zr_;      where      z;      is      defined      as      Brownian      motion:</p><p>dz,   \\n \\n=      dwt,      te      (0,      7).\\n(41)</p><p> \\n \\n  Subsequently,      we      consider      the      coupling      of      the      reference      SDE      conditioned      on      state      at      time   \\n \\n0      (i.e.,  \\n \\n  distribution      of      xo.7      conditional      on      x)      and      the      data      distribution      at      time      0,      denoting      the      resulting  \\n \\n  distribution      as      Q*°?,      Informally,      this      Q*°°?\\nis      characterized      as      follows.</p><p>Informal      characterization      of      Q*”.\\nWhen      discretizing      the      time      step,      by      denoting      the      distribution  \\n \\n  induced      by      the      reference      SDE      as      p\"(x7,--+   \\n \\n,      29),      we      define QF      (a7,      tae   \\n \\n,      20)   \\n \\n=      p(x,      ay   \\n \\n|      r9)pP\"°      (x9).</p><p>The      distribution      Q*”      above      is   \\n \\na      limiting      distribution      obtained      by      making      the      discretization      step small.</p><p>Training.\\nIn      bridge-based      diffusion      models,      our      objective      is      to      train   \\n \\na      new      SDE,      parametrized      by  \\n \\n  6      (e.g.,   \\n \\na      neural      network),      to      align      the      induced      distribution      P®      with      Q*°?.\\nSpecifically,      bridge-based training      aims      to      minimize aremin      KL(Q*>||P’).\\n(42)  \\n \\n  0  \\n \\n  While      the      method      to      minimize      the      above      is      currently      unclear,      we      can      derive   \\n \\na      trainable      objective  \\n \\n  through      algebraic      manipulation.\\nInitially,      we      observe      that      the      reference      SDE      (41),      conditioned      on  \\n \\n  state      xo      at      time   \\n \\n0      (zr   \\n \\n=      D),      is      expressed      as:</p><p>b\\n \\n \\n  b—      2;</p><p>t\\n \\n   €\\n \\n   [0,T];dz?\\n \\n \\n=      g?(z?,t)dt      +du,,  \\n \\n  g(z?,t)   \\n \\n=   \\n \\na       (43)</p><p>using      the      seminal      “Doob’s      h-transform”      (Rogers      and      Williams,      2000).\\nThis      SDE      is      commonly  \\n \\n  known      as      the      Brownian      bridge.\\nNow,      we      consider   \\n \\na      new      SDE      parameterized      by      0:</p><p>t\\n \\n   €\\n \\n   [0,7];      dz   \\n \\n=      s(z,t;      @)dt   \\n \\n+      dw.\\n(44)</p><p>Then,      we      can      demonstrate      that      minimizing      the      KL      loss      in      (42)      is      equivalent      to argmin      E,W      unil0,7],1~4\"°      cop?\\n[Ilg”      (2,      t)   \\n \\n_      s(21,      t;      0)      M3]      (45)</p><p>where      q;°      denotes      the      induced      distribution      at      time   \\n \\nt      following      SDE      (43)      conditioned      on      xo,      and  \\n \\n  s:      Xx      |0,T]   \\n \\n>   \\n \\n&      is      the      model      we      aim      to      optimize.</p><p> \\n \\n  Flow-based      diffusion      models.\\nThe      aforementioned      formulation      has      been      originally      proposed      as   \\n \\na       bridge-based      SDE      (Liu      et      al.,      2022)      because      the      conditional      SDE      is      often      referred      to      as   \\n \\na      Brownian  \\n \\n  bridge.\\nFlow-based      diffusion      models      proposed      in      Lipman      et      al.\\n(2023);      Tong      et      al.\\n(2023)      share   \\n \\na       close      relationship      with      bridge-based      diffusion      models.\\nThese      flow-based      models      use   \\n \\na      target      SDE (43)      and      an      SDE      with   \\n \\na      parameter      (44)      without      stochastic      terms      (1.e.,      no      dw,)      and      aim      to      minimize  \\n \\n  the      loss      function      (45).</p><p> \\n \\n  Comparison      with      score-based      diffusion      models.\\nBoth      score-based      diffusion      models      and      bridge-  \\n \\n  based      diffusion      models      aim      to      minimize      KL      divergences.\\nDespite      its      similarity,      in      contrast      to  \\n \\n  score-based      diffusion      models,      bridge-based      diffusion      models      target      Q*°>      rather      than      the      time-  \\n \\n  reversal      SDE      Q\"™®.\\nThe      distribution      Q*”      is      often      considered      preferable      because      it      can      impose      an  \\n \\n  SDE      that      navigates      efficiently      from   \\n \\n0      to      7’      (e.g.,   \\n \\na      Brownian      bridge      with      minimal      noise).\\nThis      can  \\n \\n  expedite      the      learning      process      since      we      lack      direct      control      over      the      time-reversal      SDE      in      score-based  \\n \\n  diffusion      models.</p><h10>9.3.      Connection      with      RL-Based      Fine-Tuning</h10><p>ning  \\n \\n  RL-based      fine-tuning      shares   \\n \\na      close      relationship      with      flow-based      training.\\nTo      illustrate      this,      consider  \\n \\n  the      reference      SDE      (from      7’      to      0)      following      the      pre-trained      diffusion      model.\\nThen,      akin      to      flow-  \\n \\n  based      training,      we      consider      the      coupling      of      the      reference      SDE      conditioned      on      state      at      time   \\n \\n0       (i.e.,      distribution      over      xo-r      conditioned      on      x9)      and      the      target      distribution      at      time   \\n \\n0      (i.e.,      p,(-)   \\n \\n«       exp(r(-)/a)p?\\n\"°(-)),      denoting      the      induced      distribution      as      Q*™.</p><p>Informal      characterization      of      Q™°*.\\nInformally,      Q*\"°      is      introduced      as      follows.\\nWhen      discretizing  \\n \\n  the      time      step,      by      denoting      the      distribution      induced      by      the      reference      SDE      as      p™\\\\\\\\\\\\\\'(xp,-+-   \\n \\n,      9),      we define Qi\"      (xr,   \\n \\na   \\n \\n,      0)   \\n \\n=      p(x,      nr      Ly   \\n \\n|      L0)Pr      (Xo).</p><p>The      distribution      Q*°      above      is   \\n \\na      limiting      distribution      obtained      by      making      the      discretization      step small.</p><p>Training.\\nNow,      we      introduce   \\n \\na      new      SDE      parameterized      by   \\n \\n@      such      as      an      neural      network      to      align  \\n \\n  the      induced      distribution      P’      with      Q®\"*.\\nActually,      in      the      continuous      formulation,      the      RL-problem      (18) in      Section      4.2      is      equal      to      solving argmin      KL(P?||Q\"™).\\n \\n \\n  0 This      formalization      has      been      presented      in      Uehara      et      al.\\n(2024).\\nIntuitively,      as      we      see      in      Theorem      3,  \\n \\n  this      is      expected      because      we      see      that      the      posterior      distribution      induced      by      pre-trained      models:</p><p>ref   \\n \\n(      PD      (ar,-++      21   \\n \\n|      to)      (=      vey      (er|er_-1)p      Pp      (ar-1   \\n \\n|      er_2)      +++      pp\"      (x1   \\n \\n|      20), remains      preserved      after      fine-tuning      based      on      the      RL-formulation      (18).</p><p> \\n \\n  Comparison      with      flow-based      training.\\nIn      contrast      to      flow-based      training,      RL-based      fine-tuning  \\n \\n  minimizes      the      inverse      KL      divergence      rather      than      the      KL      divergence.\\nIntuitively,      this      is      because,  \\n \\n  unlike      Qt™*      or      Q*>   \\n \\n,      we      cannot      sample      from      Q*”      (recall      that   \\n \\na      marginal      distribution      at      time   \\n \\n0      in  \\n \\n  Qi,      i.e.,      p,,      is      unnormalized);      on      the      other      hand,      the      marginal      distributions      at      time   \\n \\n0      in      both      Q\\\\\\\\\\\\\\'™*  \\n \\n  and      Q*°°>      are      data      distributions.</p><p> \\n \\n  Remark   \\n \\n7      (Extension      to      f-divergence).\\nEach      of      these      methods      can      be      technically      extended      by  \\n \\n  incorporating      more      general      divergences      beyond      KL      divergence.\\nFor      instance,      in      the      context      of  \\n \\n  fine-tuning,      please      see      Tang      (2024).</p><p>10      Connection      with      Sampling      from      Unnormalized      Distributions</p><p> \\n \\n  Numerous      studies      delve      into      sampling      from      an      unnormalized      distribution      proportional      to      exp(r()),  \\n \\n  commonly      known      as      the      Gibbs      distribution,      extensively      discussed      in      the      computational      statistics  \\n \\n  and      statistical      physics      community      (Robert,      2014).\\nThis      issue      is      pertinent      to      our      work,      as      during  \\n \\n  fine-tuning,      the      target      distribution      is      also      formulated      in      the      Gibbs      distribution      form      in      (10),      which      is  \\n \\n  proportional      to      exp(r(a))p?\\n\"*      (x).</p><p> \\n \\n  The      literature      employs      two      main      approaches      to      tackle      this      challenge:      Markov      Chain      Monte  \\n \\n  Carlo      (MCMC)      and      RL-based      methods.\\nIn      particular,      the      RL-based      diffusion      model      fine-tuning  \\n \\n  we      have      discussed      so      far      is      closely      related      to      the      latter      approach.\\nIn      this      section,      we      explain      and  \\n \\n  compare      these      two      approaches.</p><h10>10.1      Markov      Chain      Monte      Carlo      (MCMC)</h10><p>CMC)  \\n \\n  In      MCMC,   \\n \\na      Markov      chain      is      constructed      to      approximate   \\n \\na      target      distribution      such      that      the      equilib-  \\n \\n  rium      distribution      of      the      Markov      chain      converges      to      the      target      distribution.\\nHowever,      dealing      with  \\n \\n  high-dimensional      domain      spaces      is      challenging      for      MCMC.\\n \\n \\nA      common      strategy      involves      leveraging  \\n \\n  gradient      information,      such      as      the      Metropolis-adjusted      Langevin      algorithm      (MALA)      (Besag,      1994)  \\n \\n  or      Hamiltonian      Monte      Carlo      (HMC)      (Neal      et      al.,      2011;      Girolami      and      Calderhead,      2011).\\nMALA  \\n \\n  shares      similarities      with      classifier      guidance      and      value-weighted      sampling,      as      both      methods      depend  \\n \\n  on      the      first-order      information      from      reward      (or      value)      functions.</p><p> \\n \\n  Can      we      use      MCMC      for      fine-tuning      diffusion      models?\\nIn      the      context      of      fine-tuning      dif-  \\n \\n  fusion      models,      while      it      may      seem      intuitive      to      sample      from      the      target      distribution      (10)      (i.e.,  \\n \\n  Dy   \\n \\n&      exp(r(x))pP\"*(x))      using      MCMC,      it’s      not      straightforward      for      the      following      reasons:</p><p>¢\\n \\n   We      lack      of      an      analytical      form      of      p?\\n\"*(-).<ul><li>*\\n \\n   Additionally,      even      if      we      can      estimate      p?\\n’°(-)      in      an      unbiased      manner,      as      in      Chen      et      al.\\n(2018),  \\n \\n  the      mixing      time      for      obtaining   \\n \\na      single      sample      in      MCMC      might      be      lengthy.</li></ul></p><p> \\n \\n  Therefore,      we      explore      creating   \\n \\na      generative      model      (i.e.,      diffusion      model)      for      this      task      using      RL-  \\n \\n  based      fine-tuning      so      that      we      can      easily      sample      from      p,(-)      during      inference      (i.e.,      simulating      the  \\n \\n  time-reversal      SDE      (2)      with      policies      from   \\n \\n¢   \\n \\n=      T\\\\\\\\\\\\\\'+   \\n \\n1      to   \\n \\nt   \\n \\n=      1),      without      relying      on      MCMC.\\nThis  \\n \\n  addresses      the      concern      of      MCMC      by      avoiding      the      need      to      estimate      p?\\n\"*(-)      and      minimizing      inference  \\n \\n  time,      albeit      at      the      expense      of      increased      training      time.</p><p>10.2}      RL-Based      Approaches  \\n \\n  While      MCMC      has      historically      been      widely      used      for      this      purpose,      recent      works      have      proposed      an  \\n \\n  RL-based      approach      or      its      variant      (e.g.,      Bernton      et      al.      (2019);      Heng      et      al.\\n(2020);      Huang      et      al.\\n(2023);</p><p> \\n \\n  Vargas      et      al.\\n(2023)).\\nFor      example,   \\n \\na      seminal      study      (Zhang      and      Chen,      2021)      introduced   \\n \\na      method  \\n \\n  very      similar      to      reward      backpropagation.\\nIn      their      work,      they      initially      define      Brownian      motion      as   \\n \\na       reference      SDE      (from      7’      to      0),      similar      to      the      pre-trained      diffusion      models      in      our      context.\\nThen,      by  \\n \\n  appropriately      setting      rewards,      Zhang      and      Chen      (2021)      aims      to      learn   \\n \\na      new      SDE      (from      T\\\\\\\\\\\\\\'      to      0)      such  \\n \\n  that      we      can      sample      from      the      target      distribution      at      the      endpoint      0.</p><p> \\n \\n  These      approaches      offer      advantages      over      MCMC      due      to      their      ability      to      minimize      inference      time  \\n \\n  (amortization)      by      constructing   \\n \\na      generative      model      comprising      policies      in      the      training      time      and      solely  \\n \\n  executing      learned      policies      during      inference.\\nUnlike      MCMC,      these      methods      significantly      reduce      the  \\n \\n  inference      cost      as      we      don’t      need      to      be      concerned      about      the      mixing      time.\\nAdditionally,      RL-based  \\n \\n  approaches      have      the      potential      to      effectively      handle      high-dimensional,      complex      (i.e.,      multi-modal)  \\n \\n  distributions      by      harnessing      the      considerable      expressive      power      arising      from      latent      states.</p><p> \\n \\n  Despite      these      advantages,      it      is      often      unclear      how      to      select   \\n \\na      reference      SDE      in      these      works.\\nIn  \\n \\n  contrast,      in      RL-based      fine-tuning      of      diffusion      models,      we      directly      leverage      pre-trained      diffusion  \\n \\n  models      as      the      reference      SDEs,      i.e.,      as      our      prior      knowledge.</p><p> \\n \\n  Key      message.\\nFinally,      it      is      worth      noting      that      we      can      technically      utilize      any      off-the-shelf      RL      algo-  \\n \\n  rithms      for      training.\\nFor      example,      Zhang      and      Chen      (2021)      employs      reward      backpropagation,      while  \\n \\n  works      in      Gflownets      typically      leverage      PCL,      as      we      mention      in      Section      6.3.\\nHowever,      additionally,      it  \\n \\n  is      possible      to      use      PPO      or      reward-weighted      MLE      mentioned      in      Section      4.2.</p><p>11      Closely      Related      Directions</p><p>Lastly,      we      highlight      several      closely      related      directions      we      have      not      yet      discussed.</p><p> \\n \\n  Aligning      text-to-image      models.\\nWe      recognize      that      current      efforts      in      fine-tuning      diffusion      models  \\n \\n  primarily      revolve      around      aligning      text-to-image      models      using      human      feedback      (Lee      et      al.,      2023;  \\n \\n  Wallace      et      al.,      2023;      Wu      et      al.,      2023;      Yang      et      al.,      2023).\\nIn      this      paper,      our      goal      is      to      consider   \\n \\na      more  \\n \\n  general      and      fundamental      formulation      that      is      not      tailored      to      any      particular      task.</p><p> \\n \\n  Diffusion      models      for      RL.\\nDiffusion      models      are      well-suited      for      specific      reinforcement      learning  \\n \\n  applications      due      to      their      ability      to      model      complex      and      multimodal      distributions      as      polices      (Janner  \\n \\n  et      al.,      2022;      Ajay      et      al.,      2023;      Wang      et      al.,      2022;      Hansen-Estruch      et      al.,      2023;      Du      et      al.,      2024;      Zhu  \\n \\n  et      al.,      2023).\\nFor      example,      Wang      et      al.\\n(2022)      use      conditional      diffusion      models      as      policies      and  \\n \\n  demonstrate      their      effectiveness      in      typical      offline      RL      benchmarks.\\nWhile      RL-based      fine-tuning      also  \\n \\n  aims      to      maximize      certain      reward      functions,      the      primary      focus      of      this      tutorial      covers      methods      that  \\n \\n  leverage      pre-trained      diffusion      models      for      this      purpose.</p><p> \\n \\n  RL      from      human      feedback      (RLHF)      for      language      models.\\nRLHF      is      widely      discussed      in      the  \\n \\n  context      of      language      models,      where      many      algorithms      share      similarities      with      those      used      in      diffusion  \\n \\n  models      (Ouyang      et      al.,      2022;      Bai      et      al.,      2022;      Casper      et      al.,      2023).\\nFor      instance,      both      domains  \\n \\n  commonly      employ      PPO      as   \\n \\na      standard      approach,      and      reward-weighted      training      (or      reward-weighted  \\n \\n  MLE)      often      serves      as   \\n \\na      basic      baseline      approach.\\nDespite      these      similarities,      nuanced      differences  \\n \\n  exist      mainly      because      diffusion      models      typically      operate      in   \\n \\na      continuous      input      space      rather      than  \\n \\n  a      discrete      one.\\nMoreover,      techniques      like      value-weighted      sampling      emerge      uniquely      within      the  \\n \\n  context      of      diffusion      models.</p><p>12      Summary</p><p> \\n \\n  In      this      article,      we      comprehensively      explain      how      fine-tuning      diffusion      models      to      maximize      down-  \\n \\n  stream      reward      functions      can      be      formalized      as   \\n \\na      reinforcement      learning      (RL)      problem      in      entropy-  \\n \\n  regularized      Markov      decision      processes      (MDPs).\\nBased      on      this      viewpoint,      we      elaborate      on      the  \\n \\n  application      of      various      RL      algorithms,      such      as      PPO      and      reward-weighted      MLE,      specifically      tailored  \\n \\n  for      fine-tuning      diffusion      models.\\nAdditionally,      we      categorize      different      scenarios      based      on      how  \\n \\n  reward      feedback      is      obtained.\\nWhile      this      categorization      is      not      always      explicitly      mentioned      in      many  \\n \\n  existing      works,      it      is      crucial      when      selecting      appropriate      algorithms.\\nFinally,      we      discuss      the      rela-  \\n \\n  tionships      with      several      related      topics,      including      classifier      guidance,      Gflownets,      path      integral      control  \\n \\n  theory,      and      sampling      from      unnormalized      distributions.</p><h7>8.2.      RL-Based      Fine-Tuning      for      Conditional      Generation</h7><p>tion  \\n \\n  In      this      section,      following      Zhao      et      al.\\n(2024),      we      explain      an      RL-based      fine-tuning      approach      for  \\n \\n  conditional      generation.\\nThe      core      idea      is      that      setting      r(x,      y)   \\n \\n=      log      p(y|x)      allows      us      to      sample      from  \\n \\n  the      target      conditional      distribution      (37),      drawing      on      the      insights      discussed      in      Section      4.2.  \\n \\n  More      precisely,      introducing   \\n \\na      distribution      over   \\n \\ny      as      q(-),      we      formulate      the      following      RL      problem:</p><p>{Di      ft   \\n \\n=      argmax      Egy,      (-\\\\\\\\\\\\\\\\-y)}.yott      log      p(y|xo)   \\n \\n—      NeopyaKL(pe(-|e,      y)      ||P      (late).\\n \\n \\n  {prE[(V,.X)PA(A)]      Heyy (38)</p><p> \\n \\n  Compared      to      the      objective      function      (18)      in      Section      4.2,      the      main      differences      are:      (1)      policy      space  \\n \\n  is      expanded      to      accommodate      additional      control      (i.e.,      y):      pz(-|-,-)   \\n \\nC      [R¢   \\n \\nx   \\n \\nY   \\n \\n+      A(R®)],      (2)      the  \\n \\n  fine-tuning      objectives      are      set      as      log-likelihoods:      log      p(y|z).\\nThen,      following      Theorem      1,      we      can  \\n \\n  establish      the      following      result.</p><p>Corollary      2.\\nThe      distribution      induced      by      {p*}1_74      (ue.   \\n \\n[      {Teri      Ph      (ae-1   \\n \\n|      te,      y)}deur      is      the target      conditional      distribution      (37).</p><p> \\n \\n  The      above      corollary      suggests      that      for      conditional      generation,      we      can      utilize      various      off-the-shelf  \\n \\n  algorithms      discussed      in      Section      4.2      and      6,      such      as      PPO,      reward      backpropagation,      and      reward-  \\n \\n  weighted      MLE,      as      well      as      value-weighted      sampling.</p><p>9\\n \\n   Connection      with      Flow-Based      Diffusion      Models</p><p> \\n \\n  We      elucidate      the      close      relationship      between      bridge      matching      and      fine-tuning,      as      summarized      in  \\n \\n  Table      2.\\nBridge      (flow)      matching      (Liu      et      al.,      2022;      Tong      et      al.,      2023;      Lipman      et      al.,      2023;      Liu      et      al.,  \\n \\n  2022;      Shi      et      al.,      2024;      Albergo      and      Vanden-Eijnden,      2022),      has      recently      gained      popularity      as   \\n \\na       distinct      type      of      diffusion      model      from      score-matching-based      ones.\\nThese      works      suggest      that      training  \\n \\n  and      inference      speed      will      be      accelerated      due      to      the      ability      to      define   \\n \\na      more      direct      trajectory      from  \\n \\n  noise      to      data      distribution      than      score-matching-based      diffusion      models.</p><p> \\n \\n  The      connection      between      bridge      matching      and      fine-tuning      becomes      evident      in      the      continuous-  \\n \\n  time      formulation.\\nTo      begin,      we      first      provide   \\n \\na      brief      overview      of      the      continuous-time      formulation      of  \\n \\n  score-matching-based      diffusion      models.</p><p> \\n \\n  Table      2:      Comparison      between      training      score/flow-based      diffusion      models      and      fine-tuning.\\nWe  \\n \\n  optimize      the      parameter   \\n \\n6      in      P’,      which      represents      the      induced      distribution      over      trajectories      associated  \\n \\n  with      6.</p><h8>Loss      Note      of      Q</h8><p>Score-based      KL(Q\\\\\\\\\\\\\\'™¢|P?)      Qtime:      Time-reversal      of      reference      SDE      (reference training      SDE      is      defined      from   \\n \\n0      to      T’      where      the      initial      dis-  \\n \\n  tribute      at   \\n \\n0      is      data      distribution)</p><p> \\n \\n  Flow-based      KL(Q*°>      ||P?)      @Q*°°h:      Coupling      between      the      reference      SDE      from  \\n \\n  training   \\n \\nT      to   \\n \\n0      conditioning      at   \\n \\n0      (dist.      over      x.;|7o)      and  \\n \\n  a      data      distribution      at      time   \\n \\n0      (dis.      over      x).\\nNote  \\n \\n  that      reference      SDE      is      defined      from      7’      to      0.</p><p> \\n \\n  Fine-tuning      KL(P?||Q*°)      @*:      Coupling      between      the      pre-trained      SDE  \\n \\n  from   \\n \\nT      to   \\n \\n0      conditioned      on   \\n \\n0      (dist.      over      x7.1|X0)  \\n \\n  and   \\n \\na      data      distribution      at      time   \\n \\n0      (dis.      over      Xo) 9.1      Continuous      Time      Formulation      of      Score-Based      diffusion      models</p><p> \\n \\n  We      provide   \\n \\na      brief      overview      of      the      continuous-time      formulation      of      diffusion      models      (Song      et      al.,  \\n \\n  2021),      further      expanding      Section      1.1.1.\\nIn      this      formulation,      we      initially      define      the      forward      reference  \\n \\n  SDE,      spanning      from      time   \\n \\n0      to      7’      (e.g.,      variance      exploding      (VE)      process      or      variance      preserving      (VP)  \\n \\n  process),      by      setting      an      initial      distribution      at   \\n \\n0      as      the      data      distribution.\\nAdditionally,      the      reference  \\n \\n  SDE      is      tailored      to      converge      to      an      easy      distribution      at      time      7’,      such      as      the      normal      distribution.</p><p> \\n \\n  Subsequently,      we      consider      the      time-reversal      SDE      (Anderson,      1982),      progressing      from      T\\\\\\\\\\\\\\'      to      0.\\nIf  \\n \\n  we      could      learn      this      time-reversal      SDE,      it      would      enable      us      to      sample      from      the      data      distribution      by  \\n \\n  starting      with      the      easy      initial      distribution      (at      7’)      and      following      the      time-reversal      SDE      from   \\n \\nT      to   \\n \\n0      at  \\n \\n  inference      time.</p><p> \\n \\n  Training.\\nNow,      the      remaining      question      is      how      to      learn      this      time-reversal      SDE.\\nHere,      by      repre-  \\n \\n  senting      the      induced      distribution      over      trajectories      from   \\n \\nT      to   \\n \\n0      with      this      time-reversal      SDE      as      Q\\\\\\\\\\\\\\'™®,  \\n \\n  our      goal      is      to      train   \\n \\na      new      SDE,      parameterized      by   \\n \\n@      in      the      neural      network,      such      that      the      induced  \\n \\n  distribution      P®      over      trajectories      associated      with   \\n \\n6      aligns      with      Q\"™*.\\nSpecifically,      the      objective      is      to minimize      the      following      losses:</p><p> \\n \\n  argmin      KL(Q\"™||P*).\\n(39)  \\n \\n  0  \\n \\n  With      some      algebra,      leveraging      the      observation      that      the      time-reversal      SDE      comprises      the      original  \\n \\n  forward      SDE      and      the      scoring      term      of      the      marginal      distribution      (Anderson,      1982)      as      in      (2),      Song  \\n \\n  et      al.\\n(2021)      demonstrates      that      this      KL      loss      (39)      is      approximated      as Dore   \\n \\n=      argmin      Eeuni((0,7))01~qyo0(weleo)co~pP\"e      Fe      Fl      Vey      log      jo      (Xt   \\n \\n|      Xo)   \\n \\n_      so(Xt,      t)|I?\\n],      (40)  \\n \\n  which      is      equal      to      (4)      where      the      weight      A(t)      is      {o?\\n}?\\n(for      notation,      refer      to      Section      1.1.1).</p><h9>9.2      Flow-Based      (Bridge-Based)      Diffusion      Models</h9><p>dels We      adopt      the      explanation      outlined      in      Liu      et      al.\\n(2022).\\nIn      this      framework,      we      begin      by      considering  \\n \\n  a      reference      SDE      (from      T\\\\\\\\\\\\\\'      to      0),      x;   \\n \\n=      zr_;      where      z;      is      defined      as      Brownian      motion:</p><p>dz,   \\n \\n=      dwt,      te      (0,      7).\\n(41)</p><p> \\n \\n  Subsequently,      we      consider      the      coupling      of      the      reference      SDE      conditioned      on      state      at      time   \\n \\n0      (i.e.,  \\n \\n  distribution      of      xo.7      conditional      on      x)      and      the      data      distribution      at      time      0,      denoting      the      resulting  \\n \\n  distribution      as      Q*°?,      Informally,      this      Q*°°?\\nis      characterized      as      follows.</p><p>Informal      characterization      of      Q*”.\\nWhen      discretizing      the      time      step,      by      denoting      the      distribution  \\n \\n  induced      by      the      reference      SDE      as      p\"(x7,--+   \\n \\n,      29),      we      define QF      (a7,      tae   \\n \\n,      20)   \\n \\n=      p(x,      ay   \\n \\n|      r9)pP\"°      (x9).</p><p>The      distribution      Q*”      above      is   \\n \\na      limiting      distribution      obtained      by      making      the      discretization      step small.</p><p>Training.\\nIn      bridge-based      diffusion      models,      our      objective      is      to      train   \\n \\na      new      SDE,      parametrized      by  \\n \\n  6      (e.g.,   \\n \\na      neural      network),      to      align      the      induced      distribution      P®      with      Q*°?.\\nSpecifically,      bridge-based training      aims      to      minimize aremin      KL(Q*>||P’).\\n(42)  \\n \\n  0  \\n \\n  While      the      method      to      minimize      the      above      is      currently      unclear,      we      can      derive   \\n \\na      trainable      objective  \\n \\n  through      algebraic      manipulation.\\nInitially,      we      observe      that      the      reference      SDE      (41),      conditioned      on  \\n \\n  state      xo      at      time   \\n \\n0      (zr   \\n \\n=      D),      is      expressed      as:</p><p>b\\n \\n \\n  b—      2;</p><p>t\\n \\n   €\\n \\n   [0,T];dz?\\n \\n \\n=      g?(z?,t)dt      +du,,  \\n \\n  g(z?,t)   \\n \\n=   \\n \\na       (43)</p><p>using      the      seminal      “Doob’s      h-transform”      (Rogers      and      Williams,      2000).\\nThis      SDE      is      commonly  \\n \\n  known      as      the      Brownian      bridge.\\nNow,      we      consider   \\n \\na      new      SDE      parameterized      by      0:</p><p>t\\n \\n   €\\n \\n   [0,7];      dz   \\n \\n=      s(z,t;      @)dt   \\n \\n+      dw.\\n(44)</p><p>Then,      we      can      demonstrate      that      minimizing      the      KL      loss      in      (42)      is      equivalent      to argmin      E,W      unil0,7],1~4\"°      cop?\\n[Ilg”      (2,      t)   \\n \\n_      s(21,      t;      0)      M3]      (45)</p><p>where      q;°      denotes      the      induced      distribution      at      time   \\n \\nt      following      SDE      (43)      conditioned      on      xo,      and  \\n \\n  s:      Xx      |0,T]   \\n \\n>   \\n \\n&      is      the      model      we      aim      to      optimize.</p><p> \\n \\n  Flow-based      diffusion      models.\\nThe      aforementioned      formulation      has      been      originally      proposed      as   \\n \\na       bridge-based      SDE      (Liu      et      al.,      2022)      because      the      conditional      SDE      is      often      referred      to      as   \\n \\na      Brownian  \\n \\n  bridge.\\nFlow-based      diffusion      models      proposed      in      Lipman      et      al.\\n(2023);      Tong      et      al.\\n(2023)      share   \\n \\na       close      relationship      with      bridge-based      diffusion      models.\\nThese      flow-based      models      use   \\n \\na      target      SDE (43)      and      an      SDE      with   \\n \\na      parameter      (44)      without      stochastic      terms      (1.e.,      no      dw,)      and      aim      to      minimize  \\n \\n  the      loss      function      (45).</p><p> \\n \\n  Comparison      with      score-based      diffusion      models.\\nBoth      score-based      diffusion      models      and      bridge-  \\n \\n  based      diffusion      models      aim      to      minimize      KL      divergences.\\nDespite      its      similarity,      in      contrast      to  \\n \\n  score-based      diffusion      models,      bridge-based      diffusion      models      target      Q*°>      rather      than      the      time-  \\n \\n  reversal      SDE      Q\"™®.\\nThe      distribution      Q*”      is      often      considered      preferable      because      it      can      impose      an  \\n \\n  SDE      that      navigates      efficiently      from   \\n \\n0      to      7’      (e.g.,   \\n \\na      Brownian      bridge      with      minimal      noise).\\nThis      can  \\n \\n  expedite      the      learning      process      since      we      lack      direct      control      over      the      time-reversal      SDE      in      score-based  \\n \\n  diffusion      models.</p><h10>9.3.      Connection      with      RL-Based      Fine-Tuning</h10><p>ning  \\n \\n  RL-based      fine-tuning      shares   \\n \\na      close      relationship      with      flow-based      training.\\nTo      illustrate      this,      consider  \\n \\n  the      reference      SDE      (from      7’      to      0)      following      the      pre-trained      diffusion      model.\\nThen,      akin      to      flow-  \\n \\n  based      training,      we      consider      the      coupling      of      the      reference      SDE      conditioned      on      state      at      time   \\n \\n0       (i.e.,      distribution      over      xo-r      conditioned      on      x9)      and      the      target      distribution      at      time   \\n \\n0      (i.e.,      p,(-)   \\n \\n«       exp(r(-)/a)p?\\n\"°(-)),      denoting      the      induced      distribution      as      Q*™.</p><p>Informal      characterization      of      Q™°*.\\nInformally,      Q*\"°      is      introduced      as      follows.\\nWhen      discretizing  \\n \\n  the      time      step,      by      denoting      the      distribution      induced      by      the      reference      SDE      as      p™\\\\\\\\\\\\\\'(xp,-+-   \\n \\n,      9),      we define Qi\"      (xr,   \\n \\na   \\n \\n,      0)   \\n \\n=      p(x,      nr      Ly   \\n \\n|      L0)Pr      (Xo).</p><p>The      distribution      Q*°      above      is   \\n \\na      limiting      distribution      obtained      by      making      the      discretization      step small.</p><p>Training.\\nNow,      we      introduce   \\n \\na      new      SDE      parameterized      by   \\n \\n@      such      as      an      neural      network      to      align  \\n \\n  the      induced      distribution      P’      with      Q®\"*.\\nActually,      in      the      continuous      formulation,      the      RL-problem      (18) in      Section      4.2      is      equal      to      solving argmin      KL(P?||Q\"™).\\n \\n \\n  0 This      formalization      has      been      presented      in      Uehara      et      al.\\n(2024).\\nIntuitively,      as      we      see      in      Theorem      3,  \\n \\n  this      is      expected      because      we      see      that      the      posterior      distribution      induced      by      pre-trained      models:</p><p>ref   \\n \\n(      PD      (ar,-++      21   \\n \\n|      to)      (=      vey      (er|er_-1)p      Pp      (ar-1   \\n \\n|      er_2)      +++      pp\"      (x1   \\n \\n|      20), remains      preserved      after      fine-tuning      based      on      the      RL-formulation      (18).</p><p> \\n \\n  Comparison      with      flow-based      training.\\nIn      contrast      to      flow-based      training,      RL-based      fine-tuning  \\n \\n  minimizes      the      inverse      KL      divergence      rather      than      the      KL      divergence.\\nIntuitively,      this      is      because,  \\n \\n  unlike      Qt™*      or      Q*>   \\n \\n,      we      cannot      sample      from      Q*”      (recall      that   \\n \\na      marginal      distribution      at      time   \\n \\n0      in  \\n \\n  Qi,      i.e.,      p,,      is      unnormalized);      on      the      other      hand,      the      marginal      distributions      at      time   \\n \\n0      in      both      Q\\\\\\\\\\\\\\'™*  \\n \\n  and      Q*°°>      are      data      distributions.</p><p> \\n \\n  Remark   \\n \\n7      (Extension      to      f-divergence).\\nEach      of      these      methods      can      be      technically      extended      by  \\n \\n  incorporating      more      general      divergences      beyond      KL      divergence.\\nFor      instance,      in      the      context      of  \\n \\n  fine-tuning,      please      see      Tang      (2024).</p><p>10      Connection      with      Sampling      from      Unnormalized      Distributions</p><p> \\n \\n  Numerous      studies      delve      into      sampling      from      an      unnormalized      distribution      proportional      to      exp(r()),  \\n \\n  commonly      known      as      the      Gibbs      distribution,      extensively      discussed      in      the      computational      statistics  \\n \\n  and      statistical      physics      community      (Robert,      2014).\\nThis      issue      is      pertinent      to      our      work,      as      during  \\n \\n  fine-tuning,      the      target      distribution      is      also      formulated      in      the      Gibbs      distribution      form      in      (10),      which      is  \\n \\n  proportional      to      exp(r(a))p?\\n\"*      (x).</p><p> \\n \\n  The      literature      employs      two      main      approaches      to      tackle      this      challenge:      Markov      Chain      Monte  \\n \\n  Carlo      (MCMC)      and      RL-based      methods.\\nIn      particular,      the      RL-based      diffusion      model      fine-tuning  \\n \\n  we      have      discussed      so      far      is      closely      related      to      the      latter      approach.\\nIn      this      section,      we      explain      and  \\n \\n  compare      these      two      approaches.</p><h10>10.1      Markov      Chain      Monte      Carlo      (MCMC)</h10><p>CMC)  \\n \\n  In      MCMC,   \\n \\na      Markov      chain      is      constructed      to      approximate   \\n \\na      target      distribution      such      that      the      equilib-  \\n \\n  rium      distribution      of      the      Markov      chain      converges      to      the      target      distribution.\\nHowever,      dealing      with  \\n \\n  high-dimensional      domain      spaces      is      challenging      for      MCMC.\\n \\n \\nA      common      strategy      involves      leveraging  \\n \\n  gradient      information,      such      as      the      Metropolis-adjusted      Langevin      algorithm      (MALA)      (Besag,      1994)  \\n \\n  or      Hamiltonian      Monte      Carlo      (HMC)      (Neal      et      al.,      2011;      Girolami      and      Calderhead,      2011).\\nMALA  \\n \\n  shares      similarities      with      classifier      guidance      and      value-weighted      sampling,      as      both      methods      depend  \\n \\n  on      the      first-order      information      from      reward      (or      value)      functions.</p><p> \\n \\n  Can      we      use      MCMC      for      fine-tuning      diffusion      models?\\nIn      the      context      of      fine-tuning      dif-  \\n \\n  fusion      models,      while      it      may      seem      intuitive      to      sample      from      the      target      distribution      (10)      (i.e.,  \\n \\n  Dy   \\n \\n&      exp(r(x))pP\"*(x))      using      MCMC,      it’s      not      straightforward      for      the      following      reasons:</p><p>¢\\n \\n   We      lack      of      an      analytical      form      of      p?\\n\"*(-).<ul><li>*\\n \\n   Additionally,      even      if      we      can      estimate      p?\\n’°(-)      in      an      unbiased      manner,      as      in      Chen      et      al.\\n(2018),  \\n \\n  the      mixing      time      for      obtaining   \\n \\na      single      sample      in      MCMC      might      be      lengthy.</li></ul></p><p> \\n \\n  Therefore,      we      explore      creating   \\n \\na      generative      model      (i.e.,      diffusion      model)      for      this      task      using      RL-  \\n \\n  based      fine-tuning      so      that      we      can      easily      sample      from      p,(-)      during      inference      (i.e.,      simulating      the  \\n \\n  time-reversal      SDE      (2)      with      policies      from   \\n \\n¢   \\n \\n=      T\\\\\\\\\\\\\\'+   \\n \\n1      to   \\n \\nt   \\n \\n=      1),      without      relying      on      MCMC.\\nThis  \\n \\n  addresses      the      concern      of      MCMC      by      avoiding      the      need      to      estimate      p?\\n\"*(-)      and      minimizing      inference  \\n \\n  time,      albeit      at      the      expense      of      increased      training      time.</p><p>10.2}      RL-Based      Approaches  \\n \\n  While      MCMC      has      historically      been      widely      used      for      this      purpose,      recent      works      have      proposed      an  \\n \\n  RL-based      approach      or      its      variant      (e.g.,      Bernton      et      al.      (2019);      Heng      et      al.\\n(2020);      Huang      et      al.\\n(2023);</p><p> \\n \\n  Vargas      et      al.\\n(2023)).\\nFor      example,   \\n \\na      seminal      study      (Zhang      and      Chen,      2021)      introduced   \\n \\na      method  \\n \\n  very      similar      to      reward      backpropagation.\\nIn      their      work,      they      initially      define      Brownian      motion      as   \\n \\na       reference      SDE      (from      7’      to      0),      similar      to      the      pre-trained      diffusion      models      in      our      context.\\nThen,      by  \\n \\n  appropriately      setting      rewards,      Zhang      and      Chen      (2021)      aims      to      learn   \\n \\na      new      SDE      (from      T\\\\\\\\\\\\\\'      to      0)      such  \\n \\n  that      we      can      sample      from      the      target      distribution      at      the      endpoint      0.</p><p> \\n \\n  These      approaches      offer      advantages      over      MCMC      due      to      their      ability      to      minimize      inference      time  \\n \\n  (amortization)      by      constructing   \\n \\na      generative      model      comprising      policies      in      the      training      time      and      solely  \\n \\n  executing      learned      policies      during      inference.\\nUnlike      MCMC,      these      methods      significantly      reduce      the  \\n \\n  inference      cost      as      we      don’t      need      to      be      concerned      about      the      mixing      time.\\nAdditionally,      RL-based  \\n \\n  approaches      have      the      potential      to      effectively      handle      high-dimensional,      complex      (i.e.,      multi-modal)  \\n \\n  distributions      by      harnessing      the      considerable      expressive      power      arising      from      latent      states.</p><p> \\n \\n  Despite      these      advantages,      it      is      often      unclear      how      to      select   \\n \\na      reference      SDE      in      these      works.\\nIn  \\n \\n  contrast,      in      RL-based      fine-tuning      of      diffusion      models,      we      directly      leverage      pre-trained      diffusion  \\n \\n  models      as      the      reference      SDEs,      i.e.,      as      our      prior      knowledge.</p><p> \\n \\n  Key      message.\\nFinally,      it      is      worth      noting      that      we      can      technically      utilize      any      off-the-shelf      RL      algo-  \\n \\n  rithms      for      training.\\nFor      example,      Zhang      and      Chen      (2021)      employs      reward      backpropagation,      while  \\n \\n  works      in      Gflownets      typically      leverage      PCL,      as      we      mention      in      Section      6.3.\\nHowever,      additionally,      it  \\n \\n  is      possible      to      use      PPO      or      reward-weighted      MLE      mentioned      in      Section      4.2.</p><p>11      Closely      Related      Directions</p><p>Lastly,      we      highlight      several      closely      related      directions      we      have      not      yet      discussed.</p><p> \\n \\n  Aligning      text-to-image      models.\\nWe      recognize      that      current      efforts      in      fine-tuning      diffusion      models  \\n \\n  primarily      revolve      around      aligning      text-to-image      models      using      human      feedback      (Lee      et      al.,      2023;  \\n \\n  Wallace      et      al.,      2023;      Wu      et      al.,      2023;      Yang      et      al.,      2023).\\nIn      this      paper,      our      goal      is      to      consider   \\n \\na      more  \\n \\n  general      and      fundamental      formulation      that      is      not      tailored      to      any      particular      task.</p><p> \\n \\n  Diffusion      models      for      RL.\\nDiffusion      models      are      well-suited      for      specific      reinforcement      learning  \\n \\n  applications      due      to      their      ability      to      model      complex      and      multimodal      distributions      as      polices      (Janner  \\n \\n  et      al.,      2022;      Ajay      et      al.,      2023;      Wang      et      al.,      2022;      Hansen-Estruch      et      al.,      2023;      Du      et      al.,      2024;      Zhu  \\n \\n  et      al.,      2023).\\nFor      example,      Wang      et      al.\\n(2022)      use      conditional      diffusion      models      as      policies      and  \\n \\n  demonstrate      their      effectiveness      in      typical      offline      RL      benchmarks.\\nWhile      RL-based      fine-tuning      also  \\n \\n  aims      to      maximize      certain      reward      functions,      the      primary      focus      of      this      tutorial      covers      methods      that  \\n \\n  leverage      pre-trained      diffusion      models      for      this      purpose.</p><p> \\n \\n  RL      from      human      feedback      (RLHF)      for      language      models.\\nRLHF      is      widely      discussed      in      the  \\n \\n  context      of      language      models,      where      many      algorithms      share      similarities      with      those      used      in      diffusion  \\n \\n  models      (Ouyang      et      al.,      2022;      Bai      et      al.,      2022;      Casper      et      al.,      2023).\\nFor      instance,      both      domains  \\n \\n  commonly      employ      PPO      as   \\n \\na      standard      approach,      and      reward-weighted      training      (or      reward-weighted  \\n \\n  MLE)      often      serves      as   \\n \\na      basic      baseline      approach.\\nDespite      these      similarities,      nuanced      differences  \\n \\n  exist      mainly      because      diffusion      models      typically      operate      in   \\n \\na      continuous      input      space      rather      than  \\n \\n  a      discrete      one.\\nMoreover,      techniques      like      value-weighted      sampling      emerge      uniquely      within      the  \\n \\n  context      of      diffusion      models.</p><p>12      Summary</p><p> \\n \\n  In      this      article,      we      comprehensively      explain      how      fine-tuning      diffusion      models      to      maximize      down-  \\n \\n  stream      reward      functions      can      be      formalized      as   \\n \\na      reinforcement      learning      (RL)      problem      in      entropy-  \\n \\n  regularized      Markov      decision      processes      (MDPs).\\nBased      on      this      viewpoint,      we      elaborate      on      the  \\n \\n  application      of      various      RL      algorithms,      such      as      PPO      and      reward-weighted      MLE,      specifically      tailored  \\n \\n  for      fine-tuning      diffusion      models.\\nAdditionally,      we      categorize      different      scenarios      based      on      how  \\n \\n  reward      feedback      is      obtained.\\nWhile      this      categorization      is      not      always      explicitly      mentioned      in      many  \\n \\n  existing      works,      it      is      crucial      when      selecting      appropriate      algorithms.\\nFinally,      we      discuss      the      rela-  \\n \\n  tionships      with      several      related      topics,      including      classifier      guidance,      Gflownets,      path      integral      control  \\n \\n  theory,      and      sampling      from      unnormalized      distributions.</p><h8>Loss      Note      of      Q</h8><p>Score-based      KL(Q\\\\\\\\\\\\\\'™¢|P?)      Qtime:      Time-reversal      of      reference      SDE      (reference training      SDE      is      defined      from   \\n \\n0      to      T’      where      the      initial      dis-  \\n \\n  tribute      at   \\n \\n0      is      data      distribution)</p><p> \\n \\n  Flow-based      KL(Q*°>      ||P?)      @Q*°°h:      Coupling      between      the      reference      SDE      from  \\n \\n  training   \\n \\nT      to   \\n \\n0      conditioning      at   \\n \\n0      (dist.      over      x.;|7o)      and  \\n \\n  a      data      distribution      at      time   \\n \\n0      (dis.      over      x).\\nNote  \\n \\n  that      reference      SDE      is      defined      from      7’      to      0.</p><p> \\n \\n  Fine-tuning      KL(P?||Q*°)      @*:      Coupling      between      the      pre-trained      SDE  \\n \\n  from   \\n \\nT      to   \\n \\n0      conditioned      on   \\n \\n0      (dist.      over      x7.1|X0)  \\n \\n  and   \\n \\na      data      distribution      at      time   \\n \\n0      (dis.      over      Xo) 9.1      Continuous      Time      Formulation      of      Score-Based      diffusion      models</p><p> \\n \\n  We      provide   \\n \\na      brief      overview      of      the      continuous-time      formulation      of      diffusion      models      (Song      et      al.,  \\n \\n  2021),      further      expanding      Section      1.1.1.\\nIn      this      formulation,      we      initially      define      the      forward      reference  \\n \\n  SDE,      spanning      from      time   \\n \\n0      to      7’      (e.g.,      variance      exploding      (VE)      process      or      variance      preserving      (VP)  \\n \\n  process),      by      setting      an      initial      distribution      at   \\n \\n0      as      the      data      distribution.\\nAdditionally,      the      reference  \\n \\n  SDE      is      tailored      to      converge      to      an      easy      distribution      at      time      7’,      such      as      the      normal      distribution.</p><p> \\n \\n  Subsequently,      we      consider      the      time-reversal      SDE      (Anderson,      1982),      progressing      from      T\\\\\\\\\\\\\\'      to      0.\\nIf  \\n \\n  we      could      learn      this      time-reversal      SDE,      it      would      enable      us      to      sample      from      the      data      distribution      by  \\n \\n  starting      with      the      easy      initial      distribution      (at      7’)      and      following      the      time-reversal      SDE      from   \\n \\nT      to   \\n \\n0      at  \\n \\n  inference      time.</p><p> \\n \\n  Training.\\nNow,      the      remaining      question      is      how      to      learn      this      time-reversal      SDE.\\nHere,      by      repre-  \\n \\n  senting      the      induced      distribution      over      trajectories      from   \\n \\nT      to   \\n \\n0      with      this      time-reversal      SDE      as      Q\\\\\\\\\\\\\\'™®,  \\n \\n  our      goal      is      to      train   \\n \\na      new      SDE,      parameterized      by   \\n \\n@      in      the      neural      network,      such      that      the      induced  \\n \\n  distribution      P®      over      trajectories      associated      with   \\n \\n6      aligns      with      Q\"™*.\\nSpecifically,      the      objective      is      to minimize      the      following      losses:</p><p> \\n \\n  argmin      KL(Q\"™||P*).\\n(39)  \\n \\n  0  \\n \\n  With      some      algebra,      leveraging      the      observation      that      the      time-reversal      SDE      comprises      the      original  \\n \\n  forward      SDE      and      the      scoring      term      of      the      marginal      distribution      (Anderson,      1982)      as      in      (2),      Song  \\n \\n  et      al.\\n(2021)      demonstrates      that      this      KL      loss      (39)      is      approximated      as Dore   \\n \\n=      argmin      Eeuni((0,7))01~qyo0(weleo)co~pP\"e      Fe      Fl      Vey      log      jo      (Xt   \\n \\n|      Xo)   \\n \\n_      so(Xt,      t)|I?\\n],      (40)  \\n \\n  which      is      equal      to      (4)      where      the      weight      A(t)      is      {o?\\n}?\\n(for      notation,      refer      to      Section      1.1.1).</p><h9>9.2      Flow-Based      (Bridge-Based)      Diffusion      Models</h9><p>dels We      adopt      the      explanation      outlined      in      Liu      et      al.\\n(2022).\\nIn      this      framework,      we      begin      by      considering  \\n \\n  a      reference      SDE      (from      T\\\\\\\\\\\\\\'      to      0),      x;   \\n \\n=      zr_;      where      z;      is      defined      as      Brownian      motion:</p><p>dz,   \\n \\n=      dwt,      te      (0,      7).\\n(41)</p><p> \\n \\n  Subsequently,      we      consider      the      coupling      of      the      reference      SDE      conditioned      on      state      at      time   \\n \\n0      (i.e.,  \\n \\n  distribution      of      xo.7      conditional      on      x)      and      the      data      distribution      at      time      0,      denoting      the      resulting  \\n \\n  distribution      as      Q*°?,      Informally,      this      Q*°°?\\nis      characterized      as      follows.</p><p>Informal      characterization      of      Q*”.\\nWhen      discretizing      the      time      step,      by      denoting      the      distribution  \\n \\n  induced      by      the      reference      SDE      as      p\"(x7,--+   \\n \\n,      29),      we      define QF      (a7,      tae   \\n \\n,      20)   \\n \\n=      p(x,      ay   \\n \\n|      r9)pP\"°      (x9).</p><p>The      distribution      Q*”      above      is   \\n \\na      limiting      distribution      obtained      by      making      the      discretization      step small.</p><p>Training.\\nIn      bridge-based      diffusion      models,      our      objective      is      to      train   \\n \\na      new      SDE,      parametrized      by  \\n \\n  6      (e.g.,   \\n \\na      neural      network),      to      align      the      induced      distribution      P®      with      Q*°?.\\nSpecifically,      bridge-based training      aims      to      minimize aremin      KL(Q*>||P’).\\n(42)  \\n \\n  0  \\n \\n  While      the      method      to      minimize      the      above      is      currently      unclear,      we      can      derive   \\n \\na      trainable      objective  \\n \\n  through      algebraic      manipulation.\\nInitially,      we      observe      that      the      reference      SDE      (41),      conditioned      on  \\n \\n  state      xo      at      time   \\n \\n0      (zr   \\n \\n=      D),      is      expressed      as:</p><p>b\\n \\n \\n  b—      2;</p><p>t\\n \\n   €\\n \\n   [0,T];dz?\\n \\n \\n=      g?(z?,t)dt      +du,,  \\n \\n  g(z?,t)   \\n \\n=   \\n \\na       (43)</p><p>using      the      seminal      “Doob’s      h-transform”      (Rogers      and      Williams,      2000).\\nThis      SDE      is      commonly  \\n \\n  known      as      the      Brownian      bridge.\\nNow,      we      consider   \\n \\na      new      SDE      parameterized      by      0:</p><p>t\\n \\n   €\\n \\n   [0,7];      dz   \\n \\n=      s(z,t;      @)dt   \\n \\n+      dw.\\n(44)</p><p>Then,      we      can      demonstrate      that      minimizing      the      KL      loss      in      (42)      is      equivalent      to argmin      E,W      unil0,7],1~4\"°      cop?\\n[Ilg”      (2,      t)   \\n \\n_      s(21,      t;      0)      M3]      (45)</p><p>where      q;°      denotes      the      induced      distribution      at      time   \\n \\nt      following      SDE      (43)      conditioned      on      xo,      and  \\n \\n  s:      Xx      |0,T]   \\n \\n>   \\n \\n&      is      the      model      we      aim      to      optimize.</p><p> \\n \\n  Flow-based      diffusion      models.\\nThe      aforementioned      formulation      has      been      originally      proposed      as   \\n \\na       bridge-based      SDE      (Liu      et      al.,      2022)      because      the      conditional      SDE      is      often      referred      to      as   \\n \\na      Brownian  \\n \\n  bridge.\\nFlow-based      diffusion      models      proposed      in      Lipman      et      al.\\n(2023);      Tong      et      al.\\n(2023)      share   \\n \\na       close      relationship      with      bridge-based      diffusion      models.\\nThese      flow-based      models      use   \\n \\na      target      SDE (43)      and      an      SDE      with   \\n \\na      parameter      (44)      without      stochastic      terms      (1.e.,      no      dw,)      and      aim      to      minimize  \\n \\n  the      loss      function      (45).</p><p> \\n \\n  Comparison      with      score-based      diffusion      models.\\nBoth      score-based      diffusion      models      and      bridge-  \\n \\n  based      diffusion      models      aim      to      minimize      KL      divergences.\\nDespite      its      similarity,      in      contrast      to  \\n \\n  score-based      diffusion      models,      bridge-based      diffusion      models      target      Q*°>      rather      than      the      time-  \\n \\n  reversal      SDE      Q\"™®.\\nThe      distribution      Q*”      is      often      considered      preferable      because      it      can      impose      an  \\n \\n  SDE      that      navigates      efficiently      from   \\n \\n0      to      7’      (e.g.,   \\n \\na      Brownian      bridge      with      minimal      noise).\\nThis      can  \\n \\n  expedite      the      learning      process      since      we      lack      direct      control      over      the      time-reversal      SDE      in      score-based  \\n \\n  diffusion      models.</p><h10>9.3.      Connection      with      RL-Based      Fine-Tuning</h10><p>ning  \\n \\n  RL-based      fine-tuning      shares   \\n \\na      close      relationship      with      flow-based      training.\\nTo      illustrate      this,      consider  \\n \\n  the      reference      SDE      (from      7’      to      0)      following      the      pre-trained      diffusion      model.\\nThen,      akin      to      flow-  \\n \\n  based      training,      we      consider      the      coupling      of      the      reference      SDE      conditioned      on      state      at      time   \\n \\n0       (i.e.,      distribution      over      xo-r      conditioned      on      x9)      and      the      target      distribution      at      time   \\n \\n0      (i.e.,      p,(-)   \\n \\n«       exp(r(-)/a)p?\\n\"°(-)),      denoting      the      induced      distribution      as      Q*™.</p><p>Informal      characterization      of      Q™°*.\\nInformally,      Q*\"°      is      introduced      as      follows.\\nWhen      discretizing  \\n \\n  the      time      step,      by      denoting      the      distribution      induced      by      the      reference      SDE      as      p™\\\\\\\\\\\\\\'(xp,-+-   \\n \\n,      9),      we define Qi\"      (xr,   \\n \\na   \\n \\n,      0)   \\n \\n=      p(x,      nr      Ly   \\n \\n|      L0)Pr      (Xo).</p><p>The      distribution      Q*°      above      is   \\n \\na      limiting      distribution      obtained      by      making      the      discretization      step small.</p><p>Training.\\nNow,      we      introduce   \\n \\na      new      SDE      parameterized      by   \\n \\n@      such      as      an      neural      network      to      align  \\n \\n  the      induced      distribution      P’      with      Q®\"*.\\nActually,      in      the      continuous      formulation,      the      RL-problem      (18) in      Section      4.2      is      equal      to      solving argmin      KL(P?||Q\"™).\\n \\n \\n  0 This      formalization      has      been      presented      in      Uehara      et      al.\\n(2024).\\nIntuitively,      as      we      see      in      Theorem      3,  \\n \\n  this      is      expected      because      we      see      that      the      posterior      distribution      induced      by      pre-trained      models:</p><p>ref   \\n \\n(      PD      (ar,-++      21   \\n \\n|      to)      (=      vey      (er|er_-1)p      Pp      (ar-1   \\n \\n|      er_2)      +++      pp\"      (x1   \\n \\n|      20), remains      preserved      after      fine-tuning      based      on      the      RL-formulation      (18).</p><p> \\n \\n  Comparison      with      flow-based      training.\\nIn      contrast      to      flow-based      training,      RL-based      fine-tuning  \\n \\n  minimizes      the      inverse      KL      divergence      rather      than      the      KL      divergence.\\nIntuitively,      this      is      because,  \\n \\n  unlike      Qt™*      or      Q*>   \\n \\n,      we      cannot      sample      from      Q*”      (recall      that   \\n \\na      marginal      distribution      at      time   \\n \\n0      in  \\n \\n  Qi,      i.e.,      p,,      is      unnormalized);      on      the      other      hand,      the      marginal      distributions      at      time   \\n \\n0      in      both      Q\\\\\\\\\\\\\\'™*  \\n \\n  and      Q*°°>      are      data      distributions.</p><p> \\n \\n  Remark   \\n \\n7      (Extension      to      f-divergence).\\nEach      of      these      methods      can      be      technically      extended      by  \\n \\n  incorporating      more      general      divergences      beyond      KL      divergence.\\nFor      instance,      in      the      context      of  \\n \\n  fine-tuning,      please      see      Tang      (2024).</p><p>10      Connection      with      Sampling      from      Unnormalized      Distributions</p><p> \\n \\n  Numerous      studies      delve      into      sampling      from      an      unnormalized      distribution      proportional      to      exp(r()),  \\n \\n  commonly      known      as      the      Gibbs      distribution,      extensively      discussed      in      the      computational      statistics  \\n \\n  and      statistical      physics      community      (Robert,      2014).\\nThis      issue      is      pertinent      to      our      work,      as      during  \\n \\n  fine-tuning,      the      target      distribution      is      also      formulated      in      the      Gibbs      distribution      form      in      (10),      which      is  \\n \\n  proportional      to      exp(r(a))p?\\n\"*      (x).</p><p> \\n \\n  The      literature      employs      two      main      approaches      to      tackle      this      challenge:      Markov      Chain      Monte  \\n \\n  Carlo      (MCMC)      and      RL-based      methods.\\nIn      particular,      the      RL-based      diffusion      model      fine-tuning  \\n \\n  we      have      discussed      so      far      is      closely      related      to      the      latter      approach.\\nIn      this      section,      we      explain      and  \\n \\n  compare      these      two      approaches.</p><h10>10.1      Markov      Chain      Monte      Carlo      (MCMC)</h10><p>CMC)  \\n \\n  In      MCMC,   \\n \\na      Markov      chain      is      constructed      to      approximate   \\n \\na      target      distribution      such      that      the      equilib-  \\n \\n  rium      distribution      of      the      Markov      chain      converges      to      the      target      distribution.\\nHowever,      dealing      with  \\n \\n  high-dimensional      domain      spaces      is      challenging      for      MCMC.\\n \\n \\nA      common      strategy      involves      leveraging  \\n \\n  gradient      information,      such      as      the      Metropolis-adjusted      Langevin      algorithm      (MALA)      (Besag,      1994)  \\n \\n  or      Hamiltonian      Monte      Carlo      (HMC)      (Neal      et      al.,      2011;      Girolami      and      Calderhead,      2011).\\nMALA  \\n \\n  shares      similarities      with      classifier      guidance      and      value-weighted      sampling,      as      both      methods      depend  \\n \\n  on      the      first-order      information      from      reward      (or      value)      functions.</p><p> \\n \\n  Can      we      use      MCMC      for      fine-tuning      diffusion      models?\\nIn      the      context      of      fine-tuning      dif-  \\n \\n  fusion      models,      while      it      may      seem      intuitive      to      sample      from      the      target      distribution      (10)      (i.e.,  \\n \\n  Dy   \\n \\n&      exp(r(x))pP\"*(x))      using      MCMC,      it’s      not      straightforward      for      the      following      reasons:</p><p>¢\\n \\n   We      lack      of      an      analytical      form      of      p?\\n\"*(-).<ul><li>*\\n \\n   Additionally,      even      if      we      can      estimate      p?\\n’°(-)      in      an      unbiased      manner,      as      in      Chen      et      al.\\n(2018),  \\n \\n  the      mixing      time      for      obtaining   \\n \\na      single      sample      in      MCMC      might      be      lengthy.</li></ul></p><p> \\n \\n  Therefore,      we      explore      creating   \\n \\na      generative      model      (i.e.,      diffusion      model)      for      this      task      using      RL-  \\n \\n  based      fine-tuning      so      that      we      can      easily      sample      from      p,(-)      during      inference      (i.e.,      simulating      the  \\n \\n  time-reversal      SDE      (2)      with      policies      from   \\n \\n¢   \\n \\n=      T\\\\\\\\\\\\\\'+   \\n \\n1      to   \\n \\nt   \\n \\n=      1),      without      relying      on      MCMC.\\nThis  \\n \\n  addresses      the      concern      of      MCMC      by      avoiding      the      need      to      estimate      p?\\n\"*(-)      and      minimizing      inference  \\n \\n  time,      albeit      at      the      expense      of      increased      training      time.</p><p>10.2}      RL-Based      Approaches  \\n \\n  While      MCMC      has      historically      been      widely      used      for      this      purpose,      recent      works      have      proposed      an  \\n \\n  RL-based      approach      or      its      variant      (e.g.,      Bernton      et      al.      (2019);      Heng      et      al.\\n(2020);      Huang      et      al.\\n(2023);</p><p> \\n \\n  Vargas      et      al.\\n(2023)).\\nFor      example,   \\n \\na      seminal      study      (Zhang      and      Chen,      2021)      introduced   \\n \\na      method  \\n \\n  very      similar      to      reward      backpropagation.\\nIn      their      work,      they      initially      define      Brownian      motion      as   \\n \\na       reference      SDE      (from      7’      to      0),      similar      to      the      pre-trained      diffusion      models      in      our      context.\\nThen,      by  \\n \\n  appropriately      setting      rewards,      Zhang      and      Chen      (2021)      aims      to      learn   \\n \\na      new      SDE      (from      T\\\\\\\\\\\\\\'      to      0)      such  \\n \\n  that      we      can      sample      from      the      target      distribution      at      the      endpoint      0.</p><p> \\n \\n  These      approaches      offer      advantages      over      MCMC      due      to      their      ability      to      minimize      inference      time  \\n \\n  (amortization)      by      constructing   \\n \\na      generative      model      comprising      policies      in      the      training      time      and      solely  \\n \\n  executing      learned      policies      during      inference.\\nUnlike      MCMC,      these      methods      significantly      reduce      the  \\n \\n  inference      cost      as      we      don’t      need      to      be      concerned      about      the      mixing      time.\\nAdditionally,      RL-based  \\n \\n  approaches      have      the      potential      to      effectively      handle      high-dimensional,      complex      (i.e.,      multi-modal)  \\n \\n  distributions      by      harnessing      the      considerable      expressive      power      arising      from      latent      states.</p><p> \\n \\n  Despite      these      advantages,      it      is      often      unclear      how      to      select   \\n \\na      reference      SDE      in      these      works.\\nIn  \\n \\n  contrast,      in      RL-based      fine-tuning      of      diffusion      models,      we      directly      leverage      pre-trained      diffusion  \\n \\n  models      as      the      reference      SDEs,      i.e.,      as      our      prior      knowledge.</p><p> \\n \\n  Key      message.\\nFinally,      it      is      worth      noting      that      we      can      technically      utilize      any      off-the-shelf      RL      algo-  \\n \\n  rithms      for      training.\\nFor      example,      Zhang      and      Chen      (2021)      employs      reward      backpropagation,      while  \\n \\n  works      in      Gflownets      typically      leverage      PCL,      as      we      mention      in      Section      6.3.\\nHowever,      additionally,      it  \\n \\n  is      possible      to      use      PPO      or      reward-weighted      MLE      mentioned      in      Section      4.2.</p><p>11      Closely      Related      Directions</p><p>Lastly,      we      highlight      several      closely      related      directions      we      have      not      yet      discussed.</p><p> \\n \\n  Aligning      text-to-image      models.\\nWe      recognize      that      current      efforts      in      fine-tuning      diffusion      models  \\n \\n  primarily      revolve      around      aligning      text-to-image      models      using      human      feedback      (Lee      et      al.,      2023;  \\n \\n  Wallace      et      al.,      2023;      Wu      et      al.,      2023;      Yang      et      al.,      2023).\\nIn      this      paper,      our      goal      is      to      consider   \\n \\na      more  \\n \\n  general      and      fundamental      formulation      that      is      not      tailored      to      any      particular      task.</p><p> \\n \\n  Diffusion      models      for      RL.\\nDiffusion      models      are      well-suited      for      specific      reinforcement      learning  \\n \\n  applications      due      to      their      ability      to      model      complex      and      multimodal      distributions      as      polices      (Janner  \\n \\n  et      al.,      2022;      Ajay      et      al.,      2023;      Wang      et      al.,      2022;      Hansen-Estruch      et      al.,      2023;      Du      et      al.,      2024;      Zhu  \\n \\n  et      al.,      2023).\\nFor      example,      Wang      et      al.\\n(2022)      use      conditional      diffusion      models      as      policies      and  \\n \\n  demonstrate      their      effectiveness      in      typical      offline      RL      benchmarks.\\nWhile      RL-based      fine-tuning      also  \\n \\n  aims      to      maximize      certain      reward      functions,      the      primary      focus      of      this      tutorial      covers      methods      that  \\n \\n  leverage      pre-trained      diffusion      models      for      this      purpose.</p><p> \\n \\n  RL      from      human      feedback      (RLHF)      for      language      models.\\nRLHF      is      widely      discussed      in      the  \\n \\n  context      of      language      models,      where      many      algorithms      share      similarities      with      those      used      in      diffusion  \\n \\n  models      (Ouyang      et      al.,      2022;      Bai      et      al.,      2022;      Casper      et      al.,      2023).\\nFor      instance,      both      domains  \\n \\n  commonly      employ      PPO      as   \\n \\na      standard      approach,      and      reward-weighted      training      (or      reward-weighted  \\n \\n  MLE)      often      serves      as   \\n \\na      basic      baseline      approach.\\nDespite      these      similarities,      nuanced      differences  \\n \\n  exist      mainly      because      diffusion      models      typically      operate      in   \\n \\na      continuous      input      space      rather      than  \\n \\n  a      discrete      one.\\nMoreover,      techniques      like      value-weighted      sampling      emerge      uniquely      within      the  \\n \\n  context      of      diffusion      models.</p><p>12      Summary</p><p> \\n \\n  In      this      article,      we      comprehensively      explain      how      fine-tuning      diffusion      models      to      maximize      down-  \\n \\n  stream      reward      functions      can      be      formalized      as   \\n \\na      reinforcement      learning      (RL)      problem      in      entropy-  \\n \\n  regularized      Markov      decision      processes      (MDPs).\\nBased      on      this      viewpoint,      we      elaborate      on      the  \\n \\n  application      of      various      RL      algorithms,      such      as      PPO      and      reward-weighted      MLE,      specifically      tailored  \\n \\n  for      fine-tuning      diffusion      models.\\nAdditionally,      we      categorize      different      scenarios      based      on      how  \\n \\n  reward      feedback      is      obtained.\\nWhile      this      categorization      is      not      always      explicitly      mentioned      in      many  \\n \\n  existing      works,      it      is      crucial      when      selecting      appropriate      algorithms.\\nFinally,      we      discuss      the      rela-  \\n \\n  tionships      with      several      related      topics,      including      classifier      guidance,      Gflownets,      path      integral      control  \\n \\n  theory,      and      sampling      from      unnormalized      distributions.</p><h9>9.2      Flow-Based      (Bridge-Based)      Diffusion      Models</h9><p>dels We      adopt      the      explanation      outlined      in      Liu      et      al.\\n(2022).\\nIn      this      framework,      we      begin      by      considering  \\n \\n  a      reference      SDE      (from      T\\\\\\\\\\\\\\'      to      0),      x;   \\n \\n=      zr_;      where      z;      is      defined      as      Brownian      motion:</p><p>dz,   \\n \\n=      dwt,      te      (0,      7).\\n(41)</p><p> \\n \\n  Subsequently,      we      consider      the      coupling      of      the      reference      SDE      conditioned      on      state      at      time   \\n \\n0      (i.e.,  \\n \\n  distribution      of      xo.7      conditional      on      x)      and      the      data      distribution      at      time      0,      denoting      the      resulting  \\n \\n  distribution      as      Q*°?,      Informally,      this      Q*°°?\\nis      characterized      as      follows.</p><p>Informal      characterization      of      Q*”.\\nWhen      discretizing      the      time      step,      by      denoting      the      distribution  \\n \\n  induced      by      the      reference      SDE      as      p\"(x7,--+   \\n \\n,      29),      we      define QF      (a7,      tae   \\n \\n,      20)   \\n \\n=      p(x,      ay   \\n \\n|      r9)pP\"°      (x9).</p><p>The      distribution      Q*”      above      is   \\n \\na      limiting      distribution      obtained      by      making      the      discretization      step small.</p><p>Training.\\nIn      bridge-based      diffusion      models,      our      objective      is      to      train   \\n \\na      new      SDE,      parametrized      by  \\n \\n  6      (e.g.,   \\n \\na      neural      network),      to      align      the      induced      distribution      P®      with      Q*°?.\\nSpecifically,      bridge-based training      aims      to      minimize aremin      KL(Q*>||P’).\\n(42)  \\n \\n  0  \\n \\n  While      the      method      to      minimize      the      above      is      currently      unclear,      we      can      derive   \\n \\na      trainable      objective  \\n \\n  through      algebraic      manipulation.\\nInitially,      we      observe      that      the      reference      SDE      (41),      conditioned      on  \\n \\n  state      xo      at      time   \\n \\n0      (zr   \\n \\n=      D),      is      expressed      as:</p><p>b\\n \\n \\n  b—      2;</p><p>t\\n \\n   €\\n \\n   [0,T];dz?\\n \\n \\n=      g?(z?,t)dt      +du,,  \\n \\n  g(z?,t)   \\n \\n=   \\n \\na       (43)</p><p>using      the      seminal      “Doob’s      h-transform”      (Rogers      and      Williams,      2000).\\nThis      SDE      is      commonly  \\n \\n  known      as      the      Brownian      bridge.\\nNow,      we      consider   \\n \\na      new      SDE      parameterized      by      0:</p><p>t\\n \\n   €\\n \\n   [0,7];      dz   \\n \\n=      s(z,t;      @)dt   \\n \\n+      dw.\\n(44)</p><p>Then,      we      can      demonstrate      that      minimizing      the      KL      loss      in      (42)      is      equivalent      to argmin      E,W      unil0,7],1~4\"°      cop?\\n[Ilg”      (2,      t)   \\n \\n_      s(21,      t;      0)      M3]      (45)</p><p>where      q;°      denotes      the      induced      distribution      at      time   \\n \\nt      following      SDE      (43)      conditioned      on      xo,      and  \\n \\n  s:      Xx      |0,T]   \\n \\n>   \\n \\n&      is      the      model      we      aim      to      optimize.</p><p> \\n \\n  Flow-based      diffusion      models.\\nThe      aforementioned      formulation      has      been      originally      proposed      as   \\n \\na       bridge-based      SDE      (Liu      et      al.,      2022)      because      the      conditional      SDE      is      often      referred      to      as   \\n \\na      Brownian  \\n \\n  bridge.\\nFlow-based      diffusion      models      proposed      in      Lipman      et      al.\\n(2023);      Tong      et      al.\\n(2023)      share   \\n \\na       close      relationship      with      bridge-based      diffusion      models.\\nThese      flow-based      models      use   \\n \\na      target      SDE (43)      and      an      SDE      with   \\n \\na      parameter      (44)      without      stochastic      terms      (1.e.,      no      dw,)      and      aim      to      minimize  \\n \\n  the      loss      function      (45).</p><p> \\n \\n  Comparison      with      score-based      diffusion      models.\\nBoth      score-based      diffusion      models      and      bridge-  \\n \\n  based      diffusion      models      aim      to      minimize      KL      divergences.\\nDespite      its      similarity,      in      contrast      to  \\n \\n  score-based      diffusion      models,      bridge-based      diffusion      models      target      Q*°>      rather      than      the      time-  \\n \\n  reversal      SDE      Q\"™®.\\nThe      distribution      Q*”      is      often      considered      preferable      because      it      can      impose      an  \\n \\n  SDE      that      navigates      efficiently      from   \\n \\n0      to      7’      (e.g.,   \\n \\na      Brownian      bridge      with      minimal      noise).\\nThis      can  \\n \\n  expedite      the      learning      process      since      we      lack      direct      control      over      the      time-reversal      SDE      in      score-based  \\n \\n  diffusion      models.</p><h10>9.3.      Connection      with      RL-Based      Fine-Tuning</h10><p>ning  \\n \\n  RL-based      fine-tuning      shares   \\n \\na      close      relationship      with      flow-based      training.\\nTo      illustrate      this,      consider  \\n \\n  the      reference      SDE      (from      7’      to      0)      following      the      pre-trained      diffusion      model.\\nThen,      akin      to      flow-  \\n \\n  based      training,      we      consider      the      coupling      of      the      reference      SDE      conditioned      on      state      at      time   \\n \\n0       (i.e.,      distribution      over      xo-r      conditioned      on      x9)      and      the      target      distribution      at      time   \\n \\n0      (i.e.,      p,(-)   \\n \\n«       exp(r(-)/a)p?\\n\"°(-)),      denoting      the      induced      distribution      as      Q*™.</p><p>Informal      characterization      of      Q™°*.\\nInformally,      Q*\"°      is      introduced      as      follows.\\nWhen      discretizing  \\n \\n  the      time      step,      by      denoting      the      distribution      induced      by      the      reference      SDE      as      p™\\\\\\\\\\\\\\'(xp,-+-   \\n \\n,      9),      we define Qi\"      (xr,   \\n \\na   \\n \\n,      0)   \\n \\n=      p(x,      nr      Ly   \\n \\n|      L0)Pr      (Xo).</p><p>The      distribution      Q*°      above      is   \\n \\na      limiting      distribution      obtained      by      making      the      discretization      step small.</p><p>Training.\\nNow,      we      introduce   \\n \\na      new      SDE      parameterized      by   \\n \\n@      such      as      an      neural      network      to      align  \\n \\n  the      induced      distribution      P’      with      Q®\"*.\\nActually,      in      the      continuous      formulation,      the      RL-problem      (18) in      Section      4.2      is      equal      to      solving argmin      KL(P?||Q\"™).\\n \\n \\n  0 This      formalization      has      been      presented      in      Uehara      et      al.\\n(2024).\\nIntuitively,      as      we      see      in      Theorem      3,  \\n \\n  this      is      expected      because      we      see      that      the      posterior      distribution      induced      by      pre-trained      models:</p><p>ref   \\n \\n(      PD      (ar,-++      21   \\n \\n|      to)      (=      vey      (er|er_-1)p      Pp      (ar-1   \\n \\n|      er_2)      +++      pp\"      (x1   \\n \\n|      20), remains      preserved      after      fine-tuning      based      on      the      RL-formulation      (18).</p><p> \\n \\n  Comparison      with      flow-based      training.\\nIn      contrast      to      flow-based      training,      RL-based      fine-tuning  \\n \\n  minimizes      the      inverse      KL      divergence      rather      than      the      KL      divergence.\\nIntuitively,      this      is      because,  \\n \\n  unlike      Qt™*      or      Q*>   \\n \\n,      we      cannot      sample      from      Q*”      (recall      that   \\n \\na      marginal      distribution      at      time   \\n \\n0      in  \\n \\n  Qi,      i.e.,      p,,      is      unnormalized);      on      the      other      hand,      the      marginal      distributions      at      time   \\n \\n0      in      both      Q\\\\\\\\\\\\\\'™*  \\n \\n  and      Q*°°>      are      data      distributions.</p><p> \\n \\n  Remark   \\n \\n7      (Extension      to      f-divergence).\\nEach      of      these      methods      can      be      technically      extended      by  \\n \\n  incorporating      more      general      divergences      beyond      KL      divergence.\\nFor      instance,      in      the      context      of  \\n \\n  fine-tuning,      please      see      Tang      (2024).</p><p>10      Connection      with      Sampling      from      Unnormalized      Distributions</p><p> \\n \\n  Numerous      studies      delve      into      sampling      from      an      unnormalized      distribution      proportional      to      exp(r()),  \\n \\n  commonly      known      as      the      Gibbs      distribution,      extensively      discussed      in      the      computational      statistics  \\n \\n  and      statistical      physics      community      (Robert,      2014).\\nThis      issue      is      pertinent      to      our      work,      as      during  \\n \\n  fine-tuning,      the      target      distribution      is      also      formulated      in      the      Gibbs      distribution      form      in      (10),      which      is  \\n \\n  proportional      to      exp(r(a))p?\\n\"*      (x).</p><p> \\n \\n  The      literature      employs      two      main      approaches      to      tackle      this      challenge:      Markov      Chain      Monte  \\n \\n  Carlo      (MCMC)      and      RL-based      methods.\\nIn      particular,      the      RL-based      diffusion      model      fine-tuning  \\n \\n  we      have      discussed      so      far      is      closely      related      to      the      latter      approach.\\nIn      this      section,      we      explain      and  \\n \\n  compare      these      two      approaches.</p><h10>10.1      Markov      Chain      Monte      Carlo      (MCMC)</h10><p>CMC)  \\n \\n  In      MCMC,   \\n \\na      Markov      chain      is      constructed      to      approximate   \\n \\na      target      distribution      such      that      the      equilib-  \\n \\n  rium      distribution      of      the      Markov      chain      converges      to      the      target      distribution.\\nHowever,      dealing      with  \\n \\n  high-dimensional      domain      spaces      is      challenging      for      MCMC.\\n \\n \\nA      common      strategy      involves      leveraging  \\n \\n  gradient      information,      such      as      the      Metropolis-adjusted      Langevin      algorithm      (MALA)      (Besag,      1994)  \\n \\n  or      Hamiltonian      Monte      Carlo      (HMC)      (Neal      et      al.,      2011;      Girolami      and      Calderhead,      2011).\\nMALA  \\n \\n  shares      similarities      with      classifier      guidance      and      value-weighted      sampling,      as      both      methods      depend  \\n \\n  on      the      first-order      information      from      reward      (or      value)      functions.</p><p> \\n \\n  Can      we      use      MCMC      for      fine-tuning      diffusion      models?\\nIn      the      context      of      fine-tuning      dif-  \\n \\n  fusion      models,      while      it      may      seem      intuitive      to      sample      from      the      target      distribution      (10)      (i.e.,  \\n \\n  Dy   \\n \\n&      exp(r(x))pP\"*(x))      using      MCMC,      it’s      not      straightforward      for      the      following      reasons:</p><p>¢\\n \\n   We      lack      of      an      analytical      form      of      p?\\n\"*(-).<ul><li>*\\n \\n   Additionally,      even      if      we      can      estimate      p?\\n’°(-)      in      an      unbiased      manner,      as      in      Chen      et      al.\\n(2018),  \\n \\n  the      mixing      time      for      obtaining   \\n \\na      single      sample      in      MCMC      might      be      lengthy.</li></ul></p><p> \\n \\n  Therefore,      we      explore      creating   \\n \\na      generative      model      (i.e.,      diffusion      model)      for      this      task      using      RL-  \\n \\n  based      fine-tuning      so      that      we      can      easily      sample      from      p,(-)      during      inference      (i.e.,      simulating      the  \\n \\n  time-reversal      SDE      (2)      with      policies      from   \\n \\n¢   \\n \\n=      T\\\\\\\\\\\\\\'+   \\n \\n1      to   \\n \\nt   \\n \\n=      1),      without      relying      on      MCMC.\\nThis  \\n \\n  addresses      the      concern      of      MCMC      by      avoiding      the      need      to      estimate      p?\\n\"*(-)      and      minimizing      inference  \\n \\n  time,      albeit      at      the      expense      of      increased      training      time.</p><p>10.2}      RL-Based      Approaches  \\n \\n  While      MCMC      has      historically      been      widely      used      for      this      purpose,      recent      works      have      proposed      an  \\n \\n  RL-based      approach      or      its      variant      (e.g.,      Bernton      et      al.      (2019);      Heng      et      al.\\n(2020);      Huang      et      al.\\n(2023);</p><p> \\n \\n  Vargas      et      al.\\n(2023)).\\nFor      example,   \\n \\na      seminal      study      (Zhang      and      Chen,      2021)      introduced   \\n \\na      method  \\n \\n  very      similar      to      reward      backpropagation.\\nIn      their      work,      they      initially      define      Brownian      motion      as   \\n \\na       reference      SDE      (from      7’      to      0),      similar      to      the      pre-trained      diffusion      models      in      our      context.\\nThen,      by  \\n \\n  appropriately      setting      rewards,      Zhang      and      Chen      (2021)      aims      to      learn   \\n \\na      new      SDE      (from      T\\\\\\\\\\\\\\'      to      0)      such  \\n \\n  that      we      can      sample      from      the      target      distribution      at      the      endpoint      0.</p><p> \\n \\n  These      approaches      offer      advantages      over      MCMC      due      to      their      ability      to      minimize      inference      time  \\n \\n  (amortization)      by      constructing   \\n \\na      generative      model      comprising      policies      in      the      training      time      and      solely  \\n \\n  executing      learned      policies      during      inference.\\nUnlike      MCMC,      these      methods      significantly      reduce      the  \\n \\n  inference      cost      as      we      don’t      need      to      be      concerned      about      the      mixing      time.\\nAdditionally,      RL-based  \\n \\n  approaches      have      the      potential      to      effectively      handle      high-dimensional,      complex      (i.e.,      multi-modal)  \\n \\n  distributions      by      harnessing      the      considerable      expressive      power      arising      from      latent      states.</p><p> \\n \\n  Despite      these      advantages,      it      is      often      unclear      how      to      select   \\n \\na      reference      SDE      in      these      works.\\nIn  \\n \\n  contrast,      in      RL-based      fine-tuning      of      diffusion      models,      we      directly      leverage      pre-trained      diffusion  \\n \\n  models      as      the      reference      SDEs,      i.e.,      as      our      prior      knowledge.</p><p> \\n \\n  Key      message.\\nFinally,      it      is      worth      noting      that      we      can      technically      utilize      any      off-the-shelf      RL      algo-  \\n \\n  rithms      for      training.\\nFor      example,      Zhang      and      Chen      (2021)      employs      reward      backpropagation,      while  \\n \\n  works      in      Gflownets      typically      leverage      PCL,      as      we      mention      in      Section      6.3.\\nHowever,      additionally,      it  \\n \\n  is      possible      to      use      PPO      or      reward-weighted      MLE      mentioned      in      Section      4.2.</p><p>11      Closely      Related      Directions</p><p>Lastly,      we      highlight      several      closely      related      directions      we      have      not      yet      discussed.</p><p> \\n \\n  Aligning      text-to-image      models.\\nWe      recognize      that      current      efforts      in      fine-tuning      diffusion      models  \\n \\n  primarily      revolve      around      aligning      text-to-image      models      using      human      feedback      (Lee      et      al.,      2023;  \\n \\n  Wallace      et      al.,      2023;      Wu      et      al.,      2023;      Yang      et      al.,      2023).\\nIn      this      paper,      our      goal      is      to      consider   \\n \\na      more  \\n \\n  general      and      fundamental      formulation      that      is      not      tailored      to      any      particular      task.</p><p> \\n \\n  Diffusion      models      for      RL.\\nDiffusion      models      are      well-suited      for      specific      reinforcement      learning  \\n \\n  applications      due      to      their      ability      to      model      complex      and      multimodal      distributions      as      polices      (Janner  \\n \\n  et      al.,      2022;      Ajay      et      al.,      2023;      Wang      et      al.,      2022;      Hansen-Estruch      et      al.,      2023;      Du      et      al.,      2024;      Zhu  \\n \\n  et      al.,      2023).\\nFor      example,      Wang      et      al.\\n(2022)      use      conditional      diffusion      models      as      policies      and  \\n \\n  demonstrate      their      effectiveness      in      typical      offline      RL      benchmarks.\\nWhile      RL-based      fine-tuning      also  \\n \\n  aims      to      maximize      certain      reward      functions,      the      primary      focus      of      this      tutorial      covers      methods      that  \\n \\n  leverage      pre-trained      diffusion      models      for      this      purpose.</p><p> \\n \\n  RL      from      human      feedback      (RLHF)      for      language      models.\\nRLHF      is      widely      discussed      in      the  \\n \\n  context      of      language      models,      where      many      algorithms      share      similarities      with      those      used      in      diffusion  \\n \\n  models      (Ouyang      et      al.,      2022;      Bai      et      al.,      2022;      Casper      et      al.,      2023).\\nFor      instance,      both      domains  \\n \\n  commonly      employ      PPO      as   \\n \\na      standard      approach,      and      reward-weighted      training      (or      reward-weighted  \\n \\n  MLE)      often      serves      as   \\n \\na      basic      baseline      approach.\\nDespite      these      similarities,      nuanced      differences  \\n \\n  exist      mainly      because      diffusion      models      typically      operate      in   \\n \\na      continuous      input      space      rather      than  \\n \\n  a      discrete      one.\\nMoreover,      techniques      like      value-weighted      sampling      emerge      uniquely      within      the  \\n \\n  context      of      diffusion      models.</p><p>12      Summary</p><p> \\n \\n  In      this      article,      we      comprehensively      explain      how      fine-tuning      diffusion      models      to      maximize      down-  \\n \\n  stream      reward      functions      can      be      formalized      as   \\n \\na      reinforcement      learning      (RL)      problem      in      entropy-  \\n \\n  regularized      Markov      decision      processes      (MDPs).\\nBased      on      this      viewpoint,      we      elaborate      on      the  \\n \\n  application      of      various      RL      algorithms,      such      as      PPO      and      reward-weighted      MLE,      specifically      tailored  \\n \\n  for      fine-tuning      diffusion      models.\\nAdditionally,      we      categorize      different      scenarios      based      on      how  \\n \\n  reward      feedback      is      obtained.\\nWhile      this      categorization      is      not      always      explicitly      mentioned      in      many  \\n \\n  existing      works,      it      is      crucial      when      selecting      appropriate      algorithms.\\nFinally,      we      discuss      the      rela-  \\n \\n  tionships      with      several      related      topics,      including      classifier      guidance,      Gflownets,      path      integral      control  \\n \\n  theory,      and      sampling      from      unnormalized      distributions.</p><h10>9.3.      Connection      with      RL-Based      Fine-Tuning</h10><p>ning  \\n \\n  RL-based      fine-tuning      shares   \\n \\na      close      relationship      with      flow-based      training.\\nTo      illustrate      this,      consider  \\n \\n  the      reference      SDE      (from      7’      to      0)      following      the      pre-trained      diffusion      model.\\nThen,      akin      to      flow-  \\n \\n  based      training,      we      consider      the      coupling      of      the      reference      SDE      conditioned      on      state      at      time   \\n \\n0       (i.e.,      distribution      over      xo-r      conditioned      on      x9)      and      the      target      distribution      at      time   \\n \\n0      (i.e.,      p,(-)   \\n \\n«       exp(r(-)/a)p?\\n\"°(-)),      denoting      the      induced      distribution      as      Q*™.</p><p>Informal      characterization      of      Q™°*.\\nInformally,      Q*\"°      is      introduced      as      follows.\\nWhen      discretizing  \\n \\n  the      time      step,      by      denoting      the      distribution      induced      by      the      reference      SDE      as      p™\\\\\\\\\\\\\\'(xp,-+-   \\n \\n,      9),      we define Qi\"      (xr,   \\n \\na   \\n \\n,      0)   \\n \\n=      p(x,      nr      Ly   \\n \\n|      L0)Pr      (Xo).</p><p>The      distribution      Q*°      above      is   \\n \\na      limiting      distribution      obtained      by      making      the      discretization      step small.</p><p>Training.\\nNow,      we      introduce   \\n \\na      new      SDE      parameterized      by   \\n \\n@      such      as      an      neural      network      to      align  \\n \\n  the      induced      distribution      P’      with      Q®\"*.\\nActually,      in      the      continuous      formulation,      the      RL-problem      (18) in      Section      4.2      is      equal      to      solving argmin      KL(P?||Q\"™).\\n \\n \\n  0 This      formalization      has      been      presented      in      Uehara      et      al.\\n(2024).\\nIntuitively,      as      we      see      in      Theorem      3,  \\n \\n  this      is      expected      because      we      see      that      the      posterior      distribution      induced      by      pre-trained      models:</p><p>ref   \\n \\n(      PD      (ar,-++      21   \\n \\n|      to)      (=      vey      (er|er_-1)p      Pp      (ar-1   \\n \\n|      er_2)      +++      pp\"      (x1   \\n \\n|      20), remains      preserved      after      fine-tuning      based      on      the      RL-formulation      (18).</p><p> \\n \\n  Comparison      with      flow-based      training.\\nIn      contrast      to      flow-based      training,      RL-based      fine-tuning  \\n \\n  minimizes      the      inverse      KL      divergence      rather      than      the      KL      divergence.\\nIntuitively,      this      is      because,  \\n \\n  unlike      Qt™*      or      Q*>   \\n \\n,      we      cannot      sample      from      Q*”      (recall      that   \\n \\na      marginal      distribution      at      time   \\n \\n0      in  \\n \\n  Qi,      i.e.,      p,,      is      unnormalized);      on      the      other      hand,      the      marginal      distributions      at      time   \\n \\n0      in      both      Q\\\\\\\\\\\\\\'™*  \\n \\n  and      Q*°°>      are      data      distributions.</p><p> \\n \\n  Remark   \\n \\n7      (Extension      to      f-divergence).\\nEach      of      these      methods      can      be      technically      extended      by  \\n \\n  incorporating      more      general      divergences      beyond      KL      divergence.\\nFor      instance,      in      the      context      of  \\n \\n  fine-tuning,      please      see      Tang      (2024).</p><p>10      Connection      with      Sampling      from      Unnormalized      Distributions</p><p> \\n \\n  Numerous      studies      delve      into      sampling      from      an      unnormalized      distribution      proportional      to      exp(r()),  \\n \\n  commonly      known      as      the      Gibbs      distribution,      extensively      discussed      in      the      computational      statistics  \\n \\n  and      statistical      physics      community      (Robert,      2014).\\nThis      issue      is      pertinent      to      our      work,      as      during  \\n \\n  fine-tuning,      the      target      distribution      is      also      formulated      in      the      Gibbs      distribution      form      in      (10),      which      is  \\n \\n  proportional      to      exp(r(a))p?\\n\"*      (x).</p><p> \\n \\n  The      literature      employs      two      main      approaches      to      tackle      this      challenge:      Markov      Chain      Monte  \\n \\n  Carlo      (MCMC)      and      RL-based      methods.\\nIn      particular,      the      RL-based      diffusion      model      fine-tuning  \\n \\n  we      have      discussed      so      far      is      closely      related      to      the      latter      approach.\\nIn      this      section,      we      explain      and  \\n \\n  compare      these      two      approaches.</p><h10>10.1      Markov      Chain      Monte      Carlo      (MCMC)</h10><p>CMC)  \\n \\n  In      MCMC,   \\n \\na      Markov      chain      is      constructed      to      approximate   \\n \\na      target      distribution      such      that      the      equilib-  \\n \\n  rium      distribution      of      the      Markov      chain      converges      to      the      target      distribution.\\nHowever,      dealing      with  \\n \\n  high-dimensional      domain      spaces      is      challenging      for      MCMC.\\n \\n \\nA      common      strategy      involves      leveraging  \\n \\n  gradient      information,      such      as      the      Metropolis-adjusted      Langevin      algorithm      (MALA)      (Besag,      1994)  \\n \\n  or      Hamiltonian      Monte      Carlo      (HMC)      (Neal      et      al.,      2011;      Girolami      and      Calderhead,      2011).\\nMALA  \\n \\n  shares      similarities      with      classifier      guidance      and      value-weighted      sampling,      as      both      methods      depend  \\n \\n  on      the      first-order      information      from      reward      (or      value)      functions.</p><p> \\n \\n  Can      we      use      MCMC      for      fine-tuning      diffusion      models?\\nIn      the      context      of      fine-tuning      dif-  \\n \\n  fusion      models,      while      it      may      seem      intuitive      to      sample      from      the      target      distribution      (10)      (i.e.,  \\n \\n  Dy   \\n \\n&      exp(r(x))pP\"*(x))      using      MCMC,      it’s      not      straightforward      for      the      following      reasons:</p><p>¢\\n \\n   We      lack      of      an      analytical      form      of      p?\\n\"*(-).<ul><li>*\\n \\n   Additionally,      even      if      we      can      estimate      p?\\n’°(-)      in      an      unbiased      manner,      as      in      Chen      et      al.\\n(2018),  \\n \\n  the      mixing      time      for      obtaining   \\n \\na      single      sample      in      MCMC      might      be      lengthy.</li></ul></p><p> \\n \\n  Therefore,      we      explore      creating   \\n \\na      generative      model      (i.e.,      diffusion      model)      for      this      task      using      RL-  \\n \\n  based      fine-tuning      so      that      we      can      easily      sample      from      p,(-)      during      inference      (i.e.,      simulating      the  \\n \\n  time-reversal      SDE      (2)      with      policies      from   \\n \\n¢   \\n \\n=      T\\\\\\\\\\\\\\'+   \\n \\n1      to   \\n \\nt   \\n \\n=      1),      without      relying      on      MCMC.\\nThis  \\n \\n  addresses      the      concern      of      MCMC      by      avoiding      the      need      to      estimate      p?\\n\"*(-)      and      minimizing      inference  \\n \\n  time,      albeit      at      the      expense      of      increased      training      time.</p><p>10.2}      RL-Based      Approaches  \\n \\n  While      MCMC      has      historically      been      widely      used      for      this      purpose,      recent      works      have      proposed      an  \\n \\n  RL-based      approach      or      its      variant      (e.g.,      Bernton      et      al.      (2019);      Heng      et      al.\\n(2020);      Huang      et      al.\\n(2023);</p><p> \\n \\n  Vargas      et      al.\\n(2023)).\\nFor      example,   \\n \\na      seminal      study      (Zhang      and      Chen,      2021)      introduced   \\n \\na      method  \\n \\n  very      similar      to      reward      backpropagation.\\nIn      their      work,      they      initially      define      Brownian      motion      as   \\n \\na       reference      SDE      (from      7’      to      0),      similar      to      the      pre-trained      diffusion      models      in      our      context.\\nThen,      by  \\n \\n  appropriately      setting      rewards,      Zhang      and      Chen      (2021)      aims      to      learn   \\n \\na      new      SDE      (from      T\\\\\\\\\\\\\\'      to      0)      such  \\n \\n  that      we      can      sample      from      the      target      distribution      at      the      endpoint      0.</p><p> \\n \\n  These      approaches      offer      advantages      over      MCMC      due      to      their      ability      to      minimize      inference      time  \\n \\n  (amortization)      by      constructing   \\n \\na      generative      model      comprising      policies      in      the      training      time      and      solely  \\n \\n  executing      learned      policies      during      inference.\\nUnlike      MCMC,      these      methods      significantly      reduce      the  \\n \\n  inference      cost      as      we      don’t      need      to      be      concerned      about      the      mixing      time.\\nAdditionally,      RL-based  \\n \\n  approaches      have      the      potential      to      effectively      handle      high-dimensional,      complex      (i.e.,      multi-modal)  \\n \\n  distributions      by      harnessing      the      considerable      expressive      power      arising      from      latent      states.</p><p> \\n \\n  Despite      these      advantages,      it      is      often      unclear      how      to      select   \\n \\na      reference      SDE      in      these      works.\\nIn  \\n \\n  contrast,      in      RL-based      fine-tuning      of      diffusion      models,      we      directly      leverage      pre-trained      diffusion  \\n \\n  models      as      the      reference      SDEs,      i.e.,      as      our      prior      knowledge.</p><p> \\n \\n  Key      message.\\nFinally,      it      is      worth      noting      that      we      can      technically      utilize      any      off-the-shelf      RL      algo-  \\n \\n  rithms      for      training.\\nFor      example,      Zhang      and      Chen      (2021)      employs      reward      backpropagation,      while  \\n \\n  works      in      Gflownets      typically      leverage      PCL,      as      we      mention      in      Section      6.3.\\nHowever,      additionally,      it  \\n \\n  is      possible      to      use      PPO      or      reward-weighted      MLE      mentioned      in      Section      4.2.</p><p>11      Closely      Related      Directions</p><p>Lastly,      we      highlight      several      closely      related      directions      we      have      not      yet      discussed.</p><p> \\n \\n  Aligning      text-to-image      models.\\nWe      recognize      that      current      efforts      in      fine-tuning      diffusion      models  \\n \\n  primarily      revolve      around      aligning      text-to-image      models      using      human      feedback      (Lee      et      al.,      2023;  \\n \\n  Wallace      et      al.,      2023;      Wu      et      al.,      2023;      Yang      et      al.,      2023).\\nIn      this      paper,      our      goal      is      to      consider   \\n \\na      more  \\n \\n  general      and      fundamental      formulation      that      is      not      tailored      to      any      particular      task.</p><p> \\n \\n  Diffusion      models      for      RL.\\nDiffusion      models      are      well-suited      for      specific      reinforcement      learning  \\n \\n  applications      due      to      their      ability      to      model      complex      and      multimodal      distributions      as      polices      (Janner  \\n \\n  et      al.,      2022;      Ajay      et      al.,      2023;      Wang      et      al.,      2022;      Hansen-Estruch      et      al.,      2023;      Du      et      al.,      2024;      Zhu  \\n \\n  et      al.,      2023).\\nFor      example,      Wang      et      al.\\n(2022)      use      conditional      diffusion      models      as      policies      and  \\n \\n  demonstrate      their      effectiveness      in      typical      offline      RL      benchmarks.\\nWhile      RL-based      fine-tuning      also  \\n \\n  aims      to      maximize      certain      reward      functions,      the      primary      focus      of      this      tutorial      covers      methods      that  \\n \\n  leverage      pre-trained      diffusion      models      for      this      purpose.</p><p> \\n \\n  RL      from      human      feedback      (RLHF)      for      language      models.\\nRLHF      is      widely      discussed      in      the  \\n \\n  context      of      language      models,      where      many      algorithms      share      similarities      with      those      used      in      diffusion  \\n \\n  models      (Ouyang      et      al.,      2022;      Bai      et      al.,      2022;      Casper      et      al.,      2023).\\nFor      instance,      both      domains  \\n \\n  commonly      employ      PPO      as   \\n \\na      standard      approach,      and      reward-weighted      training      (or      reward-weighted  \\n \\n  MLE)      often      serves      as   \\n \\na      basic      baseline      approach.\\nDespite      these      similarities,      nuanced      differences  \\n \\n  exist      mainly      because      diffusion      models      typically      operate      in   \\n \\na      continuous      input      space      rather      than  \\n \\n  a      discrete      one.\\nMoreover,      techniques      like      value-weighted      sampling      emerge      uniquely      within      the  \\n \\n  context      of      diffusion      models.</p><p>12      Summary</p><p> \\n \\n  In      this      article,      we      comprehensively      explain      how      fine-tuning      diffusion      models      to      maximize      down-  \\n \\n  stream      reward      functions      can      be      formalized      as   \\n \\na      reinforcement      learning      (RL)      problem      in      entropy-  \\n \\n  regularized      Markov      decision      processes      (MDPs).\\nBased      on      this      viewpoint,      we      elaborate      on      the  \\n \\n  application      of      various      RL      algorithms,      such      as      PPO      and      reward-weighted      MLE,      specifically      tailored  \\n \\n  for      fine-tuning      diffusion      models.\\nAdditionally,      we      categorize      different      scenarios      based      on      how  \\n \\n  reward      feedback      is      obtained.\\nWhile      this      categorization      is      not      always      explicitly      mentioned      in      many  \\n \\n  existing      works,      it      is      crucial      when      selecting      appropriate      algorithms.\\nFinally,      we      discuss      the      rela-  \\n \\n  tionships      with      several      related      topics,      including      classifier      guidance,      Gflownets,      path      integral      control  \\n \\n  theory,      and      sampling      from      unnormalized      distributions.</p><h4>References</h4><p>Agarwal,      A.,      N.      Jiang,      S.      M.      Kakade,      and      W.      Sun      (2019).\\nReinforcement      learning:      Theory      and  \\n \\n  algorithms.\\nCS      Dept.,      UW      Seattle,      Seattle,      WA,      USA,      Tech.\\nRep,      10-4.</p><p>Agarwal,      V.      and      D.      R.      Kelley      (2022).\\nThe      genetic      and      biochemical      determinants      of      mrna      degradation  \\n \\n  rates      in      mammals.\\nGenome      biology      23(1),      245.</p><p> \\n \\n  Ajay,      A.,      Y.      Du,      A.      Gupta,      J.      B.      Tenenbaum,      T.      S.      Jaakkola,      and      P.      Agrawal      (2023).\\nIs      conditional  \\n \\n  generative      modeling      all      you      need      for      decision      making?\\nIn      The      Eleventh      International      Conference  \\n \\n  on      Learning      Representations.</p><p>Albergo,      M.      S.      and      E.      Vanden-Eijnden      (2022).\\nBuilding      normalizing      flows      with      stochastic      inter-  \\n \\n  polants.\\narXiv      preprint      arXiv:2209.15571.</p><p>Anderson,      B.      D.      (1982).\\nReverse-time      diffusion      equation      models.\\nStochastic      Processes      and      their  \\n \\n  Applications      12(3),      313-326.</p><p> \\n \\n  Austin,      J.,      D.      D.      Johnson,      J.      Ho,      D.      Tarlow,      and      R.      Van      Den      Berg      (2021).\\nStructured      denoising  \\n \\n  diffusion      models      in      discrete      state-spaces.\\nAdvances      in      Neural      Information      Processing      Systems      34,  \\n \\n  17981-17993.</p><p>Avdeyey,      P.,      C.      Shi,      Y.      Tan,      K.      Dudnyk,      and      J.      Zhou      (2023).\\nDirichlet      diffusion      score      model      for  \\n \\n  biological      sequence      generation.\\narXiv      preprint      arXiv:2305.\\n10699.</p><p>Bai,      Y.,      S.      Kadavath,      S.      Kundu,      A.      Askell,      J.      Kernion,      A.      Jones,      A.      Chen,      A.      Goldie,      A.      Mirhoseini, C.      McKinnon,      et      al.\\n(2022).\\nConstitutional      ai:      Harmlessness      from      ai      feedback.\\narXiv      preprint  \\n \\n  arXiv:2212.08073.</p><p> \\n \\n  Bansal,      A.,      H.-M.\\nChu,      A.      Schwarzschild,      S.      Sengupta,      M.      Goldblum,      J.      Geiping,      and      T.      Goldstein  \\n \\n  (2023).\\nUniversal      guidance      for      diffusion      models.\\nIn      Proceedings      of      the      IEEE/CVF      Conference  \\n \\n  on      Computer      Vision      and      Pattern      Recognition,      pp.\\n843-852.</p><p>Bengio,      Y.,      S.      Lahlou,      T.      Deleu,      E.      J.      Hu,      M.      Tiwari,      and      E.      Bengio      (2023).\\nGflownet      foundations.\\n \\n \\n  Journal      of      Machine      Learning      Research      24(210),      1-55.</p><p> \\n \\n  Benton,      J.,      Y.      Shi,      V.      De      Bortoli,      G.      Deligiannidis,      and      A.      Doucet      (2024).\\nFrom      denoising      diffusions  \\n \\n  to      denoising      markov      models.\\nJournal      of      the      Royal      Statistical      Society      Series      B:      Statistical  \\n \\n  Methodology      86(2),      286-301.</p><p>Bernton,      E.,      J.      Heng,      A.      Doucet,      and      P.      E.      Jacob      (2019).\\nSchr\\\\\\\\\\\\\\\\””      odinger      bridge      samplers.\\narXiv  \\n \\n  preprint      arXiv:      1912.13170.</p><p>Besag,      J.\\n(1994).\\nComments      on      “representations      of      knowledge      in      complex      systems”      by      u.      grenander  \\n \\n  and      mi      miller.\\nJ.      Roy.\\nStatist.\\nSoc.      Ser.\\n \\n \\nB      56(591-592),      4.</p><p>Black,      K.,      M.      Janner,      Y.      Du,      I.      Kostrikov,      and      S.      Levine      (2023).\\nTraining      diffusion      models      with  \\n \\n  reinforcement      learning.\\narXiv      preprint      arXiv:2305.13301.</p><p> \\n \\n  Campbell,      A.,      J.      Benton,      V.      De      Bortoli,      T.      Rainforth,      G.      Deligiannidis,      and      A.      Doucet      (2022).\\n \\n \\nA       continuous      time      framework      for      discrete      denoising      models.\\nAdvances      in      Neural      Information  \\n \\n  Processing      Systems      35,      28266-28279.</p><p> \\n \\n  Campbell,      A.,      J.      Yim,      R.      Barzilay,      T.      Rainforth,      and      T.      Jaakkola      (2024).\\nGenerative      flows      on  \\n \\n  discrete      state-spaces:      Enabling      multimodal      flows      with      applications      to      protein      co-design.\\narXiv  \\n \\n  preprint      arXiv:2402.04997.</p><p>Cao,      H.,      C.      Tan,      Z.      Gao,      Y.      Xu,      G.      Chen,      P.-A.\\nHeng,      and      S.      Z.      Li      (2024).\\n \\n \\nA      survey      on      generative  \\n \\n  diffusion      models.\\n[EEE      Transactions      on      Knowledge      and      Data      Engineering.</p><p>Casper,      S.,      X.      Davies,      C.      Shi,      T.      K.      Gilbert,      J.      Scheurer,      J.      Rando,      R.      Freedman,      T.      Korbak, D.      Lindner,   \\n \\nP P.      Freire,      et      al.\\n(2023).\\nOpen      problems      and      fundamental      limitations      of      reinforcement  \\n \\n  learning      from      human      feedback.\\narXiv      preprint      arXiv:2307.15217.</p><p>Castillo-Hair,      S.      M.      and      G.      Seelig      (2021).\\nMachine      learning      for      designing      next-generation      mrna  \\n \\n  therapeutics.\\nAccounts      of      Chemical      Research      55(1),      24-34.</p><p>Chang,      J.      D.,      M.      Uehara,      D.      Sreenivas,      R.      Kidambi,      and      W.      Sun      (2021).\\nMitigating      covariate      shift  \\n \\n  in      imitation      learning      via      offline      data      without      great      coverage.\\narXiv      preprint      arXiv:2106.03207.</p><p>Chen,      M.,      S.      Mei,      J.      Fan,      and      M.      Wang      (2024).\\nAn      overview      of      diffusion      models:      Applications,  \\n \\n  guided      generation,      statistical      rates      and      optimization.\\narXiv      preprint      arXiv:2404.07771.</p><p>Chen,      R.      T.,      Y.      Rubanova,      J.      Bettencourt,      and      D.      K.      Duvenaud      (2018).\\nNeural      ordinary      differential  \\n \\n  equations.\\nAdvances      in      neural      information      processing      systems      31.</p><p>Chung,      H.,      J.      Kim,      M.      T.      Mccann,      M.      L.      Klasky,      and      J.      C.      Ye      (2022).\\nDiffusion      posterior      sampling  \\n \\n  for      general      noisy      inverse      problems.\\narXiv      preprint      arXiv:2209.\\n14687.</p><p> \\n \\n  Chung,      H.,      B.      Sim,      D.      Ryu,      and      J.      C.      Ye      (2022).\\nImproving      diffusion      models      for      inverse      problems  \\n \\n  using      manifold      constraints.\\nAdvances      in      Neural      Information      Processing      Systems      35,      25683-—  \\n \\n  25696.</p><p>Clark,      K.,      P.      Vicol,      K.      Swersky,      and      D.      J.      Fleet      (2023).\\nDirectly      fine-tuning      diffusion      models      on  \\n \\n  differentiable      rewards.\\narXiv      preprint      arXiv:2309.\\n17400.</p><p> \\n \\n  De      Bortoli,      V.,      E.      Mathieu,      M.      Hutchinson,      J.      Thornton,      Y.      W.      Teh,      and      A.      Doucet      (2022).\\nRieman-  \\n \\n  nian      score-based      generative      modelling.\\nAdvances      in      Neural      Information      Processing      Systems      35,  \\n \\n  2406-2422.</p><p>Deleu,      T.,      P.      Nouri,      N.      Malkin,      D.      Precup,      and      Y.      Bengio      (2024).\\nDiscrete      probabilistic      inference      as  \\n \\n  control      in      multi-path      environments.\\narXiv      preprint      arXiv:2402.10309.</p><p>Dhariwal,      P.      and      A.      Nichol      (2021).\\nDiffusion      models      beat      gans      on      image      synthesis.\\nAdvances      in  \\n \\n  neural      information      processing      systems      34,      8780-8794.</p><p> \\n \\n  Du,      Y.,      S.      Yang,      B.      Dai,      H.      Dai,      O.      Nachum,      J.      Tenenbaum,      D.      Schuurmans,      and      P.      Abbeel      (2024).\\n \\n \\n  Learning      universal      policies      via      text-guided      video      generation.\\nAdvances      in      Neural      Information  \\n \\n  Processing      Systems      36.</p><p>Fan,      Y.,      O.      Watkins,      Y.      Du,      H.      Liu,      M.      Ryu,      C.      Boutilier,      P.      Abbeel,      M.      Ghavamzadeh,      K.      Lee,      and K.      Lee      (2023) 023).\\nDPOK:      Reinforcement      learning      for      fine-tuning      text-to-image      diffusion      models.\\n \\n \\n  arXiv      preprint      arXiv:2305.16381.</p><p>Finn,      C.,      S.      Levine,      and      P.      Abbeel      (2016).\\nGuided      cost      learning:      Deep      inverse      optimal      control      via  \\n \\n  policy      optimization.\\nIn      International      conference      on      machine      learning,      pp.\\n49-58.\\nPMLR.</p><p>Fox,      R.,      A.      Pakman,      and      N.      Tishby      (2015).\\nTaming      the      noise      in      reinforcement      learning      via      soft  \\n \\n  updates.\\narXiv      preprint      arXiv:      1512.08562.</p><p>Frey,      N.      C.,      D.      Berenberg,      K.      Zadorozhny,      J.      Kleinhenz,      J.      Lafrance-Vanasse,      I.      Hotzel,      Y.      Wu, S.      Ra,   \\n \\nR R.      Bonneau,      K.      Cho,      et      al.\\n(2023).\\nProtein      discovery      with      discrete      walk-jump      sampling.\\n \\n \\n  arXiv      preprint      arXiv:2306.\\n12360.</p><p>Geist,      M.,      B.      Scherrer,      and      O.      Pietquin      (2019).\\n \\n \\nA      theory      of      regularized      markov      decision      processes.\\n \\n \\n  In      International      Conference      on      Machine      Learning,      pp.\\n2160-2169.\\nPMLR.</p><p> \\n \\n  Girolami,      M.      and      B.      Calderhead      (2011).\\nRiemann      manifold      langevin      and      hamiltonian      monte  \\n \\n  carlo      methods.\\nJournal      of      the      Royal      Statistical      Society      Series      B:      Statistical      Methodology      73(2),  \\n \\n  123-214.</p><p>Gosai,      S.      J.,      R.      I.      Castro,      N.      Fuentes,      J.      C.      Butts,      S.      Kales,      R.      R.      Noche,      K.      Mouri,      P.      C.      Sabeti, S.   \\n \\nK K.      Reilly,      and      R.      Tewhey      (2023).\\nMachine-guided      design      of      synthetic      cell      type-specific  \\n \\n  cis-regulatory      elements.\\nbioRxiv.</p><p>Haarnoja,      T.,      H.      Tang,      P.      Abbeel,      and      S.      Levine      (2017).\\nReinforcement      learning      with      deep      energy-  \\n \\n  based      policies.\\nIn      International      conference      on      machine      learning,      pp.\\n1352-1361.\\nPMLR.</p><p>Hansen-Estruch,      P.,      I.      Kostrikov,      M.      Janner,      J.      G.      Kuba,      and      S.      Levine      (2023).\\nIdql:      Implicit  \\n \\n  q-learning      as      an      actor-critic      method      with      diffusion      policies.\\narXiv      preprint      arXiv:2304.10573.</p><p>Heng,      J.,      A.      N.      Bishop,      G.      Deligiannidis,      and      A.      Doucet      (2020).\\nControlled      sequential      monte      carlo.</p><p>Ho,      J.,      A.      Jain,      and      P.      Abbeel      (2020).\\nDenoising      diffusion      probabilistic      models.\\nAdvances      in      neural  \\n \\n  information      processing      systems      33,      6840-6851.</p><p>Ho,      J.      and      T.      Salimans      (2022).\\nClassifier-free      diffusion      guidance.\\narXiv      preprint      arXiv:2207.12598.</p><p>Ho,      J.,      T.      Salimans,      A.      Gritsenko,      W.      Chan,      M.      Norouzi,      and      D.      J.      Fleet      (2022).\\nVideo      diffusion  \\n \\n  models.\\nAdvances      in      Neural      Information      Processing      Systems      35,      8633-8646.</p><p> \\n \\n  Hoogeboom,      E.,      V.      G.      Satorras,      C.      Vignac,      and      M.      Welling      (2022).\\nEquivariant      diffusion      for  \\n \\n  molecule      generation      in      3d.\\nIn      International      conference      on      machine      learning,      pp.\\n8867-8887.</p><h4>      PMLR.</h4><p>Huang,      X.,      H.      Dong,      H.      Yifan,      Y.      Ma,      and      T.      Zhang      (2023).\\nReverse      diffusion      monte      carlo.\\nIn      The  \\n \\n  Twelfth      International      Conference      on      Learning      Representations.</p><p>Janner,      M.,      Y.      Du,      J.      B.      Tenenbaum,      and      S.      Levine      (2022).\\nPlanning      with      diffusion      for      flexible  \\n \\n  behavior      synthesis.\\narXiv      preprint      arXiv:2205.09991.</p><p> \\n \\n  Jo,      J.,      S.      Lee,      and      S.      J.      Hwang      (2022).\\nScore-based      generative      modeling      of      graphs      via      the      system  \\n \\n  of      stochastic      differential      equations.\\nIn      International      Conference      on      Machine      Learning,      pp.\\n \\n \\n  10362-10383.\\nPMLR.</p><p> \\n \\n  Kazim,      M.,      J.      Hong,      M.-G.\\nKim,      and      K.-K.\\nK.      Kim      (2024).\\nRecent      advances      in      path      integral      control  \\n \\n  for      trajectory      optimization:      An      overview      in      theoretical      and      algorithmic      perspectives.\\nAnnual  \\n \\n  Reviews      in      Control      57,      100931.</p><p>Krishnamoorthy,      S.,      S.      M.      Mashkaria,      and      A.      Grover      (2023).\\nDiffusion      models      for      black-box  \\n \\n  optimization.\\narXiv      preprint      arXiv:2306.07180.</p><p>Kumar,      A.,      A.      Zhou,      G.      Tucker,      and      S.      Levine      (2020).\\nConservative      q-learning      for      offline      reinforce-  \\n \\n  ment      learning.\\nAdvances      in      Neural      Information      Processing      Systems      33,      1179-1191.</p><p>Lal,      A.,      D.      Garfield,      T.      Biancalani,      and      G.      Eraslan      (2024).\\nreglm:      Designing      realistic      regulatory  \\n \\n  dna      with      autoregressive      language      models.\\nbioRxiv,      2024-02.</p><p>Lee,      K.,      H.      Liu,      M.      Ryu,      O.      Watkins,      Y.      Du,      C.      Boutilier,      P.      Abbeel,      M.      Ghavamzadeh,      and      S.      S.      Gu  \\n \\n  (2023).\\nAligning      text-to-image      models      using      human      feedback.\\narXiv      preprint      arXiv:2302.12192.</p><p>Levine,      S.      (2018).\\nReinforcement      learning      and      control      as      probabilistic      inference:      Tutorial      and  \\n \\n  review.\\narXiv      preprint      arXiv:      1805.00909.</p><p>Li,      Z.,      Y.      Ni,      T.      A.      B.      Huygelen,      A.      Das,      G.      Xia,      G.-B.\\nStan,      and      Y.      Zhao      (2023).\\nLatent      diffusion  \\n \\n  model      for      dna      sequence      generation.\\narXiv      preprint      arXiv:2310.06150.</p><p>Lipman,      Y.,      R.      T.      Chen,      H.      Ben-Hamu,      M.      Nickel,      and      M.      Le      (2023).\\nFlow      matching      for      generative  \\n \\n  modeling.\\nJCLR      2023.</p><p>Liu,      X.,      C.      Gong,      and      Q.      Liu      (2022).\\nFlow      straight      and      fast:      Learning      to      generate      and      transfer      data  \\n \\n  with      rectified      flow.\\narXiv      preprint      arXiv:2209.03003.</p><p>Liu,      X.,      L.      Wu,      M.      Ye,      and      Q.      Liu      (2022).\\nLet      us      build      bridges:      Understanding      and      extending  \\n \\n  diffusion      generative      models.\\narXiv      preprint      arXiv:2208.14699.</p><p>Lou,      A.,      C.      Meng,      and      S.      Ermon      (2023).\\nDiscrete      diffusion      language      modeling      by      estimating      the  \\n \\n  ratios      of      the      data      distribution.\\narXiv      preprint      arXiv:2310.16834.</p><p>Malkin,      N.,      M.      Jain,      E.      Bengio,      C.      Sun,      and      Y.      Bengio      (2022).\\nTrajectory      balance:      Improved      credit  \\n \\n  assignment      in      gflownets.\\nAdvances      in      Neural      Information      Processing      Systems      35,      5955-5967.</p><p>Mohammadpour,      S.,      E.      Bengio,      E.      Frejinger,      and      P.-L.\\nBacon      (2023).\\nMaximum      entropy      gflownets  \\n \\n  with      soft      q-learning.\\narXiv      preprint      arXiv:2312.14331.</p><p>Nachum,      O.,      M.      Norouzi,      K.      Xu,      and      D.      Schuurmans      (2017).\\nBridging      the      gap      between      value      and  \\n \\n  policy      based      reinforcement      learning.\\nAdvances      in      neural      information      processing      systems      30.</p><p>Neal,      R.      M.      et      al.\\n(2011).\\nMcmce      using      hamiltonian      dynamics.\\nHandbook      of      markov      chain      monte  \\n \\n  carlo      2(11),      2.</p><p>Neu,      G.,      A.      Jonsson,      and      V.      Gémez      (2017).\\n \\n \\nA      unified      view      of      entropy-regularized      markov      decision  \\n \\n  processes.\\narXiv      preprint      arXiv:      1705.07798.</p><p>Ouyang,      L.,      J.      Wu,      X.      Jiang,      D.      Almeida,      C.      Wainwright,      P.      Mishkin,      C.      Zhang,      S.      Agarwal, K.      Slama,      A A.      Ray,      et      al.\\n(2022).\\nTraining      language      models      to      follow      instructions      with      human  \\n \\n  feedback.\\nAdvances      in      Neural      Information      Processing      Systems      35,      27730-27744.</p><p>Pattanaik,      L.      and      C.      W.      Coley      (2020).\\nMolecular      representation:      going      long      on      fingerprints.\\n \\n \\n  Chem      6(6),      1204-1207.</p><p>Peng,      X.      B.,      A.      Kumar,      G.      Zhang,      and      S.      Levine      (2019).\\nAdvantage-weighted      regression:      Simple  \\n \\n  and      scalable      off-policy      reinforcement      learning.\\narXiv      preprint      arXiv:1910.00177.</p><p>Peters,      J.,      K.      Mulling,      and      Y.      Altun      (2010).\\nRelative      entropy      policy      search.\\nIn      Proceedings      of      the  \\n \\n  AAAI      Conference      on      Artificial      Intelligence,      Volume      24,      pp.\\n1607-1612.</p><p> \\n \\n  Podell,      D.,      Z.      English,      K.      Lacey,      A.      Blattmann,      T.      Dockhorn,      J.      Miiller,      J.      Penna,      and      R.      Rombach  \\n \\n  (2023).\\nSdxl:      Improving      latent      diffusion      models      for      high-resolution      image      synthesis.\\narXiv  \\n \\n  preprint      arXiv:2307.01952.</p><p>Prabhudesai,      M.,      A.      Goyal,      D.      Pathak,      and      K.      Fragkiadaki      (2023).\\nAligning      text-to-image      diffusion  \\n \\n  models      with      reward      backpropagation.\\narXiv      preprint      arXiv:2310.03739.</p><p>Ramesh,      A.,      P.      Dhariwal,      A.      Nichol,      C.      Chu,      and      M.      Chen      (2022).\\nHierarchical      text-conditional  \\n \\n  image      generation      with      clip      latents.\\narXiv      preprint      arXiv:2204.06125      1(2),      3.</p><p>Rigter,      M.,      B.      Lacerda,      and      N.      Hawes      (2022).\\nRambo-rl:      Robust      adversarial      model-based      offline  \\n \\n  reinforcement      learning.\\nAdvances      in      neural      information      processing      systems      35,      16082-16097.</p><p>Robert,      C.      (2014).\\nStatistical      modeling      and      computation.</p><p>Rogers,      L.      C.      G.      and      D.      Williams      (2000).\\nDiffusions,      Markov      processes      and      martingales:      Volume      2,  \\n \\n  It6é      calculus,      Volume      2.\\nCambridge      university      press.</p><p> \\n \\n  Rombach,      R.,      A.      Blattmann,      D.      Lorenz,      P.      Esser,      and      B.      Ommer      (2022,      June).\\nHigh-resolution  \\n \\n  image      synthesis      with      latent      diffusion      models.\\nIn      Proceedings      of      the      IEEE/CVF      Conference      on  \\n \\n  Computer      Vision      and      Pattern      Recognition      (CVPR),      pp.\\n10684-10695.</p><p>Sarkar,      A.,      Z.      Tang,      C.      Zhao,      and      P.      Koo      (2024).\\nDesigning      dna      with      tunable      regulatory      activity  \\n \\n  using      discrete      diffusion.\\nbioRxiv,      2024-05.</p><p>Schulman,      J.,      X.      Chen,      and      P.      Abbeel      (2017).\\nEquivalence      between      policy      gradients      and      soft  \\n \\n  q-learning.\\narXiv      preprint      arXiv:      1704.06440.</p><p>Schulman,      J.,      S.      Levine,      P.      Abbeel,      M.      Jordan,      and      P.      Moritz      (2015).\\nTrust      region      policy      optimization.\\n \\n \\n  In      International      conference      on      machine      learning,      pp.\\n1889-1897.\\nPMLR.</p><p>Schulman,      J.,      F.      Wolski,      P.      Dhariwal,      A.      Radford,      and      O.      Klimov      (2017).\\nProximal      policy      optimiza-  \\n \\n  tion      algorithms.\\narXiv      preprint      arXiv:      1707.06347.</p><p>Shi,      Y.,      V.      De      Bortoli,      A.      Campbell,      and      A.      Doucet      (2024).\\nDiffusion      schrédinger      bridge      matching.\\n \\n \\n  Advances      in      Neural      Information      Processing      Systems      36.</p><p> \\n \\n  Sohl-Dickstein,      J.,      E.      Weiss,      N.      Maheswaranathan,      and      S.      Ganguli      (2015).\\nDeep      unsupervised  \\n \\n  learning      using      nonequilibrium      thermodynamics.\\nIn      /nternational      conference      on      machine      learning,  \\n \\n  pp.\\n2256-2265.\\nPMLR.</p><p>Song,      J.,      C.      Meng,      and      S.      Ermon      (2020).\\nDenoising      diffusion      implicit      models.\\narXiv      preprint  \\n \\n  arXiv:2010.02502.</p><p>Song,      Y.,      C.      Durkan,      I.      Murray,      and      S.      Ermon      (2021).\\nMaximum      likelihood      training      of      score-based  \\n \\n  diffusion      models.\\nAdvances      in      neural      information      processing      systems      34,      1415-1428.</p><p>Stark,      H.,      B.      Jing,      C.      Wang,      G.      Corso,      B.      Berger,      R.      Barzilay,      and      T.      Jaakkola      (2024).\\nDirichlet  \\n \\n  flow      matching      with      applications      to      dna      sequence      design.\\narXiv      preprint      arXiv:2402.05841.</p><p>Sutton,      R.      S.      and      A.      G.      Barto      (2018).\\nReinforcement      learning:      An      introduction.\\nMIT      press.</p><p>Tang,      W.      (2024).\\nFine-tuning      of      diffusion      models      via      stochastic      control:      entropy      regularization      and  \\n \\n  beyond.\\narXiv      preprint      arXiv:2403.06279.</p><p>Tang,      W.      and      H.      Zhao      (2024).\\nScore-based      diffusion      models      via      stochastic      differential      equations—a  \\n \\n  technical      tutorial.\\narXiv      preprint      arXiv:2402.07487.</p><p>Theodorou,      E.,      J.      Buchli,      and      S.      Schaal      (2010).\\n \\n \\nA      generalized      path      integral      control      approach      to  \\n \\n  reinforcement      learning.\\nThe      Journal      of      Machine      Learning      Research      11,      3137-3181.</p><p>Tiapkin,      D.,      N.      Morozov,      A.      Naumov,      and      D.      Vetrov      (2023).\\nGenerative      flow      networks      as      entropy-  \\n \\n  regularized      rl.\\narXiv      preprint      arXiv:2310.12934.</p><p> \\n \\n  Tong,      A.,      N.      Malkin,      G.      Huguet,      Y.      Zhang,      J.      Rector-Brooks,      K.      Fatras,      G.      Wolf,      and      Y.      Bengio  \\n \\n  (2023).\\nConditional      flow      matching:      Simulation-free      dynamic      optimal      transport.\\narXiv      preprint  \\n \\n  arXiv:2302.00482.</p><p>Uehara,      M.      and      W.      Sun      (2021).\\nPessimistic      model-based      offline      reinforcement      learning      under  \\n \\n  partial      coverage.\\narXiv      preprint      arXiv:2107.06226.</p><p>Uehara,      M.,      Y.      Zhao,      K.      Black,      E.      Hajiramezanali,      G.      Scalia,      N.      L.      Diamant,      A.      M.      Tseng, T.      Biancalani,      and      S S.      Levine      (2024).\\nFine-tuning      of      continuous-time      diffusion      models      as      entropy-  \\n \\n  regularized      control.\\narXiv      preprint      arXiv:2402.15194.</p><p> \\n \\n  Uehara,      M.,      Y.      Zhao,      K.      Black,      E.      Hajiramezanali,      G.      Scalia,      N.      L.      Diamant,      A.      M.      Tseng,      S.      Levine,  \\n \\n  and      T.      Biancalani      (2024).\\nFeedback      efficient      online      fine-tuning      of      diffusion      models.\\narXiv      preprint  \\n \\n  arXiv:2402.\\n16359.</p><p> \\n \\n  Uehara,      M.,      Y.      Zhao,      E.      Hajiramezanali,      G.      Scalia,      G.      Eraslan,      A.      Lal,      S.      Levine,      and      T.      Biancalani  \\n \\n  (2024).\\nBridging      model-based      optimization      and      generative      modeling      via      conservative      fine-tuning  \\n \\n  of      diffusion      models.\\narXiv      preprint      arXiv:2405.19673.</p><p>Vargas,      F.,      W.      Grathwohl,      and      A.      Doucet      (2023).\\nDenoising      diffusion      samplers.\\narXiv      preprint  \\n \\n  arXiv:2302.\\n13834.</p><p> \\n \\n  Wallace,      B.,      M.      Dang,      R.      Rafailov,      L.      Zhou,      A.      Lou,      S.      Purushwalkam,      S.      Ermon,      C.      Xiong,      S.      Joty,  \\n \\n  and      N.      Naik      (2023).\\nDiffusion      model      alignment      using      direct      preference      optimization.\\narXiv  \\n \\n  preprint      arXiv:2311.12908.</p><p>Wang,      Z.,      J.      J.      Hunt,      and      M.      Zhou      (2022).\\nDiffusion      policies      as      an      expressive      policy      class      for      offline  \\n \\n  reinforcement      learning.\\narXiv      preprint      arXiv:2208.06193.</p><p>Widatalla,      T.,      R.      Rafailov,      and      B.      Hie      (2024).\\nAligning      protein      generative      models      with      experimental  \\n \\n  fitness      via      direct      preference      optimization.\\nbioRxiv,      2024-05.</p><p>Williams,      G.,      A.      Aldrich,      and      E.      A.      Theodorou      (2017).\\nModel      predictive      path      integral      control:      From  \\n \\n  theory      to      parallel      computation.\\nJournal      of      Guidance,      Control,      and      Dynamics      40(2),      344-357.</p><p>Wu,      X.,      K.      Sun,      F.      Zhu,      R.      Zhao,      and      H.      Li      (2023).\\nBetter      aligning      text-to-image      models      with  \\n \\n  human      preference.\\narXiv      preprint      arXiv:2303.14420.</p><p>Wu,      Y.,      G.      Tucker,      and      O.      Nachum      (2019).\\nBehavior      regularized      offline      reinforcement      learning.\\n \\n \\n  arXiv      preprint      arXiv:      1911.11361.</p><p>Wulfmeier,      M.,      P.      Ondruska,      and      I.      Posner      (2015).\\nMaximum      entropy      deep      inverse      reinforcement  \\n \\n  learning.\\narXiv      preprint      arXiv:      1507.04888.</p><p>Xie,      T.,      C.-A.\\nCheng,      N.      Jiang,      P.      Mineiro,      and      A.      Agarwal      (2021).\\nBellman-consistent      pessimism  \\n \\n  for      offline      reinforcement      learning.\\nAdvances      in      neural      information      processing      systems      34.</p><p>Xu,      M.,      L.      Yu,      Y.      Song,      C.      Shi,      S.      Ermon,      and      J.      Tang      (2022).\\nGeodiff:   \\n \\nA      geometric      diffusion  \\n \\n  model      for      molecular      conformation      generation.\\narXiv      preprint      arXiv:2203.02923.</p><p> \\n \\n  Yang,      L.,      Z.      Zhang,      Y.      Song,      S.      Hong,      R.      Xu,      Y.      Zhao,      W.      Zhang,      B.      Cui,      and      M.-H.\\nYang      (2023).\\n \\n \\n  Diffusion      models:   \\n \\nA      comprehensive      survey      of      methods      and      applications.\\nACM      Computing  \\n \\n  Surveys      56(4),      1-39.</p><p> \\n \\n  Yu,      T.,      G.      Thomas,      L.      Yu,      S.      Ermon,      J.      Y.      Zou,      S.      Levine,      C.      Finn,      and      T.      Ma      (2020).\\nMopo:  \\n \\n  Model-based      offline      policy      optimization.\\nAdvances      in      Neural      Information      Processing      Systems      33,  \\n \\n  14129-14142.</p><p> \\n \\n  Yuan,      H.,      K.      Huang,      C.      Ni,      M.      Chen,      and      M.      Wang      (2023).\\nReward-directed      conditional      diffusion:  \\n \\n  Provable      distribution      estimation      and      reward      improvement.\\nIn      Thirty-seventh      Conference      on  \\n \\n  Neural      Information      Processing      Systems.</p><p>Zhang,      H.      and      T.      Xu      (2023).\\nTowards      controllable      diffusion      models      via      reward-guided      exploration.\\n \\n \\n  arXiv      preprint      arXiv:2304.07132.</p><p>Zhang,      Q.      and      Y.      Chen      (2021).\\nPath      integral      sampler:   \\n \\na      stochastic      control      approach      for      sampling.\\n \\n \\n  arXiv      preprint      arXiv:2111,15141.</p><p> \\n \\n  Zhao,      Y.,      M.      Uehara,      G.      Scalia,      T.      Biancalani,      S.      Levine,      and      E.      Hajiramezanali      (2024).\\n \\n \\n  Adding      conditional      control      to      diffusion      models      with      reinforcement      learning.\\narXiv      preprint  \\n \\n  arXiv:2406.\\n12120.</p><p>Zhou,      Z.,      S.      Kearnes,      L.      Li,      R.      N.      Zare,      and      P.      Riley      (2019).\\nOptimization      of      molecules      via      deep  \\n \\n  reinforcement      learning.\\nScientific      reports      9(1),      10752.</p><p>Zhu,      Z.,      H.      Zhao,      H.      He,      Y.      Zhong,      S.      Zhang,      Y.      Yu,      and      W.      Zhang      (2023).\\nDiffusion      models      for  \\n \\n  reinforcement      learning:   \\n \\nA      survey.\\narXiv      preprint      arXiv:2311.01223.</p><p>Ziebart,      B.      D.,      A.      L.      Maas,      J.      A.      Bagnell,      A.      K.      Dey,      et      al.\\n(2008).\\nMaximum      entropy      inverse  \\n \\n  reinforcement      learning.\\nIn      Aaai,      Volume      8,      pp.\\n1433-1438.\\nChicago,      IL,      USA.</p></html>')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Document' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdocs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpage_content\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Document' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
