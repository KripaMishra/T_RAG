{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     strategy=\"sections\"\n",
    "from langchain_community.document_loaders.llmsherpa import LLMSherpaFileLoader\n",
    "loader = LLMSherpaFileLoader(\n",
    "    file_path=\"https://arxiv.org/pdf/1910.13461.pdf\",\n",
    "    new_indent_parser=True,\n",
    "    apply_ocr=True,\n",
    "    strategy=\"sections\",\n",
    "    llmsherpa_api_url=\"http://localhost:5010/api/parseDocument?renderFormat=all\",\n",
    ")\n",
    "section_docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://arxiv.org/pdf/1910.13461.pdf', 'section_number': 0, 'section_title': '2.1      Architecture'}, page_content='2.1      Architecture\\nture  \\n \\n  BART      uses      the      standard      sequence-to-sequence      Trans-  \\n \\n  former      architecture      from      (Vaswani      et      al.,      2017),      ex-  \\n \\n  cept,      following      GPT,      that      we      modify      ReLU      activa-  \\n \\n  tion      functions      to      GeLUs      (Hendrycks   \\n \\n&      Gimpel,      2016)  \\n \\n  and      initialise      parameters      from      A/(0,0.02).\\nFor      our  \\n \\n  base      model,      we      use   \\n \\n6      layers      in      the      encoder      and      de-  \\n \\n  coder,      and      for      our      large      model      we      use      12      layers      in  \\n \\n  each.\\nThe      architecture      is      closely      related      to      that      used      in  \\n \\n  BERT,      with      the      following      differences:      (1)      each      layer      of  \\n \\n  the      decoder      additionally      performs      cross-attention      over  \\n \\n  the      final      hidden      layer      of      the      encoder      (as      in      the      trans-  \\n \\n  former      sequence-to-sequence      model);      and      (2)      BERT  \\n \\n  uses      an      additional      feed-forward      network      before      word-  \\n \\n  prediction,      which      BART      does      not.\\nIn      total,      BART      con-  \\n \\n  tains      roughly      10%      more      parameters      than      the      equiva-  \\n \\n  lently      sized      BERT      model.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/1910.13461.pdf', 'section_number': 1, 'section_title': '2.2      Pre-training      BART'}, page_content='2.2      Pre-training      BART\\nBART  \\n \\n  BART      is      trained      by      corrupting      documents      and      then      op-  \\n \\n  timizing   \\n \\na      reconstruction      loss—the      cross-entropy      be-  \\n \\n  tween      the      decoder’s      output      and      the      original      document.\\n \\n \\n  Unlike      existing      denoising      autoencoders,      which      are      tai-  \\n \\n  lored      to      specific      noising      schemes,      BART      allows      us      to  \\n \\n  apply      any      type      of      document      corruption.\\nIn      the      extreme  \\n \\n  case,      where      all      information      about      the      source      is      lost,  \\n \\n  BART      is      equivalent      to   \\n \\na      language      model.\\n \\n \\n  We      experiment      with      several      previously      proposed      and  \\n \\n  novel      transformations,      but      we      believe      there      is   \\n \\na      sig-  \\n \\n  nificant      potential      for      development      of      other      new      alter-  \\n \\n  natives.\\nThe      transformations      we      used      are      summarized  \\n \\n  below,      and      examples      are      shown      in      Figure      2.\\n \\n \\n  Token      Masking      Following      BERT      (Devlin      et      al.,  \\n \\n  2019),      random      tokens      are      sampled      and      replaced      with  \\n \\n  [MASK]      elements.\\n \\n \\n  Token      Deletion      Random      tokens      are      deleted      from      the  \\n \\n  input.\\nIn      contrast      to      token      masking,      the      model      must  \\n \\n  decide      which      positions      are      missing      inputs.\\nToken      Masking'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/1910.13461.pdf', 'section_number': 2, 'section_title': 'Token      Masking'}, page_content='Token      Masking'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/1910.13461.pdf', 'section_number': 3, 'section_title': 'C.DE.AB'}, page_content='C.DE.AB\\nToken      Deletion Text      Infilling\\n \\n \\n  Text      Infilling   \\n \\nA      number      of      text      spans      are      sampled,  \\n \\n  with      span      lengths      drawn      from   \\n \\na      Poisson      distribution  \\n \\n  (A   \\n \\n=      3).\\nEach      span      is      replaced      with   \\n \\na      single      [MASK]  \\n \\n  token.\\nO-length      spans      correspond      to      the      insertion      of  \\n \\n  [MASK]      tokens.\\nText      infilling      is      inspired      by      Span-  \\n \\n  BERT      (Joshi      et      al.,      2019),      but      SpanBERT      samples  \\n \\n  span      lengths      from   \\n \\na      different      (clamped      geometric)      dis-  \\n \\n  tribution,      and      replaces      each      span      with   \\n \\na      sequence      of  \\n \\n  [MASK]      tokens      of      exactly      the      same      length.\\nText      infill-  \\n \\n  ing      teaches      the      model      to      predict      how      many      tokens      are  \\n \\n  missing      from   \\n \\na      span.\\n \\n \\n  Sentence      Permutation   \\n \\nA      document      is      divided      into  \\n \\n  sentences      based      on      full      stops,      and      these      sentences      are  \\n \\n  shuffled      in   \\n \\na      random      order.\\n \\n \\n  Document      Rotation   \\n \\nA      token      is      chosen      uniformly      at  \\n \\n  random,      and      the      document      is      rotated      so      that      it      begins  \\n \\n  with      that      token.\\nThis      task      trains      the      model      to      identify  \\n \\n  the      start      of      the      document.\\n3\\n \\n   Fine-tuning      BART The      representations      produced      by      BART      can      be      used      in  \\n \\n  several      ways      for      downstream      applications.\\n3.1      Sequence      Classification      Tasks\\nasks  \\n \\n  For      sequence      classification      tasks,      the      same      input      is      fed  \\n \\n  into      the      encoder      and      decoder,      and      the      final      hidden      state  \\n \\n  of      the      final      decoder      token      is      fed      into      new      multi-class  \\n \\n  linear      classifier.\\nThis      approach      is      related      to      the      CLS  \\n \\n  token      in      BERT;      however      we      add      the      additional      token  \\n \\n  to      the      end      so      that      representation      for      the      token      in      the  \\n \\n  decoder      can      attend      to      decoder      states      from      the      complete  \\n \\n  input      (Figure      3a).\\n3.2.      Token      Classification      Tasks\\nasks  \\n \\n  For      token      classification      tasks,      such      as      answer      endpoint  \\n \\n  classification      for      SQUAD,      we      feed      the      complete      doc-  \\n \\n  ument      into      the      encoder      and      decoder,      and      use      the      top  \\n \\n  hidden      state      of      the      decoder      as   \\n \\na      representation      for      each  \\n \\n  word.\\nThis      representation      is      used      to      classify      the      token.\\n3.3      Sequence      Generation      Tasks\\nasks  \\n \\n  Because      BART      has      an      autoregressive      decoder,      it      can      be  \\n \\n  directly      fine      tuned      for      sequence      generation      tasks      such  \\n \\n  as      abstractive      question      answering      and      summarization.\\n \\n \\n  In      both      of      these      tasks,      information      is      copied      from      the  \\n \\n  input      but      manipulated,      which      is      closely      related      to      the  \\n \\n  denoising      pre-training      objective.\\nHere,      the      encoder      in-  \\n \\n  put      is      the      input      sequence,      and      the      decoder      generates  \\n \\n  outputs      autoregressively.\\n3.4      Machine      Translation\\ntion  \\n \\n  We      also      explore      using      BART      to      improve      machine      trans-  \\n \\n  lation      decoders      for      translating      into      English.\\nPrevious  \\n \\n  work      Edunov      et      al.\\n(2019)      has      shown      that      models      can  \\n \\n  be      improved      by      incorporating      pre-trained      encoders,      but  \\n \\n  gains      from      using      pre-trained      language      models      in      de-  \\n \\n  coders      have      been      limited.\\nWe      show      that      it      is      possible  \\n \\n  to      use      the      entire      BART      model      (both      encoder      and      de-  \\n \\n  coder)      as   \\n \\na      single      pretrained      decoder      for      machine      trans-  \\n \\n  lation,      by      adding   \\n \\na      new      set      of      encoder      parameters      that  \\n \\n  are      learned      from      bitext      (see      Figure      3b).\\n \\n \\n  More      precisely,      we      replace      BART’s      encoder      embed-  \\n \\n  ding      layer      with   \\n \\na      new      randomly      initialized      encoder.\\n \\n \\n  The      model      is      trained      end-to-end,      which      trains      the      new       encoder      to      map      foreign      words      into      an      input      that      BART  \\n \\n  can      de-noise      to      English.\\nThe      new      encoder      can      use   \\n \\na       separate      vocabulary      from      the      original      BART      model.\\n \\n \\n  We      train      the      source      encoder      in      two      steps,      in      both  \\n \\n  cases      backpropagating      the      cross-entropy      loss      from      the  \\n \\n  output      of      the      BART      model.\\nIn      the      first      step,      we      freeze  \\n \\n  most      of      BART      parameters      and      only      update      the      ran-  \\n \\n  domly      initialized      source      encoder,      the      BART      positional  \\n \\n  embeddings,      and      the      self-attention      input      projection      ma-  \\n \\n  trix      of      BART’s      encoder      first      layer.\\nIn      the      second      step,  \\n \\n  we      train      all      model      parameters      for   \\n \\na      small      number      of  \\n \\n  iterations.\\n4\\n \\n   Comparing      Pre-training      Objectives  \\n \\n  BART      supports   \\n \\na      much      wider      range      of      noising      schemes  \\n \\n  during      pre-training      than      previous      work.\\nWe      compare   \\n \\na       range      of      options      using      base-size      models      (6      encoder      and  \\n \\n  6      decoder      layers,      with   \\n \\na      hidden      size      of      768),      evaluated  \\n \\n  on   \\n \\na      representative      subset      of      the      tasks      we      will      consider  \\n \\n  for      the      full      large      scale      experiments      in      85.\\n4.1      Comparison      Objectives\\nives  \\n \\n  While      many      pre-training      objectives      have      been      pro-  \\n \\n  posed,      fair      comparisons      between      these      have      been      dif-  \\n \\n  ficult      to      perform,      at      least      in      part      due      to      differences      in  \\n \\n  training      data,      training      resources,      architectural      differ-  \\n \\n  ences      between      models,      and      fine-tuning      procedures.\\nWe  \\n \\n  ABCDE  \\n \\n  weee  \\n \\n  Pre-trained      Pre-trained  \\n \\n  Craters      Joo      rezapes cise)      Te      TrET Pre-trained  \\n \\n  Encoder Pre-trained  \\n \\n  Decoder aa      AB  \\n \\n  trrrt  \\n \\n  tr      ttt  \\n \\n  <s>A      BCD  \\n \\n  Randomly  \\n \\n  Initialized      Encoder  \\n \\n  TFT  \\n \\n  aBpByoe sentation      from      the      final      output      is      used.\\n \\n \\n  re-implement      strong      pre-training      approaches      recently  \\n \\n  proposed      for      discriminative      and      generation      tasks.\\nWe  \\n \\n  aim,      as      much      as      possible,      to      control      for      differences      un-  \\n \\n  related      to      the      pre-training      objective.\\nHowever,      we      do  \\n \\n  make      minor      changes      to      the      learning      rate      and      usage      of  \\n \\n  layer      normalisation      in      order      to      improve      performance  \\n \\n  (tuning      these      separately      for      each      objective).\\nFor      refer-  \\n \\n  ence,      we      compare      our      implementations      with      published  \\n \\n  numbers      from      BERT,      which      was      also      trained      for      1M  \\n \\n  steps      on   \\n \\na      combination      of      books      and      Wikipedia      data.\\n \\n \\n  We      compare      the      following      approaches:\\n \\n \\n  Language      Model      Similarly      to      GPT      (Radford      et      al.,  \\n \\n  2018),      we      train   \\n \\na      left-to-right      Transformer      language  \\n \\n  model.\\nThis      model      is      equivalent      to      the      BART      decoder,  \\n \\n  without      cross-attention.\\n \\n \\n  Permuted      Language      Model      Based      on      XLNet      (Yang  \\n \\n  et      al.,      2019),      we      sample      1/6      of      the      tokens,      and      gener-  \\n \\n  ate      them      in   \\n \\na      random      order      autoregressively.\\nFor      con-  \\n \\n  sistency      with      other      models,      we      do      not      implement      the  \\n \\n  relative      positional      embeddings      or      attention      across      seg-  \\n \\n  ments      from      XLNet.\\n \\n \\n  Masked      Language      Model      Following      BERT      (Devlin  \\n \\n  et      al.,      2019),      we      replace      15%      of      tokens      with      [MASK]  \\n \\n  symbols,      and      train      the      model      to      independently      predict  \\n \\n  the      original      tokens.\\n \\n \\n  Multitask      Masked      Language      Model      As      in      UniLM  \\n \\n  (Dong      et      al.,      2019),      we      train   \\n \\na      Masked      Language  \\n \\n  Model      with      additional      self-attention      masks.\\nSelf      at-  \\n \\n  tention      masks      are      chosen      randomly      in      with      the      follow  \\n \\n  proportions:      1/6      left-to-right,      1/6      right-to-left,      1/3      un-  \\n \\n  masked,      and      1/3      with      the      first      50%      of      tokens      unmasked  \\n \\n  and   \\n \\na      left-to-right      mask      for      the      remainder.\\n \\n \\n  Masked      Seq-to-Seq      Inspired      by      MASS      (Song      et      al.,  \\n \\n  2019),      we      mask   \\n \\na      span      containing      50%      of      tokens,  \\n \\n  and      train   \\n \\na      sequence      to      sequence      model      to      predict      the  \\n \\n  masked      tokens.\\n \\n \\n  For      the      Permuted      LM,      Masked      LM      and      Multitask  \\n \\n  Masked      LM,      we      use      two-stream      attention      (Yang      et      al.,  \\n \\n  2019)      to      efficiently      compute      likelihoods      of      the      output  \\n \\n  part      of      the      sequence      (using   \\n \\na      diagonal      self-attention  \\n \\n  mask      on      the      output      to      predict      words      left-to-right).\\n \\n \\n  We      experiment      with      (1)      treating      the      task      as   \\n \\na      stan-  \\n \\n  dard      sequence-to-sequence      problem,      where      the      source  \\n \\n  input      to      the      encoder      and      the      target      is      the      decoder      out-  \\n \\n  put,      or      (2)      adding      the      source      as      prefix      to      the      target      in  \\n \\n  the      decoder,      with   \\n \\na      loss      only      on      the      target      part      of      the  \\n \\n  sequence.\\nWe      find      the      former      works      better      for      BART  \\n \\n  models,      and      the      latter      for      other      models.\\n \\n \\n  To      most      directly      compare      our      models      on      their      ability  \\n \\n  to      model      their      fine-tuning      objective      (the      log      likelihood  \\n \\n  of      the      human      text),      we      report      perplexity      in      Table      1.\\n4.2      Tasks\\nasks  \\n \\n  SQuAD  \\n \\n  (Rajpurkar      et      al.,      2016)a      an      extractive      ques-  \\n \\n  tion      answering      task      on      Wikipedia      paragraphs.\\nAnswers  \\n \\n  are      text      spans      extracted      from   \\n \\na      given      document      context.\\n \\n \\n  Similar      to      BERT      (Devlin      et      al.,      2019),      we      use      concate-  \\n \\n  nated      question      and      context      as      input      to      the      encoder      of  \\n \\n  BART,      and      additionally      pass      them      to      the      decoder.\\nThe  \\n \\n  model      includes      classifiers      to      predict      the      start      and      end  \\n \\n  indices      of      each      token.\\n \\n \\n  MNLI      (Williams      et      al.,      2017),   \\n \\na      bitext      classification  \\n \\n  task      to      predict      whether      one      sentence      entails      another.\\n \\n \\n  The      fine-tuned      model      concatenates      the      two      sentences  \\n \\n  with      appended      an      EOS      token,      and      passes      them      to      both  \\n \\n  the      BART      encoder      and      decoder.\\nIn      contrast      to      BERT,  \\n \\n  the      representation      of      the      EOS      token      is      used      to      classify  \\n \\n  the      sentences      relations.\\n \\n \\n  ELIS      (Fanetal.,      2019),   \\n \\na      long-form      abstractive      ques-  \\n \\n  tion      answering      dataset.\\nModels      generate      answers      con-  \\n \\n  ditioned      on      the      concatenation      of   \\n \\na      question      and      sup-  \\n \\n  porting      documents.\\nXSum_      (Narayan      et      al.,      2018),   \\n \\na      news      summarization  \\n \\n  dataset      with      highly      abstractive      summaries.\\nConvAI2_      (Dinan      et      al.,      2019),   \\n \\na      dialogue      response  \\n \\n  generation      task,      conditioned      on      context      and   \\n \\na      persona.\\n \\n \\n  CNN/DM_      (Hermann      et      al.,      2015),   \\n \\na      news      summa-  \\n \\n  rization      dataset.\\nSummaries      here      are      typically      closely  \\n \\n  related      to      source      sentences.\\n4.3      Results\\nults Results      are      shown      in      Table      1.\\nSeveral      trends      are      clear:\\nModel      SQuAD      1.1      MNLI      ELIS      XSum   \\n \\n_      ConvAI2      CNN/DM  \\n \\n  Fl      Acc      PPL      PPL      PPL      PPL BERT      Base      (Devlin      et      al.,      2019)      88.5      84.3   \\n \\n-   \\n \\n-   \\n \\n-   \\n \\n-       Masked      Language      Model      90.0      83.5      24.77      7.87      12.59      7.06  \\n \\n  Masked      Seq2seq      87.0      82.1      23.40      6.80      11.43      6.19  \\n \\n  Language      Model      76.7      80.1      21.40      7.00      11.51      6.56  \\n \\n  Permuted      Language      Model      89.1      83.7      24.03      7.69      12.23      6.96  \\n \\n  Multitask      Masked      Language      Model      89.2      82.4      23.73      7.50      12.39      6.74  \\n \\n  BART      Base  \\n \\n  w/      Token      Masking      90.4      84.1      25.05      7.08      11.73      6.10  \\n \\n  w/      Token      Deletion      90.4      84.1      24.61      6.90      11.46      5.87  \\n \\n  w/      Text      Infilling      90.8      84.0      24.26      6.61      11.05      5.83  \\n \\n  w/      Document      Rotation      77.2      75.3      53.69      17.14      19.87      10.59  \\n \\n  w/      Sentence      Shuffling      85.4      81.5      41.87      10.93      16.67      7.89  \\n \\n  w/      Text      Infilling   \\n \\n+      Sentence      Shuffling      90.8      83.8      24.17      6.62      11.12      5.41\\nconsistently      strong      performance.\\n \\n \\n  Performance      of      pre-training      methods      varies      signifi-  \\n \\n  cantly      across      tasks      The      effectiveness      of      pre-training  \\n \\n  methods      is      highly      dependent      on      the      task.\\nFor      exam-  \\n \\n  ple,   \\n \\na      simple      language      model      achieves      the      best      ELIS  \\n \\n  performance,      but      the      worst      SQUAD      results.\\n \\n \\n  Token      masking      is      crucial      Pre-training      objectives  \\n \\n  based      on      rotating      documents      or      permuting      sentences  \\n \\n  perform      poorly      in      isolation.\\nThe      successful      methods  \\n \\n  either      use      token      deletion      or      masking,      or      self-attention  \\n \\n  masks.\\nDeletion      appears      to      outperform      masking      on  \\n \\n  generation      tasks.\\n \\n \\n  Left-to-right      pre-training      improves      generation  \\n \\n  The      Masked      Language      Model      and      the      Permuted  \\n \\n  Language      Model      perform      less      well      than      others      on  \\n \\n  generation,      and      are      the      only      models      we      consider      that  \\n \\n  do      not      include      left-to-right      auto-regressive      language  \\n \\n  modelling      during      pre-training.\\n \\n \\n  Bidirectional      encoders      are      crucial      for      SQUAD      As  \\n \\n  noted      in      previous      work      (Devlin      et      al.,      2019),      just  \\n \\n  left-to-right      decoder      performs      poorly      on      SQuAD,      be-  \\n \\n  cause      future      context      is      crucial      in      classification      deci-  \\n \\n  sions.\\nHowever,      BART      achieves      similar      performance  \\n \\n  with      only      half      the      number      of      bidirectional      layers.\\n \\n \\n  The      pre-training      objective      is      not      the      only      important  \\n \\n  factor      Our      Permuted      Language      Model      performs      less  \\n \\n  well      than      XLNet      (Yang      et      al.,      2019).\\nSome      of      this      dif-  \\n \\n  ference      is      likely      due      to      not      including      other      architectural  \\n \\n  improvements,      such      as      relative-position      embeddings      or  \\n \\n  segment-level      recurrence.\\n \\n \\n  Pure      language      models      perform      best      on      ELIS      The  \\n \\n  ELIS      dataset      is      an      outlier,      with      much      higher      perplex-  \\n \\n  ities      than      other      tasks,      and      is      the      only      generation      task  \\n \\n  where      other      models      outperform      BART.\\n \\n \\nA      pure      lan-  \\n \\n  guage      model      performs      best,      suggesting      that      BART      is  \\n \\n  less      effective      when      the      output      is      only      loosely      con-  \\n \\n  strained      by      the      input.\\n \\n \\n  BART      achieves      the      most      consistently      strong      perfor-  \\n \\n  mance.\\nWith      the      exception      of      ELI5,      BART      models  \\n \\n  using      text-infilling      perform      well      on      all      tasks.\\n5\\n \\n   Large-scale      Pre-training      Experiments  \\n \\n  Recent      work      has      shown      that      downstream      performance  \\n \\n  can      dramatically      improve      when      pre-training      is      scaled  \\n \\n  to      large      batch      sizes      (Yang      et      al.,      2019;      Liu      et      al.,      2019)  \\n \\n  and      corpora.\\nTo      test      how      well      BART      performs      in      this  \\n \\n  regime,      and      to      create   \\n \\na      useful      model      for      downstream  \\n \\n  tasks,      we      trained      BART      using      the      same      scale      as      the  \\n \\n  RoBERTa      model.\\n5.1      Experimental      Setup\\netup  \\n \\n  We      pre-train   \\n \\na      large      model      with      12      layers      in      each      of      the  \\n \\n  encoder      and      decoder,      and   \\n \\na      hidden      size      of      1024.\\nFol-  \\n \\n  lowing      RoBERTa      (Liu      et      al.,      2019),      we      use   \\n \\na      batch      size  \\n \\n  of      8000,      and      train      the      model      for      500000      steps.\\nDocu-  \\n \\n  ments      are      tokenized      with      the      same      byte-pair      encoding  \\n \\n  as      GPT-2      (Radford      et      al.,      2019).\\nBased      on      the      results      in  \\n \\n  Section      §4,      we      use   \\n \\na      combination      of      text      infilling      and  \\n \\n  sentence      permutation.\\nWe      mask      30%      of      tokens      in      each  \\n \\n  document,      and      permute      all      sentences.\\nAlthough      sen-  \\n \\n  tence      permutation      only      shows      significant      additive      gains SQuAD      1.1      SQuAD      2.0      MNLI      SST      QQP      QNLI      STS-B      RTE      MRPC      CoLA  \\n \\n  EM/F1      EM/F1      m/mm      Acc      Acc      Acc      Acc      Acc      Acc      Mcc\\n \\n \\n  BERT      84.1/90.9      79.0/81.8      86.6/-      93.2      91.3      92.3      90.0      70.4      88.0      60.6  \\n \\n  UniLM      -/-      80.5/83.4      87.0/85.9      94.5   \\n \\n-      92.7   \\n \\n-      70.9   \\n \\n-      61.1  \\n \\n  XLNet      89.0/94.5      86.1/88.8      89.8/-      95.6      91.8      93.9      91.8      83.8      89.2      63.6  \\n \\n  RoBERTa      88.9/94.6      86.5/89.4      90.2/90.2      96.4      92.2      94.7      92.4      86.6      90.9      68.0  \\n \\n  BART      88.8/94.6      86.1/89.2      89.9/90.1      96.6      92.5      94.9      91.2      87.0      90.4      62.8 Table      2:      Results      for      large      models      on      SQUAD      and      GLUE      tasks.\\nBART      performs      comparably      to      ROBERTa      and  \\n \\n  XLNet,      suggesting      that      BART’s      uni-directional      decoder      layers      do      not      reduce      performance      on      discriminative      tasks.\\nCNN/DailyMail      XSum       RI      R2      RL      RI      R2      RL\\n \\n \\n  Lead-3      40.42      17.62      36.67      16.30      1.60      11.95  \\n \\n  PTGEN      (See      et      al.,      2017)      36.44      15.66      33.42      29.70      9.21      23.24  \\n \\n  PTGEN+COV      (See      et      al.,      2017)      39.53      17.28      36.38      28.10      8.02      21.72  \\n \\n  UniLM      43.33      20.21      40.51   \\n \\n-   \\n \\n-   \\n \\n-       BERTSUMABS      (Liu   \\n \\n&      Lapata,      2019)      41.72      19.39      38.76      38.76      16.33      31.15  \\n \\n  BERTSUMEXTABS      (Liu   \\n \\n&      Lapata,      2019)      42.13      19.60      39.18      38.81      16.50      31.27 BART      44.16      21.28      40.90      45.14      22.27      37.25 Table      3:      Results      on      two      standard      summarization      datasets.\\nBART      outperforms      previous      work      on      summarization      on\\n \\n \\n  on      the      CNN/DM      summarization      dataset,      we      hypothe-      ConvAI2  \\n \\n  sised      that      larger      pre-trained      models      may      be      better      able      Valid      F1      Valid      PPL  \\n \\n  to   \\n \\n|      fi      this      task.\\nTo      help      th      del      better      fit      th  \\n \\n  RE      aa      ae      Pai      Cae      Seq2Seq+      Attention      16.02      35.07  \\n \\n  data,      we      disabled      dropout      for      the      final      10%      of      training   \\n \\n.    \\n.\\n     Best      System      19.09      17.51  \\n \\n  steps.\\nWe      use      the      same      pre-training      data      as      Liu      et      al.\\n(2019),      consisting      of      160Gb      of      news,      books,      stories,  \\n \\n  and      web      text.\\nBART      20.72      11.85 5.2      Discriminative      Tasks  \\n \\n  Table   \\n \\n2      compares      the      performance      of      BART      with      sev-  \\n \\n  eral      recent      approaches      on      the      well-studied      SQUAD      and  \\n \\n  GLUE      tasks      (Warstadt      et      al.,      2018;      Socher      et      al.,      2013;  \\n \\n  Dolan   \\n \\n&      Brockett,      2005;      Agirre      et      al.,      2007;      Williams  \\n \\n  et      al.,      2018;      Dagan      et      al.,      2006;      Levesque      et      al.,      2011).\\n \\n \\n  Table      4:      BART      outperforms      previous      work      on      conver-  \\n \\n  sational      response      generation.\\nPerplexities      are      renor-  \\n \\n  malized      based      on      official      tokenizer      for      ConvAI2.\\n \\n \\n  The      most      directly      comparable      baseline      is      ROBERTa,  \\n \\n  which      was      pre-trained      with      the      same      resources,      but  \\n \\n  a      different      objective.\\nOverall,      BART      performs      simi-  \\n \\n  larly,      with      only      small      differences      between      the      models  \\n \\n  on      most      tasks.\\nsuggesting      that      BART’s      improvements  \\n \\n  on      generation      tasks      do      not      come      at      the      expense      of      clas-  \\n \\n  sification      performance.\\n \\n \\n  Summarization      To      provide   \\n \\na      comparison      with      the  \\n \\n  state-of-the-art      in      summarization,      we      present      results  \\n \\n  on      two      summarization      datasets,      CNN/DailyMail      and  \\n \\n  XSum,      which      have      distinct      properties.\\n \\n \\n  Summaries      in      the      CNN/DailyMail      tend      to      resemble  \\n \\n  source      sentences.\\nExtractive      models      do      well      here,      and  \\n \\n  even      the      baseline      of      the      first-three      source      sentences      is  \\n \\n  highly      competitive.\\nNevertheless,      BART      outperforms  \\n \\n  all      existing      work.\\n5.3.      Generation      Tasks\\nGeneration      Tasks\\n |       We      also      experiment      with      several      text      generation      tasks.       BART      is      fine-tuned      as      a      standard      sequence-to-sequence       model      from      the      input      to      the      output      text.      During      fine-       tuning      we      use      a      label      smoothed      cross      entropy      loss       (Pereyra      et      al.,      2017),      with      the      smoothing      parameter       set      to      0.1.      During      generation,      we      set      beam      size      as      5,       remove      duplicated      trigrams      in      beam      search,      and      tuned       the      model      with      min-len,      max-len,      length      penalty      on      the       validation      set      (Fan      et      al.,      2017). | 6.0      points      on      all      ROUGE      metrics—trepresenting   \\n \\na      sig-  \\n \\n  nificant      advance      in      performance      on      this      problem.\\nQual-  \\n \\n  itatively,      sample      quality      is      high      (see      86).\\n \\n \\n  In      contrast,      XSum      is      highly      abstractive,      and      extrac-  \\n \\n  tive      models      perform      poorly.\\nBART      outperforms      the  \\n \\n  best      previous      work,      which      leverages      BERT,      by      roughly  \\n \\n  Dialogue      We      evaluate      dialogue      response      generation  \\n \\n  on      CONVAI2      (Dinan      et      al.,      2019),      in      which      agents  \\n \\n  must      generate      responses      conditioned      on      both      the      pre-  \\n \\n  vious      context      and   \\n \\na      textually-specified      persona.\\nBART  \\n \\n  outperforms      previous      work      on      two      automated      metrics.\\n\\nRl      R2      RL\\n \\n \\n  Best      Extractive      23.55      3.1      17.5  \\n \\n  Language      Model      27.8      47      23.1  \\n \\n  Seq2Seq      28.3      5.1      22.8  \\n \\n  Seq2Seq      Multitask      28.9      54      23.1  \\n \\n  BART      30.6      6.2      24.3  \\n \\n  Table      5:      BART      achieves      state-of-the-art      results      on  \\n \\n  the      challenging      ELI5      abstractive      question      answering  \\n \\n  dataset.\\nComparison      models      are      from      Fan      et      al.\\n(2019).'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/1910.13461.pdf', 'section_number': 4, 'section_title': 'Token      Deletion Text      Infilling'}, page_content='Token      Deletion Text      Infilling\\n \\n \\n  Text      Infilling   \\n \\nA      number      of      text      spans      are      sampled,  \\n \\n  with      span      lengths      drawn      from   \\n \\na      Poisson      distribution  \\n \\n  (A   \\n \\n=      3).\\nEach      span      is      replaced      with   \\n \\na      single      [MASK]  \\n \\n  token.\\nO-length      spans      correspond      to      the      insertion      of  \\n \\n  [MASK]      tokens.\\nText      infilling      is      inspired      by      Span-  \\n \\n  BERT      (Joshi      et      al.,      2019),      but      SpanBERT      samples  \\n \\n  span      lengths      from   \\n \\na      different      (clamped      geometric)      dis-  \\n \\n  tribution,      and      replaces      each      span      with   \\n \\na      sequence      of  \\n \\n  [MASK]      tokens      of      exactly      the      same      length.\\nText      infill-  \\n \\n  ing      teaches      the      model      to      predict      how      many      tokens      are  \\n \\n  missing      from   \\n \\na      span.\\n \\n \\n  Sentence      Permutation   \\n \\nA      document      is      divided      into  \\n \\n  sentences      based      on      full      stops,      and      these      sentences      are  \\n \\n  shuffled      in   \\n \\na      random      order.\\n \\n \\n  Document      Rotation   \\n \\nA      token      is      chosen      uniformly      at  \\n \\n  random,      and      the      document      is      rotated      so      that      it      begins  \\n \\n  with      that      token.\\nThis      task      trains      the      model      to      identify  \\n \\n  the      start      of      the      document.\\n3\\n \\n   Fine-tuning      BART The      representations      produced      by      BART      can      be      used      in  \\n \\n  several      ways      for      downstream      applications.\\n3.1      Sequence      Classification      Tasks\\nasks  \\n \\n  For      sequence      classification      tasks,      the      same      input      is      fed  \\n \\n  into      the      encoder      and      decoder,      and      the      final      hidden      state  \\n \\n  of      the      final      decoder      token      is      fed      into      new      multi-class  \\n \\n  linear      classifier.\\nThis      approach      is      related      to      the      CLS  \\n \\n  token      in      BERT;      however      we      add      the      additional      token  \\n \\n  to      the      end      so      that      representation      for      the      token      in      the  \\n \\n  decoder      can      attend      to      decoder      states      from      the      complete  \\n \\n  input      (Figure      3a).\\n3.2.      Token      Classification      Tasks\\nasks  \\n \\n  For      token      classification      tasks,      such      as      answer      endpoint  \\n \\n  classification      for      SQUAD,      we      feed      the      complete      doc-  \\n \\n  ument      into      the      encoder      and      decoder,      and      use      the      top  \\n \\n  hidden      state      of      the      decoder      as   \\n \\na      representation      for      each  \\n \\n  word.\\nThis      representation      is      used      to      classify      the      token.\\n3.3      Sequence      Generation      Tasks\\nasks  \\n \\n  Because      BART      has      an      autoregressive      decoder,      it      can      be  \\n \\n  directly      fine      tuned      for      sequence      generation      tasks      such  \\n \\n  as      abstractive      question      answering      and      summarization.\\n \\n \\n  In      both      of      these      tasks,      information      is      copied      from      the  \\n \\n  input      but      manipulated,      which      is      closely      related      to      the  \\n \\n  denoising      pre-training      objective.\\nHere,      the      encoder      in-  \\n \\n  put      is      the      input      sequence,      and      the      decoder      generates  \\n \\n  outputs      autoregressively.\\n3.4      Machine      Translation\\ntion  \\n \\n  We      also      explore      using      BART      to      improve      machine      trans-  \\n \\n  lation      decoders      for      translating      into      English.\\nPrevious  \\n \\n  work      Edunov      et      al.\\n(2019)      has      shown      that      models      can  \\n \\n  be      improved      by      incorporating      pre-trained      encoders,      but  \\n \\n  gains      from      using      pre-trained      language      models      in      de-  \\n \\n  coders      have      been      limited.\\nWe      show      that      it      is      possible  \\n \\n  to      use      the      entire      BART      model      (both      encoder      and      de-  \\n \\n  coder)      as   \\n \\na      single      pretrained      decoder      for      machine      trans-  \\n \\n  lation,      by      adding   \\n \\na      new      set      of      encoder      parameters      that  \\n \\n  are      learned      from      bitext      (see      Figure      3b).\\n \\n \\n  More      precisely,      we      replace      BART’s      encoder      embed-  \\n \\n  ding      layer      with   \\n \\na      new      randomly      initialized      encoder.\\n \\n \\n  The      model      is      trained      end-to-end,      which      trains      the      new       encoder      to      map      foreign      words      into      an      input      that      BART  \\n \\n  can      de-noise      to      English.\\nThe      new      encoder      can      use   \\n \\na       separate      vocabulary      from      the      original      BART      model.\\n \\n \\n  We      train      the      source      encoder      in      two      steps,      in      both  \\n \\n  cases      backpropagating      the      cross-entropy      loss      from      the  \\n \\n  output      of      the      BART      model.\\nIn      the      first      step,      we      freeze  \\n \\n  most      of      BART      parameters      and      only      update      the      ran-  \\n \\n  domly      initialized      source      encoder,      the      BART      positional  \\n \\n  embeddings,      and      the      self-attention      input      projection      ma-  \\n \\n  trix      of      BART’s      encoder      first      layer.\\nIn      the      second      step,  \\n \\n  we      train      all      model      parameters      for   \\n \\na      small      number      of  \\n \\n  iterations.\\n4\\n \\n   Comparing      Pre-training      Objectives  \\n \\n  BART      supports   \\n \\na      much      wider      range      of      noising      schemes  \\n \\n  during      pre-training      than      previous      work.\\nWe      compare   \\n \\na       range      of      options      using      base-size      models      (6      encoder      and  \\n \\n  6      decoder      layers,      with   \\n \\na      hidden      size      of      768),      evaluated  \\n \\n  on   \\n \\na      representative      subset      of      the      tasks      we      will      consider  \\n \\n  for      the      full      large      scale      experiments      in      85.\\n4.1      Comparison      Objectives\\nives  \\n \\n  While      many      pre-training      objectives      have      been      pro-  \\n \\n  posed,      fair      comparisons      between      these      have      been      dif-  \\n \\n  ficult      to      perform,      at      least      in      part      due      to      differences      in  \\n \\n  training      data,      training      resources,      architectural      differ-  \\n \\n  ences      between      models,      and      fine-tuning      procedures.\\nWe  \\n \\n  ABCDE  \\n \\n  weee  \\n \\n  Pre-trained      Pre-trained  \\n \\n  Craters      Joo      rezapes cise)      Te      TrET Pre-trained  \\n \\n  Encoder Pre-trained  \\n \\n  Decoder aa      AB  \\n \\n  trrrt  \\n \\n  tr      ttt  \\n \\n  <s>A      BCD  \\n \\n  Randomly  \\n \\n  Initialized      Encoder  \\n \\n  TFT  \\n \\n  aBpByoe sentation      from      the      final      output      is      used.\\n \\n \\n  re-implement      strong      pre-training      approaches      recently  \\n \\n  proposed      for      discriminative      and      generation      tasks.\\nWe  \\n \\n  aim,      as      much      as      possible,      to      control      for      differences      un-  \\n \\n  related      to      the      pre-training      objective.\\nHowever,      we      do  \\n \\n  make      minor      changes      to      the      learning      rate      and      usage      of  \\n \\n  layer      normalisation      in      order      to      improve      performance  \\n \\n  (tuning      these      separately      for      each      objective).\\nFor      refer-  \\n \\n  ence,      we      compare      our      implementations      with      published  \\n \\n  numbers      from      BERT,      which      was      also      trained      for      1M  \\n \\n  steps      on   \\n \\na      combination      of      books      and      Wikipedia      data.\\n \\n \\n  We      compare      the      following      approaches:\\n \\n \\n  Language      Model      Similarly      to      GPT      (Radford      et      al.,  \\n \\n  2018),      we      train   \\n \\na      left-to-right      Transformer      language  \\n \\n  model.\\nThis      model      is      equivalent      to      the      BART      decoder,  \\n \\n  without      cross-attention.\\n \\n \\n  Permuted      Language      Model      Based      on      XLNet      (Yang  \\n \\n  et      al.,      2019),      we      sample      1/6      of      the      tokens,      and      gener-  \\n \\n  ate      them      in   \\n \\na      random      order      autoregressively.\\nFor      con-  \\n \\n  sistency      with      other      models,      we      do      not      implement      the  \\n \\n  relative      positional      embeddings      or      attention      across      seg-  \\n \\n  ments      from      XLNet.\\n \\n \\n  Masked      Language      Model      Following      BERT      (Devlin  \\n \\n  et      al.,      2019),      we      replace      15%      of      tokens      with      [MASK]  \\n \\n  symbols,      and      train      the      model      to      independently      predict  \\n \\n  the      original      tokens.\\n \\n \\n  Multitask      Masked      Language      Model      As      in      UniLM  \\n \\n  (Dong      et      al.,      2019),      we      train   \\n \\na      Masked      Language  \\n \\n  Model      with      additional      self-attention      masks.\\nSelf      at-  \\n \\n  tention      masks      are      chosen      randomly      in      with      the      follow  \\n \\n  proportions:      1/6      left-to-right,      1/6      right-to-left,      1/3      un-  \\n \\n  masked,      and      1/3      with      the      first      50%      of      tokens      unmasked  \\n \\n  and   \\n \\na      left-to-right      mask      for      the      remainder.\\n \\n \\n  Masked      Seq-to-Seq      Inspired      by      MASS      (Song      et      al.,  \\n \\n  2019),      we      mask   \\n \\na      span      containing      50%      of      tokens,  \\n \\n  and      train   \\n \\na      sequence      to      sequence      model      to      predict      the  \\n \\n  masked      tokens.\\n \\n \\n  For      the      Permuted      LM,      Masked      LM      and      Multitask  \\n \\n  Masked      LM,      we      use      two-stream      attention      (Yang      et      al.,  \\n \\n  2019)      to      efficiently      compute      likelihoods      of      the      output  \\n \\n  part      of      the      sequence      (using   \\n \\na      diagonal      self-attention  \\n \\n  mask      on      the      output      to      predict      words      left-to-right).\\n \\n \\n  We      experiment      with      (1)      treating      the      task      as   \\n \\na      stan-  \\n \\n  dard      sequence-to-sequence      problem,      where      the      source  \\n \\n  input      to      the      encoder      and      the      target      is      the      decoder      out-  \\n \\n  put,      or      (2)      adding      the      source      as      prefix      to      the      target      in  \\n \\n  the      decoder,      with   \\n \\na      loss      only      on      the      target      part      of      the  \\n \\n  sequence.\\nWe      find      the      former      works      better      for      BART  \\n \\n  models,      and      the      latter      for      other      models.\\n \\n \\n  To      most      directly      compare      our      models      on      their      ability  \\n \\n  to      model      their      fine-tuning      objective      (the      log      likelihood  \\n \\n  of      the      human      text),      we      report      perplexity      in      Table      1.\\n4.2      Tasks\\nasks  \\n \\n  SQuAD  \\n \\n  (Rajpurkar      et      al.,      2016)a      an      extractive      ques-  \\n \\n  tion      answering      task      on      Wikipedia      paragraphs.\\nAnswers  \\n \\n  are      text      spans      extracted      from   \\n \\na      given      document      context.\\n \\n \\n  Similar      to      BERT      (Devlin      et      al.,      2019),      we      use      concate-  \\n \\n  nated      question      and      context      as      input      to      the      encoder      of  \\n \\n  BART,      and      additionally      pass      them      to      the      decoder.\\nThe  \\n \\n  model      includes      classifiers      to      predict      the      start      and      end  \\n \\n  indices      of      each      token.\\n \\n \\n  MNLI      (Williams      et      al.,      2017),   \\n \\na      bitext      classification  \\n \\n  task      to      predict      whether      one      sentence      entails      another.\\n \\n \\n  The      fine-tuned      model      concatenates      the      two      sentences  \\n \\n  with      appended      an      EOS      token,      and      passes      them      to      both  \\n \\n  the      BART      encoder      and      decoder.\\nIn      contrast      to      BERT,  \\n \\n  the      representation      of      the      EOS      token      is      used      to      classify  \\n \\n  the      sentences      relations.\\n \\n \\n  ELIS      (Fanetal.,      2019),   \\n \\na      long-form      abstractive      ques-  \\n \\n  tion      answering      dataset.\\nModels      generate      answers      con-  \\n \\n  ditioned      on      the      concatenation      of   \\n \\na      question      and      sup-  \\n \\n  porting      documents.\\nXSum_      (Narayan      et      al.,      2018),   \\n \\na      news      summarization  \\n \\n  dataset      with      highly      abstractive      summaries.\\nConvAI2_      (Dinan      et      al.,      2019),   \\n \\na      dialogue      response  \\n \\n  generation      task,      conditioned      on      context      and   \\n \\na      persona.\\n \\n \\n  CNN/DM_      (Hermann      et      al.,      2015),   \\n \\na      news      summa-  \\n \\n  rization      dataset.\\nSummaries      here      are      typically      closely  \\n \\n  related      to      source      sentences.\\n4.3      Results\\nults Results      are      shown      in      Table      1.\\nSeveral      trends      are      clear:\\nModel      SQuAD      1.1      MNLI      ELIS      XSum   \\n \\n_      ConvAI2      CNN/DM  \\n \\n  Fl      Acc      PPL      PPL      PPL      PPL BERT      Base      (Devlin      et      al.,      2019)      88.5      84.3   \\n \\n-   \\n \\n-   \\n \\n-   \\n \\n-       Masked      Language      Model      90.0      83.5      24.77      7.87      12.59      7.06  \\n \\n  Masked      Seq2seq      87.0      82.1      23.40      6.80      11.43      6.19  \\n \\n  Language      Model      76.7      80.1      21.40      7.00      11.51      6.56  \\n \\n  Permuted      Language      Model      89.1      83.7      24.03      7.69      12.23      6.96  \\n \\n  Multitask      Masked      Language      Model      89.2      82.4      23.73      7.50      12.39      6.74  \\n \\n  BART      Base  \\n \\n  w/      Token      Masking      90.4      84.1      25.05      7.08      11.73      6.10  \\n \\n  w/      Token      Deletion      90.4      84.1      24.61      6.90      11.46      5.87  \\n \\n  w/      Text      Infilling      90.8      84.0      24.26      6.61      11.05      5.83  \\n \\n  w/      Document      Rotation      77.2      75.3      53.69      17.14      19.87      10.59  \\n \\n  w/      Sentence      Shuffling      85.4      81.5      41.87      10.93      16.67      7.89  \\n \\n  w/      Text      Infilling   \\n \\n+      Sentence      Shuffling      90.8      83.8      24.17      6.62      11.12      5.41\\nconsistently      strong      performance.\\n \\n \\n  Performance      of      pre-training      methods      varies      signifi-  \\n \\n  cantly      across      tasks      The      effectiveness      of      pre-training  \\n \\n  methods      is      highly      dependent      on      the      task.\\nFor      exam-  \\n \\n  ple,   \\n \\na      simple      language      model      achieves      the      best      ELIS  \\n \\n  performance,      but      the      worst      SQUAD      results.\\n \\n \\n  Token      masking      is      crucial      Pre-training      objectives  \\n \\n  based      on      rotating      documents      or      permuting      sentences  \\n \\n  perform      poorly      in      isolation.\\nThe      successful      methods  \\n \\n  either      use      token      deletion      or      masking,      or      self-attention  \\n \\n  masks.\\nDeletion      appears      to      outperform      masking      on  \\n \\n  generation      tasks.\\n \\n \\n  Left-to-right      pre-training      improves      generation  \\n \\n  The      Masked      Language      Model      and      the      Permuted  \\n \\n  Language      Model      perform      less      well      than      others      on  \\n \\n  generation,      and      are      the      only      models      we      consider      that  \\n \\n  do      not      include      left-to-right      auto-regressive      language  \\n \\n  modelling      during      pre-training.\\n \\n \\n  Bidirectional      encoders      are      crucial      for      SQUAD      As  \\n \\n  noted      in      previous      work      (Devlin      et      al.,      2019),      just  \\n \\n  left-to-right      decoder      performs      poorly      on      SQuAD,      be-  \\n \\n  cause      future      context      is      crucial      in      classification      deci-  \\n \\n  sions.\\nHowever,      BART      achieves      similar      performance  \\n \\n  with      only      half      the      number      of      bidirectional      layers.\\n \\n \\n  The      pre-training      objective      is      not      the      only      important  \\n \\n  factor      Our      Permuted      Language      Model      performs      less  \\n \\n  well      than      XLNet      (Yang      et      al.,      2019).\\nSome      of      this      dif-  \\n \\n  ference      is      likely      due      to      not      including      other      architectural  \\n \\n  improvements,      such      as      relative-position      embeddings      or  \\n \\n  segment-level      recurrence.\\n \\n \\n  Pure      language      models      perform      best      on      ELIS      The  \\n \\n  ELIS      dataset      is      an      outlier,      with      much      higher      perplex-  \\n \\n  ities      than      other      tasks,      and      is      the      only      generation      task  \\n \\n  where      other      models      outperform      BART.\\n \\n \\nA      pure      lan-  \\n \\n  guage      model      performs      best,      suggesting      that      BART      is  \\n \\n  less      effective      when      the      output      is      only      loosely      con-  \\n \\n  strained      by      the      input.\\n \\n \\n  BART      achieves      the      most      consistently      strong      perfor-  \\n \\n  mance.\\nWith      the      exception      of      ELI5,      BART      models  \\n \\n  using      text-infilling      perform      well      on      all      tasks.\\n5\\n \\n   Large-scale      Pre-training      Experiments  \\n \\n  Recent      work      has      shown      that      downstream      performance  \\n \\n  can      dramatically      improve      when      pre-training      is      scaled  \\n \\n  to      large      batch      sizes      (Yang      et      al.,      2019;      Liu      et      al.,      2019)  \\n \\n  and      corpora.\\nTo      test      how      well      BART      performs      in      this  \\n \\n  regime,      and      to      create   \\n \\na      useful      model      for      downstream  \\n \\n  tasks,      we      trained      BART      using      the      same      scale      as      the  \\n \\n  RoBERTa      model.\\n5.1      Experimental      Setup\\netup  \\n \\n  We      pre-train   \\n \\na      large      model      with      12      layers      in      each      of      the  \\n \\n  encoder      and      decoder,      and   \\n \\na      hidden      size      of      1024.\\nFol-  \\n \\n  lowing      RoBERTa      (Liu      et      al.,      2019),      we      use   \\n \\na      batch      size  \\n \\n  of      8000,      and      train      the      model      for      500000      steps.\\nDocu-  \\n \\n  ments      are      tokenized      with      the      same      byte-pair      encoding  \\n \\n  as      GPT-2      (Radford      et      al.,      2019).\\nBased      on      the      results      in  \\n \\n  Section      §4,      we      use   \\n \\na      combination      of      text      infilling      and  \\n \\n  sentence      permutation.\\nWe      mask      30%      of      tokens      in      each  \\n \\n  document,      and      permute      all      sentences.\\nAlthough      sen-  \\n \\n  tence      permutation      only      shows      significant      additive      gains SQuAD      1.1      SQuAD      2.0      MNLI      SST      QQP      QNLI      STS-B      RTE      MRPC      CoLA  \\n \\n  EM/F1      EM/F1      m/mm      Acc      Acc      Acc      Acc      Acc      Acc      Mcc\\n \\n \\n  BERT      84.1/90.9      79.0/81.8      86.6/-      93.2      91.3      92.3      90.0      70.4      88.0      60.6  \\n \\n  UniLM      -/-      80.5/83.4      87.0/85.9      94.5   \\n \\n-      92.7   \\n \\n-      70.9   \\n \\n-      61.1  \\n \\n  XLNet      89.0/94.5      86.1/88.8      89.8/-      95.6      91.8      93.9      91.8      83.8      89.2      63.6  \\n \\n  RoBERTa      88.9/94.6      86.5/89.4      90.2/90.2      96.4      92.2      94.7      92.4      86.6      90.9      68.0  \\n \\n  BART      88.8/94.6      86.1/89.2      89.9/90.1      96.6      92.5      94.9      91.2      87.0      90.4      62.8 Table      2:      Results      for      large      models      on      SQUAD      and      GLUE      tasks.\\nBART      performs      comparably      to      ROBERTa      and  \\n \\n  XLNet,      suggesting      that      BART’s      uni-directional      decoder      layers      do      not      reduce      performance      on      discriminative      tasks.\\nCNN/DailyMail      XSum       RI      R2      RL      RI      R2      RL\\n \\n \\n  Lead-3      40.42      17.62      36.67      16.30      1.60      11.95  \\n \\n  PTGEN      (See      et      al.,      2017)      36.44      15.66      33.42      29.70      9.21      23.24  \\n \\n  PTGEN+COV      (See      et      al.,      2017)      39.53      17.28      36.38      28.10      8.02      21.72  \\n \\n  UniLM      43.33      20.21      40.51   \\n \\n-   \\n \\n-   \\n \\n-       BERTSUMABS      (Liu   \\n \\n&      Lapata,      2019)      41.72      19.39      38.76      38.76      16.33      31.15  \\n \\n  BERTSUMEXTABS      (Liu   \\n \\n&      Lapata,      2019)      42.13      19.60      39.18      38.81      16.50      31.27 BART      44.16      21.28      40.90      45.14      22.27      37.25 Table      3:      Results      on      two      standard      summarization      datasets.\\nBART      outperforms      previous      work      on      summarization      on\\n \\n \\n  on      the      CNN/DM      summarization      dataset,      we      hypothe-      ConvAI2  \\n \\n  sised      that      larger      pre-trained      models      may      be      better      able      Valid      F1      Valid      PPL  \\n \\n  to   \\n \\n|      fi      this      task.\\nTo      help      th      del      better      fit      th  \\n \\n  RE      aa      ae      Pai      Cae      Seq2Seq+      Attention      16.02      35.07  \\n \\n  data,      we      disabled      dropout      for      the      final      10%      of      training   \\n \\n.    \\n.\\n     Best      System      19.09      17.51  \\n \\n  steps.\\nWe      use      the      same      pre-training      data      as      Liu      et      al.\\n(2019),      consisting      of      160Gb      of      news,      books,      stories,  \\n \\n  and      web      text.\\nBART      20.72      11.85 5.2      Discriminative      Tasks  \\n \\n  Table   \\n \\n2      compares      the      performance      of      BART      with      sev-  \\n \\n  eral      recent      approaches      on      the      well-studied      SQUAD      and  \\n \\n  GLUE      tasks      (Warstadt      et      al.,      2018;      Socher      et      al.,      2013;  \\n \\n  Dolan   \\n \\n&      Brockett,      2005;      Agirre      et      al.,      2007;      Williams  \\n \\n  et      al.,      2018;      Dagan      et      al.,      2006;      Levesque      et      al.,      2011).\\n \\n \\n  Table      4:      BART      outperforms      previous      work      on      conver-  \\n \\n  sational      response      generation.\\nPerplexities      are      renor-  \\n \\n  malized      based      on      official      tokenizer      for      ConvAI2.\\n \\n \\n  The      most      directly      comparable      baseline      is      ROBERTa,  \\n \\n  which      was      pre-trained      with      the      same      resources,      but  \\n \\n  a      different      objective.\\nOverall,      BART      performs      simi-  \\n \\n  larly,      with      only      small      differences      between      the      models  \\n \\n  on      most      tasks.\\nsuggesting      that      BART’s      improvements  \\n \\n  on      generation      tasks      do      not      come      at      the      expense      of      clas-  \\n \\n  sification      performance.\\n \\n \\n  Summarization      To      provide   \\n \\na      comparison      with      the  \\n \\n  state-of-the-art      in      summarization,      we      present      results  \\n \\n  on      two      summarization      datasets,      CNN/DailyMail      and  \\n \\n  XSum,      which      have      distinct      properties.\\n \\n \\n  Summaries      in      the      CNN/DailyMail      tend      to      resemble  \\n \\n  source      sentences.\\nExtractive      models      do      well      here,      and  \\n \\n  even      the      baseline      of      the      first-three      source      sentences      is  \\n \\n  highly      competitive.\\nNevertheless,      BART      outperforms  \\n \\n  all      existing      work.\\n5.3.      Generation      Tasks\\nGeneration      Tasks\\n |       We      also      experiment      with      several      text      generation      tasks.       BART      is      fine-tuned      as      a      standard      sequence-to-sequence       model      from      the      input      to      the      output      text.      During      fine-       tuning      we      use      a      label      smoothed      cross      entropy      loss       (Pereyra      et      al.,      2017),      with      the      smoothing      parameter       set      to      0.1.      During      generation,      we      set      beam      size      as      5,       remove      duplicated      trigrams      in      beam      search,      and      tuned       the      model      with      min-len,      max-len,      length      penalty      on      the       validation      set      (Fan      et      al.,      2017). | 6.0      points      on      all      ROUGE      metrics—trepresenting   \\n \\na      sig-  \\n \\n  nificant      advance      in      performance      on      this      problem.\\nQual-  \\n \\n  itatively,      sample      quality      is      high      (see      86).\\n \\n \\n  In      contrast,      XSum      is      highly      abstractive,      and      extrac-  \\n \\n  tive      models      perform      poorly.\\nBART      outperforms      the  \\n \\n  best      previous      work,      which      leverages      BERT,      by      roughly  \\n \\n  Dialogue      We      evaluate      dialogue      response      generation  \\n \\n  on      CONVAI2      (Dinan      et      al.,      2019),      in      which      agents  \\n \\n  must      generate      responses      conditioned      on      both      the      pre-  \\n \\n  vious      context      and   \\n \\na      textually-specified      persona.\\nBART  \\n \\n  outperforms      previous      work      on      two      automated      metrics.\\n'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/1910.13461.pdf', 'section_number': 5, 'section_title': '3.1      Sequence      Classification      Tasks'}, page_content='3.1      Sequence      Classification      Tasks\\nasks  \\n \\n  For      sequence      classification      tasks,      the      same      input      is      fed  \\n \\n  into      the      encoder      and      decoder,      and      the      final      hidden      state  \\n \\n  of      the      final      decoder      token      is      fed      into      new      multi-class  \\n \\n  linear      classifier.\\nThis      approach      is      related      to      the      CLS  \\n \\n  token      in      BERT;      however      we      add      the      additional      token  \\n \\n  to      the      end      so      that      representation      for      the      token      in      the  \\n \\n  decoder      can      attend      to      decoder      states      from      the      complete  \\n \\n  input      (Figure      3a).'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/1910.13461.pdf', 'section_number': 6, 'section_title': '3.2.      Token      Classification      Tasks'}, page_content='3.2.      Token      Classification      Tasks\\nasks  \\n \\n  For      token      classification      tasks,      such      as      answer      endpoint  \\n \\n  classification      for      SQUAD,      we      feed      the      complete      doc-  \\n \\n  ument      into      the      encoder      and      decoder,      and      use      the      top  \\n \\n  hidden      state      of      the      decoder      as   \\n \\na      representation      for      each  \\n \\n  word.\\nThis      representation      is      used      to      classify      the      token.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/1910.13461.pdf', 'section_number': 7, 'section_title': '3.3      Sequence      Generation      Tasks'}, page_content='3.3      Sequence      Generation      Tasks\\nasks  \\n \\n  Because      BART      has      an      autoregressive      decoder,      it      can      be  \\n \\n  directly      fine      tuned      for      sequence      generation      tasks      such  \\n \\n  as      abstractive      question      answering      and      summarization.\\n \\n \\n  In      both      of      these      tasks,      information      is      copied      from      the  \\n \\n  input      but      manipulated,      which      is      closely      related      to      the  \\n \\n  denoising      pre-training      objective.\\nHere,      the      encoder      in-  \\n \\n  put      is      the      input      sequence,      and      the      decoder      generates  \\n \\n  outputs      autoregressively.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/1910.13461.pdf', 'section_number': 8, 'section_title': '3.4      Machine      Translation'}, page_content='3.4      Machine      Translation\\ntion  \\n \\n  We      also      explore      using      BART      to      improve      machine      trans-  \\n \\n  lation      decoders      for      translating      into      English.\\nPrevious  \\n \\n  work      Edunov      et      al.\\n(2019)      has      shown      that      models      can  \\n \\n  be      improved      by      incorporating      pre-trained      encoders,      but  \\n \\n  gains      from      using      pre-trained      language      models      in      de-  \\n \\n  coders      have      been      limited.\\nWe      show      that      it      is      possible  \\n \\n  to      use      the      entire      BART      model      (both      encoder      and      de-  \\n \\n  coder)      as   \\n \\na      single      pretrained      decoder      for      machine      trans-  \\n \\n  lation,      by      adding   \\n \\na      new      set      of      encoder      parameters      that  \\n \\n  are      learned      from      bitext      (see      Figure      3b).\\n \\n \\n  More      precisely,      we      replace      BART’s      encoder      embed-  \\n \\n  ding      layer      with   \\n \\na      new      randomly      initialized      encoder.\\n \\n \\n  The      model      is      trained      end-to-end,      which      trains      the      new       encoder      to      map      foreign      words      into      an      input      that      BART  \\n \\n  can      de-noise      to      English.\\nThe      new      encoder      can      use   \\n \\na       separate      vocabulary      from      the      original      BART      model.\\n \\n \\n  We      train      the      source      encoder      in      two      steps,      in      both  \\n \\n  cases      backpropagating      the      cross-entropy      loss      from      the  \\n \\n  output      of      the      BART      model.\\nIn      the      first      step,      we      freeze  \\n \\n  most      of      BART      parameters      and      only      update      the      ran-  \\n \\n  domly      initialized      source      encoder,      the      BART      positional  \\n \\n  embeddings,      and      the      self-attention      input      projection      ma-  \\n \\n  trix      of      BART’s      encoder      first      layer.\\nIn      the      second      step,  \\n \\n  we      train      all      model      parameters      for   \\n \\na      small      number      of  \\n \\n  iterations.\\n4\\n \\n   Comparing      Pre-training      Objectives  \\n \\n  BART      supports   \\n \\na      much      wider      range      of      noising      schemes  \\n \\n  during      pre-training      than      previous      work.\\nWe      compare   \\n \\na       range      of      options      using      base-size      models      (6      encoder      and  \\n \\n  6      decoder      layers,      with   \\n \\na      hidden      size      of      768),      evaluated  \\n \\n  on   \\n \\na      representative      subset      of      the      tasks      we      will      consider  \\n \\n  for      the      full      large      scale      experiments      in      85.\\n4.1      Comparison      Objectives\\nives  \\n \\n  While      many      pre-training      objectives      have      been      pro-  \\n \\n  posed,      fair      comparisons      between      these      have      been      dif-  \\n \\n  ficult      to      perform,      at      least      in      part      due      to      differences      in  \\n \\n  training      data,      training      resources,      architectural      differ-  \\n \\n  ences      between      models,      and      fine-tuning      procedures.\\nWe  \\n \\n  ABCDE  \\n \\n  weee  \\n \\n  Pre-trained      Pre-trained  \\n \\n  Craters      Joo      rezapes cise)      Te      TrET Pre-trained  \\n \\n  Encoder Pre-trained  \\n \\n  Decoder aa      AB  \\n \\n  trrrt  \\n \\n  tr      ttt  \\n \\n  <s>A      BCD  \\n \\n  Randomly  \\n \\n  Initialized      Encoder  \\n \\n  TFT  \\n \\n  aBpByoe sentation      from      the      final      output      is      used.\\n \\n \\n  re-implement      strong      pre-training      approaches      recently  \\n \\n  proposed      for      discriminative      and      generation      tasks.\\nWe  \\n \\n  aim,      as      much      as      possible,      to      control      for      differences      un-  \\n \\n  related      to      the      pre-training      objective.\\nHowever,      we      do  \\n \\n  make      minor      changes      to      the      learning      rate      and      usage      of  \\n \\n  layer      normalisation      in      order      to      improve      performance  \\n \\n  (tuning      these      separately      for      each      objective).\\nFor      refer-  \\n \\n  ence,      we      compare      our      implementations      with      published  \\n \\n  numbers      from      BERT,      which      was      also      trained      for      1M  \\n \\n  steps      on   \\n \\na      combination      of      books      and      Wikipedia      data.\\n \\n \\n  We      compare      the      following      approaches:\\n \\n \\n  Language      Model      Similarly      to      GPT      (Radford      et      al.,  \\n \\n  2018),      we      train   \\n \\na      left-to-right      Transformer      language  \\n \\n  model.\\nThis      model      is      equivalent      to      the      BART      decoder,  \\n \\n  without      cross-attention.\\n \\n \\n  Permuted      Language      Model      Based      on      XLNet      (Yang  \\n \\n  et      al.,      2019),      we      sample      1/6      of      the      tokens,      and      gener-  \\n \\n  ate      them      in   \\n \\na      random      order      autoregressively.\\nFor      con-  \\n \\n  sistency      with      other      models,      we      do      not      implement      the  \\n \\n  relative      positional      embeddings      or      attention      across      seg-  \\n \\n  ments      from      XLNet.\\n \\n \\n  Masked      Language      Model      Following      BERT      (Devlin  \\n \\n  et      al.,      2019),      we      replace      15%      of      tokens      with      [MASK]  \\n \\n  symbols,      and      train      the      model      to      independently      predict  \\n \\n  the      original      tokens.\\n \\n \\n  Multitask      Masked      Language      Model      As      in      UniLM  \\n \\n  (Dong      et      al.,      2019),      we      train   \\n \\na      Masked      Language  \\n \\n  Model      with      additional      self-attention      masks.\\nSelf      at-  \\n \\n  tention      masks      are      chosen      randomly      in      with      the      follow  \\n \\n  proportions:      1/6      left-to-right,      1/6      right-to-left,      1/3      un-  \\n \\n  masked,      and      1/3      with      the      first      50%      of      tokens      unmasked  \\n \\n  and   \\n \\na      left-to-right      mask      for      the      remainder.\\n \\n \\n  Masked      Seq-to-Seq      Inspired      by      MASS      (Song      et      al.,  \\n \\n  2019),      we      mask   \\n \\na      span      containing      50%      of      tokens,  \\n \\n  and      train   \\n \\na      sequence      to      sequence      model      to      predict      the  \\n \\n  masked      tokens.\\n \\n \\n  For      the      Permuted      LM,      Masked      LM      and      Multitask  \\n \\n  Masked      LM,      we      use      two-stream      attention      (Yang      et      al.,  \\n \\n  2019)      to      efficiently      compute      likelihoods      of      the      output  \\n \\n  part      of      the      sequence      (using   \\n \\na      diagonal      self-attention  \\n \\n  mask      on      the      output      to      predict      words      left-to-right).\\n \\n \\n  We      experiment      with      (1)      treating      the      task      as   \\n \\na      stan-  \\n \\n  dard      sequence-to-sequence      problem,      where      the      source  \\n \\n  input      to      the      encoder      and      the      target      is      the      decoder      out-  \\n \\n  put,      or      (2)      adding      the      source      as      prefix      to      the      target      in  \\n \\n  the      decoder,      with   \\n \\na      loss      only      on      the      target      part      of      the  \\n \\n  sequence.\\nWe      find      the      former      works      better      for      BART  \\n \\n  models,      and      the      latter      for      other      models.\\n \\n \\n  To      most      directly      compare      our      models      on      their      ability  \\n \\n  to      model      their      fine-tuning      objective      (the      log      likelihood  \\n \\n  of      the      human      text),      we      report      perplexity      in      Table      1.\\n4.2      Tasks\\nasks  \\n \\n  SQuAD  \\n \\n  (Rajpurkar      et      al.,      2016)a      an      extractive      ques-  \\n \\n  tion      answering      task      on      Wikipedia      paragraphs.\\nAnswers  \\n \\n  are      text      spans      extracted      from   \\n \\na      given      document      context.\\n \\n \\n  Similar      to      BERT      (Devlin      et      al.,      2019),      we      use      concate-  \\n \\n  nated      question      and      context      as      input      to      the      encoder      of  \\n \\n  BART,      and      additionally      pass      them      to      the      decoder.\\nThe  \\n \\n  model      includes      classifiers      to      predict      the      start      and      end  \\n \\n  indices      of      each      token.\\n \\n \\n  MNLI      (Williams      et      al.,      2017),   \\n \\na      bitext      classification  \\n \\n  task      to      predict      whether      one      sentence      entails      another.\\n \\n \\n  The      fine-tuned      model      concatenates      the      two      sentences  \\n \\n  with      appended      an      EOS      token,      and      passes      them      to      both  \\n \\n  the      BART      encoder      and      decoder.\\nIn      contrast      to      BERT,  \\n \\n  the      representation      of      the      EOS      token      is      used      to      classify  \\n \\n  the      sentences      relations.\\n \\n \\n  ELIS      (Fanetal.,      2019),   \\n \\na      long-form      abstractive      ques-  \\n \\n  tion      answering      dataset.\\nModels      generate      answers      con-  \\n \\n  ditioned      on      the      concatenation      of   \\n \\na      question      and      sup-  \\n \\n  porting      documents.\\nXSum_      (Narayan      et      al.,      2018),   \\n \\na      news      summarization  \\n \\n  dataset      with      highly      abstractive      summaries.\\nConvAI2_      (Dinan      et      al.,      2019),   \\n \\na      dialogue      response  \\n \\n  generation      task,      conditioned      on      context      and   \\n \\na      persona.\\n \\n \\n  CNN/DM_      (Hermann      et      al.,      2015),   \\n \\na      news      summa-  \\n \\n  rization      dataset.\\nSummaries      here      are      typically      closely  \\n \\n  related      to      source      sentences.\\n4.3      Results\\nults Results      are      shown      in      Table      1.\\nSeveral      trends      are      clear:\\nModel      SQuAD      1.1      MNLI      ELIS      XSum   \\n \\n_      ConvAI2      CNN/DM  \\n \\n  Fl      Acc      PPL      PPL      PPL      PPL BERT      Base      (Devlin      et      al.,      2019)      88.5      84.3   \\n \\n-   \\n \\n-   \\n \\n-   \\n \\n-       Masked      Language      Model      90.0      83.5      24.77      7.87      12.59      7.06  \\n \\n  Masked      Seq2seq      87.0      82.1      23.40      6.80      11.43      6.19  \\n \\n  Language      Model      76.7      80.1      21.40      7.00      11.51      6.56  \\n \\n  Permuted      Language      Model      89.1      83.7      24.03      7.69      12.23      6.96  \\n \\n  Multitask      Masked      Language      Model      89.2      82.4      23.73      7.50      12.39      6.74  \\n \\n  BART      Base  \\n \\n  w/      Token      Masking      90.4      84.1      25.05      7.08      11.73      6.10  \\n \\n  w/      Token      Deletion      90.4      84.1      24.61      6.90      11.46      5.87  \\n \\n  w/      Text      Infilling      90.8      84.0      24.26      6.61      11.05      5.83  \\n \\n  w/      Document      Rotation      77.2      75.3      53.69      17.14      19.87      10.59  \\n \\n  w/      Sentence      Shuffling      85.4      81.5      41.87      10.93      16.67      7.89  \\n \\n  w/      Text      Infilling   \\n \\n+      Sentence      Shuffling      90.8      83.8      24.17      6.62      11.12      5.41\\nconsistently      strong      performance.\\n \\n \\n  Performance      of      pre-training      methods      varies      signifi-  \\n \\n  cantly      across      tasks      The      effectiveness      of      pre-training  \\n \\n  methods      is      highly      dependent      on      the      task.\\nFor      exam-  \\n \\n  ple,   \\n \\na      simple      language      model      achieves      the      best      ELIS  \\n \\n  performance,      but      the      worst      SQUAD      results.\\n \\n \\n  Token      masking      is      crucial      Pre-training      objectives  \\n \\n  based      on      rotating      documents      or      permuting      sentences  \\n \\n  perform      poorly      in      isolation.\\nThe      successful      methods  \\n \\n  either      use      token      deletion      or      masking,      or      self-attention  \\n \\n  masks.\\nDeletion      appears      to      outperform      masking      on  \\n \\n  generation      tasks.\\n \\n \\n  Left-to-right      pre-training      improves      generation  \\n \\n  The      Masked      Language      Model      and      the      Permuted  \\n \\n  Language      Model      perform      less      well      than      others      on  \\n \\n  generation,      and      are      the      only      models      we      consider      that  \\n \\n  do      not      include      left-to-right      auto-regressive      language  \\n \\n  modelling      during      pre-training.\\n \\n \\n  Bidirectional      encoders      are      crucial      for      SQUAD      As  \\n \\n  noted      in      previous      work      (Devlin      et      al.,      2019),      just  \\n \\n  left-to-right      decoder      performs      poorly      on      SQuAD,      be-  \\n \\n  cause      future      context      is      crucial      in      classification      deci-  \\n \\n  sions.\\nHowever,      BART      achieves      similar      performance  \\n \\n  with      only      half      the      number      of      bidirectional      layers.\\n \\n \\n  The      pre-training      objective      is      not      the      only      important  \\n \\n  factor      Our      Permuted      Language      Model      performs      less  \\n \\n  well      than      XLNet      (Yang      et      al.,      2019).\\nSome      of      this      dif-  \\n \\n  ference      is      likely      due      to      not      including      other      architectural  \\n \\n  improvements,      such      as      relative-position      embeddings      or  \\n \\n  segment-level      recurrence.\\n \\n \\n  Pure      language      models      perform      best      on      ELIS      The  \\n \\n  ELIS      dataset      is      an      outlier,      with      much      higher      perplex-  \\n \\n  ities      than      other      tasks,      and      is      the      only      generation      task  \\n \\n  where      other      models      outperform      BART.\\n \\n \\nA      pure      lan-  \\n \\n  guage      model      performs      best,      suggesting      that      BART      is  \\n \\n  less      effective      when      the      output      is      only      loosely      con-  \\n \\n  strained      by      the      input.\\n \\n \\n  BART      achieves      the      most      consistently      strong      perfor-  \\n \\n  mance.\\nWith      the      exception      of      ELI5,      BART      models  \\n \\n  using      text-infilling      perform      well      on      all      tasks.\\n5\\n \\n   Large-scale      Pre-training      Experiments  \\n \\n  Recent      work      has      shown      that      downstream      performance  \\n \\n  can      dramatically      improve      when      pre-training      is      scaled  \\n \\n  to      large      batch      sizes      (Yang      et      al.,      2019;      Liu      et      al.,      2019)  \\n \\n  and      corpora.\\nTo      test      how      well      BART      performs      in      this  \\n \\n  regime,      and      to      create   \\n \\na      useful      model      for      downstream  \\n \\n  tasks,      we      trained      BART      using      the      same      scale      as      the  \\n \\n  RoBERTa      model.\\n5.1      Experimental      Setup\\netup  \\n \\n  We      pre-train   \\n \\na      large      model      with      12      layers      in      each      of      the  \\n \\n  encoder      and      decoder,      and   \\n \\na      hidden      size      of      1024.\\nFol-  \\n \\n  lowing      RoBERTa      (Liu      et      al.,      2019),      we      use   \\n \\na      batch      size  \\n \\n  of      8000,      and      train      the      model      for      500000      steps.\\nDocu-  \\n \\n  ments      are      tokenized      with      the      same      byte-pair      encoding  \\n \\n  as      GPT-2      (Radford      et      al.,      2019).\\nBased      on      the      results      in  \\n \\n  Section      §4,      we      use   \\n \\na      combination      of      text      infilling      and  \\n \\n  sentence      permutation.\\nWe      mask      30%      of      tokens      in      each  \\n \\n  document,      and      permute      all      sentences.\\nAlthough      sen-  \\n \\n  tence      permutation      only      shows      significant      additive      gains SQuAD      1.1      SQuAD      2.0      MNLI      SST      QQP      QNLI      STS-B      RTE      MRPC      CoLA  \\n \\n  EM/F1      EM/F1      m/mm      Acc      Acc      Acc      Acc      Acc      Acc      Mcc\\n \\n \\n  BERT      84.1/90.9      79.0/81.8      86.6/-      93.2      91.3      92.3      90.0      70.4      88.0      60.6  \\n \\n  UniLM      -/-      80.5/83.4      87.0/85.9      94.5   \\n \\n-      92.7   \\n \\n-      70.9   \\n \\n-      61.1  \\n \\n  XLNet      89.0/94.5      86.1/88.8      89.8/-      95.6      91.8      93.9      91.8      83.8      89.2      63.6  \\n \\n  RoBERTa      88.9/94.6      86.5/89.4      90.2/90.2      96.4      92.2      94.7      92.4      86.6      90.9      68.0  \\n \\n  BART      88.8/94.6      86.1/89.2      89.9/90.1      96.6      92.5      94.9      91.2      87.0      90.4      62.8 Table      2:      Results      for      large      models      on      SQUAD      and      GLUE      tasks.\\nBART      performs      comparably      to      ROBERTa      and  \\n \\n  XLNet,      suggesting      that      BART’s      uni-directional      decoder      layers      do      not      reduce      performance      on      discriminative      tasks.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/1910.13461.pdf', 'section_number': 9, 'section_title': '4.1      Comparison      Objectives'}, page_content='4.1      Comparison      Objectives\\nives  \\n \\n  While      many      pre-training      objectives      have      been      pro-  \\n \\n  posed,      fair      comparisons      between      these      have      been      dif-  \\n \\n  ficult      to      perform,      at      least      in      part      due      to      differences      in  \\n \\n  training      data,      training      resources,      architectural      differ-  \\n \\n  ences      between      models,      and      fine-tuning      procedures.\\nWe  \\n \\n  ABCDE  \\n \\n  weee  \\n \\n  Pre-trained      Pre-trained  \\n \\n  Craters      Joo      rezapes cise)      Te      TrET Pre-trained  \\n \\n  Encoder Pre-trained  \\n \\n  Decoder aa      AB  \\n \\n  trrrt  \\n \\n  tr      ttt  \\n \\n  <s>A      BCD  \\n \\n  Randomly  \\n \\n  Initialized      Encoder  \\n \\n  TFT  \\n \\n  aBpByoe sentation      from      the      final      output      is      used.\\n \\n \\n  re-implement      strong      pre-training      approaches      recently  \\n \\n  proposed      for      discriminative      and      generation      tasks.\\nWe  \\n \\n  aim,      as      much      as      possible,      to      control      for      differences      un-  \\n \\n  related      to      the      pre-training      objective.\\nHowever,      we      do  \\n \\n  make      minor      changes      to      the      learning      rate      and      usage      of  \\n \\n  layer      normalisation      in      order      to      improve      performance  \\n \\n  (tuning      these      separately      for      each      objective).\\nFor      refer-  \\n \\n  ence,      we      compare      our      implementations      with      published  \\n \\n  numbers      from      BERT,      which      was      also      trained      for      1M  \\n \\n  steps      on   \\n \\na      combination      of      books      and      Wikipedia      data.\\n \\n \\n  We      compare      the      following      approaches:\\n \\n \\n  Language      Model      Similarly      to      GPT      (Radford      et      al.,  \\n \\n  2018),      we      train   \\n \\na      left-to-right      Transformer      language  \\n \\n  model.\\nThis      model      is      equivalent      to      the      BART      decoder,  \\n \\n  without      cross-attention.\\n \\n \\n  Permuted      Language      Model      Based      on      XLNet      (Yang  \\n \\n  et      al.,      2019),      we      sample      1/6      of      the      tokens,      and      gener-  \\n \\n  ate      them      in   \\n \\na      random      order      autoregressively.\\nFor      con-  \\n \\n  sistency      with      other      models,      we      do      not      implement      the  \\n \\n  relative      positional      embeddings      or      attention      across      seg-  \\n \\n  ments      from      XLNet.\\n \\n \\n  Masked      Language      Model      Following      BERT      (Devlin  \\n \\n  et      al.,      2019),      we      replace      15%      of      tokens      with      [MASK]  \\n \\n  symbols,      and      train      the      model      to      independently      predict  \\n \\n  the      original      tokens.\\n \\n \\n  Multitask      Masked      Language      Model      As      in      UniLM  \\n \\n  (Dong      et      al.,      2019),      we      train   \\n \\na      Masked      Language  \\n \\n  Model      with      additional      self-attention      masks.\\nSelf      at-  \\n \\n  tention      masks      are      chosen      randomly      in      with      the      follow  \\n \\n  proportions:      1/6      left-to-right,      1/6      right-to-left,      1/3      un-  \\n \\n  masked,      and      1/3      with      the      first      50%      of      tokens      unmasked  \\n \\n  and   \\n \\na      left-to-right      mask      for      the      remainder.\\n \\n \\n  Masked      Seq-to-Seq      Inspired      by      MASS      (Song      et      al.,  \\n \\n  2019),      we      mask   \\n \\na      span      containing      50%      of      tokens,  \\n \\n  and      train   \\n \\na      sequence      to      sequence      model      to      predict      the  \\n \\n  masked      tokens.\\n \\n \\n  For      the      Permuted      LM,      Masked      LM      and      Multitask  \\n \\n  Masked      LM,      we      use      two-stream      attention      (Yang      et      al.,  \\n \\n  2019)      to      efficiently      compute      likelihoods      of      the      output  \\n \\n  part      of      the      sequence      (using   \\n \\na      diagonal      self-attention  \\n \\n  mask      on      the      output      to      predict      words      left-to-right).\\n \\n \\n  We      experiment      with      (1)      treating      the      task      as   \\n \\na      stan-  \\n \\n  dard      sequence-to-sequence      problem,      where      the      source  \\n \\n  input      to      the      encoder      and      the      target      is      the      decoder      out-  \\n \\n  put,      or      (2)      adding      the      source      as      prefix      to      the      target      in  \\n \\n  the      decoder,      with   \\n \\na      loss      only      on      the      target      part      of      the  \\n \\n  sequence.\\nWe      find      the      former      works      better      for      BART  \\n \\n  models,      and      the      latter      for      other      models.\\n \\n \\n  To      most      directly      compare      our      models      on      their      ability  \\n \\n  to      model      their      fine-tuning      objective      (the      log      likelihood  \\n \\n  of      the      human      text),      we      report      perplexity      in      Table      1.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/1910.13461.pdf', 'section_number': 10, 'section_title': '4.2      Tasks'}, page_content='4.2      Tasks\\nasks  \\n \\n  SQuAD  \\n \\n  (Rajpurkar      et      al.,      2016)a      an      extractive      ques-  \\n \\n  tion      answering      task      on      Wikipedia      paragraphs.\\nAnswers  \\n \\n  are      text      spans      extracted      from   \\n \\na      given      document      context.\\n \\n \\n  Similar      to      BERT      (Devlin      et      al.,      2019),      we      use      concate-  \\n \\n  nated      question      and      context      as      input      to      the      encoder      of  \\n \\n  BART,      and      additionally      pass      them      to      the      decoder.\\nThe  \\n \\n  model      includes      classifiers      to      predict      the      start      and      end  \\n \\n  indices      of      each      token.\\n \\n \\n  MNLI      (Williams      et      al.,      2017),   \\n \\na      bitext      classification  \\n \\n  task      to      predict      whether      one      sentence      entails      another.\\n \\n \\n  The      fine-tuned      model      concatenates      the      two      sentences  \\n \\n  with      appended      an      EOS      token,      and      passes      them      to      both  \\n \\n  the      BART      encoder      and      decoder.\\nIn      contrast      to      BERT,  \\n \\n  the      representation      of      the      EOS      token      is      used      to      classify  \\n \\n  the      sentences      relations.\\n \\n \\n  ELIS      (Fanetal.,      2019),   \\n \\na      long-form      abstractive      ques-  \\n \\n  tion      answering      dataset.\\nModels      generate      answers      con-  \\n \\n  ditioned      on      the      concatenation      of   \\n \\na      question      and      sup-  \\n \\n  porting      documents.\\nXSum_      (Narayan      et      al.,      2018),   \\n \\na      news      summarization  \\n \\n  dataset      with      highly      abstractive      summaries.\\nConvAI2_      (Dinan      et      al.,      2019),   \\n \\na      dialogue      response  \\n \\n  generation      task,      conditioned      on      context      and   \\n \\na      persona.\\n \\n \\n  CNN/DM_      (Hermann      et      al.,      2015),   \\n \\na      news      summa-  \\n \\n  rization      dataset.\\nSummaries      here      are      typically      closely  \\n \\n  related      to      source      sentences.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/1910.13461.pdf', 'section_number': 11, 'section_title': '4.3      Results'}, page_content='4.3      Results\\nults Results      are      shown      in      Table      1.\\nSeveral      trends      are      clear:\\nModel      SQuAD      1.1      MNLI      ELIS      XSum   \\n \\n_      ConvAI2      CNN/DM  \\n \\n  Fl      Acc      PPL      PPL      PPL      PPL BERT      Base      (Devlin      et      al.,      2019)      88.5      84.3   \\n \\n-   \\n \\n-   \\n \\n-   \\n \\n-       Masked      Language      Model      90.0      83.5      24.77      7.87      12.59      7.06  \\n \\n  Masked      Seq2seq      87.0      82.1      23.40      6.80      11.43      6.19  \\n \\n  Language      Model      76.7      80.1      21.40      7.00      11.51      6.56  \\n \\n  Permuted      Language      Model      89.1      83.7      24.03      7.69      12.23      6.96  \\n \\n  Multitask      Masked      Language      Model      89.2      82.4      23.73      7.50      12.39      6.74  \\n \\n  BART      Base  \\n \\n  w/      Token      Masking      90.4      84.1      25.05      7.08      11.73      6.10  \\n \\n  w/      Token      Deletion      90.4      84.1      24.61      6.90      11.46      5.87  \\n \\n  w/      Text      Infilling      90.8      84.0      24.26      6.61      11.05      5.83  \\n \\n  w/      Document      Rotation      77.2      75.3      53.69      17.14      19.87      10.59  \\n \\n  w/      Sentence      Shuffling      85.4      81.5      41.87      10.93      16.67      7.89  \\n \\n  w/      Text      Infilling   \\n \\n+      Sentence      Shuffling      90.8      83.8      24.17      6.62      11.12      5.41\\nconsistently      strong      performance.\\n \\n \\n  Performance      of      pre-training      methods      varies      signifi-  \\n \\n  cantly      across      tasks      The      effectiveness      of      pre-training  \\n \\n  methods      is      highly      dependent      on      the      task.\\nFor      exam-  \\n \\n  ple,   \\n \\na      simple      language      model      achieves      the      best      ELIS  \\n \\n  performance,      but      the      worst      SQUAD      results.\\n \\n \\n  Token      masking      is      crucial      Pre-training      objectives  \\n \\n  based      on      rotating      documents      or      permuting      sentences  \\n \\n  perform      poorly      in      isolation.\\nThe      successful      methods  \\n \\n  either      use      token      deletion      or      masking,      or      self-attention  \\n \\n  masks.\\nDeletion      appears      to      outperform      masking      on  \\n \\n  generation      tasks.\\n \\n \\n  Left-to-right      pre-training      improves      generation  \\n \\n  The      Masked      Language      Model      and      the      Permuted  \\n \\n  Language      Model      perform      less      well      than      others      on  \\n \\n  generation,      and      are      the      only      models      we      consider      that  \\n \\n  do      not      include      left-to-right      auto-regressive      language  \\n \\n  modelling      during      pre-training.\\n \\n \\n  Bidirectional      encoders      are      crucial      for      SQUAD      As  \\n \\n  noted      in      previous      work      (Devlin      et      al.,      2019),      just  \\n \\n  left-to-right      decoder      performs      poorly      on      SQuAD,      be-  \\n \\n  cause      future      context      is      crucial      in      classification      deci-  \\n \\n  sions.\\nHowever,      BART      achieves      similar      performance  \\n \\n  with      only      half      the      number      of      bidirectional      layers.\\n \\n \\n  The      pre-training      objective      is      not      the      only      important  \\n \\n  factor      Our      Permuted      Language      Model      performs      less  \\n \\n  well      than      XLNet      (Yang      et      al.,      2019).\\nSome      of      this      dif-  \\n \\n  ference      is      likely      due      to      not      including      other      architectural  \\n \\n  improvements,      such      as      relative-position      embeddings      or  \\n \\n  segment-level      recurrence.\\n \\n \\n  Pure      language      models      perform      best      on      ELIS      The  \\n \\n  ELIS      dataset      is      an      outlier,      with      much      higher      perplex-  \\n \\n  ities      than      other      tasks,      and      is      the      only      generation      task  \\n \\n  where      other      models      outperform      BART.\\n \\n \\nA      pure      lan-  \\n \\n  guage      model      performs      best,      suggesting      that      BART      is  \\n \\n  less      effective      when      the      output      is      only      loosely      con-  \\n \\n  strained      by      the      input.\\n \\n \\n  BART      achieves      the      most      consistently      strong      perfor-  \\n \\n  mance.\\nWith      the      exception      of      ELI5,      BART      models  \\n \\n  using      text-infilling      perform      well      on      all      tasks.\\n5\\n \\n   Large-scale      Pre-training      Experiments  \\n \\n  Recent      work      has      shown      that      downstream      performance  \\n \\n  can      dramatically      improve      when      pre-training      is      scaled  \\n \\n  to      large      batch      sizes      (Yang      et      al.,      2019;      Liu      et      al.,      2019)  \\n \\n  and      corpora.\\nTo      test      how      well      BART      performs      in      this  \\n \\n  regime,      and      to      create   \\n \\na      useful      model      for      downstream  \\n \\n  tasks,      we      trained      BART      using      the      same      scale      as      the  \\n \\n  RoBERTa      model.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/1910.13461.pdf', 'section_number': 12, 'section_title': '5.1      Experimental      Setup'}, page_content='5.1      Experimental      Setup\\netup  \\n \\n  We      pre-train   \\n \\na      large      model      with      12      layers      in      each      of      the  \\n \\n  encoder      and      decoder,      and   \\n \\na      hidden      size      of      1024.\\nFol-  \\n \\n  lowing      RoBERTa      (Liu      et      al.,      2019),      we      use   \\n \\na      batch      size  \\n \\n  of      8000,      and      train      the      model      for      500000      steps.\\nDocu-  \\n \\n  ments      are      tokenized      with      the      same      byte-pair      encoding  \\n \\n  as      GPT-2      (Radford      et      al.,      2019).\\nBased      on      the      results      in  \\n \\n  Section      §4,      we      use   \\n \\na      combination      of      text      infilling      and  \\n \\n  sentence      permutation.\\nWe      mask      30%      of      tokens      in      each  \\n \\n  document,      and      permute      all      sentences.\\nAlthough      sen-  \\n \\n  tence      permutation      only      shows      significant      additive      gains SQuAD      1.1      SQuAD      2.0      MNLI      SST      QQP      QNLI      STS-B      RTE      MRPC      CoLA  \\n \\n  EM/F1      EM/F1      m/mm      Acc      Acc      Acc      Acc      Acc      Acc      Mcc\\n \\n \\n  BERT      84.1/90.9      79.0/81.8      86.6/-      93.2      91.3      92.3      90.0      70.4      88.0      60.6  \\n \\n  UniLM      -/-      80.5/83.4      87.0/85.9      94.5   \\n \\n-      92.7   \\n \\n-      70.9   \\n \\n-      61.1  \\n \\n  XLNet      89.0/94.5      86.1/88.8      89.8/-      95.6      91.8      93.9      91.8      83.8      89.2      63.6  \\n \\n  RoBERTa      88.9/94.6      86.5/89.4      90.2/90.2      96.4      92.2      94.7      92.4      86.6      90.9      68.0  \\n \\n  BART      88.8/94.6      86.1/89.2      89.9/90.1      96.6      92.5      94.9      91.2      87.0      90.4      62.8 Table      2:      Results      for      large      models      on      SQUAD      and      GLUE      tasks.\\nBART      performs      comparably      to      ROBERTa      and  \\n \\n  XLNet,      suggesting      that      BART’s      uni-directional      decoder      layers      do      not      reduce      performance      on      discriminative      tasks.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/1910.13461.pdf', 'section_number': 13, 'section_title': 'CNN/DailyMail      XSum       RI      R2      RL      RI      R2      RL'}, page_content='CNN/DailyMail      XSum       RI      R2      RL      RI      R2      RL\\n \\n \\n  Lead-3      40.42      17.62      36.67      16.30      1.60      11.95  \\n \\n  PTGEN      (See      et      al.,      2017)      36.44      15.66      33.42      29.70      9.21      23.24  \\n \\n  PTGEN+COV      (See      et      al.,      2017)      39.53      17.28      36.38      28.10      8.02      21.72  \\n \\n  UniLM      43.33      20.21      40.51   \\n \\n-   \\n \\n-   \\n \\n-       BERTSUMABS      (Liu   \\n \\n&      Lapata,      2019)      41.72      19.39      38.76      38.76      16.33      31.15  \\n \\n  BERTSUMEXTABS      (Liu   \\n \\n&      Lapata,      2019)      42.13      19.60      39.18      38.81      16.50      31.27 BART      44.16      21.28      40.90      45.14      22.27      37.25 Table      3:      Results      on      two      standard      summarization      datasets.\\nBART      outperforms      previous      work      on      summarization      on\\n \\n \\n  on      the      CNN/DM      summarization      dataset,      we      hypothe-      ConvAI2  \\n \\n  sised      that      larger      pre-trained      models      may      be      better      able      Valid      F1      Valid      PPL  \\n \\n  to   \\n \\n|      fi      this      task.\\nTo      help      th      del      better      fit      th  \\n \\n  RE      aa      ae      Pai      Cae      Seq2Seq+      Attention      16.02      35.07  \\n \\n  data,      we      disabled      dropout      for      the      final      10%      of      training   \\n \\n.    \\n.\\n     Best      System      19.09      17.51  \\n \\n  steps.\\nWe      use      the      same      pre-training      data      as      Liu      et      al.\\n(2019),      consisting      of      160Gb      of      news,      books,      stories,  \\n \\n  and      web      text.\\nBART      20.72      11.85 5.2      Discriminative      Tasks  \\n \\n  Table   \\n \\n2      compares      the      performance      of      BART      with      sev-  \\n \\n  eral      recent      approaches      on      the      well-studied      SQUAD      and  \\n \\n  GLUE      tasks      (Warstadt      et      al.,      2018;      Socher      et      al.,      2013;  \\n \\n  Dolan   \\n \\n&      Brockett,      2005;      Agirre      et      al.,      2007;      Williams  \\n \\n  et      al.,      2018;      Dagan      et      al.,      2006;      Levesque      et      al.,      2011).\\n \\n \\n  Table      4:      BART      outperforms      previous      work      on      conver-  \\n \\n  sational      response      generation.\\nPerplexities      are      renor-  \\n \\n  malized      based      on      official      tokenizer      for      ConvAI2.\\n \\n \\n  The      most      directly      comparable      baseline      is      ROBERTa,  \\n \\n  which      was      pre-trained      with      the      same      resources,      but  \\n \\n  a      different      objective.\\nOverall,      BART      performs      simi-  \\n \\n  larly,      with      only      small      differences      between      the      models  \\n \\n  on      most      tasks.\\nsuggesting      that      BART’s      improvements  \\n \\n  on      generation      tasks      do      not      come      at      the      expense      of      clas-  \\n \\n  sification      performance.\\n \\n \\n  Summarization      To      provide   \\n \\na      comparison      with      the  \\n \\n  state-of-the-art      in      summarization,      we      present      results  \\n \\n  on      two      summarization      datasets,      CNN/DailyMail      and  \\n \\n  XSum,      which      have      distinct      properties.\\n \\n \\n  Summaries      in      the      CNN/DailyMail      tend      to      resemble  \\n \\n  source      sentences.\\nExtractive      models      do      well      here,      and  \\n \\n  even      the      baseline      of      the      first-three      source      sentences      is  \\n \\n  highly      competitive.\\nNevertheless,      BART      outperforms  \\n \\n  all      existing      work.\\n5.3.      Generation      Tasks\\nGeneration      Tasks\\n |       We      also      experiment      with      several      text      generation      tasks.       BART      is      fine-tuned      as      a      standard      sequence-to-sequence       model      from      the      input      to      the      output      text.      During      fine-       tuning      we      use      a      label      smoothed      cross      entropy      loss       (Pereyra      et      al.,      2017),      with      the      smoothing      parameter       set      to      0.1.      During      generation,      we      set      beam      size      as      5,       remove      duplicated      trigrams      in      beam      search,      and      tuned       the      model      with      min-len,      max-len,      length      penalty      on      the       validation      set      (Fan      et      al.,      2017). | 6.0      points      on      all      ROUGE      metrics—trepresenting   \\n \\na      sig-  \\n \\n  nificant      advance      in      performance      on      this      problem.\\nQual-  \\n \\n  itatively,      sample      quality      is      high      (see      86).\\n \\n \\n  In      contrast,      XSum      is      highly      abstractive,      and      extrac-  \\n \\n  tive      models      perform      poorly.\\nBART      outperforms      the  \\n \\n  best      previous      work,      which      leverages      BERT,      by      roughly  \\n \\n  Dialogue      We      evaluate      dialogue      response      generation  \\n \\n  on      CONVAI2      (Dinan      et      al.,      2019),      in      which      agents  \\n \\n  must      generate      responses      conditioned      on      both      the      pre-  \\n \\n  vious      context      and   \\n \\na      textually-specified      persona.\\nBART  \\n \\n  outperforms      previous      work      on      two      automated      metrics.\\n'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/1910.13461.pdf', 'section_number': 14, 'section_title': '5.3.      Generation      Tasks'}, page_content='5.3.      Generation      Tasks\\nGeneration      Tasks\\n |       We      also      experiment      with      several      text      generation      tasks.       BART      is      fine-tuned      as      a      standard      sequence-to-sequence       model      from      the      input      to      the      output      text.      During      fine-       tuning      we      use      a      label      smoothed      cross      entropy      loss       (Pereyra      et      al.,      2017),      with      the      smoothing      parameter       set      to      0.1.      During      generation,      we      set      beam      size      as      5,       remove      duplicated      trigrams      in      beam      search,      and      tuned       the      model      with      min-len,      max-len,      length      penalty      on      the       validation      set      (Fan      et      al.,      2017). | 6.0      points      on      all      ROUGE      metrics—trepresenting   \\n \\na      sig-  \\n \\n  nificant      advance      in      performance      on      this      problem.\\nQual-  \\n \\n  itatively,      sample      quality      is      high      (see      86).\\n \\n \\n  In      contrast,      XSum      is      highly      abstractive,      and      extrac-  \\n \\n  tive      models      perform      poorly.\\nBART      outperforms      the  \\n \\n  best      previous      work,      which      leverages      BERT,      by      roughly  \\n \\n  Dialogue      We      evaluate      dialogue      response      generation  \\n \\n  on      CONVAI2      (Dinan      et      al.,      2019),      in      which      agents  \\n \\n  must      generate      responses      conditioned      on      both      the      pre-  \\n \\n  vious      context      and   \\n \\na      textually-specified      persona.\\nBART  \\n \\n  outperforms      previous      work      on      two      automated      metrics.\\n'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/1910.13461.pdf', 'section_number': 15, 'section_title': 'Rl      R2      RL'}, page_content='Rl      R2      RL\\n \\n \\n  Best      Extractive      23.55      3.1      17.5  \\n \\n  Language      Model      27.8      47      23.1  \\n \\n  Seq2Seq      28.3      5.1      22.8  \\n \\n  Seq2Seq      Multitask      28.9      54      23.1  \\n \\n  BART      30.6      6.2      24.3  \\n \\n  Table      5:      BART      achieves      state-of-the-art      results      on  \\n \\n  the      challenging      ELI5      abstractive      question      answering  \\n \\n  dataset.\\nComparison      models      are      from      Fan      et      al.\\n(2019).'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/1910.13461.pdf', 'section_number': 16, 'section_title': 'RO-EN'}, page_content='RO-EN\\nBaseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96\\n \\n \\n  Table      6:      The      performance      (BLEU)      of      baseline      and  \\n \\n  BART      on      WMT’16      RO-EN      augmented      with      back-  \\n \\n  translation      data.\\nBART      improves      over   \\n \\na      strong      back-  \\n \\n  translation      (BT)      baseline      by      using      monolingual      English  \\n \\n  pre-training.\\n \\n \\n  Abstractive      QA      We      use      the      recently      proposed      ELIS  \\n \\n  dataset      to      test      the      model’s      ability      to      generate      long      free-  \\n \\n  form      answers.\\nWe      find      BART      outperforms      the      best      pre-  \\n \\n  vious      work      by      1.2      ROUGE-L,      but      the      dataset      remains  \\n \\n  a      challenging,      because      answers      are      only      weakly      speci-  \\n \\n  fied      by      the      question.\\n5.4      Translation\\ntion  \\n \\n  We      also      evaluated      performance      on      WMT16      Romanian-  \\n \\n  English,      augmented      with      back-translation      data  \\n \\n  from      Sennrich      et      al.\\n(2016).\\nWe      use   \\n \\na      6-layer  \\n \\n  transformer      source      encoder      to      map      Romanian      into  \\n \\n  a      representation      that      BART      is      able      to      de-noise      into  \\n \\n  English,      following      the      approach      introduced      in      83.4.  \\n \\n  Experiment      results      are      presented      in      Table      6.      We  \\n \\n  compare      our      results      against   \\n \\na      baseline      Transformer  \\n \\n  architecture      (Vaswani      et      al.,      2017)      with      Transformer-  \\n \\n  large      settings      (the      baseline      row).\\nWe      show      the  \\n \\n  performance      of      both      steps      of      our      model      in      the      fixed  \\n \\n  BART      and      tuned      BART      rows.\\nFor      each      row      we  \\n \\n  experiment      on      the      original      WMT16      Romanian-English  \\n \\n  augmented      with      back-translation      data.\\nWe      use   \\n \\na       beam      width      of   \\n \\n5      and   \\n \\na      length      penalty      of   \\n \\na   \\n \\n=      1.  \\n \\n  Preliminary      results      suggested      that      our      approach      was  \\n \\n  less      effective      without      back-translation      data,      and      prone  \\n \\n  to      overfitting—future      work      should      explore      additional  \\n \\n  regularization      techniques.\\n6\\n \\n   Qualitative      Analysis  \\n \\n  BART      shows      large      improvements      on      summarization  \\n \\n  metrics,      of      up      to   \\n \\n6      points      over      the      prior      state-of-the-art.\\n \\n \\n  To      understand      BART’s      performance      beyond      automated  \\n \\n  metrics,      we      analyse      its      generations      qualitatively.\\n \\n \\n  Table   \\n \\n7      shows      example      summaries      generated      by  \\n \\n  BART.\\nExamples      are      taken      from      WikiNews      articles  \\n \\n  published      after      the      creation      of      the      pre-training      corpus,  \\n \\n  to      eliminate      the      possibility      of      the      events      described      be-  \\n \\n  ing      present      in      the      model’s      training      data.\\nFollowing  \\n \\n  Narayan      et      al.\\n(2018),      we      remove      the      first      sentence      of  \\n \\n  the      article      prior      to      summarizing      it,      so      there      is      no      easy  \\n \\n  extractive      summary      of      the      document.\\n \\n \\n  Unsurprisingly,      model      output      is      fluent      and      grammat-  \\n \\n  ical      English.\\nHowever,      model      output      is      also      highly      ab-  \\n \\n  stractive,      with      few      phrases      copied      from      the      input.\\nThe  \\n \\n  output      is      also      generally      factually      accurate,      and      inte-  \\n \\n  grates      supporting      evidence      from      across      the      input      doc-  \\n \\n  ument      with      background      knowledge      (for      example,      cor-  \\n \\n  rectly      completing      names,      or      inferring      that      PG&E      oper-  \\n \\n  ates      in      California).\\nIn      the      first      example,      inferring      that  \\n \\n  fish      are      protecting      reefs      from      global      warming      requires  \\n \\n  non-trivial      inference      from      the      text.\\nHowever,      the      claim  \\n \\n  that      the      work      was      published      in      Science      is      not      supported  \\n \\n  by      the      source.\\n \\n \\n  These      samples      demonstrate      that      the      BART      pretrain-  \\n \\n  ing      has      learned   \\n \\na      strong      combination      of      natural      lan-  \\n \\n  guage      understanding      and      generation.\\n7\\n \\n   Related      Work  \\n \\n  Early      methods      for      pretraining      were      based      on      language  \\n \\n  models.\\nGPT      (Radford      et      al.,      2018)      only      models      left-  \\n \\n  ward      context,      which      is      problematic      for      some      tasks.\\n \\n \\n  ELMo      (Peters      et      al.,      2018)      concatenates      left-only      and  \\n \\n  right-only      representations,      but      does      not      pre-train      inter-  \\n \\n  actions      between      these      features.\\nRadford      et      al.\\n(2019)  \\n \\n  demonstrated      that      very      large      language      models      can      act       as      unsupervised      multitask      models.\\n \\n \\n  MASS      (Song      et      al.,      2019)      is      perhaps      the      most      similar  \\n \\n  model      to      BART.\\nAn      input      sequence      where   \\n \\na      contiguous  \\n \\n  span      of      tokens      is      masked      is      mapped      to   \\n \\na      sequence      con-  \\n \\n  sisting      of      the      missing      tokens.\\nMASS      is      less      effective  \\n \\n  for      discriminative      tasks,      because      disjoint      sets      of      tokens  \\n \\n  are      fed      into      the      encoder      and      decoder.\\n \\n \\n  BERT      (Devlin      et      al.,      2019)      introduced      masked      lan-  \\n \\n  guage      modelling,      which      allows      pre-training      to      learn      in-  \\n \\n  teractions      between      left      and      right      context      words.\\nRe-  \\n \\n  cent      work      has      shown      that      very      strong      performance      can  \\n \\n  be      achieved      by      training      for      longer      (Liu      et      al.,      2019),  \\n \\n  by      tying      parameters      across      layers      (Lan      et      al.,      2019),  \\n \\n  and      by      masking      spans      instead      of      words      (Joshi      et      al.,  \\n \\n  2019).\\nPredictions      are      not      made      auto-regressively,      re-  \\n \\n  ducing      the      effectiveness      of      BERT      for      generation      tasks.\\n \\n \\n  UniLM      (Dong      et      al.,      2019)      fine-tunes      BERT      with      an  \\n \\n  ensemble      of      masks,      some      of      which      allow      only      leftward  \\n \\n  context.\\nLike      BART,      this      allows      UniLM      to      be      used      for  \\n \\n  both      generative      and      discriminative      tasks.\\n \\n \\nA      difference  \\n \\n  is      that      UniLM      predictions      are      conditionally      indepen-  \\n \\n  dent,      whereas      BART’s      are      autoregressive.\\nBART      re-  \\n \\n  duces      the      mismatch      between      pre-training      and      genera-  \\n \\n  tion      tasks,      because      the      decoder      is      always      trained      on      un-  \\n \\n  corrupted      context.\\n \\n \\n  XL-Net      (Yang      et      al.,      2019)      extends      BERT      by      pre-\\n |       Source      Document      (abbreviated) |       BART      Summary\\n |       The      researchers      examined      three      types      of      coral      in      reefs      off      the coast of Fiji The researchers found when fish were plentiful,       they      would      eat      algae      and      seaweed      off      the      corals,      which      appeared       to      leave      them      more      resistant      to      the      bacterium      Vibrio      coralliilyti-       cus,      a      bacterium      associated      with      bleaching.      The      researchers      sug-       gested      the      algae,      like      warming      temperatures,      might      render      the       corals’      chemical      defenses      less      effective,      and      the      fish      were      pro-       tecting      the      coral      by      removing      the      algae. |       Fisheries      off      the      coast      of      Fiji      are      protect-       ing      coral      reefs      from      the      effects      of      global       warming,      according      to      a      study      in      the      jour-       nal      Science.\\n |       Sacoolas,      who      has      immunity      as      a      diplomat’s      wife,      was      involved in a traffic collision Prime Minister Johnson was questioned       about      the      case      while      speaking      to      the      press      at      a      hospital      in      Wat- ford. He said, “I hope that Anne Sacoolas will come back       if      we      can’t      resolve      it      then      of      course      I      will      be      raising      it      myself       personally      with      the      White      House.” |       Boris      Johnson      has      said      he      will      raise      the      is-       sue      of      US      diplomat      Anne      Sacoolas’      diplo-       matic      immunity      with      the      White      House.\\n |       According      to      Syrian      state      media,      government      forces      began      de- ploying into previously SDF controlled territory yesterday.       On      October      6,      US      President      Donald      Trump      and      Turkish      Presi-       dent      Recep      Tayyip      Erdoan      spoke      on      the      phone.      Then      both      na-       tions      issued      statements      speaking      of      an      imminent      incursion      into northeast Syria On Wednesday, Turkey began a military       offensive      with      airstrikes      followed      by      a      ground      invasion. |       Syrian      government      forces      have      entered       territory      held      by      the      US-backed      Syrian       Democratic      Forces      (SDF)      in      response      to       Turkey’s      incursion      into      the      region.\\n |       This      is      the      first      time      anyone      has      been      recorded      to      run      a      full       marathon      of      42.195      kilometers      (approximately      26      miles)      under       this      pursued      landmark      time.      It      was      not,      however,      an      officially       sanctioned      world      record,      as      it      was      not      an      ’open      race”      of      the       IAAF.      His      time      was      1      hour      59      minutes      40.2      seconds.      Kipchoge       ran      in      Vienna,      Austria.      It      was      an      event      specifically      designed      to       help      Kipchoge      break      the      two      hour      barrier. |       Kenyan      runner      Eliud      Kipchoge      has      run      a       marathon      in      less      than      two      hours.\\n |       PG&E      stated      it      scheduled      the      blackouts      in      response      to      forecasts       for      high      winds      amid      dry      conditions.      The      aim      is      to      reduce      the      risk       of      wildfires.      Nearly      800      thousand      customers      were      scheduled      to       be      affected      by      the      shutoffs      which      were      expected      to      last      through       at      least      midday      tomorrow. |       Power      has      been      turned      off      to      millions      of       customers      in      California      as      part      of      a      power       shutoff      plan.\\n\\n \\n \\n  dicting      masked      tokens      auto-regressively      in   \\n \\na      permuted  \\n \\n  order.\\nThis      objective      allows      predictions      to      condition      on  \\n \\n  both      left      and      right      context.\\nIn      contrast,      the      BART      de-  \\n \\n  coder      works      left-to-right      during      pre-training,      matching  \\n \\n  the      setting      during      generation.\\n \\n \\n  Several      papers      have      explored      using      pre-trained      rep-  \\n \\n  resentations      to      improve      machine      translation.\\nThe  \\n \\n  largest      improvements      have      come      from      pre-training      on  \\n \\n  both      source      and      target      languages      (Song      et      al.,      2019;  \\n \\n  Lample   \\n \\n&      Conneau,      2019),      but      this      requires      pre-  \\n \\n  training      on      all      languages      of      interest.\\nOther      work      has  \\n \\n  shown      that      encoders      can      be      improved      using      pre-trained  \\n \\n  representations      (Edunov      et      al.,      2019),      but      gains      in      de-  \\n \\n  coders      are      more      limited.\\nWe      show      how      BART      can      be  \\n \\n  used      to      improve      machine      translation      decoders.\\n8\\n \\n   Conclusions  \\n \\n  We      introduced      BART,   \\n \\na      pre-training      approach      that  \\n \\n  learns      to      map      corrupted      documents      to      the      original.\\n \\n \\n  BART      achieves      similar      performance      to      ROBERTa      on  \\n \\n  discriminative      tasks,      while      achieving      new      state-of-the-  \\n \\n  art      results      on   \\n \\na      number      of      text      generation      tasks.\\nFu-  \\n \\n  ture      work      should      explore      new      methods      for      corrupting  \\n \\n  documents      for      pre-training,      perhaps      tailoring      them      to  \\n \\n  specific      end      tasks.\\nReferences\\n \\n \\n  Eneko      Agirre,      Llu’is      M‘arquez,      and      Richard      Wicen-  \\n \\n  towski      (eds.).\\nProceedings      of      the      Fourth      Interna-  \\n \\n  tional      Workshop      on      Semantic      Evaluations      (SemEval-  \\n \\n  2007).\\nAssociation      for      Computational      Linguistics,  \\n \\n  Prague,      Czech      Republic,      June      2007.\\n \\n \\n  Ido      Dagan,      Oren      Glickman,      and      Bernardo      Magnini.\\n \\n \\n  The      PASCAL      recognising      textual      entailment      chal-  \\n \\n  lenge.\\nIn      Machine      learning      challenges.\\nevaluat-  \\n \\n  ing      predictive      uncertainty,      visual      object      classifica-  \\n \\n  tion,      and      recognising      tectual      entailment,      pp.\\n177—  \\n \\n  190.\\nSpringer,      2006.\\n \\n \\n  Jacob      Devlin,      Ming-Wei      Chang,      Kenton      Lee,      and  \\n \\n  Kristina      Toutanova.\\nBERT:      Pre-training      of      deep  \\n \\n  bidirectional      transformers      for      language      understand-  \\n \\n  ing.\\nIn      Proceedings      of      the      2019      Conference      of      the  \\n \\n  North      American      Chapter      of      the      Association      for      Com-  \\n \\n  putational      Linguistics:      Human      Language      Technolo-  \\n \\n  gies,      Volume   \\n \\nI      (Long      and      Short      Papers),      pp.\\n4171-  \\n \\n  4186,      Minneapolis,      Minnesota,      June      2019.\\nAssocia-  \\n \\n  tion      for      Computational      Linguistics.\\ndoi:      10.18653/  \\n \\n  vI/N19-1423.\\nURL      https://www.aclweb.\\n \\n \\n  org/anthology/N19-1423.\\n \\n \\n  Emily      Dinan,      Varvara      Logacheva,      Valentin      Malykh,  \\n \\n  Alexander      Miller,      Kurt      Shuster,      Jack      Urbanek,  \\n \\n  Douwe      Kiela,      Arthur      Szlam,      Iulian      Serban,      Ryan  \\n \\n  Lowe,      et      al.\\nThe      second      conversational      in-  \\n \\n  telligence      challenge      (convai2).\\narXiv      preprint  \\n \\n  arXiv:      1902.00098,      2019.\\n \\n \\n  William   \\n \\nB      Dolan      and      Chris      Brockett.\\nAutomatically  \\n \\n  constructing   \\n \\na      corpus      of      sentential      paraphrases.\\nIn  \\n \\n  Proceedings      of      the      International      Workshop      on      Para-  \\n \\n  phrasing,      2005.\\n \\n \\n  Li      Dong,      Nan      Yang,      Wenhui      Wang,      Furu      Wei,      Xi-  \\n \\n  aodong      Liu,      Yu      Wang,      Jianfeng      Gao,      Ming      Zhou,  \\n \\n  and      Hsiao-Wuen      Hon.\\nUnified      language      model      pre-  \\n \\n  training      for      natural      language      understanding      and      gen-  \\n \\n  eration.\\narXiv      preprint      arXiv:      1905.03197,      2019.\\n \\n \\n  Sergey      Edunov,      Alexei      Baevski,      and      Michael      Auli.\\n \\n \\n  Pre-trained      language      model      representations      for      lan-  \\n \\n  guage      generation.\\nIn      Proceedings      of      the      2019      Con-  \\n \\n  ference      of      the      North      American      Chapter      of      the      Asso-  \\n \\n  ciation      for      Computational      Linguistics:      Human      Lan-  \\n \\n  guage      Technologies,      Volume   \\n \\n1      (Long      and      Short      Pa-  \\n \\n  pers),      2019.\\n \\n \\n  Angela      Fan,      David      Grangier,      and      Michael      Auli.\\nCon-  \\n \\n  trollable      abstractive      summarization.\\narXiv      preprint  \\n \\n  arXiv:      1711.05217,      2017.\\n \\n \\n  Angela      Fan,      Yacine      Jernite,      Ethan      Perez,      David  \\n \\n  Grangier,      Jason      Weston,      and      Michael      Auli.\\nEli5:  \\n \\n  Long      form      question      answering.\\narXiv      preprint  \\n \\n  arXiv:      1907.09190,      2019.\\n \\n \\n  Dan      Hendrycks      and      Kevin      Gimpel.\\nGaussian      error      lin-  \\n \\n  ear      units      (gelus).\\narXiv      preprint      arXiv:      1606.08415,  \\n \\n  2016.\\n \\n \\n  Karl      Moritz      Hermann,      Tomas      Kocisky,      Edward  \\n \\n  Grefenstette,      Lasse      Espeholt,      Will      Kay,      Mustafa      Su-  \\n \\n  leyman,      and      Phil      Blunsom.\\nTeaching      machines      to  \\n \\n  read      and      comprehend.\\nIn      Advances      in      neural      infor-  \\n \\n  mation      processing      systems,      pp.\\n1693-1701,      2015.\\n \\n \\n  Mandar      Joshi,      Danqi      Chen,      Yinhan      Liu,      Daniel   \\n \\nS      Weld,  \\n \\n  Luke      Zettlemoyer,      and      Omer      Levy.\\nSpanbert:      Im-  \\n \\n  proving      pre-training      by      representing      and      predicting  \\n \\n  spans.\\narXiv      preprint      arXiv:      1907.10529,      2019.\\n \\n \\n  Guillaume      Lample      and      Alexis      Conneau.\\n \\n \\n—      Cross-  \\n \\n  lingual      language      model      pretraining.\\narXiv      preprint  \\n \\n  arXiv:      1901.07291,      2019.\\n \\n \\n  Zhenzhong      Lan,      Mingda      Chen,      Sebastian      Goodman,  \\n \\n  Kevin      Gimpel,      Piyush      Sharma,      and      Radu      Sori-  \\n \\n  cut.\\nAlbert:   \\n \\nA      lite      bert      for      self-supervised      learn-  \\n \\n  ing      of      language      representations.\\narXiv      preprint  \\n \\n  arXiv:      1909.11942,      2019.\\n \\n \\n  Hector   \\n \\nJ      Levesque,      Ernest      Davis,      and      Leora      Morgen-  \\n \\n  stern.\\nThe      Winograd      schema      challenge.\\nIn      AAAI  \\n \\n  Spring      Symposium:      Logical      Formalizations      of      Com-  \\n \\n  monsense      Reasoning,      volume      46,      pp.\\n47,      2011.\\n \\n \\n  Yang      Liu      and      Mirella      Lapata.\\n \\n \\n  tion      with      pretrained      encoders.\\n \\n \\n  arXiv:      1908.08345,      2019.  \\n \\n  Text      summariza-  \\n \\n  arXiv      preprint  \\n \\n  Yinhan      Liu,      Myle      Ott,      Naman      Goyal,      Jingfei      Du,      Man-  \\n \\n  dar      Joshi,      Dangi      Chen,      Omer      Levy,      Mike      Lewis,  \\n \\n  Luke      Zettlemoyer,      and      Veselin      Stoyanov.\\nRoberta:\\n \\n \\n \\nA      robustly      optimized      bert      pretraining      approach.\\n \\n \\n  arXiv      preprint      arXiv:      1907.11692,      2019.\\n \\n \\n  Tomas      Mikolov,      Kai      Chen,      Greg      Corrado,      and      Jeffrey  \\n \\n  Dean.\\nEfficient      estimation      of      word      representations  \\n \\n  in      vector      space.\\narXiv      preprint      arXiv:1301.3781,  \\n \\n  2013.\\n \\n \\n  Shashi      Narayan,      Shay   \\n \\nB      Cohen,      and      Mirella      Lapata.\\n \\n \\n  Don’t      give      me      the      details,      just      the      summary!\\ntopic-  \\n \\n  aware      convolutional      neural      networks      for      extreme  \\n \\n  summarization.\\narXiv      preprint      arXiv:      1808.08745,  \\n \\n  2018.\\n \\n \\n  Gabriel      Pereyra,      George      Tucker,      Jan      Chorowski,  \\n \\n  Lukasz      Kaiser,      and      Geoffrey      Hinton.\\nRegularizing  \\n \\n  neural      networks      by      penalizing      confident      output      dis-  \\n \\n  tributions.\\narXiv      preprint      arXiv:      1701.06548,      2017.\\n \\n \\n  Matthew      E      Peters,      Mark      Neumann,      Mohit      Iyyer,      Matt  \\n \\n  Gardner,      Christopher      Clark,      Kenton      Lee,      and      Luke  \\n \\n  Zettlemoyer.\\nDeep      contextualized      word      representa-  \\n \\n  tions.\\narXiv      preprint      arXiv:      1802.05365,      2018.\\n \\n \\n  Alec      Radford,      Karthik      Narasimhan,      Tim      Salimans,  \\n \\n  and      Ilya      Sutskever.\\nImproving      language      un-  \\n \\n  derstanding      by      generative      pre-training.\\nURL  \\n \\n  https://s3-us-west-2.\\n \\n \\n|      amazonaws.\\n \\n \\n—      com/openai-  \\n \\n  assets/researchcovers/languageunsupervised/language  \\n \\n  understanding      paper.\\npdf,      2018.\\n \\n \\n  Alec      Radford,      Jeffrey      Wu,      Rewon      Child,      David      Luan,  \\n \\n  Dario      Amodei,      and      Ilya      Sutskever.\\nLanguage      mod-  \\n \\n  els      are      unsupervised      multitask      learners.\\nOpenAI  \\n \\n  Blog,      1(8),      2019.\\n \\n \\n  Pranav      Rajpurkar,      Jian      Zhang,      Konstantin      Lopyrev,  \\n \\n  and      Percy      Liang.\\nSquad:      100,000+      questions      for  \\n \\n  machine      comprehension      of      text.\\narXiv      preprint  \\n \\n  arXiv:      1606.05250,      2016.\\n \\n \\n  Abigail      See,      Peter   \\n \\nJ      Liu,      and      Christopher   \\n \\nD       Manning.\\nGet      to      the      point:      Summarization  \\n \\n  with      pointer-generator      networks.\\narXiv      preprint  \\n \\n  arXiv:      1704.04368,      2017.\\n \\n \\n  Rico      Sennrich,      Barry      Haddow,      and      Alexandra      Birch.\\n \\n \\n  Edinburgh      neural      machine      translation      systems      for  \\n \\n  WMT      16.\\nIn      Proceedings      of      the      First      Conference  \\n \\n  on      Machine      Translation:      Volume      2,      Shared      Task      Pa-  \\n \\n  pers,      2016.\\n \\n \\n  Richard      Socher,      Alex      Perelygin,      Jean      Wu,      Jason  \\n \\n  Chuang,      Christopher   \\n \\nD      Manning,      Andrew      Ng,      and  \\n \\n  Christopher      Potts.\\nRecursive      deep      models      for      se-  \\n \\n  mantic      compositionality      over   \\n \\na      sentiment      treebank.\\n \\n \\n  In      Proceedings      of      EMNLP,      pp.\\n1631-1642,      2013.\\n \\n \\n  Kaitao      Song,      Xu      Tan,      Tao      Qin,      Jianfeng      Lu,      and      Tie-  \\n \\n  Yan      Liu.\\nMass:      Masked      sequence      to      sequence      pre-  \\n \\n  training      for      language      generation.\\nIn      International  \\n \\n  Conference      on      Machine      Learning,      2019.\\n \\n \\n  Ashish      Vaswani,      Noam      Shazeer,      Niki      Parmar,      Jakob  \\n \\n  Uszkoreit,      Llion      Jones,      Aidan      N      Gomez,      Lukasz  \\n \\n  Kaiser,      and      Ilia      Polosukhin.\\nAttention      is      all      you  \\n \\n  need.\\nIn      Advances      in      neural      information      processing  \\n \\n  systems,      pp.\\n5998-6008,      2017.\\n \\n \\n  Alex      Wang,      Amanpreet      Singh,      Julian      Michael,      Felix  \\n \\n  Hill,      Omer      Levy,      and      Samuel      R      Bowman.\\nGlue:\\n \\n \\n \\nA      multi-task      benchmark      and      analysis      platform      for  \\n \\n  natural      language      understanding.\\narXiv      preprint  \\n \\n  arXiv:      1804.07461,      2018.\\n \\n \\n  Alex      Warstadt,      Amanpreet      Singh,      and      Samuel      R.  \\n \\n  Bowman.\\nNeural      network      acceptability      judgments.\\n \\n \\n  arXiv      preprint      1805.12471,      2018.\\n \\n \\n  Adina      Williams,      Nikita      Nangia,      and      Samuel      R      Bow-  \\n \\n  man.\\n \\n \\nA   \\n \\n_      broad-coverage      challenge      corpus      for  \\n \\n  sentence      understanding      through      inference.\\narXiv  \\n \\n  preprint      arXiv:      1704.05426,      2017.\\n \\n \\n  Adina      Williams,      Nikita      Nangia,      and      Samuel      R.      Bow-  \\n \\n  man.\\n \\n \\nA      broad-coverage      challenge      corpus      for      sen-  \\n \\n  tence      understanding      through      inference.\\nIn      Proceed-  \\n \\n  ings      of      NAACL-HLT,      2018.\\n \\n \\n  Zhilin      Yang,      Zihang      Dai,      Yiming      Yang,      Jaime  \\n \\n  Carbonell,      Ruslan      Salakhutdinov,      and      Quoc   \\n \\nV       Le.\\nXlInet:      Generalized      autoregressive      pretrain-  \\n \\n  ing      for      language      understanding.\\narXiv      preprint  \\n \\n  arXiv:      1906.08237,      2019.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/1910.13461.pdf', 'section_number': 17, 'section_title': 'Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96'}, page_content='Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96\\n \\n \\n  Table      6:      The      performance      (BLEU)      of      baseline      and  \\n \\n  BART      on      WMT’16      RO-EN      augmented      with      back-  \\n \\n  translation      data.\\nBART      improves      over   \\n \\na      strong      back-  \\n \\n  translation      (BT)      baseline      by      using      monolingual      English  \\n \\n  pre-training.\\n \\n \\n  Abstractive      QA      We      use      the      recently      proposed      ELIS  \\n \\n  dataset      to      test      the      model’s      ability      to      generate      long      free-  \\n \\n  form      answers.\\nWe      find      BART      outperforms      the      best      pre-  \\n \\n  vious      work      by      1.2      ROUGE-L,      but      the      dataset      remains  \\n \\n  a      challenging,      because      answers      are      only      weakly      speci-  \\n \\n  fied      by      the      question.\\n5.4      Translation\\ntion  \\n \\n  We      also      evaluated      performance      on      WMT16      Romanian-  \\n \\n  English,      augmented      with      back-translation      data  \\n \\n  from      Sennrich      et      al.\\n(2016).\\nWe      use   \\n \\na      6-layer  \\n \\n  transformer      source      encoder      to      map      Romanian      into  \\n \\n  a      representation      that      BART      is      able      to      de-noise      into  \\n \\n  English,      following      the      approach      introduced      in      83.4.  \\n \\n  Experiment      results      are      presented      in      Table      6.      We  \\n \\n  compare      our      results      against   \\n \\na      baseline      Transformer  \\n \\n  architecture      (Vaswani      et      al.,      2017)      with      Transformer-  \\n \\n  large      settings      (the      baseline      row).\\nWe      show      the  \\n \\n  performance      of      both      steps      of      our      model      in      the      fixed  \\n \\n  BART      and      tuned      BART      rows.\\nFor      each      row      we  \\n \\n  experiment      on      the      original      WMT16      Romanian-English  \\n \\n  augmented      with      back-translation      data.\\nWe      use   \\n \\na       beam      width      of   \\n \\n5      and   \\n \\na      length      penalty      of   \\n \\na   \\n \\n=      1.  \\n \\n  Preliminary      results      suggested      that      our      approach      was  \\n \\n  less      effective      without      back-translation      data,      and      prone  \\n \\n  to      overfitting—future      work      should      explore      additional  \\n \\n  regularization      techniques.\\n6\\n \\n   Qualitative      Analysis  \\n \\n  BART      shows      large      improvements      on      summarization  \\n \\n  metrics,      of      up      to   \\n \\n6      points      over      the      prior      state-of-the-art.\\n \\n \\n  To      understand      BART’s      performance      beyond      automated  \\n \\n  metrics,      we      analyse      its      generations      qualitatively.\\n \\n \\n  Table   \\n \\n7      shows      example      summaries      generated      by  \\n \\n  BART.\\nExamples      are      taken      from      WikiNews      articles  \\n \\n  published      after      the      creation      of      the      pre-training      corpus,  \\n \\n  to      eliminate      the      possibility      of      the      events      described      be-  \\n \\n  ing      present      in      the      model’s      training      data.\\nFollowing  \\n \\n  Narayan      et      al.\\n(2018),      we      remove      the      first      sentence      of  \\n \\n  the      article      prior      to      summarizing      it,      so      there      is      no      easy  \\n \\n  extractive      summary      of      the      document.\\n \\n \\n  Unsurprisingly,      model      output      is      fluent      and      grammat-  \\n \\n  ical      English.\\nHowever,      model      output      is      also      highly      ab-  \\n \\n  stractive,      with      few      phrases      copied      from      the      input.\\nThe  \\n \\n  output      is      also      generally      factually      accurate,      and      inte-  \\n \\n  grates      supporting      evidence      from      across      the      input      doc-  \\n \\n  ument      with      background      knowledge      (for      example,      cor-  \\n \\n  rectly      completing      names,      or      inferring      that      PG&E      oper-  \\n \\n  ates      in      California).\\nIn      the      first      example,      inferring      that  \\n \\n  fish      are      protecting      reefs      from      global      warming      requires  \\n \\n  non-trivial      inference      from      the      text.\\nHowever,      the      claim  \\n \\n  that      the      work      was      published      in      Science      is      not      supported  \\n \\n  by      the      source.\\n \\n \\n  These      samples      demonstrate      that      the      BART      pretrain-  \\n \\n  ing      has      learned   \\n \\na      strong      combination      of      natural      lan-  \\n \\n  guage      understanding      and      generation.\\n7\\n \\n   Related      Work  \\n \\n  Early      methods      for      pretraining      were      based      on      language  \\n \\n  models.\\nGPT      (Radford      et      al.,      2018)      only      models      left-  \\n \\n  ward      context,      which      is      problematic      for      some      tasks.\\n \\n \\n  ELMo      (Peters      et      al.,      2018)      concatenates      left-only      and  \\n \\n  right-only      representations,      but      does      not      pre-train      inter-  \\n \\n  actions      between      these      features.\\nRadford      et      al.\\n(2019)  \\n \\n  demonstrated      that      very      large      language      models      can      act       as      unsupervised      multitask      models.\\n \\n \\n  MASS      (Song      et      al.,      2019)      is      perhaps      the      most      similar  \\n \\n  model      to      BART.\\nAn      input      sequence      where   \\n \\na      contiguous  \\n \\n  span      of      tokens      is      masked      is      mapped      to   \\n \\na      sequence      con-  \\n \\n  sisting      of      the      missing      tokens.\\nMASS      is      less      effective  \\n \\n  for      discriminative      tasks,      because      disjoint      sets      of      tokens  \\n \\n  are      fed      into      the      encoder      and      decoder.\\n \\n \\n  BERT      (Devlin      et      al.,      2019)      introduced      masked      lan-  \\n \\n  guage      modelling,      which      allows      pre-training      to      learn      in-  \\n \\n  teractions      between      left      and      right      context      words.\\nRe-  \\n \\n  cent      work      has      shown      that      very      strong      performance      can  \\n \\n  be      achieved      by      training      for      longer      (Liu      et      al.,      2019),  \\n \\n  by      tying      parameters      across      layers      (Lan      et      al.,      2019),  \\n \\n  and      by      masking      spans      instead      of      words      (Joshi      et      al.,  \\n \\n  2019).\\nPredictions      are      not      made      auto-regressively,      re-  \\n \\n  ducing      the      effectiveness      of      BERT      for      generation      tasks.\\n \\n \\n  UniLM      (Dong      et      al.,      2019)      fine-tunes      BERT      with      an  \\n \\n  ensemble      of      masks,      some      of      which      allow      only      leftward  \\n \\n  context.\\nLike      BART,      this      allows      UniLM      to      be      used      for  \\n \\n  both      generative      and      discriminative      tasks.\\n \\n \\nA      difference  \\n \\n  is      that      UniLM      predictions      are      conditionally      indepen-  \\n \\n  dent,      whereas      BART’s      are      autoregressive.\\nBART      re-  \\n \\n  duces      the      mismatch      between      pre-training      and      genera-  \\n \\n  tion      tasks,      because      the      decoder      is      always      trained      on      un-  \\n \\n  corrupted      context.\\n \\n \\n  XL-Net      (Yang      et      al.,      2019)      extends      BERT      by      pre-\\n |       Source      Document      (abbreviated) |       BART      Summary\\n |       The      researchers      examined      three      types      of      coral      in      reefs      off      the coast of Fiji The researchers found when fish were plentiful,       they      would      eat      algae      and      seaweed      off      the      corals,      which      appeared       to      leave      them      more      resistant      to      the      bacterium      Vibrio      coralliilyti-       cus,      a      bacterium      associated      with      bleaching.      The      researchers      sug-       gested      the      algae,      like      warming      temperatures,      might      render      the       corals’      chemical      defenses      less      effective,      and      the      fish      were      pro-       tecting      the      coral      by      removing      the      algae. |       Fisheries      off      the      coast      of      Fiji      are      protect-       ing      coral      reefs      from      the      effects      of      global       warming,      according      to      a      study      in      the      jour-       nal      Science.\\n |       Sacoolas,      who      has      immunity      as      a      diplomat’s      wife,      was      involved in a traffic collision Prime Minister Johnson was questioned       about      the      case      while      speaking      to      the      press      at      a      hospital      in      Wat- ford. He said, “I hope that Anne Sacoolas will come back       if      we      can’t      resolve      it      then      of      course      I      will      be      raising      it      myself       personally      with      the      White      House.” |       Boris      Johnson      has      said      he      will      raise      the      is-       sue      of      US      diplomat      Anne      Sacoolas’      diplo-       matic      immunity      with      the      White      House.\\n |       According      to      Syrian      state      media,      government      forces      began      de- ploying into previously SDF controlled territory yesterday.       On      October      6,      US      President      Donald      Trump      and      Turkish      Presi-       dent      Recep      Tayyip      Erdoan      spoke      on      the      phone.      Then      both      na-       tions      issued      statements      speaking      of      an      imminent      incursion      into northeast Syria On Wednesday, Turkey began a military       offensive      with      airstrikes      followed      by      a      ground      invasion. |       Syrian      government      forces      have      entered       territory      held      by      the      US-backed      Syrian       Democratic      Forces      (SDF)      in      response      to       Turkey’s      incursion      into      the      region.\\n |       This      is      the      first      time      anyone      has      been      recorded      to      run      a      full       marathon      of      42.195      kilometers      (approximately      26      miles)      under       this      pursued      landmark      time.      It      was      not,      however,      an      officially       sanctioned      world      record,      as      it      was      not      an      ’open      race”      of      the       IAAF.      His      time      was      1      hour      59      minutes      40.2      seconds.      Kipchoge       ran      in      Vienna,      Austria.      It      was      an      event      specifically      designed      to       help      Kipchoge      break      the      two      hour      barrier. |       Kenyan      runner      Eliud      Kipchoge      has      run      a       marathon      in      less      than      two      hours.\\n |       PG&E      stated      it      scheduled      the      blackouts      in      response      to      forecasts       for      high      winds      amid      dry      conditions.      The      aim      is      to      reduce      the      risk       of      wildfires.      Nearly      800      thousand      customers      were      scheduled      to       be      affected      by      the      shutoffs      which      were      expected      to      last      through       at      least      midday      tomorrow. |       Power      has      been      turned      off      to      millions      of       customers      in      California      as      part      of      a      power       shutoff      plan.\\n\\n \\n \\n  dicting      masked      tokens      auto-regressively      in   \\n \\na      permuted  \\n \\n  order.\\nThis      objective      allows      predictions      to      condition      on  \\n \\n  both      left      and      right      context.\\nIn      contrast,      the      BART      de-  \\n \\n  coder      works      left-to-right      during      pre-training,      matching  \\n \\n  the      setting      during      generation.\\n \\n \\n  Several      papers      have      explored      using      pre-trained      rep-  \\n \\n  resentations      to      improve      machine      translation.\\nThe  \\n \\n  largest      improvements      have      come      from      pre-training      on  \\n \\n  both      source      and      target      languages      (Song      et      al.,      2019;  \\n \\n  Lample   \\n \\n&      Conneau,      2019),      but      this      requires      pre-  \\n \\n  training      on      all      languages      of      interest.\\nOther      work      has  \\n \\n  shown      that      encoders      can      be      improved      using      pre-trained  \\n \\n  representations      (Edunov      et      al.,      2019),      but      gains      in      de-  \\n \\n  coders      are      more      limited.\\nWe      show      how      BART      can      be  \\n \\n  used      to      improve      machine      translation      decoders.\\n8\\n \\n   Conclusions  \\n \\n  We      introduced      BART,   \\n \\na      pre-training      approach      that  \\n \\n  learns      to      map      corrupted      documents      to      the      original.\\n \\n \\n  BART      achieves      similar      performance      to      ROBERTa      on  \\n \\n  discriminative      tasks,      while      achieving      new      state-of-the-  \\n \\n  art      results      on   \\n \\na      number      of      text      generation      tasks.\\nFu-  \\n \\n  ture      work      should      explore      new      methods      for      corrupting  \\n \\n  documents      for      pre-training,      perhaps      tailoring      them      to  \\n \\n  specific      end      tasks.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/1910.13461.pdf', 'section_number': 18, 'section_title': '5.4      Translation'}, page_content='5.4      Translation\\ntion  \\n \\n  We      also      evaluated      performance      on      WMT16      Romanian-  \\n \\n  English,      augmented      with      back-translation      data  \\n \\n  from      Sennrich      et      al.\\n(2016).\\nWe      use   \\n \\na      6-layer  \\n \\n  transformer      source      encoder      to      map      Romanian      into  \\n \\n  a      representation      that      BART      is      able      to      de-noise      into  \\n \\n  English,      following      the      approach      introduced      in      83.4.  \\n \\n  Experiment      results      are      presented      in      Table      6.      We  \\n \\n  compare      our      results      against   \\n \\na      baseline      Transformer  \\n \\n  architecture      (Vaswani      et      al.,      2017)      with      Transformer-  \\n \\n  large      settings      (the      baseline      row).\\nWe      show      the  \\n \\n  performance      of      both      steps      of      our      model      in      the      fixed  \\n \\n  BART      and      tuned      BART      rows.\\nFor      each      row      we  \\n \\n  experiment      on      the      original      WMT16      Romanian-English  \\n \\n  augmented      with      back-translation      data.\\nWe      use   \\n \\na       beam      width      of   \\n \\n5      and   \\n \\na      length      penalty      of   \\n \\na   \\n \\n=      1.  \\n \\n  Preliminary      results      suggested      that      our      approach      was  \\n \\n  less      effective      without      back-translation      data,      and      prone  \\n \\n  to      overfitting—future      work      should      explore      additional  \\n \\n  regularization      techniques.\\n6\\n \\n   Qualitative      Analysis  \\n \\n  BART      shows      large      improvements      on      summarization  \\n \\n  metrics,      of      up      to   \\n \\n6      points      over      the      prior      state-of-the-art.\\n \\n \\n  To      understand      BART’s      performance      beyond      automated  \\n \\n  metrics,      we      analyse      its      generations      qualitatively.\\n \\n \\n  Table   \\n \\n7      shows      example      summaries      generated      by  \\n \\n  BART.\\nExamples      are      taken      from      WikiNews      articles  \\n \\n  published      after      the      creation      of      the      pre-training      corpus,  \\n \\n  to      eliminate      the      possibility      of      the      events      described      be-  \\n \\n  ing      present      in      the      model’s      training      data.\\nFollowing  \\n \\n  Narayan      et      al.\\n(2018),      we      remove      the      first      sentence      of  \\n \\n  the      article      prior      to      summarizing      it,      so      there      is      no      easy  \\n \\n  extractive      summary      of      the      document.\\n \\n \\n  Unsurprisingly,      model      output      is      fluent      and      grammat-  \\n \\n  ical      English.\\nHowever,      model      output      is      also      highly      ab-  \\n \\n  stractive,      with      few      phrases      copied      from      the      input.\\nThe  \\n \\n  output      is      also      generally      factually      accurate,      and      inte-  \\n \\n  grates      supporting      evidence      from      across      the      input      doc-  \\n \\n  ument      with      background      knowledge      (for      example,      cor-  \\n \\n  rectly      completing      names,      or      inferring      that      PG&E      oper-  \\n \\n  ates      in      California).\\nIn      the      first      example,      inferring      that  \\n \\n  fish      are      protecting      reefs      from      global      warming      requires  \\n \\n  non-trivial      inference      from      the      text.\\nHowever,      the      claim  \\n \\n  that      the      work      was      published      in      Science      is      not      supported  \\n \\n  by      the      source.\\n \\n \\n  These      samples      demonstrate      that      the      BART      pretrain-  \\n \\n  ing      has      learned   \\n \\na      strong      combination      of      natural      lan-  \\n \\n  guage      understanding      and      generation.\\n7\\n \\n   Related      Work  \\n \\n  Early      methods      for      pretraining      were      based      on      language  \\n \\n  models.\\nGPT      (Radford      et      al.,      2018)      only      models      left-  \\n \\n  ward      context,      which      is      problematic      for      some      tasks.\\n \\n \\n  ELMo      (Peters      et      al.,      2018)      concatenates      left-only      and  \\n \\n  right-only      representations,      but      does      not      pre-train      inter-  \\n \\n  actions      between      these      features.\\nRadford      et      al.\\n(2019)  \\n \\n  demonstrated      that      very      large      language      models      can      act       as      unsupervised      multitask      models.\\n \\n \\n  MASS      (Song      et      al.,      2019)      is      perhaps      the      most      similar  \\n \\n  model      to      BART.\\nAn      input      sequence      where   \\n \\na      contiguous  \\n \\n  span      of      tokens      is      masked      is      mapped      to   \\n \\na      sequence      con-  \\n \\n  sisting      of      the      missing      tokens.\\nMASS      is      less      effective  \\n \\n  for      discriminative      tasks,      because      disjoint      sets      of      tokens  \\n \\n  are      fed      into      the      encoder      and      decoder.\\n \\n \\n  BERT      (Devlin      et      al.,      2019)      introduced      masked      lan-  \\n \\n  guage      modelling,      which      allows      pre-training      to      learn      in-  \\n \\n  teractions      between      left      and      right      context      words.\\nRe-  \\n \\n  cent      work      has      shown      that      very      strong      performance      can  \\n \\n  be      achieved      by      training      for      longer      (Liu      et      al.,      2019),  \\n \\n  by      tying      parameters      across      layers      (Lan      et      al.,      2019),  \\n \\n  and      by      masking      spans      instead      of      words      (Joshi      et      al.,  \\n \\n  2019).\\nPredictions      are      not      made      auto-regressively,      re-  \\n \\n  ducing      the      effectiveness      of      BERT      for      generation      tasks.\\n \\n \\n  UniLM      (Dong      et      al.,      2019)      fine-tunes      BERT      with      an  \\n \\n  ensemble      of      masks,      some      of      which      allow      only      leftward  \\n \\n  context.\\nLike      BART,      this      allows      UniLM      to      be      used      for  \\n \\n  both      generative      and      discriminative      tasks.\\n \\n \\nA      difference  \\n \\n  is      that      UniLM      predictions      are      conditionally      indepen-  \\n \\n  dent,      whereas      BART’s      are      autoregressive.\\nBART      re-  \\n \\n  duces      the      mismatch      between      pre-training      and      genera-  \\n \\n  tion      tasks,      because      the      decoder      is      always      trained      on      un-  \\n \\n  corrupted      context.\\n \\n \\n  XL-Net      (Yang      et      al.,      2019)      extends      BERT      by      pre-\\n |       Source      Document      (abbreviated) |       BART      Summary\\n |       The      researchers      examined      three      types      of      coral      in      reefs      off      the coast of Fiji The researchers found when fish were plentiful,       they      would      eat      algae      and      seaweed      off      the      corals,      which      appeared       to      leave      them      more      resistant      to      the      bacterium      Vibrio      coralliilyti-       cus,      a      bacterium      associated      with      bleaching.      The      researchers      sug-       gested      the      algae,      like      warming      temperatures,      might      render      the       corals’      chemical      defenses      less      effective,      and      the      fish      were      pro-       tecting      the      coral      by      removing      the      algae. |       Fisheries      off      the      coast      of      Fiji      are      protect-       ing      coral      reefs      from      the      effects      of      global       warming,      according      to      a      study      in      the      jour-       nal      Science.\\n |       Sacoolas,      who      has      immunity      as      a      diplomat’s      wife,      was      involved in a traffic collision Prime Minister Johnson was questioned       about      the      case      while      speaking      to      the      press      at      a      hospital      in      Wat- ford. He said, “I hope that Anne Sacoolas will come back       if      we      can’t      resolve      it      then      of      course      I      will      be      raising      it      myself       personally      with      the      White      House.” |       Boris      Johnson      has      said      he      will      raise      the      is-       sue      of      US      diplomat      Anne      Sacoolas’      diplo-       matic      immunity      with      the      White      House.\\n |       According      to      Syrian      state      media,      government      forces      began      de- ploying into previously SDF controlled territory yesterday.       On      October      6,      US      President      Donald      Trump      and      Turkish      Presi-       dent      Recep      Tayyip      Erdoan      spoke      on      the      phone.      Then      both      na-       tions      issued      statements      speaking      of      an      imminent      incursion      into northeast Syria On Wednesday, Turkey began a military       offensive      with      airstrikes      followed      by      a      ground      invasion. |       Syrian      government      forces      have      entered       territory      held      by      the      US-backed      Syrian       Democratic      Forces      (SDF)      in      response      to       Turkey’s      incursion      into      the      region.\\n |       This      is      the      first      time      anyone      has      been      recorded      to      run      a      full       marathon      of      42.195      kilometers      (approximately      26      miles)      under       this      pursued      landmark      time.      It      was      not,      however,      an      officially       sanctioned      world      record,      as      it      was      not      an      ’open      race”      of      the       IAAF.      His      time      was      1      hour      59      minutes      40.2      seconds.      Kipchoge       ran      in      Vienna,      Austria.      It      was      an      event      specifically      designed      to       help      Kipchoge      break      the      two      hour      barrier. |       Kenyan      runner      Eliud      Kipchoge      has      run      a       marathon      in      less      than      two      hours.\\n |       PG&E      stated      it      scheduled      the      blackouts      in      response      to      forecasts       for      high      winds      amid      dry      conditions.      The      aim      is      to      reduce      the      risk       of      wildfires.      Nearly      800      thousand      customers      were      scheduled      to       be      affected      by      the      shutoffs      which      were      expected      to      last      through       at      least      midday      tomorrow. |       Power      has      been      turned      off      to      millions      of       customers      in      California      as      part      of      a      power       shutoff      plan.\\n\\n \\n \\n  dicting      masked      tokens      auto-regressively      in   \\n \\na      permuted  \\n \\n  order.\\nThis      objective      allows      predictions      to      condition      on  \\n \\n  both      left      and      right      context.\\nIn      contrast,      the      BART      de-  \\n \\n  coder      works      left-to-right      during      pre-training,      matching  \\n \\n  the      setting      during      generation.\\n \\n \\n  Several      papers      have      explored      using      pre-trained      rep-  \\n \\n  resentations      to      improve      machine      translation.\\nThe  \\n \\n  largest      improvements      have      come      from      pre-training      on  \\n \\n  both      source      and      target      languages      (Song      et      al.,      2019;  \\n \\n  Lample   \\n \\n&      Conneau,      2019),      but      this      requires      pre-  \\n \\n  training      on      all      languages      of      interest.\\nOther      work      has  \\n \\n  shown      that      encoders      can      be      improved      using      pre-trained  \\n \\n  representations      (Edunov      et      al.,      2019),      but      gains      in      de-  \\n \\n  coders      are      more      limited.\\nWe      show      how      BART      can      be  \\n \\n  used      to      improve      machine      translation      decoders.\\n8\\n \\n   Conclusions  \\n \\n  We      introduced      BART,   \\n \\na      pre-training      approach      that  \\n \\n  learns      to      map      corrupted      documents      to      the      original.\\n \\n \\n  BART      achieves      similar      performance      to      ROBERTa      on  \\n \\n  discriminative      tasks,      while      achieving      new      state-of-the-  \\n \\n  art      results      on   \\n \\na      number      of      text      generation      tasks.\\nFu-  \\n \\n  ture      work      should      explore      new      methods      for      corrupting  \\n \\n  documents      for      pre-training,      perhaps      tailoring      them      to  \\n \\n  specific      end      tasks.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/1910.13461.pdf', 'section_number': 19, 'section_title': 'References'}, page_content='References\\n \\n \\n  Eneko      Agirre,      Llu’is      M‘arquez,      and      Richard      Wicen-  \\n \\n  towski      (eds.).\\nProceedings      of      the      Fourth      Interna-  \\n \\n  tional      Workshop      on      Semantic      Evaluations      (SemEval-  \\n \\n  2007).\\nAssociation      for      Computational      Linguistics,  \\n \\n  Prague,      Czech      Republic,      June      2007.\\n \\n \\n  Ido      Dagan,      Oren      Glickman,      and      Bernardo      Magnini.\\n \\n \\n  The      PASCAL      recognising      textual      entailment      chal-  \\n \\n  lenge.\\nIn      Machine      learning      challenges.\\nevaluat-  \\n \\n  ing      predictive      uncertainty,      visual      object      classifica-  \\n \\n  tion,      and      recognising      tectual      entailment,      pp.\\n177—  \\n \\n  190.\\nSpringer,      2006.\\n \\n \\n  Jacob      Devlin,      Ming-Wei      Chang,      Kenton      Lee,      and  \\n \\n  Kristina      Toutanova.\\nBERT:      Pre-training      of      deep  \\n \\n  bidirectional      transformers      for      language      understand-  \\n \\n  ing.\\nIn      Proceedings      of      the      2019      Conference      of      the  \\n \\n  North      American      Chapter      of      the      Association      for      Com-  \\n \\n  putational      Linguistics:      Human      Language      Technolo-  \\n \\n  gies,      Volume   \\n \\nI      (Long      and      Short      Papers),      pp.\\n4171-  \\n \\n  4186,      Minneapolis,      Minnesota,      June      2019.\\nAssocia-  \\n \\n  tion      for      Computational      Linguistics.\\ndoi:      10.18653/  \\n \\n  vI/N19-1423.\\nURL      https://www.aclweb.\\n \\n \\n  org/anthology/N19-1423.\\n \\n \\n  Emily      Dinan,      Varvara      Logacheva,      Valentin      Malykh,  \\n \\n  Alexander      Miller,      Kurt      Shuster,      Jack      Urbanek,  \\n \\n  Douwe      Kiela,      Arthur      Szlam,      Iulian      Serban,      Ryan  \\n \\n  Lowe,      et      al.\\nThe      second      conversational      in-  \\n \\n  telligence      challenge      (convai2).\\narXiv      preprint  \\n \\n  arXiv:      1902.00098,      2019.\\n \\n \\n  William   \\n \\nB      Dolan      and      Chris      Brockett.\\nAutomatically  \\n \\n  constructing   \\n \\na      corpus      of      sentential      paraphrases.\\nIn  \\n \\n  Proceedings      of      the      International      Workshop      on      Para-  \\n \\n  phrasing,      2005.\\n \\n \\n  Li      Dong,      Nan      Yang,      Wenhui      Wang,      Furu      Wei,      Xi-  \\n \\n  aodong      Liu,      Yu      Wang,      Jianfeng      Gao,      Ming      Zhou,  \\n \\n  and      Hsiao-Wuen      Hon.\\nUnified      language      model      pre-  \\n \\n  training      for      natural      language      understanding      and      gen-  \\n \\n  eration.\\narXiv      preprint      arXiv:      1905.03197,      2019.\\n \\n \\n  Sergey      Edunov,      Alexei      Baevski,      and      Michael      Auli.\\n \\n \\n  Pre-trained      language      model      representations      for      lan-  \\n \\n  guage      generation.\\nIn      Proceedings      of      the      2019      Con-  \\n \\n  ference      of      the      North      American      Chapter      of      the      Asso-  \\n \\n  ciation      for      Computational      Linguistics:      Human      Lan-  \\n \\n  guage      Technologies,      Volume   \\n \\n1      (Long      and      Short      Pa-  \\n \\n  pers),      2019.\\n \\n \\n  Angela      Fan,      David      Grangier,      and      Michael      Auli.\\nCon-  \\n \\n  trollable      abstractive      summarization.\\narXiv      preprint  \\n \\n  arXiv:      1711.05217,      2017.\\n \\n \\n  Angela      Fan,      Yacine      Jernite,      Ethan      Perez,      David  \\n \\n  Grangier,      Jason      Weston,      and      Michael      Auli.\\nEli5:  \\n \\n  Long      form      question      answering.\\narXiv      preprint  \\n \\n  arXiv:      1907.09190,      2019.\\n \\n \\n  Dan      Hendrycks      and      Kevin      Gimpel.\\nGaussian      error      lin-  \\n \\n  ear      units      (gelus).\\narXiv      preprint      arXiv:      1606.08415,  \\n \\n  2016.\\n \\n \\n  Karl      Moritz      Hermann,      Tomas      Kocisky,      Edward  \\n \\n  Grefenstette,      Lasse      Espeholt,      Will      Kay,      Mustafa      Su-  \\n \\n  leyman,      and      Phil      Blunsom.\\nTeaching      machines      to  \\n \\n  read      and      comprehend.\\nIn      Advances      in      neural      infor-  \\n \\n  mation      processing      systems,      pp.\\n1693-1701,      2015.\\n \\n \\n  Mandar      Joshi,      Danqi      Chen,      Yinhan      Liu,      Daniel   \\n \\nS      Weld,  \\n \\n  Luke      Zettlemoyer,      and      Omer      Levy.\\nSpanbert:      Im-  \\n \\n  proving      pre-training      by      representing      and      predicting  \\n \\n  spans.\\narXiv      preprint      arXiv:      1907.10529,      2019.\\n \\n \\n  Guillaume      Lample      and      Alexis      Conneau.\\n \\n \\n—      Cross-  \\n \\n  lingual      language      model      pretraining.\\narXiv      preprint  \\n \\n  arXiv:      1901.07291,      2019.\\n \\n \\n  Zhenzhong      Lan,      Mingda      Chen,      Sebastian      Goodman,  \\n \\n  Kevin      Gimpel,      Piyush      Sharma,      and      Radu      Sori-  \\n \\n  cut.\\nAlbert:   \\n \\nA      lite      bert      for      self-supervised      learn-  \\n \\n  ing      of      language      representations.\\narXiv      preprint  \\n \\n  arXiv:      1909.11942,      2019.\\n \\n \\n  Hector   \\n \\nJ      Levesque,      Ernest      Davis,      and      Leora      Morgen-  \\n \\n  stern.\\nThe      Winograd      schema      challenge.\\nIn      AAAI  \\n \\n  Spring      Symposium:      Logical      Formalizations      of      Com-  \\n \\n  monsense      Reasoning,      volume      46,      pp.\\n47,      2011.\\n \\n \\n  Yang      Liu      and      Mirella      Lapata.\\n \\n \\n  tion      with      pretrained      encoders.\\n \\n \\n  arXiv:      1908.08345,      2019.  \\n \\n  Text      summariza-  \\n \\n  arXiv      preprint  \\n \\n  Yinhan      Liu,      Myle      Ott,      Naman      Goyal,      Jingfei      Du,      Man-  \\n \\n  dar      Joshi,      Dangi      Chen,      Omer      Levy,      Mike      Lewis,  \\n \\n  Luke      Zettlemoyer,      and      Veselin      Stoyanov.\\nRoberta:\\n \\n \\n \\nA      robustly      optimized      bert      pretraining      approach.\\n \\n \\n  arXiv      preprint      arXiv:      1907.11692,      2019.\\n \\n \\n  Tomas      Mikolov,      Kai      Chen,      Greg      Corrado,      and      Jeffrey  \\n \\n  Dean.\\nEfficient      estimation      of      word      representations  \\n \\n  in      vector      space.\\narXiv      preprint      arXiv:1301.3781,  \\n \\n  2013.\\n \\n \\n  Shashi      Narayan,      Shay   \\n \\nB      Cohen,      and      Mirella      Lapata.\\n \\n \\n  Don’t      give      me      the      details,      just      the      summary!\\ntopic-  \\n \\n  aware      convolutional      neural      networks      for      extreme  \\n \\n  summarization.\\narXiv      preprint      arXiv:      1808.08745,  \\n \\n  2018.\\n \\n \\n  Gabriel      Pereyra,      George      Tucker,      Jan      Chorowski,  \\n \\n  Lukasz      Kaiser,      and      Geoffrey      Hinton.\\nRegularizing  \\n \\n  neural      networks      by      penalizing      confident      output      dis-  \\n \\n  tributions.\\narXiv      preprint      arXiv:      1701.06548,      2017.\\n \\n \\n  Matthew      E      Peters,      Mark      Neumann,      Mohit      Iyyer,      Matt  \\n \\n  Gardner,      Christopher      Clark,      Kenton      Lee,      and      Luke  \\n \\n  Zettlemoyer.\\nDeep      contextualized      word      representa-  \\n \\n  tions.\\narXiv      preprint      arXiv:      1802.05365,      2018.\\n \\n \\n  Alec      Radford,      Karthik      Narasimhan,      Tim      Salimans,  \\n \\n  and      Ilya      Sutskever.\\nImproving      language      un-  \\n \\n  derstanding      by      generative      pre-training.\\nURL  \\n \\n  https://s3-us-west-2.\\n \\n \\n|      amazonaws.\\n \\n \\n—      com/openai-  \\n \\n  assets/researchcovers/languageunsupervised/language  \\n \\n  understanding      paper.\\npdf,      2018.\\n \\n \\n  Alec      Radford,      Jeffrey      Wu,      Rewon      Child,      David      Luan,  \\n \\n  Dario      Amodei,      and      Ilya      Sutskever.\\nLanguage      mod-  \\n \\n  els      are      unsupervised      multitask      learners.\\nOpenAI  \\n \\n  Blog,      1(8),      2019.\\n \\n \\n  Pranav      Rajpurkar,      Jian      Zhang,      Konstantin      Lopyrev,  \\n \\n  and      Percy      Liang.\\nSquad:      100,000+      questions      for  \\n \\n  machine      comprehension      of      text.\\narXiv      preprint  \\n \\n  arXiv:      1606.05250,      2016.\\n \\n \\n  Abigail      See,      Peter   \\n \\nJ      Liu,      and      Christopher   \\n \\nD       Manning.\\nGet      to      the      point:      Summarization  \\n \\n  with      pointer-generator      networks.\\narXiv      preprint  \\n \\n  arXiv:      1704.04368,      2017.\\n \\n \\n  Rico      Sennrich,      Barry      Haddow,      and      Alexandra      Birch.\\n \\n \\n  Edinburgh      neural      machine      translation      systems      for  \\n \\n  WMT      16.\\nIn      Proceedings      of      the      First      Conference  \\n \\n  on      Machine      Translation:      Volume      2,      Shared      Task      Pa-  \\n \\n  pers,      2016.\\n \\n \\n  Richard      Socher,      Alex      Perelygin,      Jean      Wu,      Jason  \\n \\n  Chuang,      Christopher   \\n \\nD      Manning,      Andrew      Ng,      and  \\n \\n  Christopher      Potts.\\nRecursive      deep      models      for      se-  \\n \\n  mantic      compositionality      over   \\n \\na      sentiment      treebank.\\n \\n \\n  In      Proceedings      of      EMNLP,      pp.\\n1631-1642,      2013.\\n \\n \\n  Kaitao      Song,      Xu      Tan,      Tao      Qin,      Jianfeng      Lu,      and      Tie-  \\n \\n  Yan      Liu.\\nMass:      Masked      sequence      to      sequence      pre-  \\n \\n  training      for      language      generation.\\nIn      International  \\n \\n  Conference      on      Machine      Learning,      2019.\\n \\n \\n  Ashish      Vaswani,      Noam      Shazeer,      Niki      Parmar,      Jakob  \\n \\n  Uszkoreit,      Llion      Jones,      Aidan      N      Gomez,      Lukasz  \\n \\n  Kaiser,      and      Ilia      Polosukhin.\\nAttention      is      all      you  \\n \\n  need.\\nIn      Advances      in      neural      information      processing  \\n \\n  systems,      pp.\\n5998-6008,      2017.\\n \\n \\n  Alex      Wang,      Amanpreet      Singh,      Julian      Michael,      Felix  \\n \\n  Hill,      Omer      Levy,      and      Samuel      R      Bowman.\\nGlue:\\n \\n \\n \\nA      multi-task      benchmark      and      analysis      platform      for  \\n \\n  natural      language      understanding.\\narXiv      preprint  \\n \\n  arXiv:      1804.07461,      2018.\\n \\n \\n  Alex      Warstadt,      Amanpreet      Singh,      and      Samuel      R.  \\n \\n  Bowman.\\nNeural      network      acceptability      judgments.\\n \\n \\n  arXiv      preprint      1805.12471,      2018.\\n \\n \\n  Adina      Williams,      Nikita      Nangia,      and      Samuel      R      Bow-  \\n \\n  man.\\n \\n \\nA   \\n \\n_      broad-coverage      challenge      corpus      for  \\n \\n  sentence      understanding      through      inference.\\narXiv  \\n \\n  preprint      arXiv:      1704.05426,      2017.\\n \\n \\n  Adina      Williams,      Nikita      Nangia,      and      Samuel      R.      Bow-  \\n \\n  man.\\n \\n \\nA      broad-coverage      challenge      corpus      for      sen-  \\n \\n  tence      understanding      through      inference.\\nIn      Proceed-  \\n \\n  ings      of      NAACL-HLT,      2018.\\n \\n \\n  Zhilin      Yang,      Zihang      Dai,      Yiming      Yang,      Jaime  \\n \\n  Carbonell,      Ruslan      Salakhutdinov,      and      Quoc   \\n \\nV       Le.\\nXlInet:      Generalized      autoregressive      pretrain-  \\n \\n  ing      for      language      understanding.\\narXiv      preprint  \\n \\n  arXiv:      1906.08237,      2019.')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "section_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'metadata': {'source': 'https://arxiv.org/pdf/1910.13461.pdf', 'section_number': 0, 'section_title': '2.1      Architecture'}, 'page_content': '2.1      Architecture\\nture  \\n \\n  BART      uses      the      standard      sequence-to-sequence      Trans-  \\n \\n  former      architecture      from      (Vaswani      et      al.,      2017),      ex-  \\n \\n  cept,      following      GPT,      that      we      modify      ReLU      activa-  \\n \\n  tion      functions      to      GeLUs      (Hendrycks   \\n \\n&      Gimpel,      2016)  \\n \\n  and      initialise      parameters      from      A/(0,0.02).\\nFor      our  \\n \\n  base      model,      we      use   \\n \\n6      layers      in      the      encoder      and      de-  \\n \\n  coder,      and      for      our      large      model      we      use      12      layers      in  \\n \\n  each.\\nThe      architecture      is      closely      related      to      that      used      in  \\n \\n  BERT,      with      the      following      differences:      (1)      each      layer      of  \\n \\n  the      decoder      additionally      performs      cross-attention      over  \\n \\n  the      final      hidden      layer      of      the      encoder      (as      in      the      trans-  \\n \\n  former      sequence-to-sequence      model);      and      (2)      BERT  \\n \\n  uses      an      additional      feed-forward      network      before      word-  \\n \\n  prediction,      which      BART      does      not.\\nIn      total,      BART      con-  \\n \\n  tains      roughly      10%      more      parameters      than      the      equiva-  \\n \\n  lently      sized      BERT      model.'}, {'metadata': {'source': 'https://arxiv.org/pdf/1910.13461.pdf', 'section_number': 1, 'section_title': '2.2      Pre-training      BART'}, 'page_content': '2.2      Pre-training      BART\\nBART  \\n \\n  BART      is      trained      by      corrupting      documents      and      then      op-  \\n \\n  timizing   \\n \\na      reconstruction      loss—the      cross-entropy      be-  \\n \\n  tween      the      decoder’s      output      and      the      original      document.\\n \\n \\n  Unlike      existing      denoising      autoencoders,      which      are      tai-  \\n \\n  lored      to      specific      noising      schemes,      BART      allows      us      to  \\n \\n  apply      any      type      of      document      corruption.\\nIn      the      extreme  \\n \\n  case,      where      all      information      about      the      source      is      lost,  \\n \\n  BART      is      equivalent      to   \\n \\na      language      model.\\n \\n \\n  We      experiment      with      several      previously      proposed      and  \\n \\n  novel      transformations,      but      we      believe      there      is   \\n \\na      sig-  \\n \\n  nificant      potential      for      development      of      other      new      alter-  \\n \\n  natives.\\nThe      transformations      we      used      are      summarized  \\n \\n  below,      and      examples      are      shown      in      Figure      2.\\n \\n \\n  Token      Masking      Following      BERT      (Devlin      et      al.,  \\n \\n  2019),      random      tokens      are      sampled      and      replaced      with  \\n \\n  [MASK]      elements.\\n \\n \\n  Token      Deletion      Random      tokens      are      deleted      from      the  \\n \\n  input.\\nIn      contrast      to      token      masking,      the      model      must  \\n \\n  decide      which      positions      are      missing      inputs.\\nToken      Masking'}, {'metadata': {'source': 'https://arxiv.org/pdf/1910.13461.pdf', 'section_number': 2, 'section_title': 'Token      Masking'}, 'page_content': 'Token      Masking'}, {'metadata': {'source': 'https://arxiv.org/pdf/1910.13461.pdf', 'section_number': 3, 'section_title': 'C.DE.AB'}, 'page_content': 'C.DE.AB\\nToken      Deletion Text      Infilling\\n \\n \\n  Text      Infilling   \\n \\nA      number      of      text      spans      are      sampled,  \\n \\n  with      span      lengths      drawn      from   \\n \\na      Poisson      distribution  \\n \\n  (A   \\n \\n=      3).\\nEach      span      is      replaced      with   \\n \\na      single      [MASK]  \\n \\n  token.\\nO-length      spans      correspond      to      the      insertion      of  \\n \\n  [MASK]      tokens.\\nText      infilling      is      inspired      by      Span-  \\n \\n  BERT      (Joshi      et      al.,      2019),      but      SpanBERT      samples  \\n \\n  span      lengths      from   \\n \\na      different      (clamped      geometric)      dis-  \\n \\n  tribution,      and      replaces      each      span      with   \\n \\na      sequence      of  \\n \\n  [MASK]      tokens      of      exactly      the      same      length.\\nText      infill-  \\n \\n  ing      teaches      the      model      to      predict      how      many      tokens      are  \\n \\n  missing      from   \\n \\na      span.\\n \\n \\n  Sentence      Permutation   \\n \\nA      document      is      divided      into  \\n \\n  sentences      based      on      full      stops,      and      these      sentences      are  \\n \\n  shuffled      in   \\n \\na      random      order.\\n \\n \\n  Document      Rotation   \\n \\nA      token      is      chosen      uniformly      at  \\n \\n  random,      and      the      document      is      rotated      so      that      it      begins  \\n \\n  with      that      token.\\nThis      task      trains      the      model      to      identify  \\n \\n  the      start      of      the      document.\\n3\\n \\n   Fine-tuning      BART The      representations      produced      by      BART      can      be      used      in  \\n \\n  several      ways      for      downstream      applications.\\n3.1      Sequence      Classification      Tasks\\nasks  \\n \\n  For      sequence      classification      tasks,      the      same      input      is      fed  \\n \\n  into      the      encoder      and      decoder,      and      the      final      hidden      state  \\n \\n  of      the      final      decoder      token      is      fed      into      new      multi-class  \\n \\n  linear      classifier.\\nThis      approach      is      related      to      the      CLS  \\n \\n  token      in      BERT;      however      we      add      the      additional      token  \\n \\n  to      the      end      so      that      representation      for      the      token      in      the  \\n \\n  decoder      can      attend      to      decoder      states      from      the      complete  \\n \\n  input      (Figure      3a).\\n3.2.      Token      Classification      Tasks\\nasks  \\n \\n  For      token      classification      tasks,      such      as      answer      endpoint  \\n \\n  classification      for      SQUAD,      we      feed      the      complete      doc-  \\n \\n  ument      into      the      encoder      and      decoder,      and      use      the      top  \\n \\n  hidden      state      of      the      decoder      as   \\n \\na      representation      for      each  \\n \\n  word.\\nThis      representation      is      used      to      classify      the      token.\\n3.3      Sequence      Generation      Tasks\\nasks  \\n \\n  Because      BART      has      an      autoregressive      decoder,      it      can      be  \\n \\n  directly      fine      tuned      for      sequence      generation      tasks      such  \\n \\n  as      abstractive      question      answering      and      summarization.\\n \\n \\n  In      both      of      these      tasks,      information      is      copied      from      the  \\n \\n  input      but      manipulated,      which      is      closely      related      to      the  \\n \\n  denoising      pre-training      objective.\\nHere,      the      encoder      in-  \\n \\n  put      is      the      input      sequence,      and      the      decoder      generates  \\n \\n  outputs      autoregressively.\\n3.4      Machine      Translation\\ntion  \\n \\n  We      also      explore      using      BART      to      improve      machine      trans-  \\n \\n  lation      decoders      for      translating      into      English.\\nPrevious  \\n \\n  work      Edunov      et      al.\\n(2019)      has      shown      that      models      can  \\n \\n  be      improved      by      incorporating      pre-trained      encoders,      but  \\n \\n  gains      from      using      pre-trained      language      models      in      de-  \\n \\n  coders      have      been      limited.\\nWe      show      that      it      is      possible  \\n \\n  to      use      the      entire      BART      model      (both      encoder      and      de-  \\n \\n  coder)      as   \\n \\na      single      pretrained      decoder      for      machine      trans-  \\n \\n  lation,      by      adding   \\n \\na      new      set      of      encoder      parameters      that  \\n \\n  are      learned      from      bitext      (see      Figure      3b).\\n \\n \\n  More      precisely,      we      replace      BART’s      encoder      embed-  \\n \\n  ding      layer      with   \\n \\na      new      randomly      initialized      encoder.\\n \\n \\n  The      model      is      trained      end-to-end,      which      trains      the      new       encoder      to      map      foreign      words      into      an      input      that      BART  \\n \\n  can      de-noise      to      English.\\nThe      new      encoder      can      use   \\n \\na       separate      vocabulary      from      the      original      BART      model.\\n \\n \\n  We      train      the      source      encoder      in      two      steps,      in      both  \\n \\n  cases      backpropagating      the      cross-entropy      loss      from      the  \\n \\n  output      of      the      BART      model.\\nIn      the      first      step,      we      freeze  \\n \\n  most      of      BART      parameters      and      only      update      the      ran-  \\n \\n  domly      initialized      source      encoder,      the      BART      positional  \\n \\n  embeddings,      and      the      self-attention      input      projection      ma-  \\n \\n  trix      of      BART’s      encoder      first      layer.\\nIn      the      second      step,  \\n \\n  we      train      all      model      parameters      for   \\n \\na      small      number      of  \\n \\n  iterations.\\n4\\n \\n   Comparing      Pre-training      Objectives  \\n \\n  BART      supports   \\n \\na      much      wider      range      of      noising      schemes  \\n \\n  during      pre-training      than      previous      work.\\nWe      compare   \\n \\na       range      of      options      using      base-size      models      (6      encoder      and  \\n \\n  6      decoder      layers,      with   \\n \\na      hidden      size      of      768),      evaluated  \\n \\n  on   \\n \\na      representative      subset      of      the      tasks      we      will      consider  \\n \\n  for      the      full      large      scale      experiments      in      85.\\n4.1      Comparison      Objectives\\nives  \\n \\n  While      many      pre-training      objectives      have      been      pro-  \\n \\n  posed,      fair      comparisons      between      these      have      been      dif-  \\n \\n  ficult      to      perform,      at      least      in      part      due      to      differences      in  \\n \\n  training      data,      training      resources,      architectural      differ-  \\n \\n  ences      between      models,      and      fine-tuning      procedures.\\nWe  \\n \\n  ABCDE  \\n \\n  weee  \\n \\n  Pre-trained      Pre-trained  \\n \\n  Craters      Joo      rezapes cise)      Te      TrET Pre-trained  \\n \\n  Encoder Pre-trained  \\n \\n  Decoder aa      AB  \\n \\n  trrrt  \\n \\n  tr      ttt  \\n \\n  <s>A      BCD  \\n \\n  Randomly  \\n \\n  Initialized      Encoder  \\n \\n  TFT  \\n \\n  aBpByoe sentation      from      the      final      output      is      used.\\n \\n \\n  re-implement      strong      pre-training      approaches      recently  \\n \\n  proposed      for      discriminative      and      generation      tasks.\\nWe  \\n \\n  aim,      as      much      as      possible,      to      control      for      differences      un-  \\n \\n  related      to      the      pre-training      objective.\\nHowever,      we      do  \\n \\n  make      minor      changes      to      the      learning      rate      and      usage      of  \\n \\n  layer      normalisation      in      order      to      improve      performance  \\n \\n  (tuning      these      separately      for      each      objective).\\nFor      refer-  \\n \\n  ence,      we      compare      our      implementations      with      published  \\n \\n  numbers      from      BERT,      which      was      also      trained      for      1M  \\n \\n  steps      on   \\n \\na      combination      of      books      and      Wikipedia      data.\\n \\n \\n  We      compare      the      following      approaches:\\n \\n \\n  Language      Model      Similarly      to      GPT      (Radford      et      al.,  \\n \\n  2018),      we      train   \\n \\na      left-to-right      Transformer      language  \\n \\n  model.\\nThis      model      is      equivalent      to      the      BART      decoder,  \\n \\n  without      cross-attention.\\n \\n \\n  Permuted      Language      Model      Based      on      XLNet      (Yang  \\n \\n  et      al.,      2019),      we      sample      1/6      of      the      tokens,      and      gener-  \\n \\n  ate      them      in   \\n \\na      random      order      autoregressively.\\nFor      con-  \\n \\n  sistency      with      other      models,      we      do      not      implement      the  \\n \\n  relative      positional      embeddings      or      attention      across      seg-  \\n \\n  ments      from      XLNet.\\n \\n \\n  Masked      Language      Model      Following      BERT      (Devlin  \\n \\n  et      al.,      2019),      we      replace      15%      of      tokens      with      [MASK]  \\n \\n  symbols,      and      train      the      model      to      independently      predict  \\n \\n  the      original      tokens.\\n \\n \\n  Multitask      Masked      Language      Model      As      in      UniLM  \\n \\n  (Dong      et      al.,      2019),      we      train   \\n \\na      Masked      Language  \\n \\n  Model      with      additional      self-attention      masks.\\nSelf      at-  \\n \\n  tention      masks      are      chosen      randomly      in      with      the      follow  \\n \\n  proportions:      1/6      left-to-right,      1/6      right-to-left,      1/3      un-  \\n \\n  masked,      and      1/3      with      the      first      50%      of      tokens      unmasked  \\n \\n  and   \\n \\na      left-to-right      mask      for      the      remainder.\\n \\n \\n  Masked      Seq-to-Seq      Inspired      by      MASS      (Song      et      al.,  \\n \\n  2019),      we      mask   \\n \\na      span      containing      50%      of      tokens,  \\n \\n  and      train   \\n \\na      sequence      to      sequence      model      to      predict      the  \\n \\n  masked      tokens.\\n \\n \\n  For      the      Permuted      LM,      Masked      LM      and      Multitask  \\n \\n  Masked      LM,      we      use      two-stream      attention      (Yang      et      al.,  \\n \\n  2019)      to      efficiently      compute      likelihoods      of      the      output  \\n \\n  part      of      the      sequence      (using   \\n \\na      diagonal      self-attention  \\n \\n  mask      on      the      output      to      predict      words      left-to-right).\\n \\n \\n  We      experiment      with      (1)      treating      the      task      as   \\n \\na      stan-  \\n \\n  dard      sequence-to-sequence      problem,      where      the      source  \\n \\n  input      to      the      encoder      and      the      target      is      the      decoder      out-  \\n \\n  put,      or      (2)      adding      the      source      as      prefix      to      the      target      in  \\n \\n  the      decoder,      with   \\n \\na      loss      only      on      the      target      part      of      the  \\n \\n  sequence.\\nWe      find      the      former      works      better      for      BART  \\n \\n  models,      and      the      latter      for      other      models.\\n \\n \\n  To      most      directly      compare      our      models      on      their      ability  \\n \\n  to      model      their      fine-tuning      objective      (the      log      likelihood  \\n \\n  of      the      human      text),      we      report      perplexity      in      Table      1.\\n4.2      Tasks\\nasks  \\n \\n  SQuAD  \\n \\n  (Rajpurkar      et      al.,      2016)a      an      extractive      ques-  \\n \\n  tion      answering      task      on      Wikipedia      paragraphs.\\nAnswers  \\n \\n  are      text      spans      extracted      from   \\n \\na      given      document      context.\\n \\n \\n  Similar      to      BERT      (Devlin      et      al.,      2019),      we      use      concate-  \\n \\n  nated      question      and      context      as      input      to      the      encoder      of  \\n \\n  BART,      and      additionally      pass      them      to      the      decoder.\\nThe  \\n \\n  model      includes      classifiers      to      predict      the      start      and      end  \\n \\n  indices      of      each      token.\\n \\n \\n  MNLI      (Williams      et      al.,      2017),   \\n \\na      bitext      classification  \\n \\n  task      to      predict      whether      one      sentence      entails      another.\\n \\n \\n  The      fine-tuned      model      concatenates      the      two      sentences  \\n \\n  with      appended      an      EOS      token,      and      passes      them      to      both  \\n \\n  the      BART      encoder      and      decoder.\\nIn      contrast      to      BERT,  \\n \\n  the      representation      of      the      EOS      token      is      used      to      classify  \\n \\n  the      sentences      relations.\\n \\n \\n  ELIS      (Fanetal.,      2019),   \\n \\na      long-form      abstractive      ques-  \\n \\n  tion      answering      dataset.\\nModels      generate      answers      con-  \\n \\n  ditioned      on      the      concatenation      of   \\n \\na      question      and      sup-  \\n \\n  porting      documents.\\nXSum_      (Narayan      et      al.,      2018),   \\n \\na      news      summarization  \\n \\n  dataset      with      highly      abstractive      summaries.\\nConvAI2_      (Dinan      et      al.,      2019),   \\n \\na      dialogue      response  \\n \\n  generation      task,      conditioned      on      context      and   \\n \\na      persona.\\n \\n \\n  CNN/DM_      (Hermann      et      al.,      2015),   \\n \\na      news      summa-  \\n \\n  rization      dataset.\\nSummaries      here      are      typically      closely  \\n \\n  related      to      source      sentences.\\n4.3      Results\\nults Results      are      shown      in      Table      1.\\nSeveral      trends      are      clear:\\nModel      SQuAD      1.1      MNLI      ELIS      XSum   \\n \\n_      ConvAI2      CNN/DM  \\n \\n  Fl      Acc      PPL      PPL      PPL      PPL BERT      Base      (Devlin      et      al.,      2019)      88.5      84.3   \\n \\n-   \\n \\n-   \\n \\n-   \\n \\n-       Masked      Language      Model      90.0      83.5      24.77      7.87      12.59      7.06  \\n \\n  Masked      Seq2seq      87.0      82.1      23.40      6.80      11.43      6.19  \\n \\n  Language      Model      76.7      80.1      21.40      7.00      11.51      6.56  \\n \\n  Permuted      Language      Model      89.1      83.7      24.03      7.69      12.23      6.96  \\n \\n  Multitask      Masked      Language      Model      89.2      82.4      23.73      7.50      12.39      6.74  \\n \\n  BART      Base  \\n \\n  w/      Token      Masking      90.4      84.1      25.05      7.08      11.73      6.10  \\n \\n  w/      Token      Deletion      90.4      84.1      24.61      6.90      11.46      5.87  \\n \\n  w/      Text      Infilling      90.8      84.0      24.26      6.61      11.05      5.83  \\n \\n  w/      Document      Rotation      77.2      75.3      53.69      17.14      19.87      10.59  \\n \\n  w/      Sentence      Shuffling      85.4      81.5      41.87      10.93      16.67      7.89  \\n \\n  w/      Text      Infilling   \\n \\n+      Sentence      Shuffling      90.8      83.8      24.17      6.62      11.12      5.41\\nconsistently      strong      performance.\\n \\n \\n  Performance      of      pre-training      methods      varies      signifi-  \\n \\n  cantly      across      tasks      The      effectiveness      of      pre-training  \\n \\n  methods      is      highly      dependent      on      the      task.\\nFor      exam-  \\n \\n  ple,   \\n \\na      simple      language      model      achieves      the      best      ELIS  \\n \\n  performance,      but      the      worst      SQUAD      results.\\n \\n \\n  Token      masking      is      crucial      Pre-training      objectives  \\n \\n  based      on      rotating      documents      or      permuting      sentences  \\n \\n  perform      poorly      in      isolation.\\nThe      successful      methods  \\n \\n  either      use      token      deletion      or      masking,      or      self-attention  \\n \\n  masks.\\nDeletion      appears      to      outperform      masking      on  \\n \\n  generation      tasks.\\n \\n \\n  Left-to-right      pre-training      improves      generation  \\n \\n  The      Masked      Language      Model      and      the      Permuted  \\n \\n  Language      Model      perform      less      well      than      others      on  \\n \\n  generation,      and      are      the      only      models      we      consider      that  \\n \\n  do      not      include      left-to-right      auto-regressive      language  \\n \\n  modelling      during      pre-training.\\n \\n \\n  Bidirectional      encoders      are      crucial      for      SQUAD      As  \\n \\n  noted      in      previous      work      (Devlin      et      al.,      2019),      just  \\n \\n  left-to-right      decoder      performs      poorly      on      SQuAD,      be-  \\n \\n  cause      future      context      is      crucial      in      classification      deci-  \\n \\n  sions.\\nHowever,      BART      achieves      similar      performance  \\n \\n  with      only      half      the      number      of      bidirectional      layers.\\n \\n \\n  The      pre-training      objective      is      not      the      only      important  \\n \\n  factor      Our      Permuted      Language      Model      performs      less  \\n \\n  well      than      XLNet      (Yang      et      al.,      2019).\\nSome      of      this      dif-  \\n \\n  ference      is      likely      due      to      not      including      other      architectural  \\n \\n  improvements,      such      as      relative-position      embeddings      or  \\n \\n  segment-level      recurrence.\\n \\n \\n  Pure      language      models      perform      best      on      ELIS      The  \\n \\n  ELIS      dataset      is      an      outlier,      with      much      higher      perplex-  \\n \\n  ities      than      other      tasks,      and      is      the      only      generation      task  \\n \\n  where      other      models      outperform      BART.\\n \\n \\nA      pure      lan-  \\n \\n  guage      model      performs      best,      suggesting      that      BART      is  \\n \\n  less      effective      when      the      output      is      only      loosely      con-  \\n \\n  strained      by      the      input.\\n \\n \\n  BART      achieves      the      most      consistently      strong      perfor-  \\n \\n  mance.\\nWith      the      exception      of      ELI5,      BART      models  \\n \\n  using      text-infilling      perform      well      on      all      tasks.\\n5\\n \\n   Large-scale      Pre-training      Experiments  \\n \\n  Recent      work      has      shown      that      downstream      performance  \\n \\n  can      dramatically      improve      when      pre-training      is      scaled  \\n \\n  to      large      batch      sizes      (Yang      et      al.,      2019;      Liu      et      al.,      2019)  \\n \\n  and      corpora.\\nTo      test      how      well      BART      performs      in      this  \\n \\n  regime,      and      to      create   \\n \\na      useful      model      for      downstream  \\n \\n  tasks,      we      trained      BART      using      the      same      scale      as      the  \\n \\n  RoBERTa      model.\\n5.1      Experimental      Setup\\netup  \\n \\n  We      pre-train   \\n \\na      large      model      with      12      layers      in      each      of      the  \\n \\n  encoder      and      decoder,      and   \\n \\na      hidden      size      of      1024.\\nFol-  \\n \\n  lowing      RoBERTa      (Liu      et      al.,      2019),      we      use   \\n \\na      batch      size  \\n \\n  of      8000,      and      train      the      model      for      500000      steps.\\nDocu-  \\n \\n  ments      are      tokenized      with      the      same      byte-pair      encoding  \\n \\n  as      GPT-2      (Radford      et      al.,      2019).\\nBased      on      the      results      in  \\n \\n  Section      §4,      we      use   \\n \\na      combination      of      text      infilling      and  \\n \\n  sentence      permutation.\\nWe      mask      30%      of      tokens      in      each  \\n \\n  document,      and      permute      all      sentences.\\nAlthough      sen-  \\n \\n  tence      permutation      only      shows      significant      additive      gains SQuAD      1.1      SQuAD      2.0      MNLI      SST      QQP      QNLI      STS-B      RTE      MRPC      CoLA  \\n \\n  EM/F1      EM/F1      m/mm      Acc      Acc      Acc      Acc      Acc      Acc      Mcc\\n \\n \\n  BERT      84.1/90.9      79.0/81.8      86.6/-      93.2      91.3      92.3      90.0      70.4      88.0      60.6  \\n \\n  UniLM      -/-      80.5/83.4      87.0/85.9      94.5   \\n \\n-      92.7   \\n \\n-      70.9   \\n \\n-      61.1  \\n \\n  XLNet      89.0/94.5      86.1/88.8      89.8/-      95.6      91.8      93.9      91.8      83.8      89.2      63.6  \\n \\n  RoBERTa      88.9/94.6      86.5/89.4      90.2/90.2      96.4      92.2      94.7      92.4      86.6      90.9      68.0  \\n \\n  BART      88.8/94.6      86.1/89.2      89.9/90.1      96.6      92.5      94.9      91.2      87.0      90.4      62.8 Table      2:      Results      for      large      models      on      SQUAD      and      GLUE      tasks.\\nBART      performs      comparably      to      ROBERTa      and  \\n \\n  XLNet,      suggesting      that      BART’s      uni-directional      decoder      layers      do      not      reduce      performance      on      discriminative      tasks.\\nCNN/DailyMail      XSum       RI      R2      RL      RI      R2      RL\\n \\n \\n  Lead-3      40.42      17.62      36.67      16.30      1.60      11.95  \\n \\n  PTGEN      (See      et      al.,      2017)      36.44      15.66      33.42      29.70      9.21      23.24  \\n \\n  PTGEN+COV      (See      et      al.,      2017)      39.53      17.28      36.38      28.10      8.02      21.72  \\n \\n  UniLM      43.33      20.21      40.51   \\n \\n-   \\n \\n-   \\n \\n-       BERTSUMABS      (Liu   \\n \\n&      Lapata,      2019)      41.72      19.39      38.76      38.76      16.33      31.15  \\n \\n  BERTSUMEXTABS      (Liu   \\n \\n&      Lapata,      2019)      42.13      19.60      39.18      38.81      16.50      31.27 BART      44.16      21.28      40.90      45.14      22.27      37.25 Table      3:      Results      on      two      standard      summarization      datasets.\\nBART      outperforms      previous      work      on      summarization      on\\n \\n \\n  on      the      CNN/DM      summarization      dataset,      we      hypothe-      ConvAI2  \\n \\n  sised      that      larger      pre-trained      models      may      be      better      able      Valid      F1      Valid      PPL  \\n \\n  to   \\n \\n|      fi      this      task.\\nTo      help      th      del      better      fit      th  \\n \\n  RE      aa      ae      Pai      Cae      Seq2Seq+      Attention      16.02      35.07  \\n \\n  data,      we      disabled      dropout      for      the      final      10%      of      training   \\n \\n.    \\n.\\n     Best      System      19.09      17.51  \\n \\n  steps.\\nWe      use      the      same      pre-training      data      as      Liu      et      al.\\n(2019),      consisting      of      160Gb      of      news,      books,      stories,  \\n \\n  and      web      text.\\nBART      20.72      11.85 5.2      Discriminative      Tasks  \\n \\n  Table   \\n \\n2      compares      the      performance      of      BART      with      sev-  \\n \\n  eral      recent      approaches      on      the      well-studied      SQUAD      and  \\n \\n  GLUE      tasks      (Warstadt      et      al.,      2018;      Socher      et      al.,      2013;  \\n \\n  Dolan   \\n \\n&      Brockett,      2005;      Agirre      et      al.,      2007;      Williams  \\n \\n  et      al.,      2018;      Dagan      et      al.,      2006;      Levesque      et      al.,      2011).\\n \\n \\n  Table      4:      BART      outperforms      previous      work      on      conver-  \\n \\n  sational      response      generation.\\nPerplexities      are      renor-  \\n \\n  malized      based      on      official      tokenizer      for      ConvAI2.\\n \\n \\n  The      most      directly      comparable      baseline      is      ROBERTa,  \\n \\n  which      was      pre-trained      with      the      same      resources,      but  \\n \\n  a      different      objective.\\nOverall,      BART      performs      simi-  \\n \\n  larly,      with      only      small      differences      between      the      models  \\n \\n  on      most      tasks.\\nsuggesting      that      BART’s      improvements  \\n \\n  on      generation      tasks      do      not      come      at      the      expense      of      clas-  \\n \\n  sification      performance.\\n \\n \\n  Summarization      To      provide   \\n \\na      comparison      with      the  \\n \\n  state-of-the-art      in      summarization,      we      present      results  \\n \\n  on      two      summarization      datasets,      CNN/DailyMail      and  \\n \\n  XSum,      which      have      distinct      properties.\\n \\n \\n  Summaries      in      the      CNN/DailyMail      tend      to      resemble  \\n \\n  source      sentences.\\nExtractive      models      do      well      here,      and  \\n \\n  even      the      baseline      of      the      first-three      source      sentences      is  \\n \\n  highly      competitive.\\nNevertheless,      BART      outperforms  \\n \\n  all      existing      work.\\n5.3.      Generation      Tasks\\nGeneration      Tasks\\n |       We      also      experiment      with      several      text      generation      tasks.       BART      is      fine-tuned      as      a      standard      sequence-to-sequence       model      from      the      input      to      the      output      text.      During      fine-       tuning      we      use      a      label      smoothed      cross      entropy      loss       (Pereyra      et      al.,      2017),      with      the      smoothing      parameter       set      to      0.1.      During      generation,      we      set      beam      size      as      5,       remove      duplicated      trigrams      in      beam      search,      and      tuned       the      model      with      min-len,      max-len,      length      penalty      on      the       validation      set      (Fan      et      al.,      2017). | 6.0      points      on      all      ROUGE      metrics—trepresenting   \\n \\na      sig-  \\n \\n  nificant      advance      in      performance      on      this      problem.\\nQual-  \\n \\n  itatively,      sample      quality      is      high      (see      86).\\n \\n \\n  In      contrast,      XSum      is      highly      abstractive,      and      extrac-  \\n \\n  tive      models      perform      poorly.\\nBART      outperforms      the  \\n \\n  best      previous      work,      which      leverages      BERT,      by      roughly  \\n \\n  Dialogue      We      evaluate      dialogue      response      generation  \\n \\n  on      CONVAI2      (Dinan      et      al.,      2019),      in      which      agents  \\n \\n  must      generate      responses      conditioned      on      both      the      pre-  \\n \\n  vious      context      and   \\n \\na      textually-specified      persona.\\nBART  \\n \\n  outperforms      previous      work      on      two      automated      metrics.\\n\\nRl      R2      RL\\n \\n \\n  Best      Extractive      23.55      3.1      17.5  \\n \\n  Language      Model      27.8      47      23.1  \\n \\n  Seq2Seq      28.3      5.1      22.8  \\n \\n  Seq2Seq      Multitask      28.9      54      23.1  \\n \\n  BART      30.6      6.2      24.3  \\n \\n  Table      5:      BART      achieves      state-of-the-art      results      on  \\n \\n  the      challenging      ELI5      abstractive      question      answering  \\n \\n  dataset.\\nComparison      models      are      from      Fan      et      al.\\n(2019).'}, {'metadata': {'source': 'https://arxiv.org/pdf/1910.13461.pdf', 'section_number': 4, 'section_title': 'Token      Deletion Text      Infilling'}, 'page_content': 'Token      Deletion Text      Infilling\\n \\n \\n  Text      Infilling   \\n \\nA      number      of      text      spans      are      sampled,  \\n \\n  with      span      lengths      drawn      from   \\n \\na      Poisson      distribution  \\n \\n  (A   \\n \\n=      3).\\nEach      span      is      replaced      with   \\n \\na      single      [MASK]  \\n \\n  token.\\nO-length      spans      correspond      to      the      insertion      of  \\n \\n  [MASK]      tokens.\\nText      infilling      is      inspired      by      Span-  \\n \\n  BERT      (Joshi      et      al.,      2019),      but      SpanBERT      samples  \\n \\n  span      lengths      from   \\n \\na      different      (clamped      geometric)      dis-  \\n \\n  tribution,      and      replaces      each      span      with   \\n \\na      sequence      of  \\n \\n  [MASK]      tokens      of      exactly      the      same      length.\\nText      infill-  \\n \\n  ing      teaches      the      model      to      predict      how      many      tokens      are  \\n \\n  missing      from   \\n \\na      span.\\n \\n \\n  Sentence      Permutation   \\n \\nA      document      is      divided      into  \\n \\n  sentences      based      on      full      stops,      and      these      sentences      are  \\n \\n  shuffled      in   \\n \\na      random      order.\\n \\n \\n  Document      Rotation   \\n \\nA      token      is      chosen      uniformly      at  \\n \\n  random,      and      the      document      is      rotated      so      that      it      begins  \\n \\n  with      that      token.\\nThis      task      trains      the      model      to      identify  \\n \\n  the      start      of      the      document.\\n3\\n \\n   Fine-tuning      BART The      representations      produced      by      BART      can      be      used      in  \\n \\n  several      ways      for      downstream      applications.\\n3.1      Sequence      Classification      Tasks\\nasks  \\n \\n  For      sequence      classification      tasks,      the      same      input      is      fed  \\n \\n  into      the      encoder      and      decoder,      and      the      final      hidden      state  \\n \\n  of      the      final      decoder      token      is      fed      into      new      multi-class  \\n \\n  linear      classifier.\\nThis      approach      is      related      to      the      CLS  \\n \\n  token      in      BERT;      however      we      add      the      additional      token  \\n \\n  to      the      end      so      that      representation      for      the      token      in      the  \\n \\n  decoder      can      attend      to      decoder      states      from      the      complete  \\n \\n  input      (Figure      3a).\\n3.2.      Token      Classification      Tasks\\nasks  \\n \\n  For      token      classification      tasks,      such      as      answer      endpoint  \\n \\n  classification      for      SQUAD,      we      feed      the      complete      doc-  \\n \\n  ument      into      the      encoder      and      decoder,      and      use      the      top  \\n \\n  hidden      state      of      the      decoder      as   \\n \\na      representation      for      each  \\n \\n  word.\\nThis      representation      is      used      to      classify      the      token.\\n3.3      Sequence      Generation      Tasks\\nasks  \\n \\n  Because      BART      has      an      autoregressive      decoder,      it      can      be  \\n \\n  directly      fine      tuned      for      sequence      generation      tasks      such  \\n \\n  as      abstractive      question      answering      and      summarization.\\n \\n \\n  In      both      of      these      tasks,      information      is      copied      from      the  \\n \\n  input      but      manipulated,      which      is      closely      related      to      the  \\n \\n  denoising      pre-training      objective.\\nHere,      the      encoder      in-  \\n \\n  put      is      the      input      sequence,      and      the      decoder      generates  \\n \\n  outputs      autoregressively.\\n3.4      Machine      Translation\\ntion  \\n \\n  We      also      explore      using      BART      to      improve      machine      trans-  \\n \\n  lation      decoders      for      translating      into      English.\\nPrevious  \\n \\n  work      Edunov      et      al.\\n(2019)      has      shown      that      models      can  \\n \\n  be      improved      by      incorporating      pre-trained      encoders,      but  \\n \\n  gains      from      using      pre-trained      language      models      in      de-  \\n \\n  coders      have      been      limited.\\nWe      show      that      it      is      possible  \\n \\n  to      use      the      entire      BART      model      (both      encoder      and      de-  \\n \\n  coder)      as   \\n \\na      single      pretrained      decoder      for      machine      trans-  \\n \\n  lation,      by      adding   \\n \\na      new      set      of      encoder      parameters      that  \\n \\n  are      learned      from      bitext      (see      Figure      3b).\\n \\n \\n  More      precisely,      we      replace      BART’s      encoder      embed-  \\n \\n  ding      layer      with   \\n \\na      new      randomly      initialized      encoder.\\n \\n \\n  The      model      is      trained      end-to-end,      which      trains      the      new       encoder      to      map      foreign      words      into      an      input      that      BART  \\n \\n  can      de-noise      to      English.\\nThe      new      encoder      can      use   \\n \\na       separate      vocabulary      from      the      original      BART      model.\\n \\n \\n  We      train      the      source      encoder      in      two      steps,      in      both  \\n \\n  cases      backpropagating      the      cross-entropy      loss      from      the  \\n \\n  output      of      the      BART      model.\\nIn      the      first      step,      we      freeze  \\n \\n  most      of      BART      parameters      and      only      update      the      ran-  \\n \\n  domly      initialized      source      encoder,      the      BART      positional  \\n \\n  embeddings,      and      the      self-attention      input      projection      ma-  \\n \\n  trix      of      BART’s      encoder      first      layer.\\nIn      the      second      step,  \\n \\n  we      train      all      model      parameters      for   \\n \\na      small      number      of  \\n \\n  iterations.\\n4\\n \\n   Comparing      Pre-training      Objectives  \\n \\n  BART      supports   \\n \\na      much      wider      range      of      noising      schemes  \\n \\n  during      pre-training      than      previous      work.\\nWe      compare   \\n \\na       range      of      options      using      base-size      models      (6      encoder      and  \\n \\n  6      decoder      layers,      with   \\n \\na      hidden      size      of      768),      evaluated  \\n \\n  on   \\n \\na      representative      subset      of      the      tasks      we      will      consider  \\n \\n  for      the      full      large      scale      experiments      in      85.\\n4.1      Comparison      Objectives\\nives  \\n \\n  While      many      pre-training      objectives      have      been      pro-  \\n \\n  posed,      fair      comparisons      between      these      have      been      dif-  \\n \\n  ficult      to      perform,      at      least      in      part      due      to      differences      in  \\n \\n  training      data,      training      resources,      architectural      differ-  \\n \\n  ences      between      models,      and      fine-tuning      procedures.\\nWe  \\n \\n  ABCDE  \\n \\n  weee  \\n \\n  Pre-trained      Pre-trained  \\n \\n  Craters      Joo      rezapes cise)      Te      TrET Pre-trained  \\n \\n  Encoder Pre-trained  \\n \\n  Decoder aa      AB  \\n \\n  trrrt  \\n \\n  tr      ttt  \\n \\n  <s>A      BCD  \\n \\n  Randomly  \\n \\n  Initialized      Encoder  \\n \\n  TFT  \\n \\n  aBpByoe sentation      from      the      final      output      is      used.\\n \\n \\n  re-implement      strong      pre-training      approaches      recently  \\n \\n  proposed      for      discriminative      and      generation      tasks.\\nWe  \\n \\n  aim,      as      much      as      possible,      to      control      for      differences      un-  \\n \\n  related      to      the      pre-training      objective.\\nHowever,      we      do  \\n \\n  make      minor      changes      to      the      learning      rate      and      usage      of  \\n \\n  layer      normalisation      in      order      to      improve      performance  \\n \\n  (tuning      these      separately      for      each      objective).\\nFor      refer-  \\n \\n  ence,      we      compare      our      implementations      with      published  \\n \\n  numbers      from      BERT,      which      was      also      trained      for      1M  \\n \\n  steps      on   \\n \\na      combination      of      books      and      Wikipedia      data.\\n \\n \\n  We      compare      the      following      approaches:\\n \\n \\n  Language      Model      Similarly      to      GPT      (Radford      et      al.,  \\n \\n  2018),      we      train   \\n \\na      left-to-right      Transformer      language  \\n \\n  model.\\nThis      model      is      equivalent      to      the      BART      decoder,  \\n \\n  without      cross-attention.\\n \\n \\n  Permuted      Language      Model      Based      on      XLNet      (Yang  \\n \\n  et      al.,      2019),      we      sample      1/6      of      the      tokens,      and      gener-  \\n \\n  ate      them      in   \\n \\na      random      order      autoregressively.\\nFor      con-  \\n \\n  sistency      with      other      models,      we      do      not      implement      the  \\n \\n  relative      positional      embeddings      or      attention      across      seg-  \\n \\n  ments      from      XLNet.\\n \\n \\n  Masked      Language      Model      Following      BERT      (Devlin  \\n \\n  et      al.,      2019),      we      replace      15%      of      tokens      with      [MASK]  \\n \\n  symbols,      and      train      the      model      to      independently      predict  \\n \\n  the      original      tokens.\\n \\n \\n  Multitask      Masked      Language      Model      As      in      UniLM  \\n \\n  (Dong      et      al.,      2019),      we      train   \\n \\na      Masked      Language  \\n \\n  Model      with      additional      self-attention      masks.\\nSelf      at-  \\n \\n  tention      masks      are      chosen      randomly      in      with      the      follow  \\n \\n  proportions:      1/6      left-to-right,      1/6      right-to-left,      1/3      un-  \\n \\n  masked,      and      1/3      with      the      first      50%      of      tokens      unmasked  \\n \\n  and   \\n \\na      left-to-right      mask      for      the      remainder.\\n \\n \\n  Masked      Seq-to-Seq      Inspired      by      MASS      (Song      et      al.,  \\n \\n  2019),      we      mask   \\n \\na      span      containing      50%      of      tokens,  \\n \\n  and      train   \\n \\na      sequence      to      sequence      model      to      predict      the  \\n \\n  masked      tokens.\\n \\n \\n  For      the      Permuted      LM,      Masked      LM      and      Multitask  \\n \\n  Masked      LM,      we      use      two-stream      attention      (Yang      et      al.,  \\n \\n  2019)      to      efficiently      compute      likelihoods      of      the      output  \\n \\n  part      of      the      sequence      (using   \\n \\na      diagonal      self-attention  \\n \\n  mask      on      the      output      to      predict      words      left-to-right).\\n \\n \\n  We      experiment      with      (1)      treating      the      task      as   \\n \\na      stan-  \\n \\n  dard      sequence-to-sequence      problem,      where      the      source  \\n \\n  input      to      the      encoder      and      the      target      is      the      decoder      out-  \\n \\n  put,      or      (2)      adding      the      source      as      prefix      to      the      target      in  \\n \\n  the      decoder,      with   \\n \\na      loss      only      on      the      target      part      of      the  \\n \\n  sequence.\\nWe      find      the      former      works      better      for      BART  \\n \\n  models,      and      the      latter      for      other      models.\\n \\n \\n  To      most      directly      compare      our      models      on      their      ability  \\n \\n  to      model      their      fine-tuning      objective      (the      log      likelihood  \\n \\n  of      the      human      text),      we      report      perplexity      in      Table      1.\\n4.2      Tasks\\nasks  \\n \\n  SQuAD  \\n \\n  (Rajpurkar      et      al.,      2016)a      an      extractive      ques-  \\n \\n  tion      answering      task      on      Wikipedia      paragraphs.\\nAnswers  \\n \\n  are      text      spans      extracted      from   \\n \\na      given      document      context.\\n \\n \\n  Similar      to      BERT      (Devlin      et      al.,      2019),      we      use      concate-  \\n \\n  nated      question      and      context      as      input      to      the      encoder      of  \\n \\n  BART,      and      additionally      pass      them      to      the      decoder.\\nThe  \\n \\n  model      includes      classifiers      to      predict      the      start      and      end  \\n \\n  indices      of      each      token.\\n \\n \\n  MNLI      (Williams      et      al.,      2017),   \\n \\na      bitext      classification  \\n \\n  task      to      predict      whether      one      sentence      entails      another.\\n \\n \\n  The      fine-tuned      model      concatenates      the      two      sentences  \\n \\n  with      appended      an      EOS      token,      and      passes      them      to      both  \\n \\n  the      BART      encoder      and      decoder.\\nIn      contrast      to      BERT,  \\n \\n  the      representation      of      the      EOS      token      is      used      to      classify  \\n \\n  the      sentences      relations.\\n \\n \\n  ELIS      (Fanetal.,      2019),   \\n \\na      long-form      abstractive      ques-  \\n \\n  tion      answering      dataset.\\nModels      generate      answers      con-  \\n \\n  ditioned      on      the      concatenation      of   \\n \\na      question      and      sup-  \\n \\n  porting      documents.\\nXSum_      (Narayan      et      al.,      2018),   \\n \\na      news      summarization  \\n \\n  dataset      with      highly      abstractive      summaries.\\nConvAI2_      (Dinan      et      al.,      2019),   \\n \\na      dialogue      response  \\n \\n  generation      task,      conditioned      on      context      and   \\n \\na      persona.\\n \\n \\n  CNN/DM_      (Hermann      et      al.,      2015),   \\n \\na      news      summa-  \\n \\n  rization      dataset.\\nSummaries      here      are      typically      closely  \\n \\n  related      to      source      sentences.\\n4.3      Results\\nults Results      are      shown      in      Table      1.\\nSeveral      trends      are      clear:\\nModel      SQuAD      1.1      MNLI      ELIS      XSum   \\n \\n_      ConvAI2      CNN/DM  \\n \\n  Fl      Acc      PPL      PPL      PPL      PPL BERT      Base      (Devlin      et      al.,      2019)      88.5      84.3   \\n \\n-   \\n \\n-   \\n \\n-   \\n \\n-       Masked      Language      Model      90.0      83.5      24.77      7.87      12.59      7.06  \\n \\n  Masked      Seq2seq      87.0      82.1      23.40      6.80      11.43      6.19  \\n \\n  Language      Model      76.7      80.1      21.40      7.00      11.51      6.56  \\n \\n  Permuted      Language      Model      89.1      83.7      24.03      7.69      12.23      6.96  \\n \\n  Multitask      Masked      Language      Model      89.2      82.4      23.73      7.50      12.39      6.74  \\n \\n  BART      Base  \\n \\n  w/      Token      Masking      90.4      84.1      25.05      7.08      11.73      6.10  \\n \\n  w/      Token      Deletion      90.4      84.1      24.61      6.90      11.46      5.87  \\n \\n  w/      Text      Infilling      90.8      84.0      24.26      6.61      11.05      5.83  \\n \\n  w/      Document      Rotation      77.2      75.3      53.69      17.14      19.87      10.59  \\n \\n  w/      Sentence      Shuffling      85.4      81.5      41.87      10.93      16.67      7.89  \\n \\n  w/      Text      Infilling   \\n \\n+      Sentence      Shuffling      90.8      83.8      24.17      6.62      11.12      5.41\\nconsistently      strong      performance.\\n \\n \\n  Performance      of      pre-training      methods      varies      signifi-  \\n \\n  cantly      across      tasks      The      effectiveness      of      pre-training  \\n \\n  methods      is      highly      dependent      on      the      task.\\nFor      exam-  \\n \\n  ple,   \\n \\na      simple      language      model      achieves      the      best      ELIS  \\n \\n  performance,      but      the      worst      SQUAD      results.\\n \\n \\n  Token      masking      is      crucial      Pre-training      objectives  \\n \\n  based      on      rotating      documents      or      permuting      sentences  \\n \\n  perform      poorly      in      isolation.\\nThe      successful      methods  \\n \\n  either      use      token      deletion      or      masking,      or      self-attention  \\n \\n  masks.\\nDeletion      appears      to      outperform      masking      on  \\n \\n  generation      tasks.\\n \\n \\n  Left-to-right      pre-training      improves      generation  \\n \\n  The      Masked      Language      Model      and      the      Permuted  \\n \\n  Language      Model      perform      less      well      than      others      on  \\n \\n  generation,      and      are      the      only      models      we      consider      that  \\n \\n  do      not      include      left-to-right      auto-regressive      language  \\n \\n  modelling      during      pre-training.\\n \\n \\n  Bidirectional      encoders      are      crucial      for      SQUAD      As  \\n \\n  noted      in      previous      work      (Devlin      et      al.,      2019),      just  \\n \\n  left-to-right      decoder      performs      poorly      on      SQuAD,      be-  \\n \\n  cause      future      context      is      crucial      in      classification      deci-  \\n \\n  sions.\\nHowever,      BART      achieves      similar      performance  \\n \\n  with      only      half      the      number      of      bidirectional      layers.\\n \\n \\n  The      pre-training      objective      is      not      the      only      important  \\n \\n  factor      Our      Permuted      Language      Model      performs      less  \\n \\n  well      than      XLNet      (Yang      et      al.,      2019).\\nSome      of      this      dif-  \\n \\n  ference      is      likely      due      to      not      including      other      architectural  \\n \\n  improvements,      such      as      relative-position      embeddings      or  \\n \\n  segment-level      recurrence.\\n \\n \\n  Pure      language      models      perform      best      on      ELIS      The  \\n \\n  ELIS      dataset      is      an      outlier,      with      much      higher      perplex-  \\n \\n  ities      than      other      tasks,      and      is      the      only      generation      task  \\n \\n  where      other      models      outperform      BART.\\n \\n \\nA      pure      lan-  \\n \\n  guage      model      performs      best,      suggesting      that      BART      is  \\n \\n  less      effective      when      the      output      is      only      loosely      con-  \\n \\n  strained      by      the      input.\\n \\n \\n  BART      achieves      the      most      consistently      strong      perfor-  \\n \\n  mance.\\nWith      the      exception      of      ELI5,      BART      models  \\n \\n  using      text-infilling      perform      well      on      all      tasks.\\n5\\n \\n   Large-scale      Pre-training      Experiments  \\n \\n  Recent      work      has      shown      that      downstream      performance  \\n \\n  can      dramatically      improve      when      pre-training      is      scaled  \\n \\n  to      large      batch      sizes      (Yang      et      al.,      2019;      Liu      et      al.,      2019)  \\n \\n  and      corpora.\\nTo      test      how      well      BART      performs      in      this  \\n \\n  regime,      and      to      create   \\n \\na      useful      model      for      downstream  \\n \\n  tasks,      we      trained      BART      using      the      same      scale      as      the  \\n \\n  RoBERTa      model.\\n5.1      Experimental      Setup\\netup  \\n \\n  We      pre-train   \\n \\na      large      model      with      12      layers      in      each      of      the  \\n \\n  encoder      and      decoder,      and   \\n \\na      hidden      size      of      1024.\\nFol-  \\n \\n  lowing      RoBERTa      (Liu      et      al.,      2019),      we      use   \\n \\na      batch      size  \\n \\n  of      8000,      and      train      the      model      for      500000      steps.\\nDocu-  \\n \\n  ments      are      tokenized      with      the      same      byte-pair      encoding  \\n \\n  as      GPT-2      (Radford      et      al.,      2019).\\nBased      on      the      results      in  \\n \\n  Section      §4,      we      use   \\n \\na      combination      of      text      infilling      and  \\n \\n  sentence      permutation.\\nWe      mask      30%      of      tokens      in      each  \\n \\n  document,      and      permute      all      sentences.\\nAlthough      sen-  \\n \\n  tence      permutation      only      shows      significant      additive      gains SQuAD      1.1      SQuAD      2.0      MNLI      SST      QQP      QNLI      STS-B      RTE      MRPC      CoLA  \\n \\n  EM/F1      EM/F1      m/mm      Acc      Acc      Acc      Acc      Acc      Acc      Mcc\\n \\n \\n  BERT      84.1/90.9      79.0/81.8      86.6/-      93.2      91.3      92.3      90.0      70.4      88.0      60.6  \\n \\n  UniLM      -/-      80.5/83.4      87.0/85.9      94.5   \\n \\n-      92.7   \\n \\n-      70.9   \\n \\n-      61.1  \\n \\n  XLNet      89.0/94.5      86.1/88.8      89.8/-      95.6      91.8      93.9      91.8      83.8      89.2      63.6  \\n \\n  RoBERTa      88.9/94.6      86.5/89.4      90.2/90.2      96.4      92.2      94.7      92.4      86.6      90.9      68.0  \\n \\n  BART      88.8/94.6      86.1/89.2      89.9/90.1      96.6      92.5      94.9      91.2      87.0      90.4      62.8 Table      2:      Results      for      large      models      on      SQUAD      and      GLUE      tasks.\\nBART      performs      comparably      to      ROBERTa      and  \\n \\n  XLNet,      suggesting      that      BART’s      uni-directional      decoder      layers      do      not      reduce      performance      on      discriminative      tasks.\\nCNN/DailyMail      XSum       RI      R2      RL      RI      R2      RL\\n \\n \\n  Lead-3      40.42      17.62      36.67      16.30      1.60      11.95  \\n \\n  PTGEN      (See      et      al.,      2017)      36.44      15.66      33.42      29.70      9.21      23.24  \\n \\n  PTGEN+COV      (See      et      al.,      2017)      39.53      17.28      36.38      28.10      8.02      21.72  \\n \\n  UniLM      43.33      20.21      40.51   \\n \\n-   \\n \\n-   \\n \\n-       BERTSUMABS      (Liu   \\n \\n&      Lapata,      2019)      41.72      19.39      38.76      38.76      16.33      31.15  \\n \\n  BERTSUMEXTABS      (Liu   \\n \\n&      Lapata,      2019)      42.13      19.60      39.18      38.81      16.50      31.27 BART      44.16      21.28      40.90      45.14      22.27      37.25 Table      3:      Results      on      two      standard      summarization      datasets.\\nBART      outperforms      previous      work      on      summarization      on\\n \\n \\n  on      the      CNN/DM      summarization      dataset,      we      hypothe-      ConvAI2  \\n \\n  sised      that      larger      pre-trained      models      may      be      better      able      Valid      F1      Valid      PPL  \\n \\n  to   \\n \\n|      fi      this      task.\\nTo      help      th      del      better      fit      th  \\n \\n  RE      aa      ae      Pai      Cae      Seq2Seq+      Attention      16.02      35.07  \\n \\n  data,      we      disabled      dropout      for      the      final      10%      of      training   \\n \\n.    \\n.\\n     Best      System      19.09      17.51  \\n \\n  steps.\\nWe      use      the      same      pre-training      data      as      Liu      et      al.\\n(2019),      consisting      of      160Gb      of      news,      books,      stories,  \\n \\n  and      web      text.\\nBART      20.72      11.85 5.2      Discriminative      Tasks  \\n \\n  Table   \\n \\n2      compares      the      performance      of      BART      with      sev-  \\n \\n  eral      recent      approaches      on      the      well-studied      SQUAD      and  \\n \\n  GLUE      tasks      (Warstadt      et      al.,      2018;      Socher      et      al.,      2013;  \\n \\n  Dolan   \\n \\n&      Brockett,      2005;      Agirre      et      al.,      2007;      Williams  \\n \\n  et      al.,      2018;      Dagan      et      al.,      2006;      Levesque      et      al.,      2011).\\n \\n \\n  Table      4:      BART      outperforms      previous      work      on      conver-  \\n \\n  sational      response      generation.\\nPerplexities      are      renor-  \\n \\n  malized      based      on      official      tokenizer      for      ConvAI2.\\n \\n \\n  The      most      directly      comparable      baseline      is      ROBERTa,  \\n \\n  which      was      pre-trained      with      the      same      resources,      but  \\n \\n  a      different      objective.\\nOverall,      BART      performs      simi-  \\n \\n  larly,      with      only      small      differences      between      the      models  \\n \\n  on      most      tasks.\\nsuggesting      that      BART’s      improvements  \\n \\n  on      generation      tasks      do      not      come      at      the      expense      of      clas-  \\n \\n  sification      performance.\\n \\n \\n  Summarization      To      provide   \\n \\na      comparison      with      the  \\n \\n  state-of-the-art      in      summarization,      we      present      results  \\n \\n  on      two      summarization      datasets,      CNN/DailyMail      and  \\n \\n  XSum,      which      have      distinct      properties.\\n \\n \\n  Summaries      in      the      CNN/DailyMail      tend      to      resemble  \\n \\n  source      sentences.\\nExtractive      models      do      well      here,      and  \\n \\n  even      the      baseline      of      the      first-three      source      sentences      is  \\n \\n  highly      competitive.\\nNevertheless,      BART      outperforms  \\n \\n  all      existing      work.\\n5.3.      Generation      Tasks\\nGeneration      Tasks\\n |       We      also      experiment      with      several      text      generation      tasks.       BART      is      fine-tuned      as      a      standard      sequence-to-sequence       model      from      the      input      to      the      output      text.      During      fine-       tuning      we      use      a      label      smoothed      cross      entropy      loss       (Pereyra      et      al.,      2017),      with      the      smoothing      parameter       set      to      0.1.      During      generation,      we      set      beam      size      as      5,       remove      duplicated      trigrams      in      beam      search,      and      tuned       the      model      with      min-len,      max-len,      length      penalty      on      the       validation      set      (Fan      et      al.,      2017). | 6.0      points      on      all      ROUGE      metrics—trepresenting   \\n \\na      sig-  \\n \\n  nificant      advance      in      performance      on      this      problem.\\nQual-  \\n \\n  itatively,      sample      quality      is      high      (see      86).\\n \\n \\n  In      contrast,      XSum      is      highly      abstractive,      and      extrac-  \\n \\n  tive      models      perform      poorly.\\nBART      outperforms      the  \\n \\n  best      previous      work,      which      leverages      BERT,      by      roughly  \\n \\n  Dialogue      We      evaluate      dialogue      response      generation  \\n \\n  on      CONVAI2      (Dinan      et      al.,      2019),      in      which      agents  \\n \\n  must      generate      responses      conditioned      on      both      the      pre-  \\n \\n  vious      context      and   \\n \\na      textually-specified      persona.\\nBART  \\n \\n  outperforms      previous      work      on      two      automated      metrics.\\n'}, {'metadata': {'source': 'https://arxiv.org/pdf/1910.13461.pdf', 'section_number': 5, 'section_title': '3.1      Sequence      Classification      Tasks'}, 'page_content': '3.1      Sequence      Classification      Tasks\\nasks  \\n \\n  For      sequence      classification      tasks,      the      same      input      is      fed  \\n \\n  into      the      encoder      and      decoder,      and      the      final      hidden      state  \\n \\n  of      the      final      decoder      token      is      fed      into      new      multi-class  \\n \\n  linear      classifier.\\nThis      approach      is      related      to      the      CLS  \\n \\n  token      in      BERT;      however      we      add      the      additional      token  \\n \\n  to      the      end      so      that      representation      for      the      token      in      the  \\n \\n  decoder      can      attend      to      decoder      states      from      the      complete  \\n \\n  input      (Figure      3a).'}, {'metadata': {'source': 'https://arxiv.org/pdf/1910.13461.pdf', 'section_number': 6, 'section_title': '3.2.      Token      Classification      Tasks'}, 'page_content': '3.2.      Token      Classification      Tasks\\nasks  \\n \\n  For      token      classification      tasks,      such      as      answer      endpoint  \\n \\n  classification      for      SQUAD,      we      feed      the      complete      doc-  \\n \\n  ument      into      the      encoder      and      decoder,      and      use      the      top  \\n \\n  hidden      state      of      the      decoder      as   \\n \\na      representation      for      each  \\n \\n  word.\\nThis      representation      is      used      to      classify      the      token.'}, {'metadata': {'source': 'https://arxiv.org/pdf/1910.13461.pdf', 'section_number': 7, 'section_title': '3.3      Sequence      Generation      Tasks'}, 'page_content': '3.3      Sequence      Generation      Tasks\\nasks  \\n \\n  Because      BART      has      an      autoregressive      decoder,      it      can      be  \\n \\n  directly      fine      tuned      for      sequence      generation      tasks      such  \\n \\n  as      abstractive      question      answering      and      summarization.\\n \\n \\n  In      both      of      these      tasks,      information      is      copied      from      the  \\n \\n  input      but      manipulated,      which      is      closely      related      to      the  \\n \\n  denoising      pre-training      objective.\\nHere,      the      encoder      in-  \\n \\n  put      is      the      input      sequence,      and      the      decoder      generates  \\n \\n  outputs      autoregressively.'}, {'metadata': {'source': 'https://arxiv.org/pdf/1910.13461.pdf', 'section_number': 8, 'section_title': '3.4      Machine      Translation'}, 'page_content': '3.4      Machine      Translation\\ntion  \\n \\n  We      also      explore      using      BART      to      improve      machine      trans-  \\n \\n  lation      decoders      for      translating      into      English.\\nPrevious  \\n \\n  work      Edunov      et      al.\\n(2019)      has      shown      that      models      can  \\n \\n  be      improved      by      incorporating      pre-trained      encoders,      but  \\n \\n  gains      from      using      pre-trained      language      models      in      de-  \\n \\n  coders      have      been      limited.\\nWe      show      that      it      is      possible  \\n \\n  to      use      the      entire      BART      model      (both      encoder      and      de-  \\n \\n  coder)      as   \\n \\na      single      pretrained      decoder      for      machine      trans-  \\n \\n  lation,      by      adding   \\n \\na      new      set      of      encoder      parameters      that  \\n \\n  are      learned      from      bitext      (see      Figure      3b).\\n \\n \\n  More      precisely,      we      replace      BART’s      encoder      embed-  \\n \\n  ding      layer      with   \\n \\na      new      randomly      initialized      encoder.\\n \\n \\n  The      model      is      trained      end-to-end,      which      trains      the      new       encoder      to      map      foreign      words      into      an      input      that      BART  \\n \\n  can      de-noise      to      English.\\nThe      new      encoder      can      use   \\n \\na       separate      vocabulary      from      the      original      BART      model.\\n \\n \\n  We      train      the      source      encoder      in      two      steps,      in      both  \\n \\n  cases      backpropagating      the      cross-entropy      loss      from      the  \\n \\n  output      of      the      BART      model.\\nIn      the      first      step,      we      freeze  \\n \\n  most      of      BART      parameters      and      only      update      the      ran-  \\n \\n  domly      initialized      source      encoder,      the      BART      positional  \\n \\n  embeddings,      and      the      self-attention      input      projection      ma-  \\n \\n  trix      of      BART’s      encoder      first      layer.\\nIn      the      second      step,  \\n \\n  we      train      all      model      parameters      for   \\n \\na      small      number      of  \\n \\n  iterations.\\n4\\n \\n   Comparing      Pre-training      Objectives  \\n \\n  BART      supports   \\n \\na      much      wider      range      of      noising      schemes  \\n \\n  during      pre-training      than      previous      work.\\nWe      compare   \\n \\na       range      of      options      using      base-size      models      (6      encoder      and  \\n \\n  6      decoder      layers,      with   \\n \\na      hidden      size      of      768),      evaluated  \\n \\n  on   \\n \\na      representative      subset      of      the      tasks      we      will      consider  \\n \\n  for      the      full      large      scale      experiments      in      85.\\n4.1      Comparison      Objectives\\nives  \\n \\n  While      many      pre-training      objectives      have      been      pro-  \\n \\n  posed,      fair      comparisons      between      these      have      been      dif-  \\n \\n  ficult      to      perform,      at      least      in      part      due      to      differences      in  \\n \\n  training      data,      training      resources,      architectural      differ-  \\n \\n  ences      between      models,      and      fine-tuning      procedures.\\nWe  \\n \\n  ABCDE  \\n \\n  weee  \\n \\n  Pre-trained      Pre-trained  \\n \\n  Craters      Joo      rezapes cise)      Te      TrET Pre-trained  \\n \\n  Encoder Pre-trained  \\n \\n  Decoder aa      AB  \\n \\n  trrrt  \\n \\n  tr      ttt  \\n \\n  <s>A      BCD  \\n \\n  Randomly  \\n \\n  Initialized      Encoder  \\n \\n  TFT  \\n \\n  aBpByoe sentation      from      the      final      output      is      used.\\n \\n \\n  re-implement      strong      pre-training      approaches      recently  \\n \\n  proposed      for      discriminative      and      generation      tasks.\\nWe  \\n \\n  aim,      as      much      as      possible,      to      control      for      differences      un-  \\n \\n  related      to      the      pre-training      objective.\\nHowever,      we      do  \\n \\n  make      minor      changes      to      the      learning      rate      and      usage      of  \\n \\n  layer      normalisation      in      order      to      improve      performance  \\n \\n  (tuning      these      separately      for      each      objective).\\nFor      refer-  \\n \\n  ence,      we      compare      our      implementations      with      published  \\n \\n  numbers      from      BERT,      which      was      also      trained      for      1M  \\n \\n  steps      on   \\n \\na      combination      of      books      and      Wikipedia      data.\\n \\n \\n  We      compare      the      following      approaches:\\n \\n \\n  Language      Model      Similarly      to      GPT      (Radford      et      al.,  \\n \\n  2018),      we      train   \\n \\na      left-to-right      Transformer      language  \\n \\n  model.\\nThis      model      is      equivalent      to      the      BART      decoder,  \\n \\n  without      cross-attention.\\n \\n \\n  Permuted      Language      Model      Based      on      XLNet      (Yang  \\n \\n  et      al.,      2019),      we      sample      1/6      of      the      tokens,      and      gener-  \\n \\n  ate      them      in   \\n \\na      random      order      autoregressively.\\nFor      con-  \\n \\n  sistency      with      other      models,      we      do      not      implement      the  \\n \\n  relative      positional      embeddings      or      attention      across      seg-  \\n \\n  ments      from      XLNet.\\n \\n \\n  Masked      Language      Model      Following      BERT      (Devlin  \\n \\n  et      al.,      2019),      we      replace      15%      of      tokens      with      [MASK]  \\n \\n  symbols,      and      train      the      model      to      independently      predict  \\n \\n  the      original      tokens.\\n \\n \\n  Multitask      Masked      Language      Model      As      in      UniLM  \\n \\n  (Dong      et      al.,      2019),      we      train   \\n \\na      Masked      Language  \\n \\n  Model      with      additional      self-attention      masks.\\nSelf      at-  \\n \\n  tention      masks      are      chosen      randomly      in      with      the      follow  \\n \\n  proportions:      1/6      left-to-right,      1/6      right-to-left,      1/3      un-  \\n \\n  masked,      and      1/3      with      the      first      50%      of      tokens      unmasked  \\n \\n  and   \\n \\na      left-to-right      mask      for      the      remainder.\\n \\n \\n  Masked      Seq-to-Seq      Inspired      by      MASS      (Song      et      al.,  \\n \\n  2019),      we      mask   \\n \\na      span      containing      50%      of      tokens,  \\n \\n  and      train   \\n \\na      sequence      to      sequence      model      to      predict      the  \\n \\n  masked      tokens.\\n \\n \\n  For      the      Permuted      LM,      Masked      LM      and      Multitask  \\n \\n  Masked      LM,      we      use      two-stream      attention      (Yang      et      al.,  \\n \\n  2019)      to      efficiently      compute      likelihoods      of      the      output  \\n \\n  part      of      the      sequence      (using   \\n \\na      diagonal      self-attention  \\n \\n  mask      on      the      output      to      predict      words      left-to-right).\\n \\n \\n  We      experiment      with      (1)      treating      the      task      as   \\n \\na      stan-  \\n \\n  dard      sequence-to-sequence      problem,      where      the      source  \\n \\n  input      to      the      encoder      and      the      target      is      the      decoder      out-  \\n \\n  put,      or      (2)      adding      the      source      as      prefix      to      the      target      in  \\n \\n  the      decoder,      with   \\n \\na      loss      only      on      the      target      part      of      the  \\n \\n  sequence.\\nWe      find      the      former      works      better      for      BART  \\n \\n  models,      and      the      latter      for      other      models.\\n \\n \\n  To      most      directly      compare      our      models      on      their      ability  \\n \\n  to      model      their      fine-tuning      objective      (the      log      likelihood  \\n \\n  of      the      human      text),      we      report      perplexity      in      Table      1.\\n4.2      Tasks\\nasks  \\n \\n  SQuAD  \\n \\n  (Rajpurkar      et      al.,      2016)a      an      extractive      ques-  \\n \\n  tion      answering      task      on      Wikipedia      paragraphs.\\nAnswers  \\n \\n  are      text      spans      extracted      from   \\n \\na      given      document      context.\\n \\n \\n  Similar      to      BERT      (Devlin      et      al.,      2019),      we      use      concate-  \\n \\n  nated      question      and      context      as      input      to      the      encoder      of  \\n \\n  BART,      and      additionally      pass      them      to      the      decoder.\\nThe  \\n \\n  model      includes      classifiers      to      predict      the      start      and      end  \\n \\n  indices      of      each      token.\\n \\n \\n  MNLI      (Williams      et      al.,      2017),   \\n \\na      bitext      classification  \\n \\n  task      to      predict      whether      one      sentence      entails      another.\\n \\n \\n  The      fine-tuned      model      concatenates      the      two      sentences  \\n \\n  with      appended      an      EOS      token,      and      passes      them      to      both  \\n \\n  the      BART      encoder      and      decoder.\\nIn      contrast      to      BERT,  \\n \\n  the      representation      of      the      EOS      token      is      used      to      classify  \\n \\n  the      sentences      relations.\\n \\n \\n  ELIS      (Fanetal.,      2019),   \\n \\na      long-form      abstractive      ques-  \\n \\n  tion      answering      dataset.\\nModels      generate      answers      con-  \\n \\n  ditioned      on      the      concatenation      of   \\n \\na      question      and      sup-  \\n \\n  porting      documents.\\nXSum_      (Narayan      et      al.,      2018),   \\n \\na      news      summarization  \\n \\n  dataset      with      highly      abstractive      summaries.\\nConvAI2_      (Dinan      et      al.,      2019),   \\n \\na      dialogue      response  \\n \\n  generation      task,      conditioned      on      context      and   \\n \\na      persona.\\n \\n \\n  CNN/DM_      (Hermann      et      al.,      2015),   \\n \\na      news      summa-  \\n \\n  rization      dataset.\\nSummaries      here      are      typically      closely  \\n \\n  related      to      source      sentences.\\n4.3      Results\\nults Results      are      shown      in      Table      1.\\nSeveral      trends      are      clear:\\nModel      SQuAD      1.1      MNLI      ELIS      XSum   \\n \\n_      ConvAI2      CNN/DM  \\n \\n  Fl      Acc      PPL      PPL      PPL      PPL BERT      Base      (Devlin      et      al.,      2019)      88.5      84.3   \\n \\n-   \\n \\n-   \\n \\n-   \\n \\n-       Masked      Language      Model      90.0      83.5      24.77      7.87      12.59      7.06  \\n \\n  Masked      Seq2seq      87.0      82.1      23.40      6.80      11.43      6.19  \\n \\n  Language      Model      76.7      80.1      21.40      7.00      11.51      6.56  \\n \\n  Permuted      Language      Model      89.1      83.7      24.03      7.69      12.23      6.96  \\n \\n  Multitask      Masked      Language      Model      89.2      82.4      23.73      7.50      12.39      6.74  \\n \\n  BART      Base  \\n \\n  w/      Token      Masking      90.4      84.1      25.05      7.08      11.73      6.10  \\n \\n  w/      Token      Deletion      90.4      84.1      24.61      6.90      11.46      5.87  \\n \\n  w/      Text      Infilling      90.8      84.0      24.26      6.61      11.05      5.83  \\n \\n  w/      Document      Rotation      77.2      75.3      53.69      17.14      19.87      10.59  \\n \\n  w/      Sentence      Shuffling      85.4      81.5      41.87      10.93      16.67      7.89  \\n \\n  w/      Text      Infilling   \\n \\n+      Sentence      Shuffling      90.8      83.8      24.17      6.62      11.12      5.41\\nconsistently      strong      performance.\\n \\n \\n  Performance      of      pre-training      methods      varies      signifi-  \\n \\n  cantly      across      tasks      The      effectiveness      of      pre-training  \\n \\n  methods      is      highly      dependent      on      the      task.\\nFor      exam-  \\n \\n  ple,   \\n \\na      simple      language      model      achieves      the      best      ELIS  \\n \\n  performance,      but      the      worst      SQUAD      results.\\n \\n \\n  Token      masking      is      crucial      Pre-training      objectives  \\n \\n  based      on      rotating      documents      or      permuting      sentences  \\n \\n  perform      poorly      in      isolation.\\nThe      successful      methods  \\n \\n  either      use      token      deletion      or      masking,      or      self-attention  \\n \\n  masks.\\nDeletion      appears      to      outperform      masking      on  \\n \\n  generation      tasks.\\n \\n \\n  Left-to-right      pre-training      improves      generation  \\n \\n  The      Masked      Language      Model      and      the      Permuted  \\n \\n  Language      Model      perform      less      well      than      others      on  \\n \\n  generation,      and      are      the      only      models      we      consider      that  \\n \\n  do      not      include      left-to-right      auto-regressive      language  \\n \\n  modelling      during      pre-training.\\n \\n \\n  Bidirectional      encoders      are      crucial      for      SQUAD      As  \\n \\n  noted      in      previous      work      (Devlin      et      al.,      2019),      just  \\n \\n  left-to-right      decoder      performs      poorly      on      SQuAD,      be-  \\n \\n  cause      future      context      is      crucial      in      classification      deci-  \\n \\n  sions.\\nHowever,      BART      achieves      similar      performance  \\n \\n  with      only      half      the      number      of      bidirectional      layers.\\n \\n \\n  The      pre-training      objective      is      not      the      only      important  \\n \\n  factor      Our      Permuted      Language      Model      performs      less  \\n \\n  well      than      XLNet      (Yang      et      al.,      2019).\\nSome      of      this      dif-  \\n \\n  ference      is      likely      due      to      not      including      other      architectural  \\n \\n  improvements,      such      as      relative-position      embeddings      or  \\n \\n  segment-level      recurrence.\\n \\n \\n  Pure      language      models      perform      best      on      ELIS      The  \\n \\n  ELIS      dataset      is      an      outlier,      with      much      higher      perplex-  \\n \\n  ities      than      other      tasks,      and      is      the      only      generation      task  \\n \\n  where      other      models      outperform      BART.\\n \\n \\nA      pure      lan-  \\n \\n  guage      model      performs      best,      suggesting      that      BART      is  \\n \\n  less      effective      when      the      output      is      only      loosely      con-  \\n \\n  strained      by      the      input.\\n \\n \\n  BART      achieves      the      most      consistently      strong      perfor-  \\n \\n  mance.\\nWith      the      exception      of      ELI5,      BART      models  \\n \\n  using      text-infilling      perform      well      on      all      tasks.\\n5\\n \\n   Large-scale      Pre-training      Experiments  \\n \\n  Recent      work      has      shown      that      downstream      performance  \\n \\n  can      dramatically      improve      when      pre-training      is      scaled  \\n \\n  to      large      batch      sizes      (Yang      et      al.,      2019;      Liu      et      al.,      2019)  \\n \\n  and      corpora.\\nTo      test      how      well      BART      performs      in      this  \\n \\n  regime,      and      to      create   \\n \\na      useful      model      for      downstream  \\n \\n  tasks,      we      trained      BART      using      the      same      scale      as      the  \\n \\n  RoBERTa      model.\\n5.1      Experimental      Setup\\netup  \\n \\n  We      pre-train   \\n \\na      large      model      with      12      layers      in      each      of      the  \\n \\n  encoder      and      decoder,      and   \\n \\na      hidden      size      of      1024.\\nFol-  \\n \\n  lowing      RoBERTa      (Liu      et      al.,      2019),      we      use   \\n \\na      batch      size  \\n \\n  of      8000,      and      train      the      model      for      500000      steps.\\nDocu-  \\n \\n  ments      are      tokenized      with      the      same      byte-pair      encoding  \\n \\n  as      GPT-2      (Radford      et      al.,      2019).\\nBased      on      the      results      in  \\n \\n  Section      §4,      we      use   \\n \\na      combination      of      text      infilling      and  \\n \\n  sentence      permutation.\\nWe      mask      30%      of      tokens      in      each  \\n \\n  document,      and      permute      all      sentences.\\nAlthough      sen-  \\n \\n  tence      permutation      only      shows      significant      additive      gains SQuAD      1.1      SQuAD      2.0      MNLI      SST      QQP      QNLI      STS-B      RTE      MRPC      CoLA  \\n \\n  EM/F1      EM/F1      m/mm      Acc      Acc      Acc      Acc      Acc      Acc      Mcc\\n \\n \\n  BERT      84.1/90.9      79.0/81.8      86.6/-      93.2      91.3      92.3      90.0      70.4      88.0      60.6  \\n \\n  UniLM      -/-      80.5/83.4      87.0/85.9      94.5   \\n \\n-      92.7   \\n \\n-      70.9   \\n \\n-      61.1  \\n \\n  XLNet      89.0/94.5      86.1/88.8      89.8/-      95.6      91.8      93.9      91.8      83.8      89.2      63.6  \\n \\n  RoBERTa      88.9/94.6      86.5/89.4      90.2/90.2      96.4      92.2      94.7      92.4      86.6      90.9      68.0  \\n \\n  BART      88.8/94.6      86.1/89.2      89.9/90.1      96.6      92.5      94.9      91.2      87.0      90.4      62.8 Table      2:      Results      for      large      models      on      SQUAD      and      GLUE      tasks.\\nBART      performs      comparably      to      ROBERTa      and  \\n \\n  XLNet,      suggesting      that      BART’s      uni-directional      decoder      layers      do      not      reduce      performance      on      discriminative      tasks.'}, {'metadata': {'source': 'https://arxiv.org/pdf/1910.13461.pdf', 'section_number': 9, 'section_title': '4.1      Comparison      Objectives'}, 'page_content': '4.1      Comparison      Objectives\\nives  \\n \\n  While      many      pre-training      objectives      have      been      pro-  \\n \\n  posed,      fair      comparisons      between      these      have      been      dif-  \\n \\n  ficult      to      perform,      at      least      in      part      due      to      differences      in  \\n \\n  training      data,      training      resources,      architectural      differ-  \\n \\n  ences      between      models,      and      fine-tuning      procedures.\\nWe  \\n \\n  ABCDE  \\n \\n  weee  \\n \\n  Pre-trained      Pre-trained  \\n \\n  Craters      Joo      rezapes cise)      Te      TrET Pre-trained  \\n \\n  Encoder Pre-trained  \\n \\n  Decoder aa      AB  \\n \\n  trrrt  \\n \\n  tr      ttt  \\n \\n  <s>A      BCD  \\n \\n  Randomly  \\n \\n  Initialized      Encoder  \\n \\n  TFT  \\n \\n  aBpByoe sentation      from      the      final      output      is      used.\\n \\n \\n  re-implement      strong      pre-training      approaches      recently  \\n \\n  proposed      for      discriminative      and      generation      tasks.\\nWe  \\n \\n  aim,      as      much      as      possible,      to      control      for      differences      un-  \\n \\n  related      to      the      pre-training      objective.\\nHowever,      we      do  \\n \\n  make      minor      changes      to      the      learning      rate      and      usage      of  \\n \\n  layer      normalisation      in      order      to      improve      performance  \\n \\n  (tuning      these      separately      for      each      objective).\\nFor      refer-  \\n \\n  ence,      we      compare      our      implementations      with      published  \\n \\n  numbers      from      BERT,      which      was      also      trained      for      1M  \\n \\n  steps      on   \\n \\na      combination      of      books      and      Wikipedia      data.\\n \\n \\n  We      compare      the      following      approaches:\\n \\n \\n  Language      Model      Similarly      to      GPT      (Radford      et      al.,  \\n \\n  2018),      we      train   \\n \\na      left-to-right      Transformer      language  \\n \\n  model.\\nThis      model      is      equivalent      to      the      BART      decoder,  \\n \\n  without      cross-attention.\\n \\n \\n  Permuted      Language      Model      Based      on      XLNet      (Yang  \\n \\n  et      al.,      2019),      we      sample      1/6      of      the      tokens,      and      gener-  \\n \\n  ate      them      in   \\n \\na      random      order      autoregressively.\\nFor      con-  \\n \\n  sistency      with      other      models,      we      do      not      implement      the  \\n \\n  relative      positional      embeddings      or      attention      across      seg-  \\n \\n  ments      from      XLNet.\\n \\n \\n  Masked      Language      Model      Following      BERT      (Devlin  \\n \\n  et      al.,      2019),      we      replace      15%      of      tokens      with      [MASK]  \\n \\n  symbols,      and      train      the      model      to      independently      predict  \\n \\n  the      original      tokens.\\n \\n \\n  Multitask      Masked      Language      Model      As      in      UniLM  \\n \\n  (Dong      et      al.,      2019),      we      train   \\n \\na      Masked      Language  \\n \\n  Model      with      additional      self-attention      masks.\\nSelf      at-  \\n \\n  tention      masks      are      chosen      randomly      in      with      the      follow  \\n \\n  proportions:      1/6      left-to-right,      1/6      right-to-left,      1/3      un-  \\n \\n  masked,      and      1/3      with      the      first      50%      of      tokens      unmasked  \\n \\n  and   \\n \\na      left-to-right      mask      for      the      remainder.\\n \\n \\n  Masked      Seq-to-Seq      Inspired      by      MASS      (Song      et      al.,  \\n \\n  2019),      we      mask   \\n \\na      span      containing      50%      of      tokens,  \\n \\n  and      train   \\n \\na      sequence      to      sequence      model      to      predict      the  \\n \\n  masked      tokens.\\n \\n \\n  For      the      Permuted      LM,      Masked      LM      and      Multitask  \\n \\n  Masked      LM,      we      use      two-stream      attention      (Yang      et      al.,  \\n \\n  2019)      to      efficiently      compute      likelihoods      of      the      output  \\n \\n  part      of      the      sequence      (using   \\n \\na      diagonal      self-attention  \\n \\n  mask      on      the      output      to      predict      words      left-to-right).\\n \\n \\n  We      experiment      with      (1)      treating      the      task      as   \\n \\na      stan-  \\n \\n  dard      sequence-to-sequence      problem,      where      the      source  \\n \\n  input      to      the      encoder      and      the      target      is      the      decoder      out-  \\n \\n  put,      or      (2)      adding      the      source      as      prefix      to      the      target      in  \\n \\n  the      decoder,      with   \\n \\na      loss      only      on      the      target      part      of      the  \\n \\n  sequence.\\nWe      find      the      former      works      better      for      BART  \\n \\n  models,      and      the      latter      for      other      models.\\n \\n \\n  To      most      directly      compare      our      models      on      their      ability  \\n \\n  to      model      their      fine-tuning      objective      (the      log      likelihood  \\n \\n  of      the      human      text),      we      report      perplexity      in      Table      1.'}, {'metadata': {'source': 'https://arxiv.org/pdf/1910.13461.pdf', 'section_number': 10, 'section_title': '4.2      Tasks'}, 'page_content': '4.2      Tasks\\nasks  \\n \\n  SQuAD  \\n \\n  (Rajpurkar      et      al.,      2016)a      an      extractive      ques-  \\n \\n  tion      answering      task      on      Wikipedia      paragraphs.\\nAnswers  \\n \\n  are      text      spans      extracted      from   \\n \\na      given      document      context.\\n \\n \\n  Similar      to      BERT      (Devlin      et      al.,      2019),      we      use      concate-  \\n \\n  nated      question      and      context      as      input      to      the      encoder      of  \\n \\n  BART,      and      additionally      pass      them      to      the      decoder.\\nThe  \\n \\n  model      includes      classifiers      to      predict      the      start      and      end  \\n \\n  indices      of      each      token.\\n \\n \\n  MNLI      (Williams      et      al.,      2017),   \\n \\na      bitext      classification  \\n \\n  task      to      predict      whether      one      sentence      entails      another.\\n \\n \\n  The      fine-tuned      model      concatenates      the      two      sentences  \\n \\n  with      appended      an      EOS      token,      and      passes      them      to      both  \\n \\n  the      BART      encoder      and      decoder.\\nIn      contrast      to      BERT,  \\n \\n  the      representation      of      the      EOS      token      is      used      to      classify  \\n \\n  the      sentences      relations.\\n \\n \\n  ELIS      (Fanetal.,      2019),   \\n \\na      long-form      abstractive      ques-  \\n \\n  tion      answering      dataset.\\nModels      generate      answers      con-  \\n \\n  ditioned      on      the      concatenation      of   \\n \\na      question      and      sup-  \\n \\n  porting      documents.\\nXSum_      (Narayan      et      al.,      2018),   \\n \\na      news      summarization  \\n \\n  dataset      with      highly      abstractive      summaries.\\nConvAI2_      (Dinan      et      al.,      2019),   \\n \\na      dialogue      response  \\n \\n  generation      task,      conditioned      on      context      and   \\n \\na      persona.\\n \\n \\n  CNN/DM_      (Hermann      et      al.,      2015),   \\n \\na      news      summa-  \\n \\n  rization      dataset.\\nSummaries      here      are      typically      closely  \\n \\n  related      to      source      sentences.'}, {'metadata': {'source': 'https://arxiv.org/pdf/1910.13461.pdf', 'section_number': 11, 'section_title': '4.3      Results'}, 'page_content': '4.3      Results\\nults Results      are      shown      in      Table      1.\\nSeveral      trends      are      clear:\\nModel      SQuAD      1.1      MNLI      ELIS      XSum   \\n \\n_      ConvAI2      CNN/DM  \\n \\n  Fl      Acc      PPL      PPL      PPL      PPL BERT      Base      (Devlin      et      al.,      2019)      88.5      84.3   \\n \\n-   \\n \\n-   \\n \\n-   \\n \\n-       Masked      Language      Model      90.0      83.5      24.77      7.87      12.59      7.06  \\n \\n  Masked      Seq2seq      87.0      82.1      23.40      6.80      11.43      6.19  \\n \\n  Language      Model      76.7      80.1      21.40      7.00      11.51      6.56  \\n \\n  Permuted      Language      Model      89.1      83.7      24.03      7.69      12.23      6.96  \\n \\n  Multitask      Masked      Language      Model      89.2      82.4      23.73      7.50      12.39      6.74  \\n \\n  BART      Base  \\n \\n  w/      Token      Masking      90.4      84.1      25.05      7.08      11.73      6.10  \\n \\n  w/      Token      Deletion      90.4      84.1      24.61      6.90      11.46      5.87  \\n \\n  w/      Text      Infilling      90.8      84.0      24.26      6.61      11.05      5.83  \\n \\n  w/      Document      Rotation      77.2      75.3      53.69      17.14      19.87      10.59  \\n \\n  w/      Sentence      Shuffling      85.4      81.5      41.87      10.93      16.67      7.89  \\n \\n  w/      Text      Infilling   \\n \\n+      Sentence      Shuffling      90.8      83.8      24.17      6.62      11.12      5.41\\nconsistently      strong      performance.\\n \\n \\n  Performance      of      pre-training      methods      varies      signifi-  \\n \\n  cantly      across      tasks      The      effectiveness      of      pre-training  \\n \\n  methods      is      highly      dependent      on      the      task.\\nFor      exam-  \\n \\n  ple,   \\n \\na      simple      language      model      achieves      the      best      ELIS  \\n \\n  performance,      but      the      worst      SQUAD      results.\\n \\n \\n  Token      masking      is      crucial      Pre-training      objectives  \\n \\n  based      on      rotating      documents      or      permuting      sentences  \\n \\n  perform      poorly      in      isolation.\\nThe      successful      methods  \\n \\n  either      use      token      deletion      or      masking,      or      self-attention  \\n \\n  masks.\\nDeletion      appears      to      outperform      masking      on  \\n \\n  generation      tasks.\\n \\n \\n  Left-to-right      pre-training      improves      generation  \\n \\n  The      Masked      Language      Model      and      the      Permuted  \\n \\n  Language      Model      perform      less      well      than      others      on  \\n \\n  generation,      and      are      the      only      models      we      consider      that  \\n \\n  do      not      include      left-to-right      auto-regressive      language  \\n \\n  modelling      during      pre-training.\\n \\n \\n  Bidirectional      encoders      are      crucial      for      SQUAD      As  \\n \\n  noted      in      previous      work      (Devlin      et      al.,      2019),      just  \\n \\n  left-to-right      decoder      performs      poorly      on      SQuAD,      be-  \\n \\n  cause      future      context      is      crucial      in      classification      deci-  \\n \\n  sions.\\nHowever,      BART      achieves      similar      performance  \\n \\n  with      only      half      the      number      of      bidirectional      layers.\\n \\n \\n  The      pre-training      objective      is      not      the      only      important  \\n \\n  factor      Our      Permuted      Language      Model      performs      less  \\n \\n  well      than      XLNet      (Yang      et      al.,      2019).\\nSome      of      this      dif-  \\n \\n  ference      is      likely      due      to      not      including      other      architectural  \\n \\n  improvements,      such      as      relative-position      embeddings      or  \\n \\n  segment-level      recurrence.\\n \\n \\n  Pure      language      models      perform      best      on      ELIS      The  \\n \\n  ELIS      dataset      is      an      outlier,      with      much      higher      perplex-  \\n \\n  ities      than      other      tasks,      and      is      the      only      generation      task  \\n \\n  where      other      models      outperform      BART.\\n \\n \\nA      pure      lan-  \\n \\n  guage      model      performs      best,      suggesting      that      BART      is  \\n \\n  less      effective      when      the      output      is      only      loosely      con-  \\n \\n  strained      by      the      input.\\n \\n \\n  BART      achieves      the      most      consistently      strong      perfor-  \\n \\n  mance.\\nWith      the      exception      of      ELI5,      BART      models  \\n \\n  using      text-infilling      perform      well      on      all      tasks.\\n5\\n \\n   Large-scale      Pre-training      Experiments  \\n \\n  Recent      work      has      shown      that      downstream      performance  \\n \\n  can      dramatically      improve      when      pre-training      is      scaled  \\n \\n  to      large      batch      sizes      (Yang      et      al.,      2019;      Liu      et      al.,      2019)  \\n \\n  and      corpora.\\nTo      test      how      well      BART      performs      in      this  \\n \\n  regime,      and      to      create   \\n \\na      useful      model      for      downstream  \\n \\n  tasks,      we      trained      BART      using      the      same      scale      as      the  \\n \\n  RoBERTa      model.'}, {'metadata': {'source': 'https://arxiv.org/pdf/1910.13461.pdf', 'section_number': 12, 'section_title': '5.1      Experimental      Setup'}, 'page_content': '5.1      Experimental      Setup\\netup  \\n \\n  We      pre-train   \\n \\na      large      model      with      12      layers      in      each      of      the  \\n \\n  encoder      and      decoder,      and   \\n \\na      hidden      size      of      1024.\\nFol-  \\n \\n  lowing      RoBERTa      (Liu      et      al.,      2019),      we      use   \\n \\na      batch      size  \\n \\n  of      8000,      and      train      the      model      for      500000      steps.\\nDocu-  \\n \\n  ments      are      tokenized      with      the      same      byte-pair      encoding  \\n \\n  as      GPT-2      (Radford      et      al.,      2019).\\nBased      on      the      results      in  \\n \\n  Section      §4,      we      use   \\n \\na      combination      of      text      infilling      and  \\n \\n  sentence      permutation.\\nWe      mask      30%      of      tokens      in      each  \\n \\n  document,      and      permute      all      sentences.\\nAlthough      sen-  \\n \\n  tence      permutation      only      shows      significant      additive      gains SQuAD      1.1      SQuAD      2.0      MNLI      SST      QQP      QNLI      STS-B      RTE      MRPC      CoLA  \\n \\n  EM/F1      EM/F1      m/mm      Acc      Acc      Acc      Acc      Acc      Acc      Mcc\\n \\n \\n  BERT      84.1/90.9      79.0/81.8      86.6/-      93.2      91.3      92.3      90.0      70.4      88.0      60.6  \\n \\n  UniLM      -/-      80.5/83.4      87.0/85.9      94.5   \\n \\n-      92.7   \\n \\n-      70.9   \\n \\n-      61.1  \\n \\n  XLNet      89.0/94.5      86.1/88.8      89.8/-      95.6      91.8      93.9      91.8      83.8      89.2      63.6  \\n \\n  RoBERTa      88.9/94.6      86.5/89.4      90.2/90.2      96.4      92.2      94.7      92.4      86.6      90.9      68.0  \\n \\n  BART      88.8/94.6      86.1/89.2      89.9/90.1      96.6      92.5      94.9      91.2      87.0      90.4      62.8 Table      2:      Results      for      large      models      on      SQUAD      and      GLUE      tasks.\\nBART      performs      comparably      to      ROBERTa      and  \\n \\n  XLNet,      suggesting      that      BART’s      uni-directional      decoder      layers      do      not      reduce      performance      on      discriminative      tasks.'}, {'metadata': {'source': 'https://arxiv.org/pdf/1910.13461.pdf', 'section_number': 13, 'section_title': 'CNN/DailyMail      XSum       RI      R2      RL      RI      R2      RL'}, 'page_content': 'CNN/DailyMail      XSum       RI      R2      RL      RI      R2      RL\\n \\n \\n  Lead-3      40.42      17.62      36.67      16.30      1.60      11.95  \\n \\n  PTGEN      (See      et      al.,      2017)      36.44      15.66      33.42      29.70      9.21      23.24  \\n \\n  PTGEN+COV      (See      et      al.,      2017)      39.53      17.28      36.38      28.10      8.02      21.72  \\n \\n  UniLM      43.33      20.21      40.51   \\n \\n-   \\n \\n-   \\n \\n-       BERTSUMABS      (Liu   \\n \\n&      Lapata,      2019)      41.72      19.39      38.76      38.76      16.33      31.15  \\n \\n  BERTSUMEXTABS      (Liu   \\n \\n&      Lapata,      2019)      42.13      19.60      39.18      38.81      16.50      31.27 BART      44.16      21.28      40.90      45.14      22.27      37.25 Table      3:      Results      on      two      standard      summarization      datasets.\\nBART      outperforms      previous      work      on      summarization      on\\n \\n \\n  on      the      CNN/DM      summarization      dataset,      we      hypothe-      ConvAI2  \\n \\n  sised      that      larger      pre-trained      models      may      be      better      able      Valid      F1      Valid      PPL  \\n \\n  to   \\n \\n|      fi      this      task.\\nTo      help      th      del      better      fit      th  \\n \\n  RE      aa      ae      Pai      Cae      Seq2Seq+      Attention      16.02      35.07  \\n \\n  data,      we      disabled      dropout      for      the      final      10%      of      training   \\n \\n.    \\n.\\n     Best      System      19.09      17.51  \\n \\n  steps.\\nWe      use      the      same      pre-training      data      as      Liu      et      al.\\n(2019),      consisting      of      160Gb      of      news,      books,      stories,  \\n \\n  and      web      text.\\nBART      20.72      11.85 5.2      Discriminative      Tasks  \\n \\n  Table   \\n \\n2      compares      the      performance      of      BART      with      sev-  \\n \\n  eral      recent      approaches      on      the      well-studied      SQUAD      and  \\n \\n  GLUE      tasks      (Warstadt      et      al.,      2018;      Socher      et      al.,      2013;  \\n \\n  Dolan   \\n \\n&      Brockett,      2005;      Agirre      et      al.,      2007;      Williams  \\n \\n  et      al.,      2018;      Dagan      et      al.,      2006;      Levesque      et      al.,      2011).\\n \\n \\n  Table      4:      BART      outperforms      previous      work      on      conver-  \\n \\n  sational      response      generation.\\nPerplexities      are      renor-  \\n \\n  malized      based      on      official      tokenizer      for      ConvAI2.\\n \\n \\n  The      most      directly      comparable      baseline      is      ROBERTa,  \\n \\n  which      was      pre-trained      with      the      same      resources,      but  \\n \\n  a      different      objective.\\nOverall,      BART      performs      simi-  \\n \\n  larly,      with      only      small      differences      between      the      models  \\n \\n  on      most      tasks.\\nsuggesting      that      BART’s      improvements  \\n \\n  on      generation      tasks      do      not      come      at      the      expense      of      clas-  \\n \\n  sification      performance.\\n \\n \\n  Summarization      To      provide   \\n \\na      comparison      with      the  \\n \\n  state-of-the-art      in      summarization,      we      present      results  \\n \\n  on      two      summarization      datasets,      CNN/DailyMail      and  \\n \\n  XSum,      which      have      distinct      properties.\\n \\n \\n  Summaries      in      the      CNN/DailyMail      tend      to      resemble  \\n \\n  source      sentences.\\nExtractive      models      do      well      here,      and  \\n \\n  even      the      baseline      of      the      first-three      source      sentences      is  \\n \\n  highly      competitive.\\nNevertheless,      BART      outperforms  \\n \\n  all      existing      work.\\n5.3.      Generation      Tasks\\nGeneration      Tasks\\n |       We      also      experiment      with      several      text      generation      tasks.       BART      is      fine-tuned      as      a      standard      sequence-to-sequence       model      from      the      input      to      the      output      text.      During      fine-       tuning      we      use      a      label      smoothed      cross      entropy      loss       (Pereyra      et      al.,      2017),      with      the      smoothing      parameter       set      to      0.1.      During      generation,      we      set      beam      size      as      5,       remove      duplicated      trigrams      in      beam      search,      and      tuned       the      model      with      min-len,      max-len,      length      penalty      on      the       validation      set      (Fan      et      al.,      2017). | 6.0      points      on      all      ROUGE      metrics—trepresenting   \\n \\na      sig-  \\n \\n  nificant      advance      in      performance      on      this      problem.\\nQual-  \\n \\n  itatively,      sample      quality      is      high      (see      86).\\n \\n \\n  In      contrast,      XSum      is      highly      abstractive,      and      extrac-  \\n \\n  tive      models      perform      poorly.\\nBART      outperforms      the  \\n \\n  best      previous      work,      which      leverages      BERT,      by      roughly  \\n \\n  Dialogue      We      evaluate      dialogue      response      generation  \\n \\n  on      CONVAI2      (Dinan      et      al.,      2019),      in      which      agents  \\n \\n  must      generate      responses      conditioned      on      both      the      pre-  \\n \\n  vious      context      and   \\n \\na      textually-specified      persona.\\nBART  \\n \\n  outperforms      previous      work      on      two      automated      metrics.\\n'}, {'metadata': {'source': 'https://arxiv.org/pdf/1910.13461.pdf', 'section_number': 14, 'section_title': '5.3.      Generation      Tasks'}, 'page_content': '5.3.      Generation      Tasks\\nGeneration      Tasks\\n |       We      also      experiment      with      several      text      generation      tasks.       BART      is      fine-tuned      as      a      standard      sequence-to-sequence       model      from      the      input      to      the      output      text.      During      fine-       tuning      we      use      a      label      smoothed      cross      entropy      loss       (Pereyra      et      al.,      2017),      with      the      smoothing      parameter       set      to      0.1.      During      generation,      we      set      beam      size      as      5,       remove      duplicated      trigrams      in      beam      search,      and      tuned       the      model      with      min-len,      max-len,      length      penalty      on      the       validation      set      (Fan      et      al.,      2017). | 6.0      points      on      all      ROUGE      metrics—trepresenting   \\n \\na      sig-  \\n \\n  nificant      advance      in      performance      on      this      problem.\\nQual-  \\n \\n  itatively,      sample      quality      is      high      (see      86).\\n \\n \\n  In      contrast,      XSum      is      highly      abstractive,      and      extrac-  \\n \\n  tive      models      perform      poorly.\\nBART      outperforms      the  \\n \\n  best      previous      work,      which      leverages      BERT,      by      roughly  \\n \\n  Dialogue      We      evaluate      dialogue      response      generation  \\n \\n  on      CONVAI2      (Dinan      et      al.,      2019),      in      which      agents  \\n \\n  must      generate      responses      conditioned      on      both      the      pre-  \\n \\n  vious      context      and   \\n \\na      textually-specified      persona.\\nBART  \\n \\n  outperforms      previous      work      on      two      automated      metrics.\\n'}, {'metadata': {'source': 'https://arxiv.org/pdf/1910.13461.pdf', 'section_number': 15, 'section_title': 'Rl      R2      RL'}, 'page_content': 'Rl      R2      RL\\n \\n \\n  Best      Extractive      23.55      3.1      17.5  \\n \\n  Language      Model      27.8      47      23.1  \\n \\n  Seq2Seq      28.3      5.1      22.8  \\n \\n  Seq2Seq      Multitask      28.9      54      23.1  \\n \\n  BART      30.6      6.2      24.3  \\n \\n  Table      5:      BART      achieves      state-of-the-art      results      on  \\n \\n  the      challenging      ELI5      abstractive      question      answering  \\n \\n  dataset.\\nComparison      models      are      from      Fan      et      al.\\n(2019).'}, {'metadata': {'source': 'https://arxiv.org/pdf/1910.13461.pdf', 'section_number': 16, 'section_title': 'RO-EN'}, 'page_content': 'RO-EN\\nBaseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96\\n \\n \\n  Table      6:      The      performance      (BLEU)      of      baseline      and  \\n \\n  BART      on      WMT’16      RO-EN      augmented      with      back-  \\n \\n  translation      data.\\nBART      improves      over   \\n \\na      strong      back-  \\n \\n  translation      (BT)      baseline      by      using      monolingual      English  \\n \\n  pre-training.\\n \\n \\n  Abstractive      QA      We      use      the      recently      proposed      ELIS  \\n \\n  dataset      to      test      the      model’s      ability      to      generate      long      free-  \\n \\n  form      answers.\\nWe      find      BART      outperforms      the      best      pre-  \\n \\n  vious      work      by      1.2      ROUGE-L,      but      the      dataset      remains  \\n \\n  a      challenging,      because      answers      are      only      weakly      speci-  \\n \\n  fied      by      the      question.\\n5.4      Translation\\ntion  \\n \\n  We      also      evaluated      performance      on      WMT16      Romanian-  \\n \\n  English,      augmented      with      back-translation      data  \\n \\n  from      Sennrich      et      al.\\n(2016).\\nWe      use   \\n \\na      6-layer  \\n \\n  transformer      source      encoder      to      map      Romanian      into  \\n \\n  a      representation      that      BART      is      able      to      de-noise      into  \\n \\n  English,      following      the      approach      introduced      in      83.4.  \\n \\n  Experiment      results      are      presented      in      Table      6.      We  \\n \\n  compare      our      results      against   \\n \\na      baseline      Transformer  \\n \\n  architecture      (Vaswani      et      al.,      2017)      with      Transformer-  \\n \\n  large      settings      (the      baseline      row).\\nWe      show      the  \\n \\n  performance      of      both      steps      of      our      model      in      the      fixed  \\n \\n  BART      and      tuned      BART      rows.\\nFor      each      row      we  \\n \\n  experiment      on      the      original      WMT16      Romanian-English  \\n \\n  augmented      with      back-translation      data.\\nWe      use   \\n \\na       beam      width      of   \\n \\n5      and   \\n \\na      length      penalty      of   \\n \\na   \\n \\n=      1.  \\n \\n  Preliminary      results      suggested      that      our      approach      was  \\n \\n  less      effective      without      back-translation      data,      and      prone  \\n \\n  to      overfitting—future      work      should      explore      additional  \\n \\n  regularization      techniques.\\n6\\n \\n   Qualitative      Analysis  \\n \\n  BART      shows      large      improvements      on      summarization  \\n \\n  metrics,      of      up      to   \\n \\n6      points      over      the      prior      state-of-the-art.\\n \\n \\n  To      understand      BART’s      performance      beyond      automated  \\n \\n  metrics,      we      analyse      its      generations      qualitatively.\\n \\n \\n  Table   \\n \\n7      shows      example      summaries      generated      by  \\n \\n  BART.\\nExamples      are      taken      from      WikiNews      articles  \\n \\n  published      after      the      creation      of      the      pre-training      corpus,  \\n \\n  to      eliminate      the      possibility      of      the      events      described      be-  \\n \\n  ing      present      in      the      model’s      training      data.\\nFollowing  \\n \\n  Narayan      et      al.\\n(2018),      we      remove      the      first      sentence      of  \\n \\n  the      article      prior      to      summarizing      it,      so      there      is      no      easy  \\n \\n  extractive      summary      of      the      document.\\n \\n \\n  Unsurprisingly,      model      output      is      fluent      and      grammat-  \\n \\n  ical      English.\\nHowever,      model      output      is      also      highly      ab-  \\n \\n  stractive,      with      few      phrases      copied      from      the      input.\\nThe  \\n \\n  output      is      also      generally      factually      accurate,      and      inte-  \\n \\n  grates      supporting      evidence      from      across      the      input      doc-  \\n \\n  ument      with      background      knowledge      (for      example,      cor-  \\n \\n  rectly      completing      names,      or      inferring      that      PG&E      oper-  \\n \\n  ates      in      California).\\nIn      the      first      example,      inferring      that  \\n \\n  fish      are      protecting      reefs      from      global      warming      requires  \\n \\n  non-trivial      inference      from      the      text.\\nHowever,      the      claim  \\n \\n  that      the      work      was      published      in      Science      is      not      supported  \\n \\n  by      the      source.\\n \\n \\n  These      samples      demonstrate      that      the      BART      pretrain-  \\n \\n  ing      has      learned   \\n \\na      strong      combination      of      natural      lan-  \\n \\n  guage      understanding      and      generation.\\n7\\n \\n   Related      Work  \\n \\n  Early      methods      for      pretraining      were      based      on      language  \\n \\n  models.\\nGPT      (Radford      et      al.,      2018)      only      models      left-  \\n \\n  ward      context,      which      is      problematic      for      some      tasks.\\n \\n \\n  ELMo      (Peters      et      al.,      2018)      concatenates      left-only      and  \\n \\n  right-only      representations,      but      does      not      pre-train      inter-  \\n \\n  actions      between      these      features.\\nRadford      et      al.\\n(2019)  \\n \\n  demonstrated      that      very      large      language      models      can      act       as      unsupervised      multitask      models.\\n \\n \\n  MASS      (Song      et      al.,      2019)      is      perhaps      the      most      similar  \\n \\n  model      to      BART.\\nAn      input      sequence      where   \\n \\na      contiguous  \\n \\n  span      of      tokens      is      masked      is      mapped      to   \\n \\na      sequence      con-  \\n \\n  sisting      of      the      missing      tokens.\\nMASS      is      less      effective  \\n \\n  for      discriminative      tasks,      because      disjoint      sets      of      tokens  \\n \\n  are      fed      into      the      encoder      and      decoder.\\n \\n \\n  BERT      (Devlin      et      al.,      2019)      introduced      masked      lan-  \\n \\n  guage      modelling,      which      allows      pre-training      to      learn      in-  \\n \\n  teractions      between      left      and      right      context      words.\\nRe-  \\n \\n  cent      work      has      shown      that      very      strong      performance      can  \\n \\n  be      achieved      by      training      for      longer      (Liu      et      al.,      2019),  \\n \\n  by      tying      parameters      across      layers      (Lan      et      al.,      2019),  \\n \\n  and      by      masking      spans      instead      of      words      (Joshi      et      al.,  \\n \\n  2019).\\nPredictions      are      not      made      auto-regressively,      re-  \\n \\n  ducing      the      effectiveness      of      BERT      for      generation      tasks.\\n \\n \\n  UniLM      (Dong      et      al.,      2019)      fine-tunes      BERT      with      an  \\n \\n  ensemble      of      masks,      some      of      which      allow      only      leftward  \\n \\n  context.\\nLike      BART,      this      allows      UniLM      to      be      used      for  \\n \\n  both      generative      and      discriminative      tasks.\\n \\n \\nA      difference  \\n \\n  is      that      UniLM      predictions      are      conditionally      indepen-  \\n \\n  dent,      whereas      BART’s      are      autoregressive.\\nBART      re-  \\n \\n  duces      the      mismatch      between      pre-training      and      genera-  \\n \\n  tion      tasks,      because      the      decoder      is      always      trained      on      un-  \\n \\n  corrupted      context.\\n \\n \\n  XL-Net      (Yang      et      al.,      2019)      extends      BERT      by      pre-\\n |       Source      Document      (abbreviated) |       BART      Summary\\n |       The      researchers      examined      three      types      of      coral      in      reefs      off      the coast of Fiji The researchers found when fish were plentiful,       they      would      eat      algae      and      seaweed      off      the      corals,      which      appeared       to      leave      them      more      resistant      to      the      bacterium      Vibrio      coralliilyti-       cus,      a      bacterium      associated      with      bleaching.      The      researchers      sug-       gested      the      algae,      like      warming      temperatures,      might      render      the       corals’      chemical      defenses      less      effective,      and      the      fish      were      pro-       tecting      the      coral      by      removing      the      algae. |       Fisheries      off      the      coast      of      Fiji      are      protect-       ing      coral      reefs      from      the      effects      of      global       warming,      according      to      a      study      in      the      jour-       nal      Science.\\n |       Sacoolas,      who      has      immunity      as      a      diplomat’s      wife,      was      involved in a traffic collision Prime Minister Johnson was questioned       about      the      case      while      speaking      to      the      press      at      a      hospital      in      Wat- ford. He said, “I hope that Anne Sacoolas will come back       if      we      can’t      resolve      it      then      of      course      I      will      be      raising      it      myself       personally      with      the      White      House.” |       Boris      Johnson      has      said      he      will      raise      the      is-       sue      of      US      diplomat      Anne      Sacoolas’      diplo-       matic      immunity      with      the      White      House.\\n |       According      to      Syrian      state      media,      government      forces      began      de- ploying into previously SDF controlled territory yesterday.       On      October      6,      US      President      Donald      Trump      and      Turkish      Presi-       dent      Recep      Tayyip      Erdoan      spoke      on      the      phone.      Then      both      na-       tions      issued      statements      speaking      of      an      imminent      incursion      into northeast Syria On Wednesday, Turkey began a military       offensive      with      airstrikes      followed      by      a      ground      invasion. |       Syrian      government      forces      have      entered       territory      held      by      the      US-backed      Syrian       Democratic      Forces      (SDF)      in      response      to       Turkey’s      incursion      into      the      region.\\n |       This      is      the      first      time      anyone      has      been      recorded      to      run      a      full       marathon      of      42.195      kilometers      (approximately      26      miles)      under       this      pursued      landmark      time.      It      was      not,      however,      an      officially       sanctioned      world      record,      as      it      was      not      an      ’open      race”      of      the       IAAF.      His      time      was      1      hour      59      minutes      40.2      seconds.      Kipchoge       ran      in      Vienna,      Austria.      It      was      an      event      specifically      designed      to       help      Kipchoge      break      the      two      hour      barrier. |       Kenyan      runner      Eliud      Kipchoge      has      run      a       marathon      in      less      than      two      hours.\\n |       PG&E      stated      it      scheduled      the      blackouts      in      response      to      forecasts       for      high      winds      amid      dry      conditions.      The      aim      is      to      reduce      the      risk       of      wildfires.      Nearly      800      thousand      customers      were      scheduled      to       be      affected      by      the      shutoffs      which      were      expected      to      last      through       at      least      midday      tomorrow. |       Power      has      been      turned      off      to      millions      of       customers      in      California      as      part      of      a      power       shutoff      plan.\\n\\n \\n \\n  dicting      masked      tokens      auto-regressively      in   \\n \\na      permuted  \\n \\n  order.\\nThis      objective      allows      predictions      to      condition      on  \\n \\n  both      left      and      right      context.\\nIn      contrast,      the      BART      de-  \\n \\n  coder      works      left-to-right      during      pre-training,      matching  \\n \\n  the      setting      during      generation.\\n \\n \\n  Several      papers      have      explored      using      pre-trained      rep-  \\n \\n  resentations      to      improve      machine      translation.\\nThe  \\n \\n  largest      improvements      have      come      from      pre-training      on  \\n \\n  both      source      and      target      languages      (Song      et      al.,      2019;  \\n \\n  Lample   \\n \\n&      Conneau,      2019),      but      this      requires      pre-  \\n \\n  training      on      all      languages      of      interest.\\nOther      work      has  \\n \\n  shown      that      encoders      can      be      improved      using      pre-trained  \\n \\n  representations      (Edunov      et      al.,      2019),      but      gains      in      de-  \\n \\n  coders      are      more      limited.\\nWe      show      how      BART      can      be  \\n \\n  used      to      improve      machine      translation      decoders.\\n8\\n \\n   Conclusions  \\n \\n  We      introduced      BART,   \\n \\na      pre-training      approach      that  \\n \\n  learns      to      map      corrupted      documents      to      the      original.\\n \\n \\n  BART      achieves      similar      performance      to      ROBERTa      on  \\n \\n  discriminative      tasks,      while      achieving      new      state-of-the-  \\n \\n  art      results      on   \\n \\na      number      of      text      generation      tasks.\\nFu-  \\n \\n  ture      work      should      explore      new      methods      for      corrupting  \\n \\n  documents      for      pre-training,      perhaps      tailoring      them      to  \\n \\n  specific      end      tasks.\\nReferences\\n \\n \\n  Eneko      Agirre,      Llu’is      M‘arquez,      and      Richard      Wicen-  \\n \\n  towski      (eds.).\\nProceedings      of      the      Fourth      Interna-  \\n \\n  tional      Workshop      on      Semantic      Evaluations      (SemEval-  \\n \\n  2007).\\nAssociation      for      Computational      Linguistics,  \\n \\n  Prague,      Czech      Republic,      June      2007.\\n \\n \\n  Ido      Dagan,      Oren      Glickman,      and      Bernardo      Magnini.\\n \\n \\n  The      PASCAL      recognising      textual      entailment      chal-  \\n \\n  lenge.\\nIn      Machine      learning      challenges.\\nevaluat-  \\n \\n  ing      predictive      uncertainty,      visual      object      classifica-  \\n \\n  tion,      and      recognising      tectual      entailment,      pp.\\n177—  \\n \\n  190.\\nSpringer,      2006.\\n \\n \\n  Jacob      Devlin,      Ming-Wei      Chang,      Kenton      Lee,      and  \\n \\n  Kristina      Toutanova.\\nBERT:      Pre-training      of      deep  \\n \\n  bidirectional      transformers      for      language      understand-  \\n \\n  ing.\\nIn      Proceedings      of      the      2019      Conference      of      the  \\n \\n  North      American      Chapter      of      the      Association      for      Com-  \\n \\n  putational      Linguistics:      Human      Language      Technolo-  \\n \\n  gies,      Volume   \\n \\nI      (Long      and      Short      Papers),      pp.\\n4171-  \\n \\n  4186,      Minneapolis,      Minnesota,      June      2019.\\nAssocia-  \\n \\n  tion      for      Computational      Linguistics.\\ndoi:      10.18653/  \\n \\n  vI/N19-1423.\\nURL      https://www.aclweb.\\n \\n \\n  org/anthology/N19-1423.\\n \\n \\n  Emily      Dinan,      Varvara      Logacheva,      Valentin      Malykh,  \\n \\n  Alexander      Miller,      Kurt      Shuster,      Jack      Urbanek,  \\n \\n  Douwe      Kiela,      Arthur      Szlam,      Iulian      Serban,      Ryan  \\n \\n  Lowe,      et      al.\\nThe      second      conversational      in-  \\n \\n  telligence      challenge      (convai2).\\narXiv      preprint  \\n \\n  arXiv:      1902.00098,      2019.\\n \\n \\n  William   \\n \\nB      Dolan      and      Chris      Brockett.\\nAutomatically  \\n \\n  constructing   \\n \\na      corpus      of      sentential      paraphrases.\\nIn  \\n \\n  Proceedings      of      the      International      Workshop      on      Para-  \\n \\n  phrasing,      2005.\\n \\n \\n  Li      Dong,      Nan      Yang,      Wenhui      Wang,      Furu      Wei,      Xi-  \\n \\n  aodong      Liu,      Yu      Wang,      Jianfeng      Gao,      Ming      Zhou,  \\n \\n  and      Hsiao-Wuen      Hon.\\nUnified      language      model      pre-  \\n \\n  training      for      natural      language      understanding      and      gen-  \\n \\n  eration.\\narXiv      preprint      arXiv:      1905.03197,      2019.\\n \\n \\n  Sergey      Edunov,      Alexei      Baevski,      and      Michael      Auli.\\n \\n \\n  Pre-trained      language      model      representations      for      lan-  \\n \\n  guage      generation.\\nIn      Proceedings      of      the      2019      Con-  \\n \\n  ference      of      the      North      American      Chapter      of      the      Asso-  \\n \\n  ciation      for      Computational      Linguistics:      Human      Lan-  \\n \\n  guage      Technologies,      Volume   \\n \\n1      (Long      and      Short      Pa-  \\n \\n  pers),      2019.\\n \\n \\n  Angela      Fan,      David      Grangier,      and      Michael      Auli.\\nCon-  \\n \\n  trollable      abstractive      summarization.\\narXiv      preprint  \\n \\n  arXiv:      1711.05217,      2017.\\n \\n \\n  Angela      Fan,      Yacine      Jernite,      Ethan      Perez,      David  \\n \\n  Grangier,      Jason      Weston,      and      Michael      Auli.\\nEli5:  \\n \\n  Long      form      question      answering.\\narXiv      preprint  \\n \\n  arXiv:      1907.09190,      2019.\\n \\n \\n  Dan      Hendrycks      and      Kevin      Gimpel.\\nGaussian      error      lin-  \\n \\n  ear      units      (gelus).\\narXiv      preprint      arXiv:      1606.08415,  \\n \\n  2016.\\n \\n \\n  Karl      Moritz      Hermann,      Tomas      Kocisky,      Edward  \\n \\n  Grefenstette,      Lasse      Espeholt,      Will      Kay,      Mustafa      Su-  \\n \\n  leyman,      and      Phil      Blunsom.\\nTeaching      machines      to  \\n \\n  read      and      comprehend.\\nIn      Advances      in      neural      infor-  \\n \\n  mation      processing      systems,      pp.\\n1693-1701,      2015.\\n \\n \\n  Mandar      Joshi,      Danqi      Chen,      Yinhan      Liu,      Daniel   \\n \\nS      Weld,  \\n \\n  Luke      Zettlemoyer,      and      Omer      Levy.\\nSpanbert:      Im-  \\n \\n  proving      pre-training      by      representing      and      predicting  \\n \\n  spans.\\narXiv      preprint      arXiv:      1907.10529,      2019.\\n \\n \\n  Guillaume      Lample      and      Alexis      Conneau.\\n \\n \\n—      Cross-  \\n \\n  lingual      language      model      pretraining.\\narXiv      preprint  \\n \\n  arXiv:      1901.07291,      2019.\\n \\n \\n  Zhenzhong      Lan,      Mingda      Chen,      Sebastian      Goodman,  \\n \\n  Kevin      Gimpel,      Piyush      Sharma,      and      Radu      Sori-  \\n \\n  cut.\\nAlbert:   \\n \\nA      lite      bert      for      self-supervised      learn-  \\n \\n  ing      of      language      representations.\\narXiv      preprint  \\n \\n  arXiv:      1909.11942,      2019.\\n \\n \\n  Hector   \\n \\nJ      Levesque,      Ernest      Davis,      and      Leora      Morgen-  \\n \\n  stern.\\nThe      Winograd      schema      challenge.\\nIn      AAAI  \\n \\n  Spring      Symposium:      Logical      Formalizations      of      Com-  \\n \\n  monsense      Reasoning,      volume      46,      pp.\\n47,      2011.\\n \\n \\n  Yang      Liu      and      Mirella      Lapata.\\n \\n \\n  tion      with      pretrained      encoders.\\n \\n \\n  arXiv:      1908.08345,      2019.  \\n \\n  Text      summariza-  \\n \\n  arXiv      preprint  \\n \\n  Yinhan      Liu,      Myle      Ott,      Naman      Goyal,      Jingfei      Du,      Man-  \\n \\n  dar      Joshi,      Dangi      Chen,      Omer      Levy,      Mike      Lewis,  \\n \\n  Luke      Zettlemoyer,      and      Veselin      Stoyanov.\\nRoberta:\\n \\n \\n \\nA      robustly      optimized      bert      pretraining      approach.\\n \\n \\n  arXiv      preprint      arXiv:      1907.11692,      2019.\\n \\n \\n  Tomas      Mikolov,      Kai      Chen,      Greg      Corrado,      and      Jeffrey  \\n \\n  Dean.\\nEfficient      estimation      of      word      representations  \\n \\n  in      vector      space.\\narXiv      preprint      arXiv:1301.3781,  \\n \\n  2013.\\n \\n \\n  Shashi      Narayan,      Shay   \\n \\nB      Cohen,      and      Mirella      Lapata.\\n \\n \\n  Don’t      give      me      the      details,      just      the      summary!\\ntopic-  \\n \\n  aware      convolutional      neural      networks      for      extreme  \\n \\n  summarization.\\narXiv      preprint      arXiv:      1808.08745,  \\n \\n  2018.\\n \\n \\n  Gabriel      Pereyra,      George      Tucker,      Jan      Chorowski,  \\n \\n  Lukasz      Kaiser,      and      Geoffrey      Hinton.\\nRegularizing  \\n \\n  neural      networks      by      penalizing      confident      output      dis-  \\n \\n  tributions.\\narXiv      preprint      arXiv:      1701.06548,      2017.\\n \\n \\n  Matthew      E      Peters,      Mark      Neumann,      Mohit      Iyyer,      Matt  \\n \\n  Gardner,      Christopher      Clark,      Kenton      Lee,      and      Luke  \\n \\n  Zettlemoyer.\\nDeep      contextualized      word      representa-  \\n \\n  tions.\\narXiv      preprint      arXiv:      1802.05365,      2018.\\n \\n \\n  Alec      Radford,      Karthik      Narasimhan,      Tim      Salimans,  \\n \\n  and      Ilya      Sutskever.\\nImproving      language      un-  \\n \\n  derstanding      by      generative      pre-training.\\nURL  \\n \\n  https://s3-us-west-2.\\n \\n \\n|      amazonaws.\\n \\n \\n—      com/openai-  \\n \\n  assets/researchcovers/languageunsupervised/language  \\n \\n  understanding      paper.\\npdf,      2018.\\n \\n \\n  Alec      Radford,      Jeffrey      Wu,      Rewon      Child,      David      Luan,  \\n \\n  Dario      Amodei,      and      Ilya      Sutskever.\\nLanguage      mod-  \\n \\n  els      are      unsupervised      multitask      learners.\\nOpenAI  \\n \\n  Blog,      1(8),      2019.\\n \\n \\n  Pranav      Rajpurkar,      Jian      Zhang,      Konstantin      Lopyrev,  \\n \\n  and      Percy      Liang.\\nSquad:      100,000+      questions      for  \\n \\n  machine      comprehension      of      text.\\narXiv      preprint  \\n \\n  arXiv:      1606.05250,      2016.\\n \\n \\n  Abigail      See,      Peter   \\n \\nJ      Liu,      and      Christopher   \\n \\nD       Manning.\\nGet      to      the      point:      Summarization  \\n \\n  with      pointer-generator      networks.\\narXiv      preprint  \\n \\n  arXiv:      1704.04368,      2017.\\n \\n \\n  Rico      Sennrich,      Barry      Haddow,      and      Alexandra      Birch.\\n \\n \\n  Edinburgh      neural      machine      translation      systems      for  \\n \\n  WMT      16.\\nIn      Proceedings      of      the      First      Conference  \\n \\n  on      Machine      Translation:      Volume      2,      Shared      Task      Pa-  \\n \\n  pers,      2016.\\n \\n \\n  Richard      Socher,      Alex      Perelygin,      Jean      Wu,      Jason  \\n \\n  Chuang,      Christopher   \\n \\nD      Manning,      Andrew      Ng,      and  \\n \\n  Christopher      Potts.\\nRecursive      deep      models      for      se-  \\n \\n  mantic      compositionality      over   \\n \\na      sentiment      treebank.\\n \\n \\n  In      Proceedings      of      EMNLP,      pp.\\n1631-1642,      2013.\\n \\n \\n  Kaitao      Song,      Xu      Tan,      Tao      Qin,      Jianfeng      Lu,      and      Tie-  \\n \\n  Yan      Liu.\\nMass:      Masked      sequence      to      sequence      pre-  \\n \\n  training      for      language      generation.\\nIn      International  \\n \\n  Conference      on      Machine      Learning,      2019.\\n \\n \\n  Ashish      Vaswani,      Noam      Shazeer,      Niki      Parmar,      Jakob  \\n \\n  Uszkoreit,      Llion      Jones,      Aidan      N      Gomez,      Lukasz  \\n \\n  Kaiser,      and      Ilia      Polosukhin.\\nAttention      is      all      you  \\n \\n  need.\\nIn      Advances      in      neural      information      processing  \\n \\n  systems,      pp.\\n5998-6008,      2017.\\n \\n \\n  Alex      Wang,      Amanpreet      Singh,      Julian      Michael,      Felix  \\n \\n  Hill,      Omer      Levy,      and      Samuel      R      Bowman.\\nGlue:\\n \\n \\n \\nA      multi-task      benchmark      and      analysis      platform      for  \\n \\n  natural      language      understanding.\\narXiv      preprint  \\n \\n  arXiv:      1804.07461,      2018.\\n \\n \\n  Alex      Warstadt,      Amanpreet      Singh,      and      Samuel      R.  \\n \\n  Bowman.\\nNeural      network      acceptability      judgments.\\n \\n \\n  arXiv      preprint      1805.12471,      2018.\\n \\n \\n  Adina      Williams,      Nikita      Nangia,      and      Samuel      R      Bow-  \\n \\n  man.\\n \\n \\nA   \\n \\n_      broad-coverage      challenge      corpus      for  \\n \\n  sentence      understanding      through      inference.\\narXiv  \\n \\n  preprint      arXiv:      1704.05426,      2017.\\n \\n \\n  Adina      Williams,      Nikita      Nangia,      and      Samuel      R.      Bow-  \\n \\n  man.\\n \\n \\nA      broad-coverage      challenge      corpus      for      sen-  \\n \\n  tence      understanding      through      inference.\\nIn      Proceed-  \\n \\n  ings      of      NAACL-HLT,      2018.\\n \\n \\n  Zhilin      Yang,      Zihang      Dai,      Yiming      Yang,      Jaime  \\n \\n  Carbonell,      Ruslan      Salakhutdinov,      and      Quoc   \\n \\nV       Le.\\nXlInet:      Generalized      autoregressive      pretrain-  \\n \\n  ing      for      language      understanding.\\narXiv      preprint  \\n \\n  arXiv:      1906.08237,      2019.'}, {'metadata': {'source': 'https://arxiv.org/pdf/1910.13461.pdf', 'section_number': 17, 'section_title': 'Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96'}, 'page_content': 'Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96\\n \\n \\n  Table      6:      The      performance      (BLEU)      of      baseline      and  \\n \\n  BART      on      WMT’16      RO-EN      augmented      with      back-  \\n \\n  translation      data.\\nBART      improves      over   \\n \\na      strong      back-  \\n \\n  translation      (BT)      baseline      by      using      monolingual      English  \\n \\n  pre-training.\\n \\n \\n  Abstractive      QA      We      use      the      recently      proposed      ELIS  \\n \\n  dataset      to      test      the      model’s      ability      to      generate      long      free-  \\n \\n  form      answers.\\nWe      find      BART      outperforms      the      best      pre-  \\n \\n  vious      work      by      1.2      ROUGE-L,      but      the      dataset      remains  \\n \\n  a      challenging,      because      answers      are      only      weakly      speci-  \\n \\n  fied      by      the      question.\\n5.4      Translation\\ntion  \\n \\n  We      also      evaluated      performance      on      WMT16      Romanian-  \\n \\n  English,      augmented      with      back-translation      data  \\n \\n  from      Sennrich      et      al.\\n(2016).\\nWe      use   \\n \\na      6-layer  \\n \\n  transformer      source      encoder      to      map      Romanian      into  \\n \\n  a      representation      that      BART      is      able      to      de-noise      into  \\n \\n  English,      following      the      approach      introduced      in      83.4.  \\n \\n  Experiment      results      are      presented      in      Table      6.      We  \\n \\n  compare      our      results      against   \\n \\na      baseline      Transformer  \\n \\n  architecture      (Vaswani      et      al.,      2017)      with      Transformer-  \\n \\n  large      settings      (the      baseline      row).\\nWe      show      the  \\n \\n  performance      of      both      steps      of      our      model      in      the      fixed  \\n \\n  BART      and      tuned      BART      rows.\\nFor      each      row      we  \\n \\n  experiment      on      the      original      WMT16      Romanian-English  \\n \\n  augmented      with      back-translation      data.\\nWe      use   \\n \\na       beam      width      of   \\n \\n5      and   \\n \\na      length      penalty      of   \\n \\na   \\n \\n=      1.  \\n \\n  Preliminary      results      suggested      that      our      approach      was  \\n \\n  less      effective      without      back-translation      data,      and      prone  \\n \\n  to      overfitting—future      work      should      explore      additional  \\n \\n  regularization      techniques.\\n6\\n \\n   Qualitative      Analysis  \\n \\n  BART      shows      large      improvements      on      summarization  \\n \\n  metrics,      of      up      to   \\n \\n6      points      over      the      prior      state-of-the-art.\\n \\n \\n  To      understand      BART’s      performance      beyond      automated  \\n \\n  metrics,      we      analyse      its      generations      qualitatively.\\n \\n \\n  Table   \\n \\n7      shows      example      summaries      generated      by  \\n \\n  BART.\\nExamples      are      taken      from      WikiNews      articles  \\n \\n  published      after      the      creation      of      the      pre-training      corpus,  \\n \\n  to      eliminate      the      possibility      of      the      events      described      be-  \\n \\n  ing      present      in      the      model’s      training      data.\\nFollowing  \\n \\n  Narayan      et      al.\\n(2018),      we      remove      the      first      sentence      of  \\n \\n  the      article      prior      to      summarizing      it,      so      there      is      no      easy  \\n \\n  extractive      summary      of      the      document.\\n \\n \\n  Unsurprisingly,      model      output      is      fluent      and      grammat-  \\n \\n  ical      English.\\nHowever,      model      output      is      also      highly      ab-  \\n \\n  stractive,      with      few      phrases      copied      from      the      input.\\nThe  \\n \\n  output      is      also      generally      factually      accurate,      and      inte-  \\n \\n  grates      supporting      evidence      from      across      the      input      doc-  \\n \\n  ument      with      background      knowledge      (for      example,      cor-  \\n \\n  rectly      completing      names,      or      inferring      that      PG&E      oper-  \\n \\n  ates      in      California).\\nIn      the      first      example,      inferring      that  \\n \\n  fish      are      protecting      reefs      from      global      warming      requires  \\n \\n  non-trivial      inference      from      the      text.\\nHowever,      the      claim  \\n \\n  that      the      work      was      published      in      Science      is      not      supported  \\n \\n  by      the      source.\\n \\n \\n  These      samples      demonstrate      that      the      BART      pretrain-  \\n \\n  ing      has      learned   \\n \\na      strong      combination      of      natural      lan-  \\n \\n  guage      understanding      and      generation.\\n7\\n \\n   Related      Work  \\n \\n  Early      methods      for      pretraining      were      based      on      language  \\n \\n  models.\\nGPT      (Radford      et      al.,      2018)      only      models      left-  \\n \\n  ward      context,      which      is      problematic      for      some      tasks.\\n \\n \\n  ELMo      (Peters      et      al.,      2018)      concatenates      left-only      and  \\n \\n  right-only      representations,      but      does      not      pre-train      inter-  \\n \\n  actions      between      these      features.\\nRadford      et      al.\\n(2019)  \\n \\n  demonstrated      that      very      large      language      models      can      act       as      unsupervised      multitask      models.\\n \\n \\n  MASS      (Song      et      al.,      2019)      is      perhaps      the      most      similar  \\n \\n  model      to      BART.\\nAn      input      sequence      where   \\n \\na      contiguous  \\n \\n  span      of      tokens      is      masked      is      mapped      to   \\n \\na      sequence      con-  \\n \\n  sisting      of      the      missing      tokens.\\nMASS      is      less      effective  \\n \\n  for      discriminative      tasks,      because      disjoint      sets      of      tokens  \\n \\n  are      fed      into      the      encoder      and      decoder.\\n \\n \\n  BERT      (Devlin      et      al.,      2019)      introduced      masked      lan-  \\n \\n  guage      modelling,      which      allows      pre-training      to      learn      in-  \\n \\n  teractions      between      left      and      right      context      words.\\nRe-  \\n \\n  cent      work      has      shown      that      very      strong      performance      can  \\n \\n  be      achieved      by      training      for      longer      (Liu      et      al.,      2019),  \\n \\n  by      tying      parameters      across      layers      (Lan      et      al.,      2019),  \\n \\n  and      by      masking      spans      instead      of      words      (Joshi      et      al.,  \\n \\n  2019).\\nPredictions      are      not      made      auto-regressively,      re-  \\n \\n  ducing      the      effectiveness      of      BERT      for      generation      tasks.\\n \\n \\n  UniLM      (Dong      et      al.,      2019)      fine-tunes      BERT      with      an  \\n \\n  ensemble      of      masks,      some      of      which      allow      only      leftward  \\n \\n  context.\\nLike      BART,      this      allows      UniLM      to      be      used      for  \\n \\n  both      generative      and      discriminative      tasks.\\n \\n \\nA      difference  \\n \\n  is      that      UniLM      predictions      are      conditionally      indepen-  \\n \\n  dent,      whereas      BART’s      are      autoregressive.\\nBART      re-  \\n \\n  duces      the      mismatch      between      pre-training      and      genera-  \\n \\n  tion      tasks,      because      the      decoder      is      always      trained      on      un-  \\n \\n  corrupted      context.\\n \\n \\n  XL-Net      (Yang      et      al.,      2019)      extends      BERT      by      pre-\\n |       Source      Document      (abbreviated) |       BART      Summary\\n |       The      researchers      examined      three      types      of      coral      in      reefs      off      the coast of Fiji The researchers found when fish were plentiful,       they      would      eat      algae      and      seaweed      off      the      corals,      which      appeared       to      leave      them      more      resistant      to      the      bacterium      Vibrio      coralliilyti-       cus,      a      bacterium      associated      with      bleaching.      The      researchers      sug-       gested      the      algae,      like      warming      temperatures,      might      render      the       corals’      chemical      defenses      less      effective,      and      the      fish      were      pro-       tecting      the      coral      by      removing      the      algae. |       Fisheries      off      the      coast      of      Fiji      are      protect-       ing      coral      reefs      from      the      effects      of      global       warming,      according      to      a      study      in      the      jour-       nal      Science.\\n |       Sacoolas,      who      has      immunity      as      a      diplomat’s      wife,      was      involved in a traffic collision Prime Minister Johnson was questioned       about      the      case      while      speaking      to      the      press      at      a      hospital      in      Wat- ford. He said, “I hope that Anne Sacoolas will come back       if      we      can’t      resolve      it      then      of      course      I      will      be      raising      it      myself       personally      with      the      White      House.” |       Boris      Johnson      has      said      he      will      raise      the      is-       sue      of      US      diplomat      Anne      Sacoolas’      diplo-       matic      immunity      with      the      White      House.\\n |       According      to      Syrian      state      media,      government      forces      began      de- ploying into previously SDF controlled territory yesterday.       On      October      6,      US      President      Donald      Trump      and      Turkish      Presi-       dent      Recep      Tayyip      Erdoan      spoke      on      the      phone.      Then      both      na-       tions      issued      statements      speaking      of      an      imminent      incursion      into northeast Syria On Wednesday, Turkey began a military       offensive      with      airstrikes      followed      by      a      ground      invasion. |       Syrian      government      forces      have      entered       territory      held      by      the      US-backed      Syrian       Democratic      Forces      (SDF)      in      response      to       Turkey’s      incursion      into      the      region.\\n |       This      is      the      first      time      anyone      has      been      recorded      to      run      a      full       marathon      of      42.195      kilometers      (approximately      26      miles)      under       this      pursued      landmark      time.      It      was      not,      however,      an      officially       sanctioned      world      record,      as      it      was      not      an      ’open      race”      of      the       IAAF.      His      time      was      1      hour      59      minutes      40.2      seconds.      Kipchoge       ran      in      Vienna,      Austria.      It      was      an      event      specifically      designed      to       help      Kipchoge      break      the      two      hour      barrier. |       Kenyan      runner      Eliud      Kipchoge      has      run      a       marathon      in      less      than      two      hours.\\n |       PG&E      stated      it      scheduled      the      blackouts      in      response      to      forecasts       for      high      winds      amid      dry      conditions.      The      aim      is      to      reduce      the      risk       of      wildfires.      Nearly      800      thousand      customers      were      scheduled      to       be      affected      by      the      shutoffs      which      were      expected      to      last      through       at      least      midday      tomorrow. |       Power      has      been      turned      off      to      millions      of       customers      in      California      as      part      of      a      power       shutoff      plan.\\n\\n \\n \\n  dicting      masked      tokens      auto-regressively      in   \\n \\na      permuted  \\n \\n  order.\\nThis      objective      allows      predictions      to      condition      on  \\n \\n  both      left      and      right      context.\\nIn      contrast,      the      BART      de-  \\n \\n  coder      works      left-to-right      during      pre-training,      matching  \\n \\n  the      setting      during      generation.\\n \\n \\n  Several      papers      have      explored      using      pre-trained      rep-  \\n \\n  resentations      to      improve      machine      translation.\\nThe  \\n \\n  largest      improvements      have      come      from      pre-training      on  \\n \\n  both      source      and      target      languages      (Song      et      al.,      2019;  \\n \\n  Lample   \\n \\n&      Conneau,      2019),      but      this      requires      pre-  \\n \\n  training      on      all      languages      of      interest.\\nOther      work      has  \\n \\n  shown      that      encoders      can      be      improved      using      pre-trained  \\n \\n  representations      (Edunov      et      al.,      2019),      but      gains      in      de-  \\n \\n  coders      are      more      limited.\\nWe      show      how      BART      can      be  \\n \\n  used      to      improve      machine      translation      decoders.\\n8\\n \\n   Conclusions  \\n \\n  We      introduced      BART,   \\n \\na      pre-training      approach      that  \\n \\n  learns      to      map      corrupted      documents      to      the      original.\\n \\n \\n  BART      achieves      similar      performance      to      ROBERTa      on  \\n \\n  discriminative      tasks,      while      achieving      new      state-of-the-  \\n \\n  art      results      on   \\n \\na      number      of      text      generation      tasks.\\nFu-  \\n \\n  ture      work      should      explore      new      methods      for      corrupting  \\n \\n  documents      for      pre-training,      perhaps      tailoring      them      to  \\n \\n  specific      end      tasks.'}, {'metadata': {'source': 'https://arxiv.org/pdf/1910.13461.pdf', 'section_number': 18, 'section_title': '5.4      Translation'}, 'page_content': '5.4      Translation\\ntion  \\n \\n  We      also      evaluated      performance      on      WMT16      Romanian-  \\n \\n  English,      augmented      with      back-translation      data  \\n \\n  from      Sennrich      et      al.\\n(2016).\\nWe      use   \\n \\na      6-layer  \\n \\n  transformer      source      encoder      to      map      Romanian      into  \\n \\n  a      representation      that      BART      is      able      to      de-noise      into  \\n \\n  English,      following      the      approach      introduced      in      83.4.  \\n \\n  Experiment      results      are      presented      in      Table      6.      We  \\n \\n  compare      our      results      against   \\n \\na      baseline      Transformer  \\n \\n  architecture      (Vaswani      et      al.,      2017)      with      Transformer-  \\n \\n  large      settings      (the      baseline      row).\\nWe      show      the  \\n \\n  performance      of      both      steps      of      our      model      in      the      fixed  \\n \\n  BART      and      tuned      BART      rows.\\nFor      each      row      we  \\n \\n  experiment      on      the      original      WMT16      Romanian-English  \\n \\n  augmented      with      back-translation      data.\\nWe      use   \\n \\na       beam      width      of   \\n \\n5      and   \\n \\na      length      penalty      of   \\n \\na   \\n \\n=      1.  \\n \\n  Preliminary      results      suggested      that      our      approach      was  \\n \\n  less      effective      without      back-translation      data,      and      prone  \\n \\n  to      overfitting—future      work      should      explore      additional  \\n \\n  regularization      techniques.\\n6\\n \\n   Qualitative      Analysis  \\n \\n  BART      shows      large      improvements      on      summarization  \\n \\n  metrics,      of      up      to   \\n \\n6      points      over      the      prior      state-of-the-art.\\n \\n \\n  To      understand      BART’s      performance      beyond      automated  \\n \\n  metrics,      we      analyse      its      generations      qualitatively.\\n \\n \\n  Table   \\n \\n7      shows      example      summaries      generated      by  \\n \\n  BART.\\nExamples      are      taken      from      WikiNews      articles  \\n \\n  published      after      the      creation      of      the      pre-training      corpus,  \\n \\n  to      eliminate      the      possibility      of      the      events      described      be-  \\n \\n  ing      present      in      the      model’s      training      data.\\nFollowing  \\n \\n  Narayan      et      al.\\n(2018),      we      remove      the      first      sentence      of  \\n \\n  the      article      prior      to      summarizing      it,      so      there      is      no      easy  \\n \\n  extractive      summary      of      the      document.\\n \\n \\n  Unsurprisingly,      model      output      is      fluent      and      grammat-  \\n \\n  ical      English.\\nHowever,      model      output      is      also      highly      ab-  \\n \\n  stractive,      with      few      phrases      copied      from      the      input.\\nThe  \\n \\n  output      is      also      generally      factually      accurate,      and      inte-  \\n \\n  grates      supporting      evidence      from      across      the      input      doc-  \\n \\n  ument      with      background      knowledge      (for      example,      cor-  \\n \\n  rectly      completing      names,      or      inferring      that      PG&E      oper-  \\n \\n  ates      in      California).\\nIn      the      first      example,      inferring      that  \\n \\n  fish      are      protecting      reefs      from      global      warming      requires  \\n \\n  non-trivial      inference      from      the      text.\\nHowever,      the      claim  \\n \\n  that      the      work      was      published      in      Science      is      not      supported  \\n \\n  by      the      source.\\n \\n \\n  These      samples      demonstrate      that      the      BART      pretrain-  \\n \\n  ing      has      learned   \\n \\na      strong      combination      of      natural      lan-  \\n \\n  guage      understanding      and      generation.\\n7\\n \\n   Related      Work  \\n \\n  Early      methods      for      pretraining      were      based      on      language  \\n \\n  models.\\nGPT      (Radford      et      al.,      2018)      only      models      left-  \\n \\n  ward      context,      which      is      problematic      for      some      tasks.\\n \\n \\n  ELMo      (Peters      et      al.,      2018)      concatenates      left-only      and  \\n \\n  right-only      representations,      but      does      not      pre-train      inter-  \\n \\n  actions      between      these      features.\\nRadford      et      al.\\n(2019)  \\n \\n  demonstrated      that      very      large      language      models      can      act       as      unsupervised      multitask      models.\\n \\n \\n  MASS      (Song      et      al.,      2019)      is      perhaps      the      most      similar  \\n \\n  model      to      BART.\\nAn      input      sequence      where   \\n \\na      contiguous  \\n \\n  span      of      tokens      is      masked      is      mapped      to   \\n \\na      sequence      con-  \\n \\n  sisting      of      the      missing      tokens.\\nMASS      is      less      effective  \\n \\n  for      discriminative      tasks,      because      disjoint      sets      of      tokens  \\n \\n  are      fed      into      the      encoder      and      decoder.\\n \\n \\n  BERT      (Devlin      et      al.,      2019)      introduced      masked      lan-  \\n \\n  guage      modelling,      which      allows      pre-training      to      learn      in-  \\n \\n  teractions      between      left      and      right      context      words.\\nRe-  \\n \\n  cent      work      has      shown      that      very      strong      performance      can  \\n \\n  be      achieved      by      training      for      longer      (Liu      et      al.,      2019),  \\n \\n  by      tying      parameters      across      layers      (Lan      et      al.,      2019),  \\n \\n  and      by      masking      spans      instead      of      words      (Joshi      et      al.,  \\n \\n  2019).\\nPredictions      are      not      made      auto-regressively,      re-  \\n \\n  ducing      the      effectiveness      of      BERT      for      generation      tasks.\\n \\n \\n  UniLM      (Dong      et      al.,      2019)      fine-tunes      BERT      with      an  \\n \\n  ensemble      of      masks,      some      of      which      allow      only      leftward  \\n \\n  context.\\nLike      BART,      this      allows      UniLM      to      be      used      for  \\n \\n  both      generative      and      discriminative      tasks.\\n \\n \\nA      difference  \\n \\n  is      that      UniLM      predictions      are      conditionally      indepen-  \\n \\n  dent,      whereas      BART’s      are      autoregressive.\\nBART      re-  \\n \\n  duces      the      mismatch      between      pre-training      and      genera-  \\n \\n  tion      tasks,      because      the      decoder      is      always      trained      on      un-  \\n \\n  corrupted      context.\\n \\n \\n  XL-Net      (Yang      et      al.,      2019)      extends      BERT      by      pre-\\n |       Source      Document      (abbreviated) |       BART      Summary\\n |       The      researchers      examined      three      types      of      coral      in      reefs      off      the coast of Fiji The researchers found when fish were plentiful,       they      would      eat      algae      and      seaweed      off      the      corals,      which      appeared       to      leave      them      more      resistant      to      the      bacterium      Vibrio      coralliilyti-       cus,      a      bacterium      associated      with      bleaching.      The      researchers      sug-       gested      the      algae,      like      warming      temperatures,      might      render      the       corals’      chemical      defenses      less      effective,      and      the      fish      were      pro-       tecting      the      coral      by      removing      the      algae. |       Fisheries      off      the      coast      of      Fiji      are      protect-       ing      coral      reefs      from      the      effects      of      global       warming,      according      to      a      study      in      the      jour-       nal      Science.\\n |       Sacoolas,      who      has      immunity      as      a      diplomat’s      wife,      was      involved in a traffic collision Prime Minister Johnson was questioned       about      the      case      while      speaking      to      the      press      at      a      hospital      in      Wat- ford. He said, “I hope that Anne Sacoolas will come back       if      we      can’t      resolve      it      then      of      course      I      will      be      raising      it      myself       personally      with      the      White      House.” |       Boris      Johnson      has      said      he      will      raise      the      is-       sue      of      US      diplomat      Anne      Sacoolas’      diplo-       matic      immunity      with      the      White      House.\\n |       According      to      Syrian      state      media,      government      forces      began      de- ploying into previously SDF controlled territory yesterday.       On      October      6,      US      President      Donald      Trump      and      Turkish      Presi-       dent      Recep      Tayyip      Erdoan      spoke      on      the      phone.      Then      both      na-       tions      issued      statements      speaking      of      an      imminent      incursion      into northeast Syria On Wednesday, Turkey began a military       offensive      with      airstrikes      followed      by      a      ground      invasion. |       Syrian      government      forces      have      entered       territory      held      by      the      US-backed      Syrian       Democratic      Forces      (SDF)      in      response      to       Turkey’s      incursion      into      the      region.\\n |       This      is      the      first      time      anyone      has      been      recorded      to      run      a      full       marathon      of      42.195      kilometers      (approximately      26      miles)      under       this      pursued      landmark      time.      It      was      not,      however,      an      officially       sanctioned      world      record,      as      it      was      not      an      ’open      race”      of      the       IAAF.      His      time      was      1      hour      59      minutes      40.2      seconds.      Kipchoge       ran      in      Vienna,      Austria.      It      was      an      event      specifically      designed      to       help      Kipchoge      break      the      two      hour      barrier. |       Kenyan      runner      Eliud      Kipchoge      has      run      a       marathon      in      less      than      two      hours.\\n |       PG&E      stated      it      scheduled      the      blackouts      in      response      to      forecasts       for      high      winds      amid      dry      conditions.      The      aim      is      to      reduce      the      risk       of      wildfires.      Nearly      800      thousand      customers      were      scheduled      to       be      affected      by      the      shutoffs      which      were      expected      to      last      through       at      least      midday      tomorrow. |       Power      has      been      turned      off      to      millions      of       customers      in      California      as      part      of      a      power       shutoff      plan.\\n\\n \\n \\n  dicting      masked      tokens      auto-regressively      in   \\n \\na      permuted  \\n \\n  order.\\nThis      objective      allows      predictions      to      condition      on  \\n \\n  both      left      and      right      context.\\nIn      contrast,      the      BART      de-  \\n \\n  coder      works      left-to-right      during      pre-training,      matching  \\n \\n  the      setting      during      generation.\\n \\n \\n  Several      papers      have      explored      using      pre-trained      rep-  \\n \\n  resentations      to      improve      machine      translation.\\nThe  \\n \\n  largest      improvements      have      come      from      pre-training      on  \\n \\n  both      source      and      target      languages      (Song      et      al.,      2019;  \\n \\n  Lample   \\n \\n&      Conneau,      2019),      but      this      requires      pre-  \\n \\n  training      on      all      languages      of      interest.\\nOther      work      has  \\n \\n  shown      that      encoders      can      be      improved      using      pre-trained  \\n \\n  representations      (Edunov      et      al.,      2019),      but      gains      in      de-  \\n \\n  coders      are      more      limited.\\nWe      show      how      BART      can      be  \\n \\n  used      to      improve      machine      translation      decoders.\\n8\\n \\n   Conclusions  \\n \\n  We      introduced      BART,   \\n \\na      pre-training      approach      that  \\n \\n  learns      to      map      corrupted      documents      to      the      original.\\n \\n \\n  BART      achieves      similar      performance      to      ROBERTa      on  \\n \\n  discriminative      tasks,      while      achieving      new      state-of-the-  \\n \\n  art      results      on   \\n \\na      number      of      text      generation      tasks.\\nFu-  \\n \\n  ture      work      should      explore      new      methods      for      corrupting  \\n \\n  documents      for      pre-training,      perhaps      tailoring      them      to  \\n \\n  specific      end      tasks.'}, {'metadata': {'source': 'https://arxiv.org/pdf/1910.13461.pdf', 'section_number': 19, 'section_title': 'References'}, 'page_content': 'References\\n \\n \\n  Eneko      Agirre,      Llu’is      M‘arquez,      and      Richard      Wicen-  \\n \\n  towski      (eds.).\\nProceedings      of      the      Fourth      Interna-  \\n \\n  tional      Workshop      on      Semantic      Evaluations      (SemEval-  \\n \\n  2007).\\nAssociation      for      Computational      Linguistics,  \\n \\n  Prague,      Czech      Republic,      June      2007.\\n \\n \\n  Ido      Dagan,      Oren      Glickman,      and      Bernardo      Magnini.\\n \\n \\n  The      PASCAL      recognising      textual      entailment      chal-  \\n \\n  lenge.\\nIn      Machine      learning      challenges.\\nevaluat-  \\n \\n  ing      predictive      uncertainty,      visual      object      classifica-  \\n \\n  tion,      and      recognising      tectual      entailment,      pp.\\n177—  \\n \\n  190.\\nSpringer,      2006.\\n \\n \\n  Jacob      Devlin,      Ming-Wei      Chang,      Kenton      Lee,      and  \\n \\n  Kristina      Toutanova.\\nBERT:      Pre-training      of      deep  \\n \\n  bidirectional      transformers      for      language      understand-  \\n \\n  ing.\\nIn      Proceedings      of      the      2019      Conference      of      the  \\n \\n  North      American      Chapter      of      the      Association      for      Com-  \\n \\n  putational      Linguistics:      Human      Language      Technolo-  \\n \\n  gies,      Volume   \\n \\nI      (Long      and      Short      Papers),      pp.\\n4171-  \\n \\n  4186,      Minneapolis,      Minnesota,      June      2019.\\nAssocia-  \\n \\n  tion      for      Computational      Linguistics.\\ndoi:      10.18653/  \\n \\n  vI/N19-1423.\\nURL      https://www.aclweb.\\n \\n \\n  org/anthology/N19-1423.\\n \\n \\n  Emily      Dinan,      Varvara      Logacheva,      Valentin      Malykh,  \\n \\n  Alexander      Miller,      Kurt      Shuster,      Jack      Urbanek,  \\n \\n  Douwe      Kiela,      Arthur      Szlam,      Iulian      Serban,      Ryan  \\n \\n  Lowe,      et      al.\\nThe      second      conversational      in-  \\n \\n  telligence      challenge      (convai2).\\narXiv      preprint  \\n \\n  arXiv:      1902.00098,      2019.\\n \\n \\n  William   \\n \\nB      Dolan      and      Chris      Brockett.\\nAutomatically  \\n \\n  constructing   \\n \\na      corpus      of      sentential      paraphrases.\\nIn  \\n \\n  Proceedings      of      the      International      Workshop      on      Para-  \\n \\n  phrasing,      2005.\\n \\n \\n  Li      Dong,      Nan      Yang,      Wenhui      Wang,      Furu      Wei,      Xi-  \\n \\n  aodong      Liu,      Yu      Wang,      Jianfeng      Gao,      Ming      Zhou,  \\n \\n  and      Hsiao-Wuen      Hon.\\nUnified      language      model      pre-  \\n \\n  training      for      natural      language      understanding      and      gen-  \\n \\n  eration.\\narXiv      preprint      arXiv:      1905.03197,      2019.\\n \\n \\n  Sergey      Edunov,      Alexei      Baevski,      and      Michael      Auli.\\n \\n \\n  Pre-trained      language      model      representations      for      lan-  \\n \\n  guage      generation.\\nIn      Proceedings      of      the      2019      Con-  \\n \\n  ference      of      the      North      American      Chapter      of      the      Asso-  \\n \\n  ciation      for      Computational      Linguistics:      Human      Lan-  \\n \\n  guage      Technologies,      Volume   \\n \\n1      (Long      and      Short      Pa-  \\n \\n  pers),      2019.\\n \\n \\n  Angela      Fan,      David      Grangier,      and      Michael      Auli.\\nCon-  \\n \\n  trollable      abstractive      summarization.\\narXiv      preprint  \\n \\n  arXiv:      1711.05217,      2017.\\n \\n \\n  Angela      Fan,      Yacine      Jernite,      Ethan      Perez,      David  \\n \\n  Grangier,      Jason      Weston,      and      Michael      Auli.\\nEli5:  \\n \\n  Long      form      question      answering.\\narXiv      preprint  \\n \\n  arXiv:      1907.09190,      2019.\\n \\n \\n  Dan      Hendrycks      and      Kevin      Gimpel.\\nGaussian      error      lin-  \\n \\n  ear      units      (gelus).\\narXiv      preprint      arXiv:      1606.08415,  \\n \\n  2016.\\n \\n \\n  Karl      Moritz      Hermann,      Tomas      Kocisky,      Edward  \\n \\n  Grefenstette,      Lasse      Espeholt,      Will      Kay,      Mustafa      Su-  \\n \\n  leyman,      and      Phil      Blunsom.\\nTeaching      machines      to  \\n \\n  read      and      comprehend.\\nIn      Advances      in      neural      infor-  \\n \\n  mation      processing      systems,      pp.\\n1693-1701,      2015.\\n \\n \\n  Mandar      Joshi,      Danqi      Chen,      Yinhan      Liu,      Daniel   \\n \\nS      Weld,  \\n \\n  Luke      Zettlemoyer,      and      Omer      Levy.\\nSpanbert:      Im-  \\n \\n  proving      pre-training      by      representing      and      predicting  \\n \\n  spans.\\narXiv      preprint      arXiv:      1907.10529,      2019.\\n \\n \\n  Guillaume      Lample      and      Alexis      Conneau.\\n \\n \\n—      Cross-  \\n \\n  lingual      language      model      pretraining.\\narXiv      preprint  \\n \\n  arXiv:      1901.07291,      2019.\\n \\n \\n  Zhenzhong      Lan,      Mingda      Chen,      Sebastian      Goodman,  \\n \\n  Kevin      Gimpel,      Piyush      Sharma,      and      Radu      Sori-  \\n \\n  cut.\\nAlbert:   \\n \\nA      lite      bert      for      self-supervised      learn-  \\n \\n  ing      of      language      representations.\\narXiv      preprint  \\n \\n  arXiv:      1909.11942,      2019.\\n \\n \\n  Hector   \\n \\nJ      Levesque,      Ernest      Davis,      and      Leora      Morgen-  \\n \\n  stern.\\nThe      Winograd      schema      challenge.\\nIn      AAAI  \\n \\n  Spring      Symposium:      Logical      Formalizations      of      Com-  \\n \\n  monsense      Reasoning,      volume      46,      pp.\\n47,      2011.\\n \\n \\n  Yang      Liu      and      Mirella      Lapata.\\n \\n \\n  tion      with      pretrained      encoders.\\n \\n \\n  arXiv:      1908.08345,      2019.  \\n \\n  Text      summariza-  \\n \\n  arXiv      preprint  \\n \\n  Yinhan      Liu,      Myle      Ott,      Naman      Goyal,      Jingfei      Du,      Man-  \\n \\n  dar      Joshi,      Dangi      Chen,      Omer      Levy,      Mike      Lewis,  \\n \\n  Luke      Zettlemoyer,      and      Veselin      Stoyanov.\\nRoberta:\\n \\n \\n \\nA      robustly      optimized      bert      pretraining      approach.\\n \\n \\n  arXiv      preprint      arXiv:      1907.11692,      2019.\\n \\n \\n  Tomas      Mikolov,      Kai      Chen,      Greg      Corrado,      and      Jeffrey  \\n \\n  Dean.\\nEfficient      estimation      of      word      representations  \\n \\n  in      vector      space.\\narXiv      preprint      arXiv:1301.3781,  \\n \\n  2013.\\n \\n \\n  Shashi      Narayan,      Shay   \\n \\nB      Cohen,      and      Mirella      Lapata.\\n \\n \\n  Don’t      give      me      the      details,      just      the      summary!\\ntopic-  \\n \\n  aware      convolutional      neural      networks      for      extreme  \\n \\n  summarization.\\narXiv      preprint      arXiv:      1808.08745,  \\n \\n  2018.\\n \\n \\n  Gabriel      Pereyra,      George      Tucker,      Jan      Chorowski,  \\n \\n  Lukasz      Kaiser,      and      Geoffrey      Hinton.\\nRegularizing  \\n \\n  neural      networks      by      penalizing      confident      output      dis-  \\n \\n  tributions.\\narXiv      preprint      arXiv:      1701.06548,      2017.\\n \\n \\n  Matthew      E      Peters,      Mark      Neumann,      Mohit      Iyyer,      Matt  \\n \\n  Gardner,      Christopher      Clark,      Kenton      Lee,      and      Luke  \\n \\n  Zettlemoyer.\\nDeep      contextualized      word      representa-  \\n \\n  tions.\\narXiv      preprint      arXiv:      1802.05365,      2018.\\n \\n \\n  Alec      Radford,      Karthik      Narasimhan,      Tim      Salimans,  \\n \\n  and      Ilya      Sutskever.\\nImproving      language      un-  \\n \\n  derstanding      by      generative      pre-training.\\nURL  \\n \\n  https://s3-us-west-2.\\n \\n \\n|      amazonaws.\\n \\n \\n—      com/openai-  \\n \\n  assets/researchcovers/languageunsupervised/language  \\n \\n  understanding      paper.\\npdf,      2018.\\n \\n \\n  Alec      Radford,      Jeffrey      Wu,      Rewon      Child,      David      Luan,  \\n \\n  Dario      Amodei,      and      Ilya      Sutskever.\\nLanguage      mod-  \\n \\n  els      are      unsupervised      multitask      learners.\\nOpenAI  \\n \\n  Blog,      1(8),      2019.\\n \\n \\n  Pranav      Rajpurkar,      Jian      Zhang,      Konstantin      Lopyrev,  \\n \\n  and      Percy      Liang.\\nSquad:      100,000+      questions      for  \\n \\n  machine      comprehension      of      text.\\narXiv      preprint  \\n \\n  arXiv:      1606.05250,      2016.\\n \\n \\n  Abigail      See,      Peter   \\n \\nJ      Liu,      and      Christopher   \\n \\nD       Manning.\\nGet      to      the      point:      Summarization  \\n \\n  with      pointer-generator      networks.\\narXiv      preprint  \\n \\n  arXiv:      1704.04368,      2017.\\n \\n \\n  Rico      Sennrich,      Barry      Haddow,      and      Alexandra      Birch.\\n \\n \\n  Edinburgh      neural      machine      translation      systems      for  \\n \\n  WMT      16.\\nIn      Proceedings      of      the      First      Conference  \\n \\n  on      Machine      Translation:      Volume      2,      Shared      Task      Pa-  \\n \\n  pers,      2016.\\n \\n \\n  Richard      Socher,      Alex      Perelygin,      Jean      Wu,      Jason  \\n \\n  Chuang,      Christopher   \\n \\nD      Manning,      Andrew      Ng,      and  \\n \\n  Christopher      Potts.\\nRecursive      deep      models      for      se-  \\n \\n  mantic      compositionality      over   \\n \\na      sentiment      treebank.\\n \\n \\n  In      Proceedings      of      EMNLP,      pp.\\n1631-1642,      2013.\\n \\n \\n  Kaitao      Song,      Xu      Tan,      Tao      Qin,      Jianfeng      Lu,      and      Tie-  \\n \\n  Yan      Liu.\\nMass:      Masked      sequence      to      sequence      pre-  \\n \\n  training      for      language      generation.\\nIn      International  \\n \\n  Conference      on      Machine      Learning,      2019.\\n \\n \\n  Ashish      Vaswani,      Noam      Shazeer,      Niki      Parmar,      Jakob  \\n \\n  Uszkoreit,      Llion      Jones,      Aidan      N      Gomez,      Lukasz  \\n \\n  Kaiser,      and      Ilia      Polosukhin.\\nAttention      is      all      you  \\n \\n  need.\\nIn      Advances      in      neural      information      processing  \\n \\n  systems,      pp.\\n5998-6008,      2017.\\n \\n \\n  Alex      Wang,      Amanpreet      Singh,      Julian      Michael,      Felix  \\n \\n  Hill,      Omer      Levy,      and      Samuel      R      Bowman.\\nGlue:\\n \\n \\n \\nA      multi-task      benchmark      and      analysis      platform      for  \\n \\n  natural      language      understanding.\\narXiv      preprint  \\n \\n  arXiv:      1804.07461,      2018.\\n \\n \\n  Alex      Warstadt,      Amanpreet      Singh,      and      Samuel      R.  \\n \\n  Bowman.\\nNeural      network      acceptability      judgments.\\n \\n \\n  arXiv      preprint      1805.12471,      2018.\\n \\n \\n  Adina      Williams,      Nikita      Nangia,      and      Samuel      R      Bow-  \\n \\n  man.\\n \\n \\nA   \\n \\n_      broad-coverage      challenge      corpus      for  \\n \\n  sentence      understanding      through      inference.\\narXiv  \\n \\n  preprint      arXiv:      1704.05426,      2017.\\n \\n \\n  Adina      Williams,      Nikita      Nangia,      and      Samuel      R.      Bow-  \\n \\n  man.\\n \\n \\nA      broad-coverage      challenge      corpus      for      sen-  \\n \\n  tence      understanding      through      inference.\\nIn      Proceed-  \\n \\n  ings      of      NAACL-HLT,      2018.\\n \\n \\n  Zhilin      Yang,      Zihang      Dai,      Yiming      Yang,      Jaime  \\n \\n  Carbonell,      Ruslan      Salakhutdinov,      and      Quoc   \\n \\nV       Le.\\nXlInet:      Generalized      autoregressive      pretrain-  \\n \\n  ing      for      language      understanding.\\narXiv      preprint  \\n \\n  arXiv:      1906.08237,      2019.'}]\n"
     ]
    }
   ],
   "source": [
    "class Document:\n",
    "    def __init__(self, metadata, page_content):\n",
    "        self.metadata = metadata\n",
    "        self.page_content = page_content\n",
    "\n",
    "# List of Document objects\n",
    "section_documents =section_docs\n",
    "\n",
    "# Convert to list of dictionaries\n",
    "section_documents_dicts = []\n",
    "for doc in section_documents:\n",
    "    doc_dict = {\n",
    "        'metadata': doc.metadata,\n",
    "        'page_content': doc.page_content\n",
    "    }\n",
    "    section_documents_dicts.append(doc_dict)\n",
    "\n",
    "# Print the result\n",
    "print(section_documents_dicts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(section_documents_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'metadata': {'source': 'https://arxiv.org/pdf/1910.13461.pdf',\n",
       "   'section_number': 0,\n",
       "   'section_title': '2.1      Architecture'},\n",
       "  'page_content': '2.1      Architecture\\nture  \\n \\n  BART      uses      the      standard      sequence-to-sequence      Trans-  \\n \\n  former      architecture      from      (Vaswani      et      al.,      2017),      ex-  \\n \\n  cept,      following      GPT,      that      we      modify      ReLU      activa-  \\n \\n  tion      functions      to      GeLUs      (Hendrycks   \\n \\n&      Gimpel,      2016)  \\n \\n  and      initialise      parameters      from      A/(0,0.02).\\nFor      our  \\n \\n  base      model,      we      use   \\n \\n6      layers      in      the      encoder      and      de-  \\n \\n  coder,      and      for      our      large      model      we      use      12      layers      in  \\n \\n  each.\\nThe      architecture      is      closely      related      to      that      used      in  \\n \\n  BERT,      with      the      following      differences:      (1)      each      layer      of  \\n \\n  the      decoder      additionally      performs      cross-attention      over  \\n \\n  the      final      hidden      layer      of      the      encoder      (as      in      the      trans-  \\n \\n  former      sequence-to-sequence      model);      and      (2)      BERT  \\n \\n  uses      an      additional      feed-forward      network      before      word-  \\n \\n  prediction,      which      BART      does      not.\\nIn      total,      BART      con-  \\n \\n  tains      roughly      10%      more      parameters      than      the      equiva-  \\n \\n  lently      sized      BERT      model.'},\n",
       " {'metadata': {'source': 'https://arxiv.org/pdf/1910.13461.pdf',\n",
       "   'section_number': 1,\n",
       "   'section_title': '2.2      Pre-training      BART'},\n",
       "  'page_content': '2.2      Pre-training      BART\\nBART  \\n \\n  BART      is      trained      by      corrupting      documents      and      then      op-  \\n \\n  timizing   \\n \\na      reconstruction      loss—the      cross-entropy      be-  \\n \\n  tween      the      decoder’s      output      and      the      original      document.\\n \\n \\n  Unlike      existing      denoising      autoencoders,      which      are      tai-  \\n \\n  lored      to      specific      noising      schemes,      BART      allows      us      to  \\n \\n  apply      any      type      of      document      corruption.\\nIn      the      extreme  \\n \\n  case,      where      all      information      about      the      source      is      lost,  \\n \\n  BART      is      equivalent      to   \\n \\na      language      model.\\n \\n \\n  We      experiment      with      several      previously      proposed      and  \\n \\n  novel      transformations,      but      we      believe      there      is   \\n \\na      sig-  \\n \\n  nificant      potential      for      development      of      other      new      alter-  \\n \\n  natives.\\nThe      transformations      we      used      are      summarized  \\n \\n  below,      and      examples      are      shown      in      Figure      2.\\n \\n \\n  Token      Masking      Following      BERT      (Devlin      et      al.,  \\n \\n  2019),      random      tokens      are      sampled      and      replaced      with  \\n \\n  [MASK]      elements.\\n \\n \\n  Token      Deletion      Random      tokens      are      deleted      from      the  \\n \\n  input.\\nIn      contrast      to      token      masking,      the      model      must  \\n \\n  decide      which      positions      are      missing      inputs.\\nToken      Masking'},\n",
       " {'metadata': {'source': 'https://arxiv.org/pdf/1910.13461.pdf',\n",
       "   'section_number': 2,\n",
       "   'section_title': 'Token      Masking'},\n",
       "  'page_content': 'Token      Masking'},\n",
       " {'metadata': {'source': 'https://arxiv.org/pdf/1910.13461.pdf',\n",
       "   'section_number': 3,\n",
       "   'section_title': 'C.DE.AB'},\n",
       "  'page_content': 'C.DE.AB\\nToken      Deletion Text      Infilling\\n \\n \\n  Text      Infilling   \\n \\nA      number      of      text      spans      are      sampled,  \\n \\n  with      span      lengths      drawn      from   \\n \\na      Poisson      distribution  \\n \\n  (A   \\n \\n=      3).\\nEach      span      is      replaced      with   \\n \\na      single      [MASK]  \\n \\n  token.\\nO-length      spans      correspond      to      the      insertion      of  \\n \\n  [MASK]      tokens.\\nText      infilling      is      inspired      by      Span-  \\n \\n  BERT      (Joshi      et      al.,      2019),      but      SpanBERT      samples  \\n \\n  span      lengths      from   \\n \\na      different      (clamped      geometric)      dis-  \\n \\n  tribution,      and      replaces      each      span      with   \\n \\na      sequence      of  \\n \\n  [MASK]      tokens      of      exactly      the      same      length.\\nText      infill-  \\n \\n  ing      teaches      the      model      to      predict      how      many      tokens      are  \\n \\n  missing      from   \\n \\na      span.\\n \\n \\n  Sentence      Permutation   \\n \\nA      document      is      divided      into  \\n \\n  sentences      based      on      full      stops,      and      these      sentences      are  \\n \\n  shuffled      in   \\n \\na      random      order.\\n \\n \\n  Document      Rotation   \\n \\nA      token      is      chosen      uniformly      at  \\n \\n  random,      and      the      document      is      rotated      so      that      it      begins  \\n \\n  with      that      token.\\nThis      task      trains      the      model      to      identify  \\n \\n  the      start      of      the      document.\\n3\\n \\n   Fine-tuning      BART The      representations      produced      by      BART      can      be      used      in  \\n \\n  several      ways      for      downstream      applications.\\n3.1      Sequence      Classification      Tasks\\nasks  \\n \\n  For      sequence      classification      tasks,      the      same      input      is      fed  \\n \\n  into      the      encoder      and      decoder,      and      the      final      hidden      state  \\n \\n  of      the      final      decoder      token      is      fed      into      new      multi-class  \\n \\n  linear      classifier.\\nThis      approach      is      related      to      the      CLS  \\n \\n  token      in      BERT;      however      we      add      the      additional      token  \\n \\n  to      the      end      so      that      representation      for      the      token      in      the  \\n \\n  decoder      can      attend      to      decoder      states      from      the      complete  \\n \\n  input      (Figure      3a).\\n3.2.      Token      Classification      Tasks\\nasks  \\n \\n  For      token      classification      tasks,      such      as      answer      endpoint  \\n \\n  classification      for      SQUAD,      we      feed      the      complete      doc-  \\n \\n  ument      into      the      encoder      and      decoder,      and      use      the      top  \\n \\n  hidden      state      of      the      decoder      as   \\n \\na      representation      for      each  \\n \\n  word.\\nThis      representation      is      used      to      classify      the      token.\\n3.3      Sequence      Generation      Tasks\\nasks  \\n \\n  Because      BART      has      an      autoregressive      decoder,      it      can      be  \\n \\n  directly      fine      tuned      for      sequence      generation      tasks      such  \\n \\n  as      abstractive      question      answering      and      summarization.\\n \\n \\n  In      both      of      these      tasks,      information      is      copied      from      the  \\n \\n  input      but      manipulated,      which      is      closely      related      to      the  \\n \\n  denoising      pre-training      objective.\\nHere,      the      encoder      in-  \\n \\n  put      is      the      input      sequence,      and      the      decoder      generates  \\n \\n  outputs      autoregressively.\\n3.4      Machine      Translation\\ntion  \\n \\n  We      also      explore      using      BART      to      improve      machine      trans-  \\n \\n  lation      decoders      for      translating      into      English.\\nPrevious  \\n \\n  work      Edunov      et      al.\\n(2019)      has      shown      that      models      can  \\n \\n  be      improved      by      incorporating      pre-trained      encoders,      but  \\n \\n  gains      from      using      pre-trained      language      models      in      de-  \\n \\n  coders      have      been      limited.\\nWe      show      that      it      is      possible  \\n \\n  to      use      the      entire      BART      model      (both      encoder      and      de-  \\n \\n  coder)      as   \\n \\na      single      pretrained      decoder      for      machine      trans-  \\n \\n  lation,      by      adding   \\n \\na      new      set      of      encoder      parameters      that  \\n \\n  are      learned      from      bitext      (see      Figure      3b).\\n \\n \\n  More      precisely,      we      replace      BART’s      encoder      embed-  \\n \\n  ding      layer      with   \\n \\na      new      randomly      initialized      encoder.\\n \\n \\n  The      model      is      trained      end-to-end,      which      trains      the      new       encoder      to      map      foreign      words      into      an      input      that      BART  \\n \\n  can      de-noise      to      English.\\nThe      new      encoder      can      use   \\n \\na       separate      vocabulary      from      the      original      BART      model.\\n \\n \\n  We      train      the      source      encoder      in      two      steps,      in      both  \\n \\n  cases      backpropagating      the      cross-entropy      loss      from      the  \\n \\n  output      of      the      BART      model.\\nIn      the      first      step,      we      freeze  \\n \\n  most      of      BART      parameters      and      only      update      the      ran-  \\n \\n  domly      initialized      source      encoder,      the      BART      positional  \\n \\n  embeddings,      and      the      self-attention      input      projection      ma-  \\n \\n  trix      of      BART’s      encoder      first      layer.\\nIn      the      second      step,  \\n \\n  we      train      all      model      parameters      for   \\n \\na      small      number      of  \\n \\n  iterations.\\n4\\n \\n   Comparing      Pre-training      Objectives  \\n \\n  BART      supports   \\n \\na      much      wider      range      of      noising      schemes  \\n \\n  during      pre-training      than      previous      work.\\nWe      compare   \\n \\na       range      of      options      using      base-size      models      (6      encoder      and  \\n \\n  6      decoder      layers,      with   \\n \\na      hidden      size      of      768),      evaluated  \\n \\n  on   \\n \\na      representative      subset      of      the      tasks      we      will      consider  \\n \\n  for      the      full      large      scale      experiments      in      85.\\n4.1      Comparison      Objectives\\nives  \\n \\n  While      many      pre-training      objectives      have      been      pro-  \\n \\n  posed,      fair      comparisons      between      these      have      been      dif-  \\n \\n  ficult      to      perform,      at      least      in      part      due      to      differences      in  \\n \\n  training      data,      training      resources,      architectural      differ-  \\n \\n  ences      between      models,      and      fine-tuning      procedures.\\nWe  \\n \\n  ABCDE  \\n \\n  weee  \\n \\n  Pre-trained      Pre-trained  \\n \\n  Craters      Joo      rezapes cise)      Te      TrET Pre-trained  \\n \\n  Encoder Pre-trained  \\n \\n  Decoder aa      AB  \\n \\n  trrrt  \\n \\n  tr      ttt  \\n \\n  <s>A      BCD  \\n \\n  Randomly  \\n \\n  Initialized      Encoder  \\n \\n  TFT  \\n \\n  aBpByoe sentation      from      the      final      output      is      used.\\n \\n \\n  re-implement      strong      pre-training      approaches      recently  \\n \\n  proposed      for      discriminative      and      generation      tasks.\\nWe  \\n \\n  aim,      as      much      as      possible,      to      control      for      differences      un-  \\n \\n  related      to      the      pre-training      objective.\\nHowever,      we      do  \\n \\n  make      minor      changes      to      the      learning      rate      and      usage      of  \\n \\n  layer      normalisation      in      order      to      improve      performance  \\n \\n  (tuning      these      separately      for      each      objective).\\nFor      refer-  \\n \\n  ence,      we      compare      our      implementations      with      published  \\n \\n  numbers      from      BERT,      which      was      also      trained      for      1M  \\n \\n  steps      on   \\n \\na      combination      of      books      and      Wikipedia      data.\\n \\n \\n  We      compare      the      following      approaches:\\n \\n \\n  Language      Model      Similarly      to      GPT      (Radford      et      al.,  \\n \\n  2018),      we      train   \\n \\na      left-to-right      Transformer      language  \\n \\n  model.\\nThis      model      is      equivalent      to      the      BART      decoder,  \\n \\n  without      cross-attention.\\n \\n \\n  Permuted      Language      Model      Based      on      XLNet      (Yang  \\n \\n  et      al.,      2019),      we      sample      1/6      of      the      tokens,      and      gener-  \\n \\n  ate      them      in   \\n \\na      random      order      autoregressively.\\nFor      con-  \\n \\n  sistency      with      other      models,      we      do      not      implement      the  \\n \\n  relative      positional      embeddings      or      attention      across      seg-  \\n \\n  ments      from      XLNet.\\n \\n \\n  Masked      Language      Model      Following      BERT      (Devlin  \\n \\n  et      al.,      2019),      we      replace      15%      of      tokens      with      [MASK]  \\n \\n  symbols,      and      train      the      model      to      independently      predict  \\n \\n  the      original      tokens.\\n \\n \\n  Multitask      Masked      Language      Model      As      in      UniLM  \\n \\n  (Dong      et      al.,      2019),      we      train   \\n \\na      Masked      Language  \\n \\n  Model      with      additional      self-attention      masks.\\nSelf      at-  \\n \\n  tention      masks      are      chosen      randomly      in      with      the      follow  \\n \\n  proportions:      1/6      left-to-right,      1/6      right-to-left,      1/3      un-  \\n \\n  masked,      and      1/3      with      the      first      50%      of      tokens      unmasked  \\n \\n  and   \\n \\na      left-to-right      mask      for      the      remainder.\\n \\n \\n  Masked      Seq-to-Seq      Inspired      by      MASS      (Song      et      al.,  \\n \\n  2019),      we      mask   \\n \\na      span      containing      50%      of      tokens,  \\n \\n  and      train   \\n \\na      sequence      to      sequence      model      to      predict      the  \\n \\n  masked      tokens.\\n \\n \\n  For      the      Permuted      LM,      Masked      LM      and      Multitask  \\n \\n  Masked      LM,      we      use      two-stream      attention      (Yang      et      al.,  \\n \\n  2019)      to      efficiently      compute      likelihoods      of      the      output  \\n \\n  part      of      the      sequence      (using   \\n \\na      diagonal      self-attention  \\n \\n  mask      on      the      output      to      predict      words      left-to-right).\\n \\n \\n  We      experiment      with      (1)      treating      the      task      as   \\n \\na      stan-  \\n \\n  dard      sequence-to-sequence      problem,      where      the      source  \\n \\n  input      to      the      encoder      and      the      target      is      the      decoder      out-  \\n \\n  put,      or      (2)      adding      the      source      as      prefix      to      the      target      in  \\n \\n  the      decoder,      with   \\n \\na      loss      only      on      the      target      part      of      the  \\n \\n  sequence.\\nWe      find      the      former      works      better      for      BART  \\n \\n  models,      and      the      latter      for      other      models.\\n \\n \\n  To      most      directly      compare      our      models      on      their      ability  \\n \\n  to      model      their      fine-tuning      objective      (the      log      likelihood  \\n \\n  of      the      human      text),      we      report      perplexity      in      Table      1.\\n4.2      Tasks\\nasks  \\n \\n  SQuAD  \\n \\n  (Rajpurkar      et      al.,      2016)a      an      extractive      ques-  \\n \\n  tion      answering      task      on      Wikipedia      paragraphs.\\nAnswers  \\n \\n  are      text      spans      extracted      from   \\n \\na      given      document      context.\\n \\n \\n  Similar      to      BERT      (Devlin      et      al.,      2019),      we      use      concate-  \\n \\n  nated      question      and      context      as      input      to      the      encoder      of  \\n \\n  BART,      and      additionally      pass      them      to      the      decoder.\\nThe  \\n \\n  model      includes      classifiers      to      predict      the      start      and      end  \\n \\n  indices      of      each      token.\\n \\n \\n  MNLI      (Williams      et      al.,      2017),   \\n \\na      bitext      classification  \\n \\n  task      to      predict      whether      one      sentence      entails      another.\\n \\n \\n  The      fine-tuned      model      concatenates      the      two      sentences  \\n \\n  with      appended      an      EOS      token,      and      passes      them      to      both  \\n \\n  the      BART      encoder      and      decoder.\\nIn      contrast      to      BERT,  \\n \\n  the      representation      of      the      EOS      token      is      used      to      classify  \\n \\n  the      sentences      relations.\\n \\n \\n  ELIS      (Fanetal.,      2019),   \\n \\na      long-form      abstractive      ques-  \\n \\n  tion      answering      dataset.\\nModels      generate      answers      con-  \\n \\n  ditioned      on      the      concatenation      of   \\n \\na      question      and      sup-  \\n \\n  porting      documents.\\nXSum_      (Narayan      et      al.,      2018),   \\n \\na      news      summarization  \\n \\n  dataset      with      highly      abstractive      summaries.\\nConvAI2_      (Dinan      et      al.,      2019),   \\n \\na      dialogue      response  \\n \\n  generation      task,      conditioned      on      context      and   \\n \\na      persona.\\n \\n \\n  CNN/DM_      (Hermann      et      al.,      2015),   \\n \\na      news      summa-  \\n \\n  rization      dataset.\\nSummaries      here      are      typically      closely  \\n \\n  related      to      source      sentences.\\n4.3      Results\\nults Results      are      shown      in      Table      1.\\nSeveral      trends      are      clear:\\nModel      SQuAD      1.1      MNLI      ELIS      XSum   \\n \\n_      ConvAI2      CNN/DM  \\n \\n  Fl      Acc      PPL      PPL      PPL      PPL BERT      Base      (Devlin      et      al.,      2019)      88.5      84.3   \\n \\n-   \\n \\n-   \\n \\n-   \\n \\n-       Masked      Language      Model      90.0      83.5      24.77      7.87      12.59      7.06  \\n \\n  Masked      Seq2seq      87.0      82.1      23.40      6.80      11.43      6.19  \\n \\n  Language      Model      76.7      80.1      21.40      7.00      11.51      6.56  \\n \\n  Permuted      Language      Model      89.1      83.7      24.03      7.69      12.23      6.96  \\n \\n  Multitask      Masked      Language      Model      89.2      82.4      23.73      7.50      12.39      6.74  \\n \\n  BART      Base  \\n \\n  w/      Token      Masking      90.4      84.1      25.05      7.08      11.73      6.10  \\n \\n  w/      Token      Deletion      90.4      84.1      24.61      6.90      11.46      5.87  \\n \\n  w/      Text      Infilling      90.8      84.0      24.26      6.61      11.05      5.83  \\n \\n  w/      Document      Rotation      77.2      75.3      53.69      17.14      19.87      10.59  \\n \\n  w/      Sentence      Shuffling      85.4      81.5      41.87      10.93      16.67      7.89  \\n \\n  w/      Text      Infilling   \\n \\n+      Sentence      Shuffling      90.8      83.8      24.17      6.62      11.12      5.41\\nconsistently      strong      performance.\\n \\n \\n  Performance      of      pre-training      methods      varies      signifi-  \\n \\n  cantly      across      tasks      The      effectiveness      of      pre-training  \\n \\n  methods      is      highly      dependent      on      the      task.\\nFor      exam-  \\n \\n  ple,   \\n \\na      simple      language      model      achieves      the      best      ELIS  \\n \\n  performance,      but      the      worst      SQUAD      results.\\n \\n \\n  Token      masking      is      crucial      Pre-training      objectives  \\n \\n  based      on      rotating      documents      or      permuting      sentences  \\n \\n  perform      poorly      in      isolation.\\nThe      successful      methods  \\n \\n  either      use      token      deletion      or      masking,      or      self-attention  \\n \\n  masks.\\nDeletion      appears      to      outperform      masking      on  \\n \\n  generation      tasks.\\n \\n \\n  Left-to-right      pre-training      improves      generation  \\n \\n  The      Masked      Language      Model      and      the      Permuted  \\n \\n  Language      Model      perform      less      well      than      others      on  \\n \\n  generation,      and      are      the      only      models      we      consider      that  \\n \\n  do      not      include      left-to-right      auto-regressive      language  \\n \\n  modelling      during      pre-training.\\n \\n \\n  Bidirectional      encoders      are      crucial      for      SQUAD      As  \\n \\n  noted      in      previous      work      (Devlin      et      al.,      2019),      just  \\n \\n  left-to-right      decoder      performs      poorly      on      SQuAD,      be-  \\n \\n  cause      future      context      is      crucial      in      classification      deci-  \\n \\n  sions.\\nHowever,      BART      achieves      similar      performance  \\n \\n  with      only      half      the      number      of      bidirectional      layers.\\n \\n \\n  The      pre-training      objective      is      not      the      only      important  \\n \\n  factor      Our      Permuted      Language      Model      performs      less  \\n \\n  well      than      XLNet      (Yang      et      al.,      2019).\\nSome      of      this      dif-  \\n \\n  ference      is      likely      due      to      not      including      other      architectural  \\n \\n  improvements,      such      as      relative-position      embeddings      or  \\n \\n  segment-level      recurrence.\\n \\n \\n  Pure      language      models      perform      best      on      ELIS      The  \\n \\n  ELIS      dataset      is      an      outlier,      with      much      higher      perplex-  \\n \\n  ities      than      other      tasks,      and      is      the      only      generation      task  \\n \\n  where      other      models      outperform      BART.\\n \\n \\nA      pure      lan-  \\n \\n  guage      model      performs      best,      suggesting      that      BART      is  \\n \\n  less      effective      when      the      output      is      only      loosely      con-  \\n \\n  strained      by      the      input.\\n \\n \\n  BART      achieves      the      most      consistently      strong      perfor-  \\n \\n  mance.\\nWith      the      exception      of      ELI5,      BART      models  \\n \\n  using      text-infilling      perform      well      on      all      tasks.\\n5\\n \\n   Large-scale      Pre-training      Experiments  \\n \\n  Recent      work      has      shown      that      downstream      performance  \\n \\n  can      dramatically      improve      when      pre-training      is      scaled  \\n \\n  to      large      batch      sizes      (Yang      et      al.,      2019;      Liu      et      al.,      2019)  \\n \\n  and      corpora.\\nTo      test      how      well      BART      performs      in      this  \\n \\n  regime,      and      to      create   \\n \\na      useful      model      for      downstream  \\n \\n  tasks,      we      trained      BART      using      the      same      scale      as      the  \\n \\n  RoBERTa      model.\\n5.1      Experimental      Setup\\netup  \\n \\n  We      pre-train   \\n \\na      large      model      with      12      layers      in      each      of      the  \\n \\n  encoder      and      decoder,      and   \\n \\na      hidden      size      of      1024.\\nFol-  \\n \\n  lowing      RoBERTa      (Liu      et      al.,      2019),      we      use   \\n \\na      batch      size  \\n \\n  of      8000,      and      train      the      model      for      500000      steps.\\nDocu-  \\n \\n  ments      are      tokenized      with      the      same      byte-pair      encoding  \\n \\n  as      GPT-2      (Radford      et      al.,      2019).\\nBased      on      the      results      in  \\n \\n  Section      §4,      we      use   \\n \\na      combination      of      text      infilling      and  \\n \\n  sentence      permutation.\\nWe      mask      30%      of      tokens      in      each  \\n \\n  document,      and      permute      all      sentences.\\nAlthough      sen-  \\n \\n  tence      permutation      only      shows      significant      additive      gains SQuAD      1.1      SQuAD      2.0      MNLI      SST      QQP      QNLI      STS-B      RTE      MRPC      CoLA  \\n \\n  EM/F1      EM/F1      m/mm      Acc      Acc      Acc      Acc      Acc      Acc      Mcc\\n \\n \\n  BERT      84.1/90.9      79.0/81.8      86.6/-      93.2      91.3      92.3      90.0      70.4      88.0      60.6  \\n \\n  UniLM      -/-      80.5/83.4      87.0/85.9      94.5   \\n \\n-      92.7   \\n \\n-      70.9   \\n \\n-      61.1  \\n \\n  XLNet      89.0/94.5      86.1/88.8      89.8/-      95.6      91.8      93.9      91.8      83.8      89.2      63.6  \\n \\n  RoBERTa      88.9/94.6      86.5/89.4      90.2/90.2      96.4      92.2      94.7      92.4      86.6      90.9      68.0  \\n \\n  BART      88.8/94.6      86.1/89.2      89.9/90.1      96.6      92.5      94.9      91.2      87.0      90.4      62.8 Table      2:      Results      for      large      models      on      SQUAD      and      GLUE      tasks.\\nBART      performs      comparably      to      ROBERTa      and  \\n \\n  XLNet,      suggesting      that      BART’s      uni-directional      decoder      layers      do      not      reduce      performance      on      discriminative      tasks.\\nCNN/DailyMail      XSum       RI      R2      RL      RI      R2      RL\\n \\n \\n  Lead-3      40.42      17.62      36.67      16.30      1.60      11.95  \\n \\n  PTGEN      (See      et      al.,      2017)      36.44      15.66      33.42      29.70      9.21      23.24  \\n \\n  PTGEN+COV      (See      et      al.,      2017)      39.53      17.28      36.38      28.10      8.02      21.72  \\n \\n  UniLM      43.33      20.21      40.51   \\n \\n-   \\n \\n-   \\n \\n-       BERTSUMABS      (Liu   \\n \\n&      Lapata,      2019)      41.72      19.39      38.76      38.76      16.33      31.15  \\n \\n  BERTSUMEXTABS      (Liu   \\n \\n&      Lapata,      2019)      42.13      19.60      39.18      38.81      16.50      31.27 BART      44.16      21.28      40.90      45.14      22.27      37.25 Table      3:      Results      on      two      standard      summarization      datasets.\\nBART      outperforms      previous      work      on      summarization      on\\n \\n \\n  on      the      CNN/DM      summarization      dataset,      we      hypothe-      ConvAI2  \\n \\n  sised      that      larger      pre-trained      models      may      be      better      able      Valid      F1      Valid      PPL  \\n \\n  to   \\n \\n|      fi      this      task.\\nTo      help      th      del      better      fit      th  \\n \\n  RE      aa      ae      Pai      Cae      Seq2Seq+      Attention      16.02      35.07  \\n \\n  data,      we      disabled      dropout      for      the      final      10%      of      training   \\n \\n.    \\n.\\n     Best      System      19.09      17.51  \\n \\n  steps.\\nWe      use      the      same      pre-training      data      as      Liu      et      al.\\n(2019),      consisting      of      160Gb      of      news,      books,      stories,  \\n \\n  and      web      text.\\nBART      20.72      11.85 5.2      Discriminative      Tasks  \\n \\n  Table   \\n \\n2      compares      the      performance      of      BART      with      sev-  \\n \\n  eral      recent      approaches      on      the      well-studied      SQUAD      and  \\n \\n  GLUE      tasks      (Warstadt      et      al.,      2018;      Socher      et      al.,      2013;  \\n \\n  Dolan   \\n \\n&      Brockett,      2005;      Agirre      et      al.,      2007;      Williams  \\n \\n  et      al.,      2018;      Dagan      et      al.,      2006;      Levesque      et      al.,      2011).\\n \\n \\n  Table      4:      BART      outperforms      previous      work      on      conver-  \\n \\n  sational      response      generation.\\nPerplexities      are      renor-  \\n \\n  malized      based      on      official      tokenizer      for      ConvAI2.\\n \\n \\n  The      most      directly      comparable      baseline      is      ROBERTa,  \\n \\n  which      was      pre-trained      with      the      same      resources,      but  \\n \\n  a      different      objective.\\nOverall,      BART      performs      simi-  \\n \\n  larly,      with      only      small      differences      between      the      models  \\n \\n  on      most      tasks.\\nsuggesting      that      BART’s      improvements  \\n \\n  on      generation      tasks      do      not      come      at      the      expense      of      clas-  \\n \\n  sification      performance.\\n \\n \\n  Summarization      To      provide   \\n \\na      comparison      with      the  \\n \\n  state-of-the-art      in      summarization,      we      present      results  \\n \\n  on      two      summarization      datasets,      CNN/DailyMail      and  \\n \\n  XSum,      which      have      distinct      properties.\\n \\n \\n  Summaries      in      the      CNN/DailyMail      tend      to      resemble  \\n \\n  source      sentences.\\nExtractive      models      do      well      here,      and  \\n \\n  even      the      baseline      of      the      first-three      source      sentences      is  \\n \\n  highly      competitive.\\nNevertheless,      BART      outperforms  \\n \\n  all      existing      work.\\n5.3.      Generation      Tasks\\nGeneration      Tasks\\n |       We      also      experiment      with      several      text      generation      tasks.       BART      is      fine-tuned      as      a      standard      sequence-to-sequence       model      from      the      input      to      the      output      text.      During      fine-       tuning      we      use      a      label      smoothed      cross      entropy      loss       (Pereyra      et      al.,      2017),      with      the      smoothing      parameter       set      to      0.1.      During      generation,      we      set      beam      size      as      5,       remove      duplicated      trigrams      in      beam      search,      and      tuned       the      model      with      min-len,      max-len,      length      penalty      on      the       validation      set      (Fan      et      al.,      2017). | 6.0      points      on      all      ROUGE      metrics—trepresenting   \\n \\na      sig-  \\n \\n  nificant      advance      in      performance      on      this      problem.\\nQual-  \\n \\n  itatively,      sample      quality      is      high      (see      86).\\n \\n \\n  In      contrast,      XSum      is      highly      abstractive,      and      extrac-  \\n \\n  tive      models      perform      poorly.\\nBART      outperforms      the  \\n \\n  best      previous      work,      which      leverages      BERT,      by      roughly  \\n \\n  Dialogue      We      evaluate      dialogue      response      generation  \\n \\n  on      CONVAI2      (Dinan      et      al.,      2019),      in      which      agents  \\n \\n  must      generate      responses      conditioned      on      both      the      pre-  \\n \\n  vious      context      and   \\n \\na      textually-specified      persona.\\nBART  \\n \\n  outperforms      previous      work      on      two      automated      metrics.\\n\\nRl      R2      RL\\n \\n \\n  Best      Extractive      23.55      3.1      17.5  \\n \\n  Language      Model      27.8      47      23.1  \\n \\n  Seq2Seq      28.3      5.1      22.8  \\n \\n  Seq2Seq      Multitask      28.9      54      23.1  \\n \\n  BART      30.6      6.2      24.3  \\n \\n  Table      5:      BART      achieves      state-of-the-art      results      on  \\n \\n  the      challenging      ELI5      abstractive      question      answering  \\n \\n  dataset.\\nComparison      models      are      from      Fan      et      al.\\n(2019).'},\n",
       " {'metadata': {'source': 'https://arxiv.org/pdf/1910.13461.pdf',\n",
       "   'section_number': 4,\n",
       "   'section_title': 'Token      Deletion Text      Infilling'},\n",
       "  'page_content': 'Token      Deletion Text      Infilling\\n \\n \\n  Text      Infilling   \\n \\nA      number      of      text      spans      are      sampled,  \\n \\n  with      span      lengths      drawn      from   \\n \\na      Poisson      distribution  \\n \\n  (A   \\n \\n=      3).\\nEach      span      is      replaced      with   \\n \\na      single      [MASK]  \\n \\n  token.\\nO-length      spans      correspond      to      the      insertion      of  \\n \\n  [MASK]      tokens.\\nText      infilling      is      inspired      by      Span-  \\n \\n  BERT      (Joshi      et      al.,      2019),      but      SpanBERT      samples  \\n \\n  span      lengths      from   \\n \\na      different      (clamped      geometric)      dis-  \\n \\n  tribution,      and      replaces      each      span      with   \\n \\na      sequence      of  \\n \\n  [MASK]      tokens      of      exactly      the      same      length.\\nText      infill-  \\n \\n  ing      teaches      the      model      to      predict      how      many      tokens      are  \\n \\n  missing      from   \\n \\na      span.\\n \\n \\n  Sentence      Permutation   \\n \\nA      document      is      divided      into  \\n \\n  sentences      based      on      full      stops,      and      these      sentences      are  \\n \\n  shuffled      in   \\n \\na      random      order.\\n \\n \\n  Document      Rotation   \\n \\nA      token      is      chosen      uniformly      at  \\n \\n  random,      and      the      document      is      rotated      so      that      it      begins  \\n \\n  with      that      token.\\nThis      task      trains      the      model      to      identify  \\n \\n  the      start      of      the      document.\\n3\\n \\n   Fine-tuning      BART The      representations      produced      by      BART      can      be      used      in  \\n \\n  several      ways      for      downstream      applications.\\n3.1      Sequence      Classification      Tasks\\nasks  \\n \\n  For      sequence      classification      tasks,      the      same      input      is      fed  \\n \\n  into      the      encoder      and      decoder,      and      the      final      hidden      state  \\n \\n  of      the      final      decoder      token      is      fed      into      new      multi-class  \\n \\n  linear      classifier.\\nThis      approach      is      related      to      the      CLS  \\n \\n  token      in      BERT;      however      we      add      the      additional      token  \\n \\n  to      the      end      so      that      representation      for      the      token      in      the  \\n \\n  decoder      can      attend      to      decoder      states      from      the      complete  \\n \\n  input      (Figure      3a).\\n3.2.      Token      Classification      Tasks\\nasks  \\n \\n  For      token      classification      tasks,      such      as      answer      endpoint  \\n \\n  classification      for      SQUAD,      we      feed      the      complete      doc-  \\n \\n  ument      into      the      encoder      and      decoder,      and      use      the      top  \\n \\n  hidden      state      of      the      decoder      as   \\n \\na      representation      for      each  \\n \\n  word.\\nThis      representation      is      used      to      classify      the      token.\\n3.3      Sequence      Generation      Tasks\\nasks  \\n \\n  Because      BART      has      an      autoregressive      decoder,      it      can      be  \\n \\n  directly      fine      tuned      for      sequence      generation      tasks      such  \\n \\n  as      abstractive      question      answering      and      summarization.\\n \\n \\n  In      both      of      these      tasks,      information      is      copied      from      the  \\n \\n  input      but      manipulated,      which      is      closely      related      to      the  \\n \\n  denoising      pre-training      objective.\\nHere,      the      encoder      in-  \\n \\n  put      is      the      input      sequence,      and      the      decoder      generates  \\n \\n  outputs      autoregressively.\\n3.4      Machine      Translation\\ntion  \\n \\n  We      also      explore      using      BART      to      improve      machine      trans-  \\n \\n  lation      decoders      for      translating      into      English.\\nPrevious  \\n \\n  work      Edunov      et      al.\\n(2019)      has      shown      that      models      can  \\n \\n  be      improved      by      incorporating      pre-trained      encoders,      but  \\n \\n  gains      from      using      pre-trained      language      models      in      de-  \\n \\n  coders      have      been      limited.\\nWe      show      that      it      is      possible  \\n \\n  to      use      the      entire      BART      model      (both      encoder      and      de-  \\n \\n  coder)      as   \\n \\na      single      pretrained      decoder      for      machine      trans-  \\n \\n  lation,      by      adding   \\n \\na      new      set      of      encoder      parameters      that  \\n \\n  are      learned      from      bitext      (see      Figure      3b).\\n \\n \\n  More      precisely,      we      replace      BART’s      encoder      embed-  \\n \\n  ding      layer      with   \\n \\na      new      randomly      initialized      encoder.\\n \\n \\n  The      model      is      trained      end-to-end,      which      trains      the      new       encoder      to      map      foreign      words      into      an      input      that      BART  \\n \\n  can      de-noise      to      English.\\nThe      new      encoder      can      use   \\n \\na       separate      vocabulary      from      the      original      BART      model.\\n \\n \\n  We      train      the      source      encoder      in      two      steps,      in      both  \\n \\n  cases      backpropagating      the      cross-entropy      loss      from      the  \\n \\n  output      of      the      BART      model.\\nIn      the      first      step,      we      freeze  \\n \\n  most      of      BART      parameters      and      only      update      the      ran-  \\n \\n  domly      initialized      source      encoder,      the      BART      positional  \\n \\n  embeddings,      and      the      self-attention      input      projection      ma-  \\n \\n  trix      of      BART’s      encoder      first      layer.\\nIn      the      second      step,  \\n \\n  we      train      all      model      parameters      for   \\n \\na      small      number      of  \\n \\n  iterations.\\n4\\n \\n   Comparing      Pre-training      Objectives  \\n \\n  BART      supports   \\n \\na      much      wider      range      of      noising      schemes  \\n \\n  during      pre-training      than      previous      work.\\nWe      compare   \\n \\na       range      of      options      using      base-size      models      (6      encoder      and  \\n \\n  6      decoder      layers,      with   \\n \\na      hidden      size      of      768),      evaluated  \\n \\n  on   \\n \\na      representative      subset      of      the      tasks      we      will      consider  \\n \\n  for      the      full      large      scale      experiments      in      85.\\n4.1      Comparison      Objectives\\nives  \\n \\n  While      many      pre-training      objectives      have      been      pro-  \\n \\n  posed,      fair      comparisons      between      these      have      been      dif-  \\n \\n  ficult      to      perform,      at      least      in      part      due      to      differences      in  \\n \\n  training      data,      training      resources,      architectural      differ-  \\n \\n  ences      between      models,      and      fine-tuning      procedures.\\nWe  \\n \\n  ABCDE  \\n \\n  weee  \\n \\n  Pre-trained      Pre-trained  \\n \\n  Craters      Joo      rezapes cise)      Te      TrET Pre-trained  \\n \\n  Encoder Pre-trained  \\n \\n  Decoder aa      AB  \\n \\n  trrrt  \\n \\n  tr      ttt  \\n \\n  <s>A      BCD  \\n \\n  Randomly  \\n \\n  Initialized      Encoder  \\n \\n  TFT  \\n \\n  aBpByoe sentation      from      the      final      output      is      used.\\n \\n \\n  re-implement      strong      pre-training      approaches      recently  \\n \\n  proposed      for      discriminative      and      generation      tasks.\\nWe  \\n \\n  aim,      as      much      as      possible,      to      control      for      differences      un-  \\n \\n  related      to      the      pre-training      objective.\\nHowever,      we      do  \\n \\n  make      minor      changes      to      the      learning      rate      and      usage      of  \\n \\n  layer      normalisation      in      order      to      improve      performance  \\n \\n  (tuning      these      separately      for      each      objective).\\nFor      refer-  \\n \\n  ence,      we      compare      our      implementations      with      published  \\n \\n  numbers      from      BERT,      which      was      also      trained      for      1M  \\n \\n  steps      on   \\n \\na      combination      of      books      and      Wikipedia      data.\\n \\n \\n  We      compare      the      following      approaches:\\n \\n \\n  Language      Model      Similarly      to      GPT      (Radford      et      al.,  \\n \\n  2018),      we      train   \\n \\na      left-to-right      Transformer      language  \\n \\n  model.\\nThis      model      is      equivalent      to      the      BART      decoder,  \\n \\n  without      cross-attention.\\n \\n \\n  Permuted      Language      Model      Based      on      XLNet      (Yang  \\n \\n  et      al.,      2019),      we      sample      1/6      of      the      tokens,      and      gener-  \\n \\n  ate      them      in   \\n \\na      random      order      autoregressively.\\nFor      con-  \\n \\n  sistency      with      other      models,      we      do      not      implement      the  \\n \\n  relative      positional      embeddings      or      attention      across      seg-  \\n \\n  ments      from      XLNet.\\n \\n \\n  Masked      Language      Model      Following      BERT      (Devlin  \\n \\n  et      al.,      2019),      we      replace      15%      of      tokens      with      [MASK]  \\n \\n  symbols,      and      train      the      model      to      independently      predict  \\n \\n  the      original      tokens.\\n \\n \\n  Multitask      Masked      Language      Model      As      in      UniLM  \\n \\n  (Dong      et      al.,      2019),      we      train   \\n \\na      Masked      Language  \\n \\n  Model      with      additional      self-attention      masks.\\nSelf      at-  \\n \\n  tention      masks      are      chosen      randomly      in      with      the      follow  \\n \\n  proportions:      1/6      left-to-right,      1/6      right-to-left,      1/3      un-  \\n \\n  masked,      and      1/3      with      the      first      50%      of      tokens      unmasked  \\n \\n  and   \\n \\na      left-to-right      mask      for      the      remainder.\\n \\n \\n  Masked      Seq-to-Seq      Inspired      by      MASS      (Song      et      al.,  \\n \\n  2019),      we      mask   \\n \\na      span      containing      50%      of      tokens,  \\n \\n  and      train   \\n \\na      sequence      to      sequence      model      to      predict      the  \\n \\n  masked      tokens.\\n \\n \\n  For      the      Permuted      LM,      Masked      LM      and      Multitask  \\n \\n  Masked      LM,      we      use      two-stream      attention      (Yang      et      al.,  \\n \\n  2019)      to      efficiently      compute      likelihoods      of      the      output  \\n \\n  part      of      the      sequence      (using   \\n \\na      diagonal      self-attention  \\n \\n  mask      on      the      output      to      predict      words      left-to-right).\\n \\n \\n  We      experiment      with      (1)      treating      the      task      as   \\n \\na      stan-  \\n \\n  dard      sequence-to-sequence      problem,      where      the      source  \\n \\n  input      to      the      encoder      and      the      target      is      the      decoder      out-  \\n \\n  put,      or      (2)      adding      the      source      as      prefix      to      the      target      in  \\n \\n  the      decoder,      with   \\n \\na      loss      only      on      the      target      part      of      the  \\n \\n  sequence.\\nWe      find      the      former      works      better      for      BART  \\n \\n  models,      and      the      latter      for      other      models.\\n \\n \\n  To      most      directly      compare      our      models      on      their      ability  \\n \\n  to      model      their      fine-tuning      objective      (the      log      likelihood  \\n \\n  of      the      human      text),      we      report      perplexity      in      Table      1.\\n4.2      Tasks\\nasks  \\n \\n  SQuAD  \\n \\n  (Rajpurkar      et      al.,      2016)a      an      extractive      ques-  \\n \\n  tion      answering      task      on      Wikipedia      paragraphs.\\nAnswers  \\n \\n  are      text      spans      extracted      from   \\n \\na      given      document      context.\\n \\n \\n  Similar      to      BERT      (Devlin      et      al.,      2019),      we      use      concate-  \\n \\n  nated      question      and      context      as      input      to      the      encoder      of  \\n \\n  BART,      and      additionally      pass      them      to      the      decoder.\\nThe  \\n \\n  model      includes      classifiers      to      predict      the      start      and      end  \\n \\n  indices      of      each      token.\\n \\n \\n  MNLI      (Williams      et      al.,      2017),   \\n \\na      bitext      classification  \\n \\n  task      to      predict      whether      one      sentence      entails      another.\\n \\n \\n  The      fine-tuned      model      concatenates      the      two      sentences  \\n \\n  with      appended      an      EOS      token,      and      passes      them      to      both  \\n \\n  the      BART      encoder      and      decoder.\\nIn      contrast      to      BERT,  \\n \\n  the      representation      of      the      EOS      token      is      used      to      classify  \\n \\n  the      sentences      relations.\\n \\n \\n  ELIS      (Fanetal.,      2019),   \\n \\na      long-form      abstractive      ques-  \\n \\n  tion      answering      dataset.\\nModels      generate      answers      con-  \\n \\n  ditioned      on      the      concatenation      of   \\n \\na      question      and      sup-  \\n \\n  porting      documents.\\nXSum_      (Narayan      et      al.,      2018),   \\n \\na      news      summarization  \\n \\n  dataset      with      highly      abstractive      summaries.\\nConvAI2_      (Dinan      et      al.,      2019),   \\n \\na      dialogue      response  \\n \\n  generation      task,      conditioned      on      context      and   \\n \\na      persona.\\n \\n \\n  CNN/DM_      (Hermann      et      al.,      2015),   \\n \\na      news      summa-  \\n \\n  rization      dataset.\\nSummaries      here      are      typically      closely  \\n \\n  related      to      source      sentences.\\n4.3      Results\\nults Results      are      shown      in      Table      1.\\nSeveral      trends      are      clear:\\nModel      SQuAD      1.1      MNLI      ELIS      XSum   \\n \\n_      ConvAI2      CNN/DM  \\n \\n  Fl      Acc      PPL      PPL      PPL      PPL BERT      Base      (Devlin      et      al.,      2019)      88.5      84.3   \\n \\n-   \\n \\n-   \\n \\n-   \\n \\n-       Masked      Language      Model      90.0      83.5      24.77      7.87      12.59      7.06  \\n \\n  Masked      Seq2seq      87.0      82.1      23.40      6.80      11.43      6.19  \\n \\n  Language      Model      76.7      80.1      21.40      7.00      11.51      6.56  \\n \\n  Permuted      Language      Model      89.1      83.7      24.03      7.69      12.23      6.96  \\n \\n  Multitask      Masked      Language      Model      89.2      82.4      23.73      7.50      12.39      6.74  \\n \\n  BART      Base  \\n \\n  w/      Token      Masking      90.4      84.1      25.05      7.08      11.73      6.10  \\n \\n  w/      Token      Deletion      90.4      84.1      24.61      6.90      11.46      5.87  \\n \\n  w/      Text      Infilling      90.8      84.0      24.26      6.61      11.05      5.83  \\n \\n  w/      Document      Rotation      77.2      75.3      53.69      17.14      19.87      10.59  \\n \\n  w/      Sentence      Shuffling      85.4      81.5      41.87      10.93      16.67      7.89  \\n \\n  w/      Text      Infilling   \\n \\n+      Sentence      Shuffling      90.8      83.8      24.17      6.62      11.12      5.41\\nconsistently      strong      performance.\\n \\n \\n  Performance      of      pre-training      methods      varies      signifi-  \\n \\n  cantly      across      tasks      The      effectiveness      of      pre-training  \\n \\n  methods      is      highly      dependent      on      the      task.\\nFor      exam-  \\n \\n  ple,   \\n \\na      simple      language      model      achieves      the      best      ELIS  \\n \\n  performance,      but      the      worst      SQUAD      results.\\n \\n \\n  Token      masking      is      crucial      Pre-training      objectives  \\n \\n  based      on      rotating      documents      or      permuting      sentences  \\n \\n  perform      poorly      in      isolation.\\nThe      successful      methods  \\n \\n  either      use      token      deletion      or      masking,      or      self-attention  \\n \\n  masks.\\nDeletion      appears      to      outperform      masking      on  \\n \\n  generation      tasks.\\n \\n \\n  Left-to-right      pre-training      improves      generation  \\n \\n  The      Masked      Language      Model      and      the      Permuted  \\n \\n  Language      Model      perform      less      well      than      others      on  \\n \\n  generation,      and      are      the      only      models      we      consider      that  \\n \\n  do      not      include      left-to-right      auto-regressive      language  \\n \\n  modelling      during      pre-training.\\n \\n \\n  Bidirectional      encoders      are      crucial      for      SQUAD      As  \\n \\n  noted      in      previous      work      (Devlin      et      al.,      2019),      just  \\n \\n  left-to-right      decoder      performs      poorly      on      SQuAD,      be-  \\n \\n  cause      future      context      is      crucial      in      classification      deci-  \\n \\n  sions.\\nHowever,      BART      achieves      similar      performance  \\n \\n  with      only      half      the      number      of      bidirectional      layers.\\n \\n \\n  The      pre-training      objective      is      not      the      only      important  \\n \\n  factor      Our      Permuted      Language      Model      performs      less  \\n \\n  well      than      XLNet      (Yang      et      al.,      2019).\\nSome      of      this      dif-  \\n \\n  ference      is      likely      due      to      not      including      other      architectural  \\n \\n  improvements,      such      as      relative-position      embeddings      or  \\n \\n  segment-level      recurrence.\\n \\n \\n  Pure      language      models      perform      best      on      ELIS      The  \\n \\n  ELIS      dataset      is      an      outlier,      with      much      higher      perplex-  \\n \\n  ities      than      other      tasks,      and      is      the      only      generation      task  \\n \\n  where      other      models      outperform      BART.\\n \\n \\nA      pure      lan-  \\n \\n  guage      model      performs      best,      suggesting      that      BART      is  \\n \\n  less      effective      when      the      output      is      only      loosely      con-  \\n \\n  strained      by      the      input.\\n \\n \\n  BART      achieves      the      most      consistently      strong      perfor-  \\n \\n  mance.\\nWith      the      exception      of      ELI5,      BART      models  \\n \\n  using      text-infilling      perform      well      on      all      tasks.\\n5\\n \\n   Large-scale      Pre-training      Experiments  \\n \\n  Recent      work      has      shown      that      downstream      performance  \\n \\n  can      dramatically      improve      when      pre-training      is      scaled  \\n \\n  to      large      batch      sizes      (Yang      et      al.,      2019;      Liu      et      al.,      2019)  \\n \\n  and      corpora.\\nTo      test      how      well      BART      performs      in      this  \\n \\n  regime,      and      to      create   \\n \\na      useful      model      for      downstream  \\n \\n  tasks,      we      trained      BART      using      the      same      scale      as      the  \\n \\n  RoBERTa      model.\\n5.1      Experimental      Setup\\netup  \\n \\n  We      pre-train   \\n \\na      large      model      with      12      layers      in      each      of      the  \\n \\n  encoder      and      decoder,      and   \\n \\na      hidden      size      of      1024.\\nFol-  \\n \\n  lowing      RoBERTa      (Liu      et      al.,      2019),      we      use   \\n \\na      batch      size  \\n \\n  of      8000,      and      train      the      model      for      500000      steps.\\nDocu-  \\n \\n  ments      are      tokenized      with      the      same      byte-pair      encoding  \\n \\n  as      GPT-2      (Radford      et      al.,      2019).\\nBased      on      the      results      in  \\n \\n  Section      §4,      we      use   \\n \\na      combination      of      text      infilling      and  \\n \\n  sentence      permutation.\\nWe      mask      30%      of      tokens      in      each  \\n \\n  document,      and      permute      all      sentences.\\nAlthough      sen-  \\n \\n  tence      permutation      only      shows      significant      additive      gains SQuAD      1.1      SQuAD      2.0      MNLI      SST      QQP      QNLI      STS-B      RTE      MRPC      CoLA  \\n \\n  EM/F1      EM/F1      m/mm      Acc      Acc      Acc      Acc      Acc      Acc      Mcc\\n \\n \\n  BERT      84.1/90.9      79.0/81.8      86.6/-      93.2      91.3      92.3      90.0      70.4      88.0      60.6  \\n \\n  UniLM      -/-      80.5/83.4      87.0/85.9      94.5   \\n \\n-      92.7   \\n \\n-      70.9   \\n \\n-      61.1  \\n \\n  XLNet      89.0/94.5      86.1/88.8      89.8/-      95.6      91.8      93.9      91.8      83.8      89.2      63.6  \\n \\n  RoBERTa      88.9/94.6      86.5/89.4      90.2/90.2      96.4      92.2      94.7      92.4      86.6      90.9      68.0  \\n \\n  BART      88.8/94.6      86.1/89.2      89.9/90.1      96.6      92.5      94.9      91.2      87.0      90.4      62.8 Table      2:      Results      for      large      models      on      SQUAD      and      GLUE      tasks.\\nBART      performs      comparably      to      ROBERTa      and  \\n \\n  XLNet,      suggesting      that      BART’s      uni-directional      decoder      layers      do      not      reduce      performance      on      discriminative      tasks.\\nCNN/DailyMail      XSum       RI      R2      RL      RI      R2      RL\\n \\n \\n  Lead-3      40.42      17.62      36.67      16.30      1.60      11.95  \\n \\n  PTGEN      (See      et      al.,      2017)      36.44      15.66      33.42      29.70      9.21      23.24  \\n \\n  PTGEN+COV      (See      et      al.,      2017)      39.53      17.28      36.38      28.10      8.02      21.72  \\n \\n  UniLM      43.33      20.21      40.51   \\n \\n-   \\n \\n-   \\n \\n-       BERTSUMABS      (Liu   \\n \\n&      Lapata,      2019)      41.72      19.39      38.76      38.76      16.33      31.15  \\n \\n  BERTSUMEXTABS      (Liu   \\n \\n&      Lapata,      2019)      42.13      19.60      39.18      38.81      16.50      31.27 BART      44.16      21.28      40.90      45.14      22.27      37.25 Table      3:      Results      on      two      standard      summarization      datasets.\\nBART      outperforms      previous      work      on      summarization      on\\n \\n \\n  on      the      CNN/DM      summarization      dataset,      we      hypothe-      ConvAI2  \\n \\n  sised      that      larger      pre-trained      models      may      be      better      able      Valid      F1      Valid      PPL  \\n \\n  to   \\n \\n|      fi      this      task.\\nTo      help      th      del      better      fit      th  \\n \\n  RE      aa      ae      Pai      Cae      Seq2Seq+      Attention      16.02      35.07  \\n \\n  data,      we      disabled      dropout      for      the      final      10%      of      training   \\n \\n.    \\n.\\n     Best      System      19.09      17.51  \\n \\n  steps.\\nWe      use      the      same      pre-training      data      as      Liu      et      al.\\n(2019),      consisting      of      160Gb      of      news,      books,      stories,  \\n \\n  and      web      text.\\nBART      20.72      11.85 5.2      Discriminative      Tasks  \\n \\n  Table   \\n \\n2      compares      the      performance      of      BART      with      sev-  \\n \\n  eral      recent      approaches      on      the      well-studied      SQUAD      and  \\n \\n  GLUE      tasks      (Warstadt      et      al.,      2018;      Socher      et      al.,      2013;  \\n \\n  Dolan   \\n \\n&      Brockett,      2005;      Agirre      et      al.,      2007;      Williams  \\n \\n  et      al.,      2018;      Dagan      et      al.,      2006;      Levesque      et      al.,      2011).\\n \\n \\n  Table      4:      BART      outperforms      previous      work      on      conver-  \\n \\n  sational      response      generation.\\nPerplexities      are      renor-  \\n \\n  malized      based      on      official      tokenizer      for      ConvAI2.\\n \\n \\n  The      most      directly      comparable      baseline      is      ROBERTa,  \\n \\n  which      was      pre-trained      with      the      same      resources,      but  \\n \\n  a      different      objective.\\nOverall,      BART      performs      simi-  \\n \\n  larly,      with      only      small      differences      between      the      models  \\n \\n  on      most      tasks.\\nsuggesting      that      BART’s      improvements  \\n \\n  on      generation      tasks      do      not      come      at      the      expense      of      clas-  \\n \\n  sification      performance.\\n \\n \\n  Summarization      To      provide   \\n \\na      comparison      with      the  \\n \\n  state-of-the-art      in      summarization,      we      present      results  \\n \\n  on      two      summarization      datasets,      CNN/DailyMail      and  \\n \\n  XSum,      which      have      distinct      properties.\\n \\n \\n  Summaries      in      the      CNN/DailyMail      tend      to      resemble  \\n \\n  source      sentences.\\nExtractive      models      do      well      here,      and  \\n \\n  even      the      baseline      of      the      first-three      source      sentences      is  \\n \\n  highly      competitive.\\nNevertheless,      BART      outperforms  \\n \\n  all      existing      work.\\n5.3.      Generation      Tasks\\nGeneration      Tasks\\n |       We      also      experiment      with      several      text      generation      tasks.       BART      is      fine-tuned      as      a      standard      sequence-to-sequence       model      from      the      input      to      the      output      text.      During      fine-       tuning      we      use      a      label      smoothed      cross      entropy      loss       (Pereyra      et      al.,      2017),      with      the      smoothing      parameter       set      to      0.1.      During      generation,      we      set      beam      size      as      5,       remove      duplicated      trigrams      in      beam      search,      and      tuned       the      model      with      min-len,      max-len,      length      penalty      on      the       validation      set      (Fan      et      al.,      2017). | 6.0      points      on      all      ROUGE      metrics—trepresenting   \\n \\na      sig-  \\n \\n  nificant      advance      in      performance      on      this      problem.\\nQual-  \\n \\n  itatively,      sample      quality      is      high      (see      86).\\n \\n \\n  In      contrast,      XSum      is      highly      abstractive,      and      extrac-  \\n \\n  tive      models      perform      poorly.\\nBART      outperforms      the  \\n \\n  best      previous      work,      which      leverages      BERT,      by      roughly  \\n \\n  Dialogue      We      evaluate      dialogue      response      generation  \\n \\n  on      CONVAI2      (Dinan      et      al.,      2019),      in      which      agents  \\n \\n  must      generate      responses      conditioned      on      both      the      pre-  \\n \\n  vious      context      and   \\n \\na      textually-specified      persona.\\nBART  \\n \\n  outperforms      previous      work      on      two      automated      metrics.\\n'},\n",
       " {'metadata': {'source': 'https://arxiv.org/pdf/1910.13461.pdf',\n",
       "   'section_number': 5,\n",
       "   'section_title': '3.1      Sequence      Classification      Tasks'},\n",
       "  'page_content': '3.1      Sequence      Classification      Tasks\\nasks  \\n \\n  For      sequence      classification      tasks,      the      same      input      is      fed  \\n \\n  into      the      encoder      and      decoder,      and      the      final      hidden      state  \\n \\n  of      the      final      decoder      token      is      fed      into      new      multi-class  \\n \\n  linear      classifier.\\nThis      approach      is      related      to      the      CLS  \\n \\n  token      in      BERT;      however      we      add      the      additional      token  \\n \\n  to      the      end      so      that      representation      for      the      token      in      the  \\n \\n  decoder      can      attend      to      decoder      states      from      the      complete  \\n \\n  input      (Figure      3a).'},\n",
       " {'metadata': {'source': 'https://arxiv.org/pdf/1910.13461.pdf',\n",
       "   'section_number': 6,\n",
       "   'section_title': '3.2.      Token      Classification      Tasks'},\n",
       "  'page_content': '3.2.      Token      Classification      Tasks\\nasks  \\n \\n  For      token      classification      tasks,      such      as      answer      endpoint  \\n \\n  classification      for      SQUAD,      we      feed      the      complete      doc-  \\n \\n  ument      into      the      encoder      and      decoder,      and      use      the      top  \\n \\n  hidden      state      of      the      decoder      as   \\n \\na      representation      for      each  \\n \\n  word.\\nThis      representation      is      used      to      classify      the      token.'},\n",
       " {'metadata': {'source': 'https://arxiv.org/pdf/1910.13461.pdf',\n",
       "   'section_number': 7,\n",
       "   'section_title': '3.3      Sequence      Generation      Tasks'},\n",
       "  'page_content': '3.3      Sequence      Generation      Tasks\\nasks  \\n \\n  Because      BART      has      an      autoregressive      decoder,      it      can      be  \\n \\n  directly      fine      tuned      for      sequence      generation      tasks      such  \\n \\n  as      abstractive      question      answering      and      summarization.\\n \\n \\n  In      both      of      these      tasks,      information      is      copied      from      the  \\n \\n  input      but      manipulated,      which      is      closely      related      to      the  \\n \\n  denoising      pre-training      objective.\\nHere,      the      encoder      in-  \\n \\n  put      is      the      input      sequence,      and      the      decoder      generates  \\n \\n  outputs      autoregressively.'},\n",
       " {'metadata': {'source': 'https://arxiv.org/pdf/1910.13461.pdf',\n",
       "   'section_number': 8,\n",
       "   'section_title': '3.4      Machine      Translation'},\n",
       "  'page_content': '3.4      Machine      Translation\\ntion  \\n \\n  We      also      explore      using      BART      to      improve      machine      trans-  \\n \\n  lation      decoders      for      translating      into      English.\\nPrevious  \\n \\n  work      Edunov      et      al.\\n(2019)      has      shown      that      models      can  \\n \\n  be      improved      by      incorporating      pre-trained      encoders,      but  \\n \\n  gains      from      using      pre-trained      language      models      in      de-  \\n \\n  coders      have      been      limited.\\nWe      show      that      it      is      possible  \\n \\n  to      use      the      entire      BART      model      (both      encoder      and      de-  \\n \\n  coder)      as   \\n \\na      single      pretrained      decoder      for      machine      trans-  \\n \\n  lation,      by      adding   \\n \\na      new      set      of      encoder      parameters      that  \\n \\n  are      learned      from      bitext      (see      Figure      3b).\\n \\n \\n  More      precisely,      we      replace      BART’s      encoder      embed-  \\n \\n  ding      layer      with   \\n \\na      new      randomly      initialized      encoder.\\n \\n \\n  The      model      is      trained      end-to-end,      which      trains      the      new       encoder      to      map      foreign      words      into      an      input      that      BART  \\n \\n  can      de-noise      to      English.\\nThe      new      encoder      can      use   \\n \\na       separate      vocabulary      from      the      original      BART      model.\\n \\n \\n  We      train      the      source      encoder      in      two      steps,      in      both  \\n \\n  cases      backpropagating      the      cross-entropy      loss      from      the  \\n \\n  output      of      the      BART      model.\\nIn      the      first      step,      we      freeze  \\n \\n  most      of      BART      parameters      and      only      update      the      ran-  \\n \\n  domly      initialized      source      encoder,      the      BART      positional  \\n \\n  embeddings,      and      the      self-attention      input      projection      ma-  \\n \\n  trix      of      BART’s      encoder      first      layer.\\nIn      the      second      step,  \\n \\n  we      train      all      model      parameters      for   \\n \\na      small      number      of  \\n \\n  iterations.\\n4\\n \\n   Comparing      Pre-training      Objectives  \\n \\n  BART      supports   \\n \\na      much      wider      range      of      noising      schemes  \\n \\n  during      pre-training      than      previous      work.\\nWe      compare   \\n \\na       range      of      options      using      base-size      models      (6      encoder      and  \\n \\n  6      decoder      layers,      with   \\n \\na      hidden      size      of      768),      evaluated  \\n \\n  on   \\n \\na      representative      subset      of      the      tasks      we      will      consider  \\n \\n  for      the      full      large      scale      experiments      in      85.\\n4.1      Comparison      Objectives\\nives  \\n \\n  While      many      pre-training      objectives      have      been      pro-  \\n \\n  posed,      fair      comparisons      between      these      have      been      dif-  \\n \\n  ficult      to      perform,      at      least      in      part      due      to      differences      in  \\n \\n  training      data,      training      resources,      architectural      differ-  \\n \\n  ences      between      models,      and      fine-tuning      procedures.\\nWe  \\n \\n  ABCDE  \\n \\n  weee  \\n \\n  Pre-trained      Pre-trained  \\n \\n  Craters      Joo      rezapes cise)      Te      TrET Pre-trained  \\n \\n  Encoder Pre-trained  \\n \\n  Decoder aa      AB  \\n \\n  trrrt  \\n \\n  tr      ttt  \\n \\n  <s>A      BCD  \\n \\n  Randomly  \\n \\n  Initialized      Encoder  \\n \\n  TFT  \\n \\n  aBpByoe sentation      from      the      final      output      is      used.\\n \\n \\n  re-implement      strong      pre-training      approaches      recently  \\n \\n  proposed      for      discriminative      and      generation      tasks.\\nWe  \\n \\n  aim,      as      much      as      possible,      to      control      for      differences      un-  \\n \\n  related      to      the      pre-training      objective.\\nHowever,      we      do  \\n \\n  make      minor      changes      to      the      learning      rate      and      usage      of  \\n \\n  layer      normalisation      in      order      to      improve      performance  \\n \\n  (tuning      these      separately      for      each      objective).\\nFor      refer-  \\n \\n  ence,      we      compare      our      implementations      with      published  \\n \\n  numbers      from      BERT,      which      was      also      trained      for      1M  \\n \\n  steps      on   \\n \\na      combination      of      books      and      Wikipedia      data.\\n \\n \\n  We      compare      the      following      approaches:\\n \\n \\n  Language      Model      Similarly      to      GPT      (Radford      et      al.,  \\n \\n  2018),      we      train   \\n \\na      left-to-right      Transformer      language  \\n \\n  model.\\nThis      model      is      equivalent      to      the      BART      decoder,  \\n \\n  without      cross-attention.\\n \\n \\n  Permuted      Language      Model      Based      on      XLNet      (Yang  \\n \\n  et      al.,      2019),      we      sample      1/6      of      the      tokens,      and      gener-  \\n \\n  ate      them      in   \\n \\na      random      order      autoregressively.\\nFor      con-  \\n \\n  sistency      with      other      models,      we      do      not      implement      the  \\n \\n  relative      positional      embeddings      or      attention      across      seg-  \\n \\n  ments      from      XLNet.\\n \\n \\n  Masked      Language      Model      Following      BERT      (Devlin  \\n \\n  et      al.,      2019),      we      replace      15%      of      tokens      with      [MASK]  \\n \\n  symbols,      and      train      the      model      to      independently      predict  \\n \\n  the      original      tokens.\\n \\n \\n  Multitask      Masked      Language      Model      As      in      UniLM  \\n \\n  (Dong      et      al.,      2019),      we      train   \\n \\na      Masked      Language  \\n \\n  Model      with      additional      self-attention      masks.\\nSelf      at-  \\n \\n  tention      masks      are      chosen      randomly      in      with      the      follow  \\n \\n  proportions:      1/6      left-to-right,      1/6      right-to-left,      1/3      un-  \\n \\n  masked,      and      1/3      with      the      first      50%      of      tokens      unmasked  \\n \\n  and   \\n \\na      left-to-right      mask      for      the      remainder.\\n \\n \\n  Masked      Seq-to-Seq      Inspired      by      MASS      (Song      et      al.,  \\n \\n  2019),      we      mask   \\n \\na      span      containing      50%      of      tokens,  \\n \\n  and      train   \\n \\na      sequence      to      sequence      model      to      predict      the  \\n \\n  masked      tokens.\\n \\n \\n  For      the      Permuted      LM,      Masked      LM      and      Multitask  \\n \\n  Masked      LM,      we      use      two-stream      attention      (Yang      et      al.,  \\n \\n  2019)      to      efficiently      compute      likelihoods      of      the      output  \\n \\n  part      of      the      sequence      (using   \\n \\na      diagonal      self-attention  \\n \\n  mask      on      the      output      to      predict      words      left-to-right).\\n \\n \\n  We      experiment      with      (1)      treating      the      task      as   \\n \\na      stan-  \\n \\n  dard      sequence-to-sequence      problem,      where      the      source  \\n \\n  input      to      the      encoder      and      the      target      is      the      decoder      out-  \\n \\n  put,      or      (2)      adding      the      source      as      prefix      to      the      target      in  \\n \\n  the      decoder,      with   \\n \\na      loss      only      on      the      target      part      of      the  \\n \\n  sequence.\\nWe      find      the      former      works      better      for      BART  \\n \\n  models,      and      the      latter      for      other      models.\\n \\n \\n  To      most      directly      compare      our      models      on      their      ability  \\n \\n  to      model      their      fine-tuning      objective      (the      log      likelihood  \\n \\n  of      the      human      text),      we      report      perplexity      in      Table      1.\\n4.2      Tasks\\nasks  \\n \\n  SQuAD  \\n \\n  (Rajpurkar      et      al.,      2016)a      an      extractive      ques-  \\n \\n  tion      answering      task      on      Wikipedia      paragraphs.\\nAnswers  \\n \\n  are      text      spans      extracted      from   \\n \\na      given      document      context.\\n \\n \\n  Similar      to      BERT      (Devlin      et      al.,      2019),      we      use      concate-  \\n \\n  nated      question      and      context      as      input      to      the      encoder      of  \\n \\n  BART,      and      additionally      pass      them      to      the      decoder.\\nThe  \\n \\n  model      includes      classifiers      to      predict      the      start      and      end  \\n \\n  indices      of      each      token.\\n \\n \\n  MNLI      (Williams      et      al.,      2017),   \\n \\na      bitext      classification  \\n \\n  task      to      predict      whether      one      sentence      entails      another.\\n \\n \\n  The      fine-tuned      model      concatenates      the      two      sentences  \\n \\n  with      appended      an      EOS      token,      and      passes      them      to      both  \\n \\n  the      BART      encoder      and      decoder.\\nIn      contrast      to      BERT,  \\n \\n  the      representation      of      the      EOS      token      is      used      to      classify  \\n \\n  the      sentences      relations.\\n \\n \\n  ELIS      (Fanetal.,      2019),   \\n \\na      long-form      abstractive      ques-  \\n \\n  tion      answering      dataset.\\nModels      generate      answers      con-  \\n \\n  ditioned      on      the      concatenation      of   \\n \\na      question      and      sup-  \\n \\n  porting      documents.\\nXSum_      (Narayan      et      al.,      2018),   \\n \\na      news      summarization  \\n \\n  dataset      with      highly      abstractive      summaries.\\nConvAI2_      (Dinan      et      al.,      2019),   \\n \\na      dialogue      response  \\n \\n  generation      task,      conditioned      on      context      and   \\n \\na      persona.\\n \\n \\n  CNN/DM_      (Hermann      et      al.,      2015),   \\n \\na      news      summa-  \\n \\n  rization      dataset.\\nSummaries      here      are      typically      closely  \\n \\n  related      to      source      sentences.\\n4.3      Results\\nults Results      are      shown      in      Table      1.\\nSeveral      trends      are      clear:\\nModel      SQuAD      1.1      MNLI      ELIS      XSum   \\n \\n_      ConvAI2      CNN/DM  \\n \\n  Fl      Acc      PPL      PPL      PPL      PPL BERT      Base      (Devlin      et      al.,      2019)      88.5      84.3   \\n \\n-   \\n \\n-   \\n \\n-   \\n \\n-       Masked      Language      Model      90.0      83.5      24.77      7.87      12.59      7.06  \\n \\n  Masked      Seq2seq      87.0      82.1      23.40      6.80      11.43      6.19  \\n \\n  Language      Model      76.7      80.1      21.40      7.00      11.51      6.56  \\n \\n  Permuted      Language      Model      89.1      83.7      24.03      7.69      12.23      6.96  \\n \\n  Multitask      Masked      Language      Model      89.2      82.4      23.73      7.50      12.39      6.74  \\n \\n  BART      Base  \\n \\n  w/      Token      Masking      90.4      84.1      25.05      7.08      11.73      6.10  \\n \\n  w/      Token      Deletion      90.4      84.1      24.61      6.90      11.46      5.87  \\n \\n  w/      Text      Infilling      90.8      84.0      24.26      6.61      11.05      5.83  \\n \\n  w/      Document      Rotation      77.2      75.3      53.69      17.14      19.87      10.59  \\n \\n  w/      Sentence      Shuffling      85.4      81.5      41.87      10.93      16.67      7.89  \\n \\n  w/      Text      Infilling   \\n \\n+      Sentence      Shuffling      90.8      83.8      24.17      6.62      11.12      5.41\\nconsistently      strong      performance.\\n \\n \\n  Performance      of      pre-training      methods      varies      signifi-  \\n \\n  cantly      across      tasks      The      effectiveness      of      pre-training  \\n \\n  methods      is      highly      dependent      on      the      task.\\nFor      exam-  \\n \\n  ple,   \\n \\na      simple      language      model      achieves      the      best      ELIS  \\n \\n  performance,      but      the      worst      SQUAD      results.\\n \\n \\n  Token      masking      is      crucial      Pre-training      objectives  \\n \\n  based      on      rotating      documents      or      permuting      sentences  \\n \\n  perform      poorly      in      isolation.\\nThe      successful      methods  \\n \\n  either      use      token      deletion      or      masking,      or      self-attention  \\n \\n  masks.\\nDeletion      appears      to      outperform      masking      on  \\n \\n  generation      tasks.\\n \\n \\n  Left-to-right      pre-training      improves      generation  \\n \\n  The      Masked      Language      Model      and      the      Permuted  \\n \\n  Language      Model      perform      less      well      than      others      on  \\n \\n  generation,      and      are      the      only      models      we      consider      that  \\n \\n  do      not      include      left-to-right      auto-regressive      language  \\n \\n  modelling      during      pre-training.\\n \\n \\n  Bidirectional      encoders      are      crucial      for      SQUAD      As  \\n \\n  noted      in      previous      work      (Devlin      et      al.,      2019),      just  \\n \\n  left-to-right      decoder      performs      poorly      on      SQuAD,      be-  \\n \\n  cause      future      context      is      crucial      in      classification      deci-  \\n \\n  sions.\\nHowever,      BART      achieves      similar      performance  \\n \\n  with      only      half      the      number      of      bidirectional      layers.\\n \\n \\n  The      pre-training      objective      is      not      the      only      important  \\n \\n  factor      Our      Permuted      Language      Model      performs      less  \\n \\n  well      than      XLNet      (Yang      et      al.,      2019).\\nSome      of      this      dif-  \\n \\n  ference      is      likely      due      to      not      including      other      architectural  \\n \\n  improvements,      such      as      relative-position      embeddings      or  \\n \\n  segment-level      recurrence.\\n \\n \\n  Pure      language      models      perform      best      on      ELIS      The  \\n \\n  ELIS      dataset      is      an      outlier,      with      much      higher      perplex-  \\n \\n  ities      than      other      tasks,      and      is      the      only      generation      task  \\n \\n  where      other      models      outperform      BART.\\n \\n \\nA      pure      lan-  \\n \\n  guage      model      performs      best,      suggesting      that      BART      is  \\n \\n  less      effective      when      the      output      is      only      loosely      con-  \\n \\n  strained      by      the      input.\\n \\n \\n  BART      achieves      the      most      consistently      strong      perfor-  \\n \\n  mance.\\nWith      the      exception      of      ELI5,      BART      models  \\n \\n  using      text-infilling      perform      well      on      all      tasks.\\n5\\n \\n   Large-scale      Pre-training      Experiments  \\n \\n  Recent      work      has      shown      that      downstream      performance  \\n \\n  can      dramatically      improve      when      pre-training      is      scaled  \\n \\n  to      large      batch      sizes      (Yang      et      al.,      2019;      Liu      et      al.,      2019)  \\n \\n  and      corpora.\\nTo      test      how      well      BART      performs      in      this  \\n \\n  regime,      and      to      create   \\n \\na      useful      model      for      downstream  \\n \\n  tasks,      we      trained      BART      using      the      same      scale      as      the  \\n \\n  RoBERTa      model.\\n5.1      Experimental      Setup\\netup  \\n \\n  We      pre-train   \\n \\na      large      model      with      12      layers      in      each      of      the  \\n \\n  encoder      and      decoder,      and   \\n \\na      hidden      size      of      1024.\\nFol-  \\n \\n  lowing      RoBERTa      (Liu      et      al.,      2019),      we      use   \\n \\na      batch      size  \\n \\n  of      8000,      and      train      the      model      for      500000      steps.\\nDocu-  \\n \\n  ments      are      tokenized      with      the      same      byte-pair      encoding  \\n \\n  as      GPT-2      (Radford      et      al.,      2019).\\nBased      on      the      results      in  \\n \\n  Section      §4,      we      use   \\n \\na      combination      of      text      infilling      and  \\n \\n  sentence      permutation.\\nWe      mask      30%      of      tokens      in      each  \\n \\n  document,      and      permute      all      sentences.\\nAlthough      sen-  \\n \\n  tence      permutation      only      shows      significant      additive      gains SQuAD      1.1      SQuAD      2.0      MNLI      SST      QQP      QNLI      STS-B      RTE      MRPC      CoLA  \\n \\n  EM/F1      EM/F1      m/mm      Acc      Acc      Acc      Acc      Acc      Acc      Mcc\\n \\n \\n  BERT      84.1/90.9      79.0/81.8      86.6/-      93.2      91.3      92.3      90.0      70.4      88.0      60.6  \\n \\n  UniLM      -/-      80.5/83.4      87.0/85.9      94.5   \\n \\n-      92.7   \\n \\n-      70.9   \\n \\n-      61.1  \\n \\n  XLNet      89.0/94.5      86.1/88.8      89.8/-      95.6      91.8      93.9      91.8      83.8      89.2      63.6  \\n \\n  RoBERTa      88.9/94.6      86.5/89.4      90.2/90.2      96.4      92.2      94.7      92.4      86.6      90.9      68.0  \\n \\n  BART      88.8/94.6      86.1/89.2      89.9/90.1      96.6      92.5      94.9      91.2      87.0      90.4      62.8 Table      2:      Results      for      large      models      on      SQUAD      and      GLUE      tasks.\\nBART      performs      comparably      to      ROBERTa      and  \\n \\n  XLNet,      suggesting      that      BART’s      uni-directional      decoder      layers      do      not      reduce      performance      on      discriminative      tasks.'},\n",
       " {'metadata': {'source': 'https://arxiv.org/pdf/1910.13461.pdf',\n",
       "   'section_number': 9,\n",
       "   'section_title': '4.1      Comparison      Objectives'},\n",
       "  'page_content': '4.1      Comparison      Objectives\\nives  \\n \\n  While      many      pre-training      objectives      have      been      pro-  \\n \\n  posed,      fair      comparisons      between      these      have      been      dif-  \\n \\n  ficult      to      perform,      at      least      in      part      due      to      differences      in  \\n \\n  training      data,      training      resources,      architectural      differ-  \\n \\n  ences      between      models,      and      fine-tuning      procedures.\\nWe  \\n \\n  ABCDE  \\n \\n  weee  \\n \\n  Pre-trained      Pre-trained  \\n \\n  Craters      Joo      rezapes cise)      Te      TrET Pre-trained  \\n \\n  Encoder Pre-trained  \\n \\n  Decoder aa      AB  \\n \\n  trrrt  \\n \\n  tr      ttt  \\n \\n  <s>A      BCD  \\n \\n  Randomly  \\n \\n  Initialized      Encoder  \\n \\n  TFT  \\n \\n  aBpByoe sentation      from      the      final      output      is      used.\\n \\n \\n  re-implement      strong      pre-training      approaches      recently  \\n \\n  proposed      for      discriminative      and      generation      tasks.\\nWe  \\n \\n  aim,      as      much      as      possible,      to      control      for      differences      un-  \\n \\n  related      to      the      pre-training      objective.\\nHowever,      we      do  \\n \\n  make      minor      changes      to      the      learning      rate      and      usage      of  \\n \\n  layer      normalisation      in      order      to      improve      performance  \\n \\n  (tuning      these      separately      for      each      objective).\\nFor      refer-  \\n \\n  ence,      we      compare      our      implementations      with      published  \\n \\n  numbers      from      BERT,      which      was      also      trained      for      1M  \\n \\n  steps      on   \\n \\na      combination      of      books      and      Wikipedia      data.\\n \\n \\n  We      compare      the      following      approaches:\\n \\n \\n  Language      Model      Similarly      to      GPT      (Radford      et      al.,  \\n \\n  2018),      we      train   \\n \\na      left-to-right      Transformer      language  \\n \\n  model.\\nThis      model      is      equivalent      to      the      BART      decoder,  \\n \\n  without      cross-attention.\\n \\n \\n  Permuted      Language      Model      Based      on      XLNet      (Yang  \\n \\n  et      al.,      2019),      we      sample      1/6      of      the      tokens,      and      gener-  \\n \\n  ate      them      in   \\n \\na      random      order      autoregressively.\\nFor      con-  \\n \\n  sistency      with      other      models,      we      do      not      implement      the  \\n \\n  relative      positional      embeddings      or      attention      across      seg-  \\n \\n  ments      from      XLNet.\\n \\n \\n  Masked      Language      Model      Following      BERT      (Devlin  \\n \\n  et      al.,      2019),      we      replace      15%      of      tokens      with      [MASK]  \\n \\n  symbols,      and      train      the      model      to      independently      predict  \\n \\n  the      original      tokens.\\n \\n \\n  Multitask      Masked      Language      Model      As      in      UniLM  \\n \\n  (Dong      et      al.,      2019),      we      train   \\n \\na      Masked      Language  \\n \\n  Model      with      additional      self-attention      masks.\\nSelf      at-  \\n \\n  tention      masks      are      chosen      randomly      in      with      the      follow  \\n \\n  proportions:      1/6      left-to-right,      1/6      right-to-left,      1/3      un-  \\n \\n  masked,      and      1/3      with      the      first      50%      of      tokens      unmasked  \\n \\n  and   \\n \\na      left-to-right      mask      for      the      remainder.\\n \\n \\n  Masked      Seq-to-Seq      Inspired      by      MASS      (Song      et      al.,  \\n \\n  2019),      we      mask   \\n \\na      span      containing      50%      of      tokens,  \\n \\n  and      train   \\n \\na      sequence      to      sequence      model      to      predict      the  \\n \\n  masked      tokens.\\n \\n \\n  For      the      Permuted      LM,      Masked      LM      and      Multitask  \\n \\n  Masked      LM,      we      use      two-stream      attention      (Yang      et      al.,  \\n \\n  2019)      to      efficiently      compute      likelihoods      of      the      output  \\n \\n  part      of      the      sequence      (using   \\n \\na      diagonal      self-attention  \\n \\n  mask      on      the      output      to      predict      words      left-to-right).\\n \\n \\n  We      experiment      with      (1)      treating      the      task      as   \\n \\na      stan-  \\n \\n  dard      sequence-to-sequence      problem,      where      the      source  \\n \\n  input      to      the      encoder      and      the      target      is      the      decoder      out-  \\n \\n  put,      or      (2)      adding      the      source      as      prefix      to      the      target      in  \\n \\n  the      decoder,      with   \\n \\na      loss      only      on      the      target      part      of      the  \\n \\n  sequence.\\nWe      find      the      former      works      better      for      BART  \\n \\n  models,      and      the      latter      for      other      models.\\n \\n \\n  To      most      directly      compare      our      models      on      their      ability  \\n \\n  to      model      their      fine-tuning      objective      (the      log      likelihood  \\n \\n  of      the      human      text),      we      report      perplexity      in      Table      1.'},\n",
       " {'metadata': {'source': 'https://arxiv.org/pdf/1910.13461.pdf',\n",
       "   'section_number': 10,\n",
       "   'section_title': '4.2      Tasks'},\n",
       "  'page_content': '4.2      Tasks\\nasks  \\n \\n  SQuAD  \\n \\n  (Rajpurkar      et      al.,      2016)a      an      extractive      ques-  \\n \\n  tion      answering      task      on      Wikipedia      paragraphs.\\nAnswers  \\n \\n  are      text      spans      extracted      from   \\n \\na      given      document      context.\\n \\n \\n  Similar      to      BERT      (Devlin      et      al.,      2019),      we      use      concate-  \\n \\n  nated      question      and      context      as      input      to      the      encoder      of  \\n \\n  BART,      and      additionally      pass      them      to      the      decoder.\\nThe  \\n \\n  model      includes      classifiers      to      predict      the      start      and      end  \\n \\n  indices      of      each      token.\\n \\n \\n  MNLI      (Williams      et      al.,      2017),   \\n \\na      bitext      classification  \\n \\n  task      to      predict      whether      one      sentence      entails      another.\\n \\n \\n  The      fine-tuned      model      concatenates      the      two      sentences  \\n \\n  with      appended      an      EOS      token,      and      passes      them      to      both  \\n \\n  the      BART      encoder      and      decoder.\\nIn      contrast      to      BERT,  \\n \\n  the      representation      of      the      EOS      token      is      used      to      classify  \\n \\n  the      sentences      relations.\\n \\n \\n  ELIS      (Fanetal.,      2019),   \\n \\na      long-form      abstractive      ques-  \\n \\n  tion      answering      dataset.\\nModels      generate      answers      con-  \\n \\n  ditioned      on      the      concatenation      of   \\n \\na      question      and      sup-  \\n \\n  porting      documents.\\nXSum_      (Narayan      et      al.,      2018),   \\n \\na      news      summarization  \\n \\n  dataset      with      highly      abstractive      summaries.\\nConvAI2_      (Dinan      et      al.,      2019),   \\n \\na      dialogue      response  \\n \\n  generation      task,      conditioned      on      context      and   \\n \\na      persona.\\n \\n \\n  CNN/DM_      (Hermann      et      al.,      2015),   \\n \\na      news      summa-  \\n \\n  rization      dataset.\\nSummaries      here      are      typically      closely  \\n \\n  related      to      source      sentences.'},\n",
       " {'metadata': {'source': 'https://arxiv.org/pdf/1910.13461.pdf',\n",
       "   'section_number': 11,\n",
       "   'section_title': '4.3      Results'},\n",
       "  'page_content': '4.3      Results\\nults Results      are      shown      in      Table      1.\\nSeveral      trends      are      clear:\\nModel      SQuAD      1.1      MNLI      ELIS      XSum   \\n \\n_      ConvAI2      CNN/DM  \\n \\n  Fl      Acc      PPL      PPL      PPL      PPL BERT      Base      (Devlin      et      al.,      2019)      88.5      84.3   \\n \\n-   \\n \\n-   \\n \\n-   \\n \\n-       Masked      Language      Model      90.0      83.5      24.77      7.87      12.59      7.06  \\n \\n  Masked      Seq2seq      87.0      82.1      23.40      6.80      11.43      6.19  \\n \\n  Language      Model      76.7      80.1      21.40      7.00      11.51      6.56  \\n \\n  Permuted      Language      Model      89.1      83.7      24.03      7.69      12.23      6.96  \\n \\n  Multitask      Masked      Language      Model      89.2      82.4      23.73      7.50      12.39      6.74  \\n \\n  BART      Base  \\n \\n  w/      Token      Masking      90.4      84.1      25.05      7.08      11.73      6.10  \\n \\n  w/      Token      Deletion      90.4      84.1      24.61      6.90      11.46      5.87  \\n \\n  w/      Text      Infilling      90.8      84.0      24.26      6.61      11.05      5.83  \\n \\n  w/      Document      Rotation      77.2      75.3      53.69      17.14      19.87      10.59  \\n \\n  w/      Sentence      Shuffling      85.4      81.5      41.87      10.93      16.67      7.89  \\n \\n  w/      Text      Infilling   \\n \\n+      Sentence      Shuffling      90.8      83.8      24.17      6.62      11.12      5.41\\nconsistently      strong      performance.\\n \\n \\n  Performance      of      pre-training      methods      varies      signifi-  \\n \\n  cantly      across      tasks      The      effectiveness      of      pre-training  \\n \\n  methods      is      highly      dependent      on      the      task.\\nFor      exam-  \\n \\n  ple,   \\n \\na      simple      language      model      achieves      the      best      ELIS  \\n \\n  performance,      but      the      worst      SQUAD      results.\\n \\n \\n  Token      masking      is      crucial      Pre-training      objectives  \\n \\n  based      on      rotating      documents      or      permuting      sentences  \\n \\n  perform      poorly      in      isolation.\\nThe      successful      methods  \\n \\n  either      use      token      deletion      or      masking,      or      self-attention  \\n \\n  masks.\\nDeletion      appears      to      outperform      masking      on  \\n \\n  generation      tasks.\\n \\n \\n  Left-to-right      pre-training      improves      generation  \\n \\n  The      Masked      Language      Model      and      the      Permuted  \\n \\n  Language      Model      perform      less      well      than      others      on  \\n \\n  generation,      and      are      the      only      models      we      consider      that  \\n \\n  do      not      include      left-to-right      auto-regressive      language  \\n \\n  modelling      during      pre-training.\\n \\n \\n  Bidirectional      encoders      are      crucial      for      SQUAD      As  \\n \\n  noted      in      previous      work      (Devlin      et      al.,      2019),      just  \\n \\n  left-to-right      decoder      performs      poorly      on      SQuAD,      be-  \\n \\n  cause      future      context      is      crucial      in      classification      deci-  \\n \\n  sions.\\nHowever,      BART      achieves      similar      performance  \\n \\n  with      only      half      the      number      of      bidirectional      layers.\\n \\n \\n  The      pre-training      objective      is      not      the      only      important  \\n \\n  factor      Our      Permuted      Language      Model      performs      less  \\n \\n  well      than      XLNet      (Yang      et      al.,      2019).\\nSome      of      this      dif-  \\n \\n  ference      is      likely      due      to      not      including      other      architectural  \\n \\n  improvements,      such      as      relative-position      embeddings      or  \\n \\n  segment-level      recurrence.\\n \\n \\n  Pure      language      models      perform      best      on      ELIS      The  \\n \\n  ELIS      dataset      is      an      outlier,      with      much      higher      perplex-  \\n \\n  ities      than      other      tasks,      and      is      the      only      generation      task  \\n \\n  where      other      models      outperform      BART.\\n \\n \\nA      pure      lan-  \\n \\n  guage      model      performs      best,      suggesting      that      BART      is  \\n \\n  less      effective      when      the      output      is      only      loosely      con-  \\n \\n  strained      by      the      input.\\n \\n \\n  BART      achieves      the      most      consistently      strong      perfor-  \\n \\n  mance.\\nWith      the      exception      of      ELI5,      BART      models  \\n \\n  using      text-infilling      perform      well      on      all      tasks.\\n5\\n \\n   Large-scale      Pre-training      Experiments  \\n \\n  Recent      work      has      shown      that      downstream      performance  \\n \\n  can      dramatically      improve      when      pre-training      is      scaled  \\n \\n  to      large      batch      sizes      (Yang      et      al.,      2019;      Liu      et      al.,      2019)  \\n \\n  and      corpora.\\nTo      test      how      well      BART      performs      in      this  \\n \\n  regime,      and      to      create   \\n \\na      useful      model      for      downstream  \\n \\n  tasks,      we      trained      BART      using      the      same      scale      as      the  \\n \\n  RoBERTa      model.'},\n",
       " {'metadata': {'source': 'https://arxiv.org/pdf/1910.13461.pdf',\n",
       "   'section_number': 12,\n",
       "   'section_title': '5.1      Experimental      Setup'},\n",
       "  'page_content': '5.1      Experimental      Setup\\netup  \\n \\n  We      pre-train   \\n \\na      large      model      with      12      layers      in      each      of      the  \\n \\n  encoder      and      decoder,      and   \\n \\na      hidden      size      of      1024.\\nFol-  \\n \\n  lowing      RoBERTa      (Liu      et      al.,      2019),      we      use   \\n \\na      batch      size  \\n \\n  of      8000,      and      train      the      model      for      500000      steps.\\nDocu-  \\n \\n  ments      are      tokenized      with      the      same      byte-pair      encoding  \\n \\n  as      GPT-2      (Radford      et      al.,      2019).\\nBased      on      the      results      in  \\n \\n  Section      §4,      we      use   \\n \\na      combination      of      text      infilling      and  \\n \\n  sentence      permutation.\\nWe      mask      30%      of      tokens      in      each  \\n \\n  document,      and      permute      all      sentences.\\nAlthough      sen-  \\n \\n  tence      permutation      only      shows      significant      additive      gains SQuAD      1.1      SQuAD      2.0      MNLI      SST      QQP      QNLI      STS-B      RTE      MRPC      CoLA  \\n \\n  EM/F1      EM/F1      m/mm      Acc      Acc      Acc      Acc      Acc      Acc      Mcc\\n \\n \\n  BERT      84.1/90.9      79.0/81.8      86.6/-      93.2      91.3      92.3      90.0      70.4      88.0      60.6  \\n \\n  UniLM      -/-      80.5/83.4      87.0/85.9      94.5   \\n \\n-      92.7   \\n \\n-      70.9   \\n \\n-      61.1  \\n \\n  XLNet      89.0/94.5      86.1/88.8      89.8/-      95.6      91.8      93.9      91.8      83.8      89.2      63.6  \\n \\n  RoBERTa      88.9/94.6      86.5/89.4      90.2/90.2      96.4      92.2      94.7      92.4      86.6      90.9      68.0  \\n \\n  BART      88.8/94.6      86.1/89.2      89.9/90.1      96.6      92.5      94.9      91.2      87.0      90.4      62.8 Table      2:      Results      for      large      models      on      SQUAD      and      GLUE      tasks.\\nBART      performs      comparably      to      ROBERTa      and  \\n \\n  XLNet,      suggesting      that      BART’s      uni-directional      decoder      layers      do      not      reduce      performance      on      discriminative      tasks.'},\n",
       " {'metadata': {'source': 'https://arxiv.org/pdf/1910.13461.pdf',\n",
       "   'section_number': 13,\n",
       "   'section_title': 'CNN/DailyMail      XSum       RI      R2      RL      RI      R2      RL'},\n",
       "  'page_content': 'CNN/DailyMail      XSum       RI      R2      RL      RI      R2      RL\\n \\n \\n  Lead-3      40.42      17.62      36.67      16.30      1.60      11.95  \\n \\n  PTGEN      (See      et      al.,      2017)      36.44      15.66      33.42      29.70      9.21      23.24  \\n \\n  PTGEN+COV      (See      et      al.,      2017)      39.53      17.28      36.38      28.10      8.02      21.72  \\n \\n  UniLM      43.33      20.21      40.51   \\n \\n-   \\n \\n-   \\n \\n-       BERTSUMABS      (Liu   \\n \\n&      Lapata,      2019)      41.72      19.39      38.76      38.76      16.33      31.15  \\n \\n  BERTSUMEXTABS      (Liu   \\n \\n&      Lapata,      2019)      42.13      19.60      39.18      38.81      16.50      31.27 BART      44.16      21.28      40.90      45.14      22.27      37.25 Table      3:      Results      on      two      standard      summarization      datasets.\\nBART      outperforms      previous      work      on      summarization      on\\n \\n \\n  on      the      CNN/DM      summarization      dataset,      we      hypothe-      ConvAI2  \\n \\n  sised      that      larger      pre-trained      models      may      be      better      able      Valid      F1      Valid      PPL  \\n \\n  to   \\n \\n|      fi      this      task.\\nTo      help      th      del      better      fit      th  \\n \\n  RE      aa      ae      Pai      Cae      Seq2Seq+      Attention      16.02      35.07  \\n \\n  data,      we      disabled      dropout      for      the      final      10%      of      training   \\n \\n.    \\n.\\n     Best      System      19.09      17.51  \\n \\n  steps.\\nWe      use      the      same      pre-training      data      as      Liu      et      al.\\n(2019),      consisting      of      160Gb      of      news,      books,      stories,  \\n \\n  and      web      text.\\nBART      20.72      11.85 5.2      Discriminative      Tasks  \\n \\n  Table   \\n \\n2      compares      the      performance      of      BART      with      sev-  \\n \\n  eral      recent      approaches      on      the      well-studied      SQUAD      and  \\n \\n  GLUE      tasks      (Warstadt      et      al.,      2018;      Socher      et      al.,      2013;  \\n \\n  Dolan   \\n \\n&      Brockett,      2005;      Agirre      et      al.,      2007;      Williams  \\n \\n  et      al.,      2018;      Dagan      et      al.,      2006;      Levesque      et      al.,      2011).\\n \\n \\n  Table      4:      BART      outperforms      previous      work      on      conver-  \\n \\n  sational      response      generation.\\nPerplexities      are      renor-  \\n \\n  malized      based      on      official      tokenizer      for      ConvAI2.\\n \\n \\n  The      most      directly      comparable      baseline      is      ROBERTa,  \\n \\n  which      was      pre-trained      with      the      same      resources,      but  \\n \\n  a      different      objective.\\nOverall,      BART      performs      simi-  \\n \\n  larly,      with      only      small      differences      between      the      models  \\n \\n  on      most      tasks.\\nsuggesting      that      BART’s      improvements  \\n \\n  on      generation      tasks      do      not      come      at      the      expense      of      clas-  \\n \\n  sification      performance.\\n \\n \\n  Summarization      To      provide   \\n \\na      comparison      with      the  \\n \\n  state-of-the-art      in      summarization,      we      present      results  \\n \\n  on      two      summarization      datasets,      CNN/DailyMail      and  \\n \\n  XSum,      which      have      distinct      properties.\\n \\n \\n  Summaries      in      the      CNN/DailyMail      tend      to      resemble  \\n \\n  source      sentences.\\nExtractive      models      do      well      here,      and  \\n \\n  even      the      baseline      of      the      first-three      source      sentences      is  \\n \\n  highly      competitive.\\nNevertheless,      BART      outperforms  \\n \\n  all      existing      work.\\n5.3.      Generation      Tasks\\nGeneration      Tasks\\n |       We      also      experiment      with      several      text      generation      tasks.       BART      is      fine-tuned      as      a      standard      sequence-to-sequence       model      from      the      input      to      the      output      text.      During      fine-       tuning      we      use      a      label      smoothed      cross      entropy      loss       (Pereyra      et      al.,      2017),      with      the      smoothing      parameter       set      to      0.1.      During      generation,      we      set      beam      size      as      5,       remove      duplicated      trigrams      in      beam      search,      and      tuned       the      model      with      min-len,      max-len,      length      penalty      on      the       validation      set      (Fan      et      al.,      2017). | 6.0      points      on      all      ROUGE      metrics—trepresenting   \\n \\na      sig-  \\n \\n  nificant      advance      in      performance      on      this      problem.\\nQual-  \\n \\n  itatively,      sample      quality      is      high      (see      86).\\n \\n \\n  In      contrast,      XSum      is      highly      abstractive,      and      extrac-  \\n \\n  tive      models      perform      poorly.\\nBART      outperforms      the  \\n \\n  best      previous      work,      which      leverages      BERT,      by      roughly  \\n \\n  Dialogue      We      evaluate      dialogue      response      generation  \\n \\n  on      CONVAI2      (Dinan      et      al.,      2019),      in      which      agents  \\n \\n  must      generate      responses      conditioned      on      both      the      pre-  \\n \\n  vious      context      and   \\n \\na      textually-specified      persona.\\nBART  \\n \\n  outperforms      previous      work      on      two      automated      metrics.\\n'},\n",
       " {'metadata': {'source': 'https://arxiv.org/pdf/1910.13461.pdf',\n",
       "   'section_number': 14,\n",
       "   'section_title': '5.3.      Generation      Tasks'},\n",
       "  'page_content': '5.3.      Generation      Tasks\\nGeneration      Tasks\\n |       We      also      experiment      with      several      text      generation      tasks.       BART      is      fine-tuned      as      a      standard      sequence-to-sequence       model      from      the      input      to      the      output      text.      During      fine-       tuning      we      use      a      label      smoothed      cross      entropy      loss       (Pereyra      et      al.,      2017),      with      the      smoothing      parameter       set      to      0.1.      During      generation,      we      set      beam      size      as      5,       remove      duplicated      trigrams      in      beam      search,      and      tuned       the      model      with      min-len,      max-len,      length      penalty      on      the       validation      set      (Fan      et      al.,      2017). | 6.0      points      on      all      ROUGE      metrics—trepresenting   \\n \\na      sig-  \\n \\n  nificant      advance      in      performance      on      this      problem.\\nQual-  \\n \\n  itatively,      sample      quality      is      high      (see      86).\\n \\n \\n  In      contrast,      XSum      is      highly      abstractive,      and      extrac-  \\n \\n  tive      models      perform      poorly.\\nBART      outperforms      the  \\n \\n  best      previous      work,      which      leverages      BERT,      by      roughly  \\n \\n  Dialogue      We      evaluate      dialogue      response      generation  \\n \\n  on      CONVAI2      (Dinan      et      al.,      2019),      in      which      agents  \\n \\n  must      generate      responses      conditioned      on      both      the      pre-  \\n \\n  vious      context      and   \\n \\na      textually-specified      persona.\\nBART  \\n \\n  outperforms      previous      work      on      two      automated      metrics.\\n'},\n",
       " {'metadata': {'source': 'https://arxiv.org/pdf/1910.13461.pdf',\n",
       "   'section_number': 15,\n",
       "   'section_title': 'Rl      R2      RL'},\n",
       "  'page_content': 'Rl      R2      RL\\n \\n \\n  Best      Extractive      23.55      3.1      17.5  \\n \\n  Language      Model      27.8      47      23.1  \\n \\n  Seq2Seq      28.3      5.1      22.8  \\n \\n  Seq2Seq      Multitask      28.9      54      23.1  \\n \\n  BART      30.6      6.2      24.3  \\n \\n  Table      5:      BART      achieves      state-of-the-art      results      on  \\n \\n  the      challenging      ELI5      abstractive      question      answering  \\n \\n  dataset.\\nComparison      models      are      from      Fan      et      al.\\n(2019).'},\n",
       " {'metadata': {'source': 'https://arxiv.org/pdf/1910.13461.pdf',\n",
       "   'section_number': 16,\n",
       "   'section_title': 'RO-EN'},\n",
       "  'page_content': 'RO-EN\\nBaseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96\\n \\n \\n  Table      6:      The      performance      (BLEU)      of      baseline      and  \\n \\n  BART      on      WMT’16      RO-EN      augmented      with      back-  \\n \\n  translation      data.\\nBART      improves      over   \\n \\na      strong      back-  \\n \\n  translation      (BT)      baseline      by      using      monolingual      English  \\n \\n  pre-training.\\n \\n \\n  Abstractive      QA      We      use      the      recently      proposed      ELIS  \\n \\n  dataset      to      test      the      model’s      ability      to      generate      long      free-  \\n \\n  form      answers.\\nWe      find      BART      outperforms      the      best      pre-  \\n \\n  vious      work      by      1.2      ROUGE-L,      but      the      dataset      remains  \\n \\n  a      challenging,      because      answers      are      only      weakly      speci-  \\n \\n  fied      by      the      question.\\n5.4      Translation\\ntion  \\n \\n  We      also      evaluated      performance      on      WMT16      Romanian-  \\n \\n  English,      augmented      with      back-translation      data  \\n \\n  from      Sennrich      et      al.\\n(2016).\\nWe      use   \\n \\na      6-layer  \\n \\n  transformer      source      encoder      to      map      Romanian      into  \\n \\n  a      representation      that      BART      is      able      to      de-noise      into  \\n \\n  English,      following      the      approach      introduced      in      83.4.  \\n \\n  Experiment      results      are      presented      in      Table      6.      We  \\n \\n  compare      our      results      against   \\n \\na      baseline      Transformer  \\n \\n  architecture      (Vaswani      et      al.,      2017)      with      Transformer-  \\n \\n  large      settings      (the      baseline      row).\\nWe      show      the  \\n \\n  performance      of      both      steps      of      our      model      in      the      fixed  \\n \\n  BART      and      tuned      BART      rows.\\nFor      each      row      we  \\n \\n  experiment      on      the      original      WMT16      Romanian-English  \\n \\n  augmented      with      back-translation      data.\\nWe      use   \\n \\na       beam      width      of   \\n \\n5      and   \\n \\na      length      penalty      of   \\n \\na   \\n \\n=      1.  \\n \\n  Preliminary      results      suggested      that      our      approach      was  \\n \\n  less      effective      without      back-translation      data,      and      prone  \\n \\n  to      overfitting—future      work      should      explore      additional  \\n \\n  regularization      techniques.\\n6\\n \\n   Qualitative      Analysis  \\n \\n  BART      shows      large      improvements      on      summarization  \\n \\n  metrics,      of      up      to   \\n \\n6      points      over      the      prior      state-of-the-art.\\n \\n \\n  To      understand      BART’s      performance      beyond      automated  \\n \\n  metrics,      we      analyse      its      generations      qualitatively.\\n \\n \\n  Table   \\n \\n7      shows      example      summaries      generated      by  \\n \\n  BART.\\nExamples      are      taken      from      WikiNews      articles  \\n \\n  published      after      the      creation      of      the      pre-training      corpus,  \\n \\n  to      eliminate      the      possibility      of      the      events      described      be-  \\n \\n  ing      present      in      the      model’s      training      data.\\nFollowing  \\n \\n  Narayan      et      al.\\n(2018),      we      remove      the      first      sentence      of  \\n \\n  the      article      prior      to      summarizing      it,      so      there      is      no      easy  \\n \\n  extractive      summary      of      the      document.\\n \\n \\n  Unsurprisingly,      model      output      is      fluent      and      grammat-  \\n \\n  ical      English.\\nHowever,      model      output      is      also      highly      ab-  \\n \\n  stractive,      with      few      phrases      copied      from      the      input.\\nThe  \\n \\n  output      is      also      generally      factually      accurate,      and      inte-  \\n \\n  grates      supporting      evidence      from      across      the      input      doc-  \\n \\n  ument      with      background      knowledge      (for      example,      cor-  \\n \\n  rectly      completing      names,      or      inferring      that      PG&E      oper-  \\n \\n  ates      in      California).\\nIn      the      first      example,      inferring      that  \\n \\n  fish      are      protecting      reefs      from      global      warming      requires  \\n \\n  non-trivial      inference      from      the      text.\\nHowever,      the      claim  \\n \\n  that      the      work      was      published      in      Science      is      not      supported  \\n \\n  by      the      source.\\n \\n \\n  These      samples      demonstrate      that      the      BART      pretrain-  \\n \\n  ing      has      learned   \\n \\na      strong      combination      of      natural      lan-  \\n \\n  guage      understanding      and      generation.\\n7\\n \\n   Related      Work  \\n \\n  Early      methods      for      pretraining      were      based      on      language  \\n \\n  models.\\nGPT      (Radford      et      al.,      2018)      only      models      left-  \\n \\n  ward      context,      which      is      problematic      for      some      tasks.\\n \\n \\n  ELMo      (Peters      et      al.,      2018)      concatenates      left-only      and  \\n \\n  right-only      representations,      but      does      not      pre-train      inter-  \\n \\n  actions      between      these      features.\\nRadford      et      al.\\n(2019)  \\n \\n  demonstrated      that      very      large      language      models      can      act       as      unsupervised      multitask      models.\\n \\n \\n  MASS      (Song      et      al.,      2019)      is      perhaps      the      most      similar  \\n \\n  model      to      BART.\\nAn      input      sequence      where   \\n \\na      contiguous  \\n \\n  span      of      tokens      is      masked      is      mapped      to   \\n \\na      sequence      con-  \\n \\n  sisting      of      the      missing      tokens.\\nMASS      is      less      effective  \\n \\n  for      discriminative      tasks,      because      disjoint      sets      of      tokens  \\n \\n  are      fed      into      the      encoder      and      decoder.\\n \\n \\n  BERT      (Devlin      et      al.,      2019)      introduced      masked      lan-  \\n \\n  guage      modelling,      which      allows      pre-training      to      learn      in-  \\n \\n  teractions      between      left      and      right      context      words.\\nRe-  \\n \\n  cent      work      has      shown      that      very      strong      performance      can  \\n \\n  be      achieved      by      training      for      longer      (Liu      et      al.,      2019),  \\n \\n  by      tying      parameters      across      layers      (Lan      et      al.,      2019),  \\n \\n  and      by      masking      spans      instead      of      words      (Joshi      et      al.,  \\n \\n  2019).\\nPredictions      are      not      made      auto-regressively,      re-  \\n \\n  ducing      the      effectiveness      of      BERT      for      generation      tasks.\\n \\n \\n  UniLM      (Dong      et      al.,      2019)      fine-tunes      BERT      with      an  \\n \\n  ensemble      of      masks,      some      of      which      allow      only      leftward  \\n \\n  context.\\nLike      BART,      this      allows      UniLM      to      be      used      for  \\n \\n  both      generative      and      discriminative      tasks.\\n \\n \\nA      difference  \\n \\n  is      that      UniLM      predictions      are      conditionally      indepen-  \\n \\n  dent,      whereas      BART’s      are      autoregressive.\\nBART      re-  \\n \\n  duces      the      mismatch      between      pre-training      and      genera-  \\n \\n  tion      tasks,      because      the      decoder      is      always      trained      on      un-  \\n \\n  corrupted      context.\\n \\n \\n  XL-Net      (Yang      et      al.,      2019)      extends      BERT      by      pre-\\n |       Source      Document      (abbreviated) |       BART      Summary\\n |       The      researchers      examined      three      types      of      coral      in      reefs      off      the coast of Fiji The researchers found when fish were plentiful,       they      would      eat      algae      and      seaweed      off      the      corals,      which      appeared       to      leave      them      more      resistant      to      the      bacterium      Vibrio      coralliilyti-       cus,      a      bacterium      associated      with      bleaching.      The      researchers      sug-       gested      the      algae,      like      warming      temperatures,      might      render      the       corals’      chemical      defenses      less      effective,      and      the      fish      were      pro-       tecting      the      coral      by      removing      the      algae. |       Fisheries      off      the      coast      of      Fiji      are      protect-       ing      coral      reefs      from      the      effects      of      global       warming,      according      to      a      study      in      the      jour-       nal      Science.\\n |       Sacoolas,      who      has      immunity      as      a      diplomat’s      wife,      was      involved in a traffic collision Prime Minister Johnson was questioned       about      the      case      while      speaking      to      the      press      at      a      hospital      in      Wat- ford. He said, “I hope that Anne Sacoolas will come back       if      we      can’t      resolve      it      then      of      course      I      will      be      raising      it      myself       personally      with      the      White      House.” |       Boris      Johnson      has      said      he      will      raise      the      is-       sue      of      US      diplomat      Anne      Sacoolas’      diplo-       matic      immunity      with      the      White      House.\\n |       According      to      Syrian      state      media,      government      forces      began      de- ploying into previously SDF controlled territory yesterday.       On      October      6,      US      President      Donald      Trump      and      Turkish      Presi-       dent      Recep      Tayyip      Erdoan      spoke      on      the      phone.      Then      both      na-       tions      issued      statements      speaking      of      an      imminent      incursion      into northeast Syria On Wednesday, Turkey began a military       offensive      with      airstrikes      followed      by      a      ground      invasion. |       Syrian      government      forces      have      entered       territory      held      by      the      US-backed      Syrian       Democratic      Forces      (SDF)      in      response      to       Turkey’s      incursion      into      the      region.\\n |       This      is      the      first      time      anyone      has      been      recorded      to      run      a      full       marathon      of      42.195      kilometers      (approximately      26      miles)      under       this      pursued      landmark      time.      It      was      not,      however,      an      officially       sanctioned      world      record,      as      it      was      not      an      ’open      race”      of      the       IAAF.      His      time      was      1      hour      59      minutes      40.2      seconds.      Kipchoge       ran      in      Vienna,      Austria.      It      was      an      event      specifically      designed      to       help      Kipchoge      break      the      two      hour      barrier. |       Kenyan      runner      Eliud      Kipchoge      has      run      a       marathon      in      less      than      two      hours.\\n |       PG&E      stated      it      scheduled      the      blackouts      in      response      to      forecasts       for      high      winds      amid      dry      conditions.      The      aim      is      to      reduce      the      risk       of      wildfires.      Nearly      800      thousand      customers      were      scheduled      to       be      affected      by      the      shutoffs      which      were      expected      to      last      through       at      least      midday      tomorrow. |       Power      has      been      turned      off      to      millions      of       customers      in      California      as      part      of      a      power       shutoff      plan.\\n\\n \\n \\n  dicting      masked      tokens      auto-regressively      in   \\n \\na      permuted  \\n \\n  order.\\nThis      objective      allows      predictions      to      condition      on  \\n \\n  both      left      and      right      context.\\nIn      contrast,      the      BART      de-  \\n \\n  coder      works      left-to-right      during      pre-training,      matching  \\n \\n  the      setting      during      generation.\\n \\n \\n  Several      papers      have      explored      using      pre-trained      rep-  \\n \\n  resentations      to      improve      machine      translation.\\nThe  \\n \\n  largest      improvements      have      come      from      pre-training      on  \\n \\n  both      source      and      target      languages      (Song      et      al.,      2019;  \\n \\n  Lample   \\n \\n&      Conneau,      2019),      but      this      requires      pre-  \\n \\n  training      on      all      languages      of      interest.\\nOther      work      has  \\n \\n  shown      that      encoders      can      be      improved      using      pre-trained  \\n \\n  representations      (Edunov      et      al.,      2019),      but      gains      in      de-  \\n \\n  coders      are      more      limited.\\nWe      show      how      BART      can      be  \\n \\n  used      to      improve      machine      translation      decoders.\\n8\\n \\n   Conclusions  \\n \\n  We      introduced      BART,   \\n \\na      pre-training      approach      that  \\n \\n  learns      to      map      corrupted      documents      to      the      original.\\n \\n \\n  BART      achieves      similar      performance      to      ROBERTa      on  \\n \\n  discriminative      tasks,      while      achieving      new      state-of-the-  \\n \\n  art      results      on   \\n \\na      number      of      text      generation      tasks.\\nFu-  \\n \\n  ture      work      should      explore      new      methods      for      corrupting  \\n \\n  documents      for      pre-training,      perhaps      tailoring      them      to  \\n \\n  specific      end      tasks.\\nReferences\\n \\n \\n  Eneko      Agirre,      Llu’is      M‘arquez,      and      Richard      Wicen-  \\n \\n  towski      (eds.).\\nProceedings      of      the      Fourth      Interna-  \\n \\n  tional      Workshop      on      Semantic      Evaluations      (SemEval-  \\n \\n  2007).\\nAssociation      for      Computational      Linguistics,  \\n \\n  Prague,      Czech      Republic,      June      2007.\\n \\n \\n  Ido      Dagan,      Oren      Glickman,      and      Bernardo      Magnini.\\n \\n \\n  The      PASCAL      recognising      textual      entailment      chal-  \\n \\n  lenge.\\nIn      Machine      learning      challenges.\\nevaluat-  \\n \\n  ing      predictive      uncertainty,      visual      object      classifica-  \\n \\n  tion,      and      recognising      tectual      entailment,      pp.\\n177—  \\n \\n  190.\\nSpringer,      2006.\\n \\n \\n  Jacob      Devlin,      Ming-Wei      Chang,      Kenton      Lee,      and  \\n \\n  Kristina      Toutanova.\\nBERT:      Pre-training      of      deep  \\n \\n  bidirectional      transformers      for      language      understand-  \\n \\n  ing.\\nIn      Proceedings      of      the      2019      Conference      of      the  \\n \\n  North      American      Chapter      of      the      Association      for      Com-  \\n \\n  putational      Linguistics:      Human      Language      Technolo-  \\n \\n  gies,      Volume   \\n \\nI      (Long      and      Short      Papers),      pp.\\n4171-  \\n \\n  4186,      Minneapolis,      Minnesota,      June      2019.\\nAssocia-  \\n \\n  tion      for      Computational      Linguistics.\\ndoi:      10.18653/  \\n \\n  vI/N19-1423.\\nURL      https://www.aclweb.\\n \\n \\n  org/anthology/N19-1423.\\n \\n \\n  Emily      Dinan,      Varvara      Logacheva,      Valentin      Malykh,  \\n \\n  Alexander      Miller,      Kurt      Shuster,      Jack      Urbanek,  \\n \\n  Douwe      Kiela,      Arthur      Szlam,      Iulian      Serban,      Ryan  \\n \\n  Lowe,      et      al.\\nThe      second      conversational      in-  \\n \\n  telligence      challenge      (convai2).\\narXiv      preprint  \\n \\n  arXiv:      1902.00098,      2019.\\n \\n \\n  William   \\n \\nB      Dolan      and      Chris      Brockett.\\nAutomatically  \\n \\n  constructing   \\n \\na      corpus      of      sentential      paraphrases.\\nIn  \\n \\n  Proceedings      of      the      International      Workshop      on      Para-  \\n \\n  phrasing,      2005.\\n \\n \\n  Li      Dong,      Nan      Yang,      Wenhui      Wang,      Furu      Wei,      Xi-  \\n \\n  aodong      Liu,      Yu      Wang,      Jianfeng      Gao,      Ming      Zhou,  \\n \\n  and      Hsiao-Wuen      Hon.\\nUnified      language      model      pre-  \\n \\n  training      for      natural      language      understanding      and      gen-  \\n \\n  eration.\\narXiv      preprint      arXiv:      1905.03197,      2019.\\n \\n \\n  Sergey      Edunov,      Alexei      Baevski,      and      Michael      Auli.\\n \\n \\n  Pre-trained      language      model      representations      for      lan-  \\n \\n  guage      generation.\\nIn      Proceedings      of      the      2019      Con-  \\n \\n  ference      of      the      North      American      Chapter      of      the      Asso-  \\n \\n  ciation      for      Computational      Linguistics:      Human      Lan-  \\n \\n  guage      Technologies,      Volume   \\n \\n1      (Long      and      Short      Pa-  \\n \\n  pers),      2019.\\n \\n \\n  Angela      Fan,      David      Grangier,      and      Michael      Auli.\\nCon-  \\n \\n  trollable      abstractive      summarization.\\narXiv      preprint  \\n \\n  arXiv:      1711.05217,      2017.\\n \\n \\n  Angela      Fan,      Yacine      Jernite,      Ethan      Perez,      David  \\n \\n  Grangier,      Jason      Weston,      and      Michael      Auli.\\nEli5:  \\n \\n  Long      form      question      answering.\\narXiv      preprint  \\n \\n  arXiv:      1907.09190,      2019.\\n \\n \\n  Dan      Hendrycks      and      Kevin      Gimpel.\\nGaussian      error      lin-  \\n \\n  ear      units      (gelus).\\narXiv      preprint      arXiv:      1606.08415,  \\n \\n  2016.\\n \\n \\n  Karl      Moritz      Hermann,      Tomas      Kocisky,      Edward  \\n \\n  Grefenstette,      Lasse      Espeholt,      Will      Kay,      Mustafa      Su-  \\n \\n  leyman,      and      Phil      Blunsom.\\nTeaching      machines      to  \\n \\n  read      and      comprehend.\\nIn      Advances      in      neural      infor-  \\n \\n  mation      processing      systems,      pp.\\n1693-1701,      2015.\\n \\n \\n  Mandar      Joshi,      Danqi      Chen,      Yinhan      Liu,      Daniel   \\n \\nS      Weld,  \\n \\n  Luke      Zettlemoyer,      and      Omer      Levy.\\nSpanbert:      Im-  \\n \\n  proving      pre-training      by      representing      and      predicting  \\n \\n  spans.\\narXiv      preprint      arXiv:      1907.10529,      2019.\\n \\n \\n  Guillaume      Lample      and      Alexis      Conneau.\\n \\n \\n—      Cross-  \\n \\n  lingual      language      model      pretraining.\\narXiv      preprint  \\n \\n  arXiv:      1901.07291,      2019.\\n \\n \\n  Zhenzhong      Lan,      Mingda      Chen,      Sebastian      Goodman,  \\n \\n  Kevin      Gimpel,      Piyush      Sharma,      and      Radu      Sori-  \\n \\n  cut.\\nAlbert:   \\n \\nA      lite      bert      for      self-supervised      learn-  \\n \\n  ing      of      language      representations.\\narXiv      preprint  \\n \\n  arXiv:      1909.11942,      2019.\\n \\n \\n  Hector   \\n \\nJ      Levesque,      Ernest      Davis,      and      Leora      Morgen-  \\n \\n  stern.\\nThe      Winograd      schema      challenge.\\nIn      AAAI  \\n \\n  Spring      Symposium:      Logical      Formalizations      of      Com-  \\n \\n  monsense      Reasoning,      volume      46,      pp.\\n47,      2011.\\n \\n \\n  Yang      Liu      and      Mirella      Lapata.\\n \\n \\n  tion      with      pretrained      encoders.\\n \\n \\n  arXiv:      1908.08345,      2019.  \\n \\n  Text      summariza-  \\n \\n  arXiv      preprint  \\n \\n  Yinhan      Liu,      Myle      Ott,      Naman      Goyal,      Jingfei      Du,      Man-  \\n \\n  dar      Joshi,      Dangi      Chen,      Omer      Levy,      Mike      Lewis,  \\n \\n  Luke      Zettlemoyer,      and      Veselin      Stoyanov.\\nRoberta:\\n \\n \\n \\nA      robustly      optimized      bert      pretraining      approach.\\n \\n \\n  arXiv      preprint      arXiv:      1907.11692,      2019.\\n \\n \\n  Tomas      Mikolov,      Kai      Chen,      Greg      Corrado,      and      Jeffrey  \\n \\n  Dean.\\nEfficient      estimation      of      word      representations  \\n \\n  in      vector      space.\\narXiv      preprint      arXiv:1301.3781,  \\n \\n  2013.\\n \\n \\n  Shashi      Narayan,      Shay   \\n \\nB      Cohen,      and      Mirella      Lapata.\\n \\n \\n  Don’t      give      me      the      details,      just      the      summary!\\ntopic-  \\n \\n  aware      convolutional      neural      networks      for      extreme  \\n \\n  summarization.\\narXiv      preprint      arXiv:      1808.08745,  \\n \\n  2018.\\n \\n \\n  Gabriel      Pereyra,      George      Tucker,      Jan      Chorowski,  \\n \\n  Lukasz      Kaiser,      and      Geoffrey      Hinton.\\nRegularizing  \\n \\n  neural      networks      by      penalizing      confident      output      dis-  \\n \\n  tributions.\\narXiv      preprint      arXiv:      1701.06548,      2017.\\n \\n \\n  Matthew      E      Peters,      Mark      Neumann,      Mohit      Iyyer,      Matt  \\n \\n  Gardner,      Christopher      Clark,      Kenton      Lee,      and      Luke  \\n \\n  Zettlemoyer.\\nDeep      contextualized      word      representa-  \\n \\n  tions.\\narXiv      preprint      arXiv:      1802.05365,      2018.\\n \\n \\n  Alec      Radford,      Karthik      Narasimhan,      Tim      Salimans,  \\n \\n  and      Ilya      Sutskever.\\nImproving      language      un-  \\n \\n  derstanding      by      generative      pre-training.\\nURL  \\n \\n  https://s3-us-west-2.\\n \\n \\n|      amazonaws.\\n \\n \\n—      com/openai-  \\n \\n  assets/researchcovers/languageunsupervised/language  \\n \\n  understanding      paper.\\npdf,      2018.\\n \\n \\n  Alec      Radford,      Jeffrey      Wu,      Rewon      Child,      David      Luan,  \\n \\n  Dario      Amodei,      and      Ilya      Sutskever.\\nLanguage      mod-  \\n \\n  els      are      unsupervised      multitask      learners.\\nOpenAI  \\n \\n  Blog,      1(8),      2019.\\n \\n \\n  Pranav      Rajpurkar,      Jian      Zhang,      Konstantin      Lopyrev,  \\n \\n  and      Percy      Liang.\\nSquad:      100,000+      questions      for  \\n \\n  machine      comprehension      of      text.\\narXiv      preprint  \\n \\n  arXiv:      1606.05250,      2016.\\n \\n \\n  Abigail      See,      Peter   \\n \\nJ      Liu,      and      Christopher   \\n \\nD       Manning.\\nGet      to      the      point:      Summarization  \\n \\n  with      pointer-generator      networks.\\narXiv      preprint  \\n \\n  arXiv:      1704.04368,      2017.\\n \\n \\n  Rico      Sennrich,      Barry      Haddow,      and      Alexandra      Birch.\\n \\n \\n  Edinburgh      neural      machine      translation      systems      for  \\n \\n  WMT      16.\\nIn      Proceedings      of      the      First      Conference  \\n \\n  on      Machine      Translation:      Volume      2,      Shared      Task      Pa-  \\n \\n  pers,      2016.\\n \\n \\n  Richard      Socher,      Alex      Perelygin,      Jean      Wu,      Jason  \\n \\n  Chuang,      Christopher   \\n \\nD      Manning,      Andrew      Ng,      and  \\n \\n  Christopher      Potts.\\nRecursive      deep      models      for      se-  \\n \\n  mantic      compositionality      over   \\n \\na      sentiment      treebank.\\n \\n \\n  In      Proceedings      of      EMNLP,      pp.\\n1631-1642,      2013.\\n \\n \\n  Kaitao      Song,      Xu      Tan,      Tao      Qin,      Jianfeng      Lu,      and      Tie-  \\n \\n  Yan      Liu.\\nMass:      Masked      sequence      to      sequence      pre-  \\n \\n  training      for      language      generation.\\nIn      International  \\n \\n  Conference      on      Machine      Learning,      2019.\\n \\n \\n  Ashish      Vaswani,      Noam      Shazeer,      Niki      Parmar,      Jakob  \\n \\n  Uszkoreit,      Llion      Jones,      Aidan      N      Gomez,      Lukasz  \\n \\n  Kaiser,      and      Ilia      Polosukhin.\\nAttention      is      all      you  \\n \\n  need.\\nIn      Advances      in      neural      information      processing  \\n \\n  systems,      pp.\\n5998-6008,      2017.\\n \\n \\n  Alex      Wang,      Amanpreet      Singh,      Julian      Michael,      Felix  \\n \\n  Hill,      Omer      Levy,      and      Samuel      R      Bowman.\\nGlue:\\n \\n \\n \\nA      multi-task      benchmark      and      analysis      platform      for  \\n \\n  natural      language      understanding.\\narXiv      preprint  \\n \\n  arXiv:      1804.07461,      2018.\\n \\n \\n  Alex      Warstadt,      Amanpreet      Singh,      and      Samuel      R.  \\n \\n  Bowman.\\nNeural      network      acceptability      judgments.\\n \\n \\n  arXiv      preprint      1805.12471,      2018.\\n \\n \\n  Adina      Williams,      Nikita      Nangia,      and      Samuel      R      Bow-  \\n \\n  man.\\n \\n \\nA   \\n \\n_      broad-coverage      challenge      corpus      for  \\n \\n  sentence      understanding      through      inference.\\narXiv  \\n \\n  preprint      arXiv:      1704.05426,      2017.\\n \\n \\n  Adina      Williams,      Nikita      Nangia,      and      Samuel      R.      Bow-  \\n \\n  man.\\n \\n \\nA      broad-coverage      challenge      corpus      for      sen-  \\n \\n  tence      understanding      through      inference.\\nIn      Proceed-  \\n \\n  ings      of      NAACL-HLT,      2018.\\n \\n \\n  Zhilin      Yang,      Zihang      Dai,      Yiming      Yang,      Jaime  \\n \\n  Carbonell,      Ruslan      Salakhutdinov,      and      Quoc   \\n \\nV       Le.\\nXlInet:      Generalized      autoregressive      pretrain-  \\n \\n  ing      for      language      understanding.\\narXiv      preprint  \\n \\n  arXiv:      1906.08237,      2019.'},\n",
       " {'metadata': {'source': 'https://arxiv.org/pdf/1910.13461.pdf',\n",
       "   'section_number': 17,\n",
       "   'section_title': 'Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96'},\n",
       "  'page_content': 'Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96\\n \\n \\n  Table      6:      The      performance      (BLEU)      of      baseline      and  \\n \\n  BART      on      WMT’16      RO-EN      augmented      with      back-  \\n \\n  translation      data.\\nBART      improves      over   \\n \\na      strong      back-  \\n \\n  translation      (BT)      baseline      by      using      monolingual      English  \\n \\n  pre-training.\\n \\n \\n  Abstractive      QA      We      use      the      recently      proposed      ELIS  \\n \\n  dataset      to      test      the      model’s      ability      to      generate      long      free-  \\n \\n  form      answers.\\nWe      find      BART      outperforms      the      best      pre-  \\n \\n  vious      work      by      1.2      ROUGE-L,      but      the      dataset      remains  \\n \\n  a      challenging,      because      answers      are      only      weakly      speci-  \\n \\n  fied      by      the      question.\\n5.4      Translation\\ntion  \\n \\n  We      also      evaluated      performance      on      WMT16      Romanian-  \\n \\n  English,      augmented      with      back-translation      data  \\n \\n  from      Sennrich      et      al.\\n(2016).\\nWe      use   \\n \\na      6-layer  \\n \\n  transformer      source      encoder      to      map      Romanian      into  \\n \\n  a      representation      that      BART      is      able      to      de-noise      into  \\n \\n  English,      following      the      approach      introduced      in      83.4.  \\n \\n  Experiment      results      are      presented      in      Table      6.      We  \\n \\n  compare      our      results      against   \\n \\na      baseline      Transformer  \\n \\n  architecture      (Vaswani      et      al.,      2017)      with      Transformer-  \\n \\n  large      settings      (the      baseline      row).\\nWe      show      the  \\n \\n  performance      of      both      steps      of      our      model      in      the      fixed  \\n \\n  BART      and      tuned      BART      rows.\\nFor      each      row      we  \\n \\n  experiment      on      the      original      WMT16      Romanian-English  \\n \\n  augmented      with      back-translation      data.\\nWe      use   \\n \\na       beam      width      of   \\n \\n5      and   \\n \\na      length      penalty      of   \\n \\na   \\n \\n=      1.  \\n \\n  Preliminary      results      suggested      that      our      approach      was  \\n \\n  less      effective      without      back-translation      data,      and      prone  \\n \\n  to      overfitting—future      work      should      explore      additional  \\n \\n  regularization      techniques.\\n6\\n \\n   Qualitative      Analysis  \\n \\n  BART      shows      large      improvements      on      summarization  \\n \\n  metrics,      of      up      to   \\n \\n6      points      over      the      prior      state-of-the-art.\\n \\n \\n  To      understand      BART’s      performance      beyond      automated  \\n \\n  metrics,      we      analyse      its      generations      qualitatively.\\n \\n \\n  Table   \\n \\n7      shows      example      summaries      generated      by  \\n \\n  BART.\\nExamples      are      taken      from      WikiNews      articles  \\n \\n  published      after      the      creation      of      the      pre-training      corpus,  \\n \\n  to      eliminate      the      possibility      of      the      events      described      be-  \\n \\n  ing      present      in      the      model’s      training      data.\\nFollowing  \\n \\n  Narayan      et      al.\\n(2018),      we      remove      the      first      sentence      of  \\n \\n  the      article      prior      to      summarizing      it,      so      there      is      no      easy  \\n \\n  extractive      summary      of      the      document.\\n \\n \\n  Unsurprisingly,      model      output      is      fluent      and      grammat-  \\n \\n  ical      English.\\nHowever,      model      output      is      also      highly      ab-  \\n \\n  stractive,      with      few      phrases      copied      from      the      input.\\nThe  \\n \\n  output      is      also      generally      factually      accurate,      and      inte-  \\n \\n  grates      supporting      evidence      from      across      the      input      doc-  \\n \\n  ument      with      background      knowledge      (for      example,      cor-  \\n \\n  rectly      completing      names,      or      inferring      that      PG&E      oper-  \\n \\n  ates      in      California).\\nIn      the      first      example,      inferring      that  \\n \\n  fish      are      protecting      reefs      from      global      warming      requires  \\n \\n  non-trivial      inference      from      the      text.\\nHowever,      the      claim  \\n \\n  that      the      work      was      published      in      Science      is      not      supported  \\n \\n  by      the      source.\\n \\n \\n  These      samples      demonstrate      that      the      BART      pretrain-  \\n \\n  ing      has      learned   \\n \\na      strong      combination      of      natural      lan-  \\n \\n  guage      understanding      and      generation.\\n7\\n \\n   Related      Work  \\n \\n  Early      methods      for      pretraining      were      based      on      language  \\n \\n  models.\\nGPT      (Radford      et      al.,      2018)      only      models      left-  \\n \\n  ward      context,      which      is      problematic      for      some      tasks.\\n \\n \\n  ELMo      (Peters      et      al.,      2018)      concatenates      left-only      and  \\n \\n  right-only      representations,      but      does      not      pre-train      inter-  \\n \\n  actions      between      these      features.\\nRadford      et      al.\\n(2019)  \\n \\n  demonstrated      that      very      large      language      models      can      act       as      unsupervised      multitask      models.\\n \\n \\n  MASS      (Song      et      al.,      2019)      is      perhaps      the      most      similar  \\n \\n  model      to      BART.\\nAn      input      sequence      where   \\n \\na      contiguous  \\n \\n  span      of      tokens      is      masked      is      mapped      to   \\n \\na      sequence      con-  \\n \\n  sisting      of      the      missing      tokens.\\nMASS      is      less      effective  \\n \\n  for      discriminative      tasks,      because      disjoint      sets      of      tokens  \\n \\n  are      fed      into      the      encoder      and      decoder.\\n \\n \\n  BERT      (Devlin      et      al.,      2019)      introduced      masked      lan-  \\n \\n  guage      modelling,      which      allows      pre-training      to      learn      in-  \\n \\n  teractions      between      left      and      right      context      words.\\nRe-  \\n \\n  cent      work      has      shown      that      very      strong      performance      can  \\n \\n  be      achieved      by      training      for      longer      (Liu      et      al.,      2019),  \\n \\n  by      tying      parameters      across      layers      (Lan      et      al.,      2019),  \\n \\n  and      by      masking      spans      instead      of      words      (Joshi      et      al.,  \\n \\n  2019).\\nPredictions      are      not      made      auto-regressively,      re-  \\n \\n  ducing      the      effectiveness      of      BERT      for      generation      tasks.\\n \\n \\n  UniLM      (Dong      et      al.,      2019)      fine-tunes      BERT      with      an  \\n \\n  ensemble      of      masks,      some      of      which      allow      only      leftward  \\n \\n  context.\\nLike      BART,      this      allows      UniLM      to      be      used      for  \\n \\n  both      generative      and      discriminative      tasks.\\n \\n \\nA      difference  \\n \\n  is      that      UniLM      predictions      are      conditionally      indepen-  \\n \\n  dent,      whereas      BART’s      are      autoregressive.\\nBART      re-  \\n \\n  duces      the      mismatch      between      pre-training      and      genera-  \\n \\n  tion      tasks,      because      the      decoder      is      always      trained      on      un-  \\n \\n  corrupted      context.\\n \\n \\n  XL-Net      (Yang      et      al.,      2019)      extends      BERT      by      pre-\\n |       Source      Document      (abbreviated) |       BART      Summary\\n |       The      researchers      examined      three      types      of      coral      in      reefs      off      the coast of Fiji The researchers found when fish were plentiful,       they      would      eat      algae      and      seaweed      off      the      corals,      which      appeared       to      leave      them      more      resistant      to      the      bacterium      Vibrio      coralliilyti-       cus,      a      bacterium      associated      with      bleaching.      The      researchers      sug-       gested      the      algae,      like      warming      temperatures,      might      render      the       corals’      chemical      defenses      less      effective,      and      the      fish      were      pro-       tecting      the      coral      by      removing      the      algae. |       Fisheries      off      the      coast      of      Fiji      are      protect-       ing      coral      reefs      from      the      effects      of      global       warming,      according      to      a      study      in      the      jour-       nal      Science.\\n |       Sacoolas,      who      has      immunity      as      a      diplomat’s      wife,      was      involved in a traffic collision Prime Minister Johnson was questioned       about      the      case      while      speaking      to      the      press      at      a      hospital      in      Wat- ford. He said, “I hope that Anne Sacoolas will come back       if      we      can’t      resolve      it      then      of      course      I      will      be      raising      it      myself       personally      with      the      White      House.” |       Boris      Johnson      has      said      he      will      raise      the      is-       sue      of      US      diplomat      Anne      Sacoolas’      diplo-       matic      immunity      with      the      White      House.\\n |       According      to      Syrian      state      media,      government      forces      began      de- ploying into previously SDF controlled territory yesterday.       On      October      6,      US      President      Donald      Trump      and      Turkish      Presi-       dent      Recep      Tayyip      Erdoan      spoke      on      the      phone.      Then      both      na-       tions      issued      statements      speaking      of      an      imminent      incursion      into northeast Syria On Wednesday, Turkey began a military       offensive      with      airstrikes      followed      by      a      ground      invasion. |       Syrian      government      forces      have      entered       territory      held      by      the      US-backed      Syrian       Democratic      Forces      (SDF)      in      response      to       Turkey’s      incursion      into      the      region.\\n |       This      is      the      first      time      anyone      has      been      recorded      to      run      a      full       marathon      of      42.195      kilometers      (approximately      26      miles)      under       this      pursued      landmark      time.      It      was      not,      however,      an      officially       sanctioned      world      record,      as      it      was      not      an      ’open      race”      of      the       IAAF.      His      time      was      1      hour      59      minutes      40.2      seconds.      Kipchoge       ran      in      Vienna,      Austria.      It      was      an      event      specifically      designed      to       help      Kipchoge      break      the      two      hour      barrier. |       Kenyan      runner      Eliud      Kipchoge      has      run      a       marathon      in      less      than      two      hours.\\n |       PG&E      stated      it      scheduled      the      blackouts      in      response      to      forecasts       for      high      winds      amid      dry      conditions.      The      aim      is      to      reduce      the      risk       of      wildfires.      Nearly      800      thousand      customers      were      scheduled      to       be      affected      by      the      shutoffs      which      were      expected      to      last      through       at      least      midday      tomorrow. |       Power      has      been      turned      off      to      millions      of       customers      in      California      as      part      of      a      power       shutoff      plan.\\n\\n \\n \\n  dicting      masked      tokens      auto-regressively      in   \\n \\na      permuted  \\n \\n  order.\\nThis      objective      allows      predictions      to      condition      on  \\n \\n  both      left      and      right      context.\\nIn      contrast,      the      BART      de-  \\n \\n  coder      works      left-to-right      during      pre-training,      matching  \\n \\n  the      setting      during      generation.\\n \\n \\n  Several      papers      have      explored      using      pre-trained      rep-  \\n \\n  resentations      to      improve      machine      translation.\\nThe  \\n \\n  largest      improvements      have      come      from      pre-training      on  \\n \\n  both      source      and      target      languages      (Song      et      al.,      2019;  \\n \\n  Lample   \\n \\n&      Conneau,      2019),      but      this      requires      pre-  \\n \\n  training      on      all      languages      of      interest.\\nOther      work      has  \\n \\n  shown      that      encoders      can      be      improved      using      pre-trained  \\n \\n  representations      (Edunov      et      al.,      2019),      but      gains      in      de-  \\n \\n  coders      are      more      limited.\\nWe      show      how      BART      can      be  \\n \\n  used      to      improve      machine      translation      decoders.\\n8\\n \\n   Conclusions  \\n \\n  We      introduced      BART,   \\n \\na      pre-training      approach      that  \\n \\n  learns      to      map      corrupted      documents      to      the      original.\\n \\n \\n  BART      achieves      similar      performance      to      ROBERTa      on  \\n \\n  discriminative      tasks,      while      achieving      new      state-of-the-  \\n \\n  art      results      on   \\n \\na      number      of      text      generation      tasks.\\nFu-  \\n \\n  ture      work      should      explore      new      methods      for      corrupting  \\n \\n  documents      for      pre-training,      perhaps      tailoring      them      to  \\n \\n  specific      end      tasks.'},\n",
       " {'metadata': {'source': 'https://arxiv.org/pdf/1910.13461.pdf',\n",
       "   'section_number': 18,\n",
       "   'section_title': '5.4      Translation'},\n",
       "  'page_content': '5.4      Translation\\ntion  \\n \\n  We      also      evaluated      performance      on      WMT16      Romanian-  \\n \\n  English,      augmented      with      back-translation      data  \\n \\n  from      Sennrich      et      al.\\n(2016).\\nWe      use   \\n \\na      6-layer  \\n \\n  transformer      source      encoder      to      map      Romanian      into  \\n \\n  a      representation      that      BART      is      able      to      de-noise      into  \\n \\n  English,      following      the      approach      introduced      in      83.4.  \\n \\n  Experiment      results      are      presented      in      Table      6.      We  \\n \\n  compare      our      results      against   \\n \\na      baseline      Transformer  \\n \\n  architecture      (Vaswani      et      al.,      2017)      with      Transformer-  \\n \\n  large      settings      (the      baseline      row).\\nWe      show      the  \\n \\n  performance      of      both      steps      of      our      model      in      the      fixed  \\n \\n  BART      and      tuned      BART      rows.\\nFor      each      row      we  \\n \\n  experiment      on      the      original      WMT16      Romanian-English  \\n \\n  augmented      with      back-translation      data.\\nWe      use   \\n \\na       beam      width      of   \\n \\n5      and   \\n \\na      length      penalty      of   \\n \\na   \\n \\n=      1.  \\n \\n  Preliminary      results      suggested      that      our      approach      was  \\n \\n  less      effective      without      back-translation      data,      and      prone  \\n \\n  to      overfitting—future      work      should      explore      additional  \\n \\n  regularization      techniques.\\n6\\n \\n   Qualitative      Analysis  \\n \\n  BART      shows      large      improvements      on      summarization  \\n \\n  metrics,      of      up      to   \\n \\n6      points      over      the      prior      state-of-the-art.\\n \\n \\n  To      understand      BART’s      performance      beyond      automated  \\n \\n  metrics,      we      analyse      its      generations      qualitatively.\\n \\n \\n  Table   \\n \\n7      shows      example      summaries      generated      by  \\n \\n  BART.\\nExamples      are      taken      from      WikiNews      articles  \\n \\n  published      after      the      creation      of      the      pre-training      corpus,  \\n \\n  to      eliminate      the      possibility      of      the      events      described      be-  \\n \\n  ing      present      in      the      model’s      training      data.\\nFollowing  \\n \\n  Narayan      et      al.\\n(2018),      we      remove      the      first      sentence      of  \\n \\n  the      article      prior      to      summarizing      it,      so      there      is      no      easy  \\n \\n  extractive      summary      of      the      document.\\n \\n \\n  Unsurprisingly,      model      output      is      fluent      and      grammat-  \\n \\n  ical      English.\\nHowever,      model      output      is      also      highly      ab-  \\n \\n  stractive,      with      few      phrases      copied      from      the      input.\\nThe  \\n \\n  output      is      also      generally      factually      accurate,      and      inte-  \\n \\n  grates      supporting      evidence      from      across      the      input      doc-  \\n \\n  ument      with      background      knowledge      (for      example,      cor-  \\n \\n  rectly      completing      names,      or      inferring      that      PG&E      oper-  \\n \\n  ates      in      California).\\nIn      the      first      example,      inferring      that  \\n \\n  fish      are      protecting      reefs      from      global      warming      requires  \\n \\n  non-trivial      inference      from      the      text.\\nHowever,      the      claim  \\n \\n  that      the      work      was      published      in      Science      is      not      supported  \\n \\n  by      the      source.\\n \\n \\n  These      samples      demonstrate      that      the      BART      pretrain-  \\n \\n  ing      has      learned   \\n \\na      strong      combination      of      natural      lan-  \\n \\n  guage      understanding      and      generation.\\n7\\n \\n   Related      Work  \\n \\n  Early      methods      for      pretraining      were      based      on      language  \\n \\n  models.\\nGPT      (Radford      et      al.,      2018)      only      models      left-  \\n \\n  ward      context,      which      is      problematic      for      some      tasks.\\n \\n \\n  ELMo      (Peters      et      al.,      2018)      concatenates      left-only      and  \\n \\n  right-only      representations,      but      does      not      pre-train      inter-  \\n \\n  actions      between      these      features.\\nRadford      et      al.\\n(2019)  \\n \\n  demonstrated      that      very      large      language      models      can      act       as      unsupervised      multitask      models.\\n \\n \\n  MASS      (Song      et      al.,      2019)      is      perhaps      the      most      similar  \\n \\n  model      to      BART.\\nAn      input      sequence      where   \\n \\na      contiguous  \\n \\n  span      of      tokens      is      masked      is      mapped      to   \\n \\na      sequence      con-  \\n \\n  sisting      of      the      missing      tokens.\\nMASS      is      less      effective  \\n \\n  for      discriminative      tasks,      because      disjoint      sets      of      tokens  \\n \\n  are      fed      into      the      encoder      and      decoder.\\n \\n \\n  BERT      (Devlin      et      al.,      2019)      introduced      masked      lan-  \\n \\n  guage      modelling,      which      allows      pre-training      to      learn      in-  \\n \\n  teractions      between      left      and      right      context      words.\\nRe-  \\n \\n  cent      work      has      shown      that      very      strong      performance      can  \\n \\n  be      achieved      by      training      for      longer      (Liu      et      al.,      2019),  \\n \\n  by      tying      parameters      across      layers      (Lan      et      al.,      2019),  \\n \\n  and      by      masking      spans      instead      of      words      (Joshi      et      al.,  \\n \\n  2019).\\nPredictions      are      not      made      auto-regressively,      re-  \\n \\n  ducing      the      effectiveness      of      BERT      for      generation      tasks.\\n \\n \\n  UniLM      (Dong      et      al.,      2019)      fine-tunes      BERT      with      an  \\n \\n  ensemble      of      masks,      some      of      which      allow      only      leftward  \\n \\n  context.\\nLike      BART,      this      allows      UniLM      to      be      used      for  \\n \\n  both      generative      and      discriminative      tasks.\\n \\n \\nA      difference  \\n \\n  is      that      UniLM      predictions      are      conditionally      indepen-  \\n \\n  dent,      whereas      BART’s      are      autoregressive.\\nBART      re-  \\n \\n  duces      the      mismatch      between      pre-training      and      genera-  \\n \\n  tion      tasks,      because      the      decoder      is      always      trained      on      un-  \\n \\n  corrupted      context.\\n \\n \\n  XL-Net      (Yang      et      al.,      2019)      extends      BERT      by      pre-\\n |       Source      Document      (abbreviated) |       BART      Summary\\n |       The      researchers      examined      three      types      of      coral      in      reefs      off      the coast of Fiji The researchers found when fish were plentiful,       they      would      eat      algae      and      seaweed      off      the      corals,      which      appeared       to      leave      them      more      resistant      to      the      bacterium      Vibrio      coralliilyti-       cus,      a      bacterium      associated      with      bleaching.      The      researchers      sug-       gested      the      algae,      like      warming      temperatures,      might      render      the       corals’      chemical      defenses      less      effective,      and      the      fish      were      pro-       tecting      the      coral      by      removing      the      algae. |       Fisheries      off      the      coast      of      Fiji      are      protect-       ing      coral      reefs      from      the      effects      of      global       warming,      according      to      a      study      in      the      jour-       nal      Science.\\n |       Sacoolas,      who      has      immunity      as      a      diplomat’s      wife,      was      involved in a traffic collision Prime Minister Johnson was questioned       about      the      case      while      speaking      to      the      press      at      a      hospital      in      Wat- ford. He said, “I hope that Anne Sacoolas will come back       if      we      can’t      resolve      it      then      of      course      I      will      be      raising      it      myself       personally      with      the      White      House.” |       Boris      Johnson      has      said      he      will      raise      the      is-       sue      of      US      diplomat      Anne      Sacoolas’      diplo-       matic      immunity      with      the      White      House.\\n |       According      to      Syrian      state      media,      government      forces      began      de- ploying into previously SDF controlled territory yesterday.       On      October      6,      US      President      Donald      Trump      and      Turkish      Presi-       dent      Recep      Tayyip      Erdoan      spoke      on      the      phone.      Then      both      na-       tions      issued      statements      speaking      of      an      imminent      incursion      into northeast Syria On Wednesday, Turkey began a military       offensive      with      airstrikes      followed      by      a      ground      invasion. |       Syrian      government      forces      have      entered       territory      held      by      the      US-backed      Syrian       Democratic      Forces      (SDF)      in      response      to       Turkey’s      incursion      into      the      region.\\n |       This      is      the      first      time      anyone      has      been      recorded      to      run      a      full       marathon      of      42.195      kilometers      (approximately      26      miles)      under       this      pursued      landmark      time.      It      was      not,      however,      an      officially       sanctioned      world      record,      as      it      was      not      an      ’open      race”      of      the       IAAF.      His      time      was      1      hour      59      minutes      40.2      seconds.      Kipchoge       ran      in      Vienna,      Austria.      It      was      an      event      specifically      designed      to       help      Kipchoge      break      the      two      hour      barrier. |       Kenyan      runner      Eliud      Kipchoge      has      run      a       marathon      in      less      than      two      hours.\\n |       PG&E      stated      it      scheduled      the      blackouts      in      response      to      forecasts       for      high      winds      amid      dry      conditions.      The      aim      is      to      reduce      the      risk       of      wildfires.      Nearly      800      thousand      customers      were      scheduled      to       be      affected      by      the      shutoffs      which      were      expected      to      last      through       at      least      midday      tomorrow. |       Power      has      been      turned      off      to      millions      of       customers      in      California      as      part      of      a      power       shutoff      plan.\\n\\n \\n \\n  dicting      masked      tokens      auto-regressively      in   \\n \\na      permuted  \\n \\n  order.\\nThis      objective      allows      predictions      to      condition      on  \\n \\n  both      left      and      right      context.\\nIn      contrast,      the      BART      de-  \\n \\n  coder      works      left-to-right      during      pre-training,      matching  \\n \\n  the      setting      during      generation.\\n \\n \\n  Several      papers      have      explored      using      pre-trained      rep-  \\n \\n  resentations      to      improve      machine      translation.\\nThe  \\n \\n  largest      improvements      have      come      from      pre-training      on  \\n \\n  both      source      and      target      languages      (Song      et      al.,      2019;  \\n \\n  Lample   \\n \\n&      Conneau,      2019),      but      this      requires      pre-  \\n \\n  training      on      all      languages      of      interest.\\nOther      work      has  \\n \\n  shown      that      encoders      can      be      improved      using      pre-trained  \\n \\n  representations      (Edunov      et      al.,      2019),      but      gains      in      de-  \\n \\n  coders      are      more      limited.\\nWe      show      how      BART      can      be  \\n \\n  used      to      improve      machine      translation      decoders.\\n8\\n \\n   Conclusions  \\n \\n  We      introduced      BART,   \\n \\na      pre-training      approach      that  \\n \\n  learns      to      map      corrupted      documents      to      the      original.\\n \\n \\n  BART      achieves      similar      performance      to      ROBERTa      on  \\n \\n  discriminative      tasks,      while      achieving      new      state-of-the-  \\n \\n  art      results      on   \\n \\na      number      of      text      generation      tasks.\\nFu-  \\n \\n  ture      work      should      explore      new      methods      for      corrupting  \\n \\n  documents      for      pre-training,      perhaps      tailoring      them      to  \\n \\n  specific      end      tasks.'},\n",
       " {'metadata': {'source': 'https://arxiv.org/pdf/1910.13461.pdf',\n",
       "   'section_number': 19,\n",
       "   'section_title': 'References'},\n",
       "  'page_content': 'References\\n \\n \\n  Eneko      Agirre,      Llu’is      M‘arquez,      and      Richard      Wicen-  \\n \\n  towski      (eds.).\\nProceedings      of      the      Fourth      Interna-  \\n \\n  tional      Workshop      on      Semantic      Evaluations      (SemEval-  \\n \\n  2007).\\nAssociation      for      Computational      Linguistics,  \\n \\n  Prague,      Czech      Republic,      June      2007.\\n \\n \\n  Ido      Dagan,      Oren      Glickman,      and      Bernardo      Magnini.\\n \\n \\n  The      PASCAL      recognising      textual      entailment      chal-  \\n \\n  lenge.\\nIn      Machine      learning      challenges.\\nevaluat-  \\n \\n  ing      predictive      uncertainty,      visual      object      classifica-  \\n \\n  tion,      and      recognising      tectual      entailment,      pp.\\n177—  \\n \\n  190.\\nSpringer,      2006.\\n \\n \\n  Jacob      Devlin,      Ming-Wei      Chang,      Kenton      Lee,      and  \\n \\n  Kristina      Toutanova.\\nBERT:      Pre-training      of      deep  \\n \\n  bidirectional      transformers      for      language      understand-  \\n \\n  ing.\\nIn      Proceedings      of      the      2019      Conference      of      the  \\n \\n  North      American      Chapter      of      the      Association      for      Com-  \\n \\n  putational      Linguistics:      Human      Language      Technolo-  \\n \\n  gies,      Volume   \\n \\nI      (Long      and      Short      Papers),      pp.\\n4171-  \\n \\n  4186,      Minneapolis,      Minnesota,      June      2019.\\nAssocia-  \\n \\n  tion      for      Computational      Linguistics.\\ndoi:      10.18653/  \\n \\n  vI/N19-1423.\\nURL      https://www.aclweb.\\n \\n \\n  org/anthology/N19-1423.\\n \\n \\n  Emily      Dinan,      Varvara      Logacheva,      Valentin      Malykh,  \\n \\n  Alexander      Miller,      Kurt      Shuster,      Jack      Urbanek,  \\n \\n  Douwe      Kiela,      Arthur      Szlam,      Iulian      Serban,      Ryan  \\n \\n  Lowe,      et      al.\\nThe      second      conversational      in-  \\n \\n  telligence      challenge      (convai2).\\narXiv      preprint  \\n \\n  arXiv:      1902.00098,      2019.\\n \\n \\n  William   \\n \\nB      Dolan      and      Chris      Brockett.\\nAutomatically  \\n \\n  constructing   \\n \\na      corpus      of      sentential      paraphrases.\\nIn  \\n \\n  Proceedings      of      the      International      Workshop      on      Para-  \\n \\n  phrasing,      2005.\\n \\n \\n  Li      Dong,      Nan      Yang,      Wenhui      Wang,      Furu      Wei,      Xi-  \\n \\n  aodong      Liu,      Yu      Wang,      Jianfeng      Gao,      Ming      Zhou,  \\n \\n  and      Hsiao-Wuen      Hon.\\nUnified      language      model      pre-  \\n \\n  training      for      natural      language      understanding      and      gen-  \\n \\n  eration.\\narXiv      preprint      arXiv:      1905.03197,      2019.\\n \\n \\n  Sergey      Edunov,      Alexei      Baevski,      and      Michael      Auli.\\n \\n \\n  Pre-trained      language      model      representations      for      lan-  \\n \\n  guage      generation.\\nIn      Proceedings      of      the      2019      Con-  \\n \\n  ference      of      the      North      American      Chapter      of      the      Asso-  \\n \\n  ciation      for      Computational      Linguistics:      Human      Lan-  \\n \\n  guage      Technologies,      Volume   \\n \\n1      (Long      and      Short      Pa-  \\n \\n  pers),      2019.\\n \\n \\n  Angela      Fan,      David      Grangier,      and      Michael      Auli.\\nCon-  \\n \\n  trollable      abstractive      summarization.\\narXiv      preprint  \\n \\n  arXiv:      1711.05217,      2017.\\n \\n \\n  Angela      Fan,      Yacine      Jernite,      Ethan      Perez,      David  \\n \\n  Grangier,      Jason      Weston,      and      Michael      Auli.\\nEli5:  \\n \\n  Long      form      question      answering.\\narXiv      preprint  \\n \\n  arXiv:      1907.09190,      2019.\\n \\n \\n  Dan      Hendrycks      and      Kevin      Gimpel.\\nGaussian      error      lin-  \\n \\n  ear      units      (gelus).\\narXiv      preprint      arXiv:      1606.08415,  \\n \\n  2016.\\n \\n \\n  Karl      Moritz      Hermann,      Tomas      Kocisky,      Edward  \\n \\n  Grefenstette,      Lasse      Espeholt,      Will      Kay,      Mustafa      Su-  \\n \\n  leyman,      and      Phil      Blunsom.\\nTeaching      machines      to  \\n \\n  read      and      comprehend.\\nIn      Advances      in      neural      infor-  \\n \\n  mation      processing      systems,      pp.\\n1693-1701,      2015.\\n \\n \\n  Mandar      Joshi,      Danqi      Chen,      Yinhan      Liu,      Daniel   \\n \\nS      Weld,  \\n \\n  Luke      Zettlemoyer,      and      Omer      Levy.\\nSpanbert:      Im-  \\n \\n  proving      pre-training      by      representing      and      predicting  \\n \\n  spans.\\narXiv      preprint      arXiv:      1907.10529,      2019.\\n \\n \\n  Guillaume      Lample      and      Alexis      Conneau.\\n \\n \\n—      Cross-  \\n \\n  lingual      language      model      pretraining.\\narXiv      preprint  \\n \\n  arXiv:      1901.07291,      2019.\\n \\n \\n  Zhenzhong      Lan,      Mingda      Chen,      Sebastian      Goodman,  \\n \\n  Kevin      Gimpel,      Piyush      Sharma,      and      Radu      Sori-  \\n \\n  cut.\\nAlbert:   \\n \\nA      lite      bert      for      self-supervised      learn-  \\n \\n  ing      of      language      representations.\\narXiv      preprint  \\n \\n  arXiv:      1909.11942,      2019.\\n \\n \\n  Hector   \\n \\nJ      Levesque,      Ernest      Davis,      and      Leora      Morgen-  \\n \\n  stern.\\nThe      Winograd      schema      challenge.\\nIn      AAAI  \\n \\n  Spring      Symposium:      Logical      Formalizations      of      Com-  \\n \\n  monsense      Reasoning,      volume      46,      pp.\\n47,      2011.\\n \\n \\n  Yang      Liu      and      Mirella      Lapata.\\n \\n \\n  tion      with      pretrained      encoders.\\n \\n \\n  arXiv:      1908.08345,      2019.  \\n \\n  Text      summariza-  \\n \\n  arXiv      preprint  \\n \\n  Yinhan      Liu,      Myle      Ott,      Naman      Goyal,      Jingfei      Du,      Man-  \\n \\n  dar      Joshi,      Dangi      Chen,      Omer      Levy,      Mike      Lewis,  \\n \\n  Luke      Zettlemoyer,      and      Veselin      Stoyanov.\\nRoberta:\\n \\n \\n \\nA      robustly      optimized      bert      pretraining      approach.\\n \\n \\n  arXiv      preprint      arXiv:      1907.11692,      2019.\\n \\n \\n  Tomas      Mikolov,      Kai      Chen,      Greg      Corrado,      and      Jeffrey  \\n \\n  Dean.\\nEfficient      estimation      of      word      representations  \\n \\n  in      vector      space.\\narXiv      preprint      arXiv:1301.3781,  \\n \\n  2013.\\n \\n \\n  Shashi      Narayan,      Shay   \\n \\nB      Cohen,      and      Mirella      Lapata.\\n \\n \\n  Don’t      give      me      the      details,      just      the      summary!\\ntopic-  \\n \\n  aware      convolutional      neural      networks      for      extreme  \\n \\n  summarization.\\narXiv      preprint      arXiv:      1808.08745,  \\n \\n  2018.\\n \\n \\n  Gabriel      Pereyra,      George      Tucker,      Jan      Chorowski,  \\n \\n  Lukasz      Kaiser,      and      Geoffrey      Hinton.\\nRegularizing  \\n \\n  neural      networks      by      penalizing      confident      output      dis-  \\n \\n  tributions.\\narXiv      preprint      arXiv:      1701.06548,      2017.\\n \\n \\n  Matthew      E      Peters,      Mark      Neumann,      Mohit      Iyyer,      Matt  \\n \\n  Gardner,      Christopher      Clark,      Kenton      Lee,      and      Luke  \\n \\n  Zettlemoyer.\\nDeep      contextualized      word      representa-  \\n \\n  tions.\\narXiv      preprint      arXiv:      1802.05365,      2018.\\n \\n \\n  Alec      Radford,      Karthik      Narasimhan,      Tim      Salimans,  \\n \\n  and      Ilya      Sutskever.\\nImproving      language      un-  \\n \\n  derstanding      by      generative      pre-training.\\nURL  \\n \\n  https://s3-us-west-2.\\n \\n \\n|      amazonaws.\\n \\n \\n—      com/openai-  \\n \\n  assets/researchcovers/languageunsupervised/language  \\n \\n  understanding      paper.\\npdf,      2018.\\n \\n \\n  Alec      Radford,      Jeffrey      Wu,      Rewon      Child,      David      Luan,  \\n \\n  Dario      Amodei,      and      Ilya      Sutskever.\\nLanguage      mod-  \\n \\n  els      are      unsupervised      multitask      learners.\\nOpenAI  \\n \\n  Blog,      1(8),      2019.\\n \\n \\n  Pranav      Rajpurkar,      Jian      Zhang,      Konstantin      Lopyrev,  \\n \\n  and      Percy      Liang.\\nSquad:      100,000+      questions      for  \\n \\n  machine      comprehension      of      text.\\narXiv      preprint  \\n \\n  arXiv:      1606.05250,      2016.\\n \\n \\n  Abigail      See,      Peter   \\n \\nJ      Liu,      and      Christopher   \\n \\nD       Manning.\\nGet      to      the      point:      Summarization  \\n \\n  with      pointer-generator      networks.\\narXiv      preprint  \\n \\n  arXiv:      1704.04368,      2017.\\n \\n \\n  Rico      Sennrich,      Barry      Haddow,      and      Alexandra      Birch.\\n \\n \\n  Edinburgh      neural      machine      translation      systems      for  \\n \\n  WMT      16.\\nIn      Proceedings      of      the      First      Conference  \\n \\n  on      Machine      Translation:      Volume      2,      Shared      Task      Pa-  \\n \\n  pers,      2016.\\n \\n \\n  Richard      Socher,      Alex      Perelygin,      Jean      Wu,      Jason  \\n \\n  Chuang,      Christopher   \\n \\nD      Manning,      Andrew      Ng,      and  \\n \\n  Christopher      Potts.\\nRecursive      deep      models      for      se-  \\n \\n  mantic      compositionality      over   \\n \\na      sentiment      treebank.\\n \\n \\n  In      Proceedings      of      EMNLP,      pp.\\n1631-1642,      2013.\\n \\n \\n  Kaitao      Song,      Xu      Tan,      Tao      Qin,      Jianfeng      Lu,      and      Tie-  \\n \\n  Yan      Liu.\\nMass:      Masked      sequence      to      sequence      pre-  \\n \\n  training      for      language      generation.\\nIn      International  \\n \\n  Conference      on      Machine      Learning,      2019.\\n \\n \\n  Ashish      Vaswani,      Noam      Shazeer,      Niki      Parmar,      Jakob  \\n \\n  Uszkoreit,      Llion      Jones,      Aidan      N      Gomez,      Lukasz  \\n \\n  Kaiser,      and      Ilia      Polosukhin.\\nAttention      is      all      you  \\n \\n  need.\\nIn      Advances      in      neural      information      processing  \\n \\n  systems,      pp.\\n5998-6008,      2017.\\n \\n \\n  Alex      Wang,      Amanpreet      Singh,      Julian      Michael,      Felix  \\n \\n  Hill,      Omer      Levy,      and      Samuel      R      Bowman.\\nGlue:\\n \\n \\n \\nA      multi-task      benchmark      and      analysis      platform      for  \\n \\n  natural      language      understanding.\\narXiv      preprint  \\n \\n  arXiv:      1804.07461,      2018.\\n \\n \\n  Alex      Warstadt,      Amanpreet      Singh,      and      Samuel      R.  \\n \\n  Bowman.\\nNeural      network      acceptability      judgments.\\n \\n \\n  arXiv      preprint      1805.12471,      2018.\\n \\n \\n  Adina      Williams,      Nikita      Nangia,      and      Samuel      R      Bow-  \\n \\n  man.\\n \\n \\nA   \\n \\n_      broad-coverage      challenge      corpus      for  \\n \\n  sentence      understanding      through      inference.\\narXiv  \\n \\n  preprint      arXiv:      1704.05426,      2017.\\n \\n \\n  Adina      Williams,      Nikita      Nangia,      and      Samuel      R.      Bow-  \\n \\n  man.\\n \\n \\nA      broad-coverage      challenge      corpus      for      sen-  \\n \\n  tence      understanding      through      inference.\\nIn      Proceed-  \\n \\n  ings      of      NAACL-HLT,      2018.\\n \\n \\n  Zhilin      Yang,      Zihang      Dai,      Yiming      Yang,      Jaime  \\n \\n  Carbonell,      Ruslan      Salakhutdinov,      and      Quoc   \\n \\nV       Le.\\nXlInet:      Generalized      autoregressive      pretrain-  \\n \\n  ing      for      language      understanding.\\narXiv      preprint  \\n \\n  arXiv:      1906.08237,      2019.'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "section_documents_dicts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Library structure saved to library_structure.json\n",
      "\n",
      "Loaded structure:\n",
      "[ID: 1] library: Library\n",
      "  [ID: 2] book: Book 1\n",
      "    [ID: 3] section: 2.1      Architecture\n",
      "      [ID: 4] chunk: 2.1      Architecture - Chunk 1\n",
      "        Content: 2.1      Architecture\n",
      "ture  \n",
      " \n",
      "  BART      uses   ...\n",
      "      [ID: 5] chunk: 2.1      Architecture - Chunk 2\n",
      "        Content: For      our  \n",
      " \n",
      "  base      model,      we      u...\n",
      "      [ID: 6] chunk: 2.1      Architecture - Chunk 3\n",
      "        Content: The      architecture      is      closely      re...\n",
      "      [ID: 7] chunk: 2.1      Architecture - Chunk 4\n",
      "        Content: In      total,      BART      con-  \n",
      " \n",
      "  tains    ...\n",
      "    [ID: 8] section: 2.2      Pre-training      BART\n",
      "      [ID: 9] chunk: 2.2      Pre-training      BART - Chunk 1\n",
      "        Content: 2.2      Pre-training      BART\n",
      "BART  \n",
      " \n",
      "  BART   ...\n",
      "      [ID: 10] chunk: 2.2      Pre-training      BART - Chunk 2\n",
      "        Content: Unlike      existing      denoising      autoencod...\n",
      "      [ID: 11] chunk: 2.2      Pre-training      BART - Chunk 3\n",
      "        Content: We      experiment      with      several      pre...\n",
      "      [ID: 12] chunk: 2.2      Pre-training      BART - Chunk 4\n",
      "        Content: Token      Masking      Following      BERT      (...\n",
      "    [ID: 13] section: Token      Masking\n",
      "      [ID: 14] chunk: Token      Masking - Chunk 1\n",
      "        Content: Token      Masking...\n",
      "    [ID: 15] section: C.DE.AB\n",
      "      [ID: 16] chunk: C.DE.AB - Chunk 1\n",
      "        Content: C.DE.AB\n",
      "Token      Deletion Text      Infilling\n",
      " \n",
      "...\n",
      "      [ID: 17] chunk: C.DE.AB - Chunk 2\n",
      "        Content: Text      infilling      is      inspired      by ...\n",
      "      [ID: 18] chunk: C.DE.AB - Chunk 3\n",
      "        Content: Text      infill-  \n",
      " \n",
      "  ing      teaches      the ...\n",
      "      [ID: 19] chunk: C.DE.AB - Chunk 4\n",
      "        Content: Document      Rotation   \n",
      " \n",
      "A      token      is  ...\n",
      "      [ID: 20] chunk: C.DE.AB - Chunk 5\n",
      "        Content: 3\n",
      " \n",
      "   Fine-tuning      BART The      representati...\n",
      "      [ID: 21] chunk: C.DE.AB - Chunk 6\n",
      "        Content: 3.1      Sequence      Classification      Tasks\n",
      "a...\n",
      "      [ID: 22] chunk: C.DE.AB - Chunk 7\n",
      "        Content: This      approach      is      related      to   ...\n",
      "      [ID: 23] chunk: C.DE.AB - Chunk 8\n",
      "        Content: Token      Classification      Tasks\n",
      "asks  \n",
      " \n",
      "  Fo...\n",
      "      [ID: 24] chunk: C.DE.AB - Chunk 9\n",
      "        Content: This      representation      is      used      to...\n",
      "      [ID: 25] chunk: C.DE.AB - Chunk 10\n",
      "        Content: In      both      of      these      tasks,      i...\n",
      "      [ID: 26] chunk: C.DE.AB - Chunk 11\n",
      "        Content: 3.4      Machine      Translation\n",
      "tion  \n",
      " \n",
      "  We   ...\n",
      "      [ID: 27] chunk: C.DE.AB - Chunk 12\n",
      "        Content: (2019)      has      shown      that      models  ...\n",
      "      [ID: 28] chunk: C.DE.AB - Chunk 13\n",
      "        Content: We      show      that      it      is      possib...\n",
      "      [ID: 29] chunk: C.DE.AB - Chunk 14\n",
      "        Content: More      precisely,      we      replace      BAR...\n",
      "      [ID: 30] chunk: C.DE.AB - Chunk 15\n",
      "        Content: The      new      encoder      can      use   \n",
      " \n",
      "a...\n",
      "      [ID: 31] chunk: C.DE.AB - Chunk 16\n",
      "        Content: In      the      first      step,      we      fre...\n",
      "      [ID: 32] chunk: C.DE.AB - Chunk 17\n",
      "        Content: In      the      second      step,  \n",
      " \n",
      "  we      t...\n",
      "      [ID: 33] chunk: C.DE.AB - Chunk 18\n",
      "        Content: We      compare   \n",
      " \n",
      "a       range      of      op...\n",
      "      [ID: 34] chunk: C.DE.AB - Chunk 19\n",
      "        Content: 4.1      Comparison      Objectives\n",
      "ives  \n",
      " \n",
      "  Whi...\n",
      "      [ID: 35] chunk: C.DE.AB - Chunk 20\n",
      "        Content: We  \n",
      " \n",
      "  ABCDE  \n",
      " \n",
      "  weee  \n",
      " \n",
      "  Pre-trained      P...\n",
      "      [ID: 36] chunk: C.DE.AB - Chunk 21\n",
      "        Content: re-implement      strong      pre-training      ap...\n",
      "      [ID: 37] chunk: C.DE.AB - Chunk 22\n",
      "        Content: However,      we      do  \n",
      " \n",
      "  make      minor    ...\n",
      "      [ID: 38] chunk: C.DE.AB - Chunk 23\n",
      "        Content: For      refer-  \n",
      " \n",
      "  ence,      we      compare  ...\n",
      "      [ID: 39] chunk: C.DE.AB - Chunk 24\n",
      "        Content: We      compare      the      following      appro...\n",
      "      [ID: 40] chunk: C.DE.AB - Chunk 25\n",
      "        Content: Permuted      Language      Model      Based      ...\n",
      "      [ID: 41] chunk: C.DE.AB - Chunk 26\n",
      "        Content: For      con-  \n",
      " \n",
      "  sistency      with      other ...\n",
      "      [ID: 42] chunk: C.DE.AB - Chunk 27\n",
      "        Content: Masked      Language      Model      Following    ...\n",
      "      [ID: 43] chunk: C.DE.AB - Chunk 28\n",
      "        Content: Multitask      Masked      Language      Model    ...\n",
      "      [ID: 44] chunk: C.DE.AB - Chunk 29\n",
      "        Content: Self      at-  \n",
      " \n",
      "  tention      masks      are   ...\n",
      "      [ID: 45] chunk: C.DE.AB - Chunk 30\n",
      "        Content: Masked      Seq-to-Seq      Inspired      by      ...\n",
      "      [ID: 46] chunk: C.DE.AB - Chunk 31\n",
      "        Content: For      the      Permuted      LM,      Masked   ...\n",
      "      [ID: 47] chunk: C.DE.AB - Chunk 32\n",
      "        Content: We      experiment      with      (1)      treatin...\n",
      "      [ID: 48] chunk: C.DE.AB - Chunk 33\n",
      "        Content: We      find      the      former      works      ...\n",
      "      [ID: 49] chunk: C.DE.AB - Chunk 34\n",
      "        Content: 4.2      Tasks\n",
      "asks  \n",
      " \n",
      "  SQuAD  \n",
      " \n",
      "  (Rajpurkar  ...\n",
      "      [ID: 50] chunk: C.DE.AB - Chunk 35\n",
      "        Content: Similar      to      BERT      (Devlin      et    ...\n",
      "      [ID: 51] chunk: C.DE.AB - Chunk 36\n",
      "        Content: MNLI      (Williams      et      al.,      2017), ...\n",
      "      [ID: 52] chunk: C.DE.AB - Chunk 37\n",
      "        Content: In      contrast      to      BERT,  \n",
      " \n",
      "  the     ...\n",
      "      [ID: 53] chunk: C.DE.AB - Chunk 38\n",
      "        Content: XSum_      (Narayan      et      al.,      2018), ...\n",
      "      [ID: 54] chunk: C.DE.AB - Chunk 39\n",
      "        Content: Summaries      here      are      typically      c...\n",
      "      [ID: 55] chunk: C.DE.AB - Chunk 40\n",
      "        Content: Several      trends      are      clear:\n",
      "Model    ...\n",
      "      [ID: 56] chunk: C.DE.AB - Chunk 41\n",
      "        Content: Performance      of      pre-training      methods...\n",
      "      [ID: 57] chunk: C.DE.AB - Chunk 42\n",
      "        Content: Token      masking      is      crucial      Pre-t...\n",
      "      [ID: 58] chunk: C.DE.AB - Chunk 43\n",
      "        Content: Left-to-right      pre-training      improves     ...\n",
      "      [ID: 59] chunk: C.DE.AB - Chunk 44\n",
      "        Content: Bidirectional      encoders      are      crucial ...\n",
      "      [ID: 60] chunk: C.DE.AB - Chunk 45\n",
      "        Content: However,      BART      achieves      similar     ...\n",
      "      [ID: 61] chunk: C.DE.AB - Chunk 46\n",
      "        Content: Some      of      this      dif-  \n",
      " \n",
      "  ference    ...\n",
      "      [ID: 62] chunk: C.DE.AB - Chunk 47\n",
      "        Content: Pure      language      models      perform      b...\n",
      "      [ID: 63] chunk: C.DE.AB - Chunk 48\n",
      "        Content: A      pure      lan-  \n",
      " \n",
      "  guage      model      ...\n",
      "      [ID: 64] chunk: C.DE.AB - Chunk 49\n",
      "        Content: With      the      exception      of      ELI5,   ...\n",
      "      [ID: 65] chunk: C.DE.AB - Chunk 50\n",
      "        Content: 5\n",
      " \n",
      "   Large-scale      Pre-training      Experime...\n",
      "      [ID: 66] chunk: C.DE.AB - Chunk 51\n",
      "        Content: To      test      how      well      BART      per...\n",
      "      [ID: 67] chunk: C.DE.AB - Chunk 52\n",
      "        Content: 5.1      Experimental      Setup\n",
      "etup  \n",
      " \n",
      "  We    ...\n",
      "      [ID: 68] chunk: C.DE.AB - Chunk 53\n",
      "        Content: Docu-  \n",
      " \n",
      "  ments      are      tokenized      wit...\n",
      "      [ID: 69] chunk: C.DE.AB - Chunk 54\n",
      "        Content: Although      sen-  \n",
      " \n",
      "  tence      permutation   ...\n",
      "      [ID: 70] chunk: C.DE.AB - Chunk 55\n",
      "        Content: BART      performs      comparably      to      RO...\n",
      "      [ID: 71] chunk: C.DE.AB - Chunk 56\n",
      "        Content: CNN/DailyMail      XSum       RI      R2      RL  ...\n",
      "      [ID: 72] chunk: C.DE.AB - Chunk 57\n",
      "        Content: BART      outperforms      previous      work     ...\n",
      "      [ID: 73] chunk: C.DE.AB - Chunk 58\n",
      "        Content: To      help      th      del      better      fit...\n",
      "      [ID: 74] chunk: C.DE.AB - Chunk 59\n",
      "        Content: (2019),      consisting      of      160Gb      of...\n",
      "      [ID: 75] chunk: C.DE.AB - Chunk 60\n",
      "        Content: BART      20.72      11.85 5.2      Discriminative...\n",
      "      [ID: 76] chunk: C.DE.AB - Chunk 61\n",
      "        Content: Table      4:      BART      outperforms      prev...\n",
      "      [ID: 77] chunk: C.DE.AB - Chunk 62\n",
      "        Content: Overall,      BART      performs      simi-  \n",
      " \n",
      "  ...\n",
      "      [ID: 78] chunk: C.DE.AB - Chunk 63\n",
      "        Content: Summarization      To      provide   \n",
      " \n",
      "a      com...\n",
      "      [ID: 79] chunk: C.DE.AB - Chunk 64\n",
      "        Content: Extractive      models      do      well      here...\n",
      "      [ID: 80] chunk: C.DE.AB - Chunk 65\n",
      "        Content: BART      is      fine-tuned      as      a      s...\n",
      "      [ID: 81] chunk: C.DE.AB - Chunk 66\n",
      "        Content: During      generation,      we      set      beam...\n",
      "      [ID: 82] chunk: C.DE.AB - Chunk 67\n",
      "        Content: | 6.0      points      on      all      ROUGE     ...\n",
      "      [ID: 83] chunk: C.DE.AB - Chunk 68\n",
      "        Content: BART      outperforms      the  \n",
      " \n",
      "  best      pre...\n",
      "      [ID: 84] chunk: C.DE.AB - Chunk 69\n",
      "        Content: BART  \n",
      " \n",
      "  outperforms      previous      work    ...\n",
      "      [ID: 85] chunk: C.DE.AB - Chunk 70\n",
      "        Content: Rl      R2      RL\n",
      " \n",
      " \n",
      "  Best      Extractive     ...\n",
      "      [ID: 86] chunk: C.DE.AB - Chunk 71\n",
      "        Content: Comparison      models      are      from      Fan...\n",
      "    [ID: 87] section: Token      Deletion Text      Infilling\n",
      "      [ID: 88] chunk: Token      Deletion Text      Infilling - Chunk 1\n",
      "        Content: Token      Deletion Text      Infilling\n",
      " \n",
      " \n",
      "  Text...\n",
      "      [ID: 89] chunk: Token      Deletion Text      Infilling - Chunk 2\n",
      "        Content: Text      infilling      is      inspired      by ...\n",
      "      [ID: 90] chunk: Token      Deletion Text      Infilling - Chunk 3\n",
      "        Content: Text      infill-  \n",
      " \n",
      "  ing      teaches      the ...\n",
      "      [ID: 91] chunk: Token      Deletion Text      Infilling - Chunk 4\n",
      "        Content: Document      Rotation   \n",
      " \n",
      "A      token      is  ...\n",
      "      [ID: 92] chunk: Token      Deletion Text      Infilling - Chunk 5\n",
      "        Content: 3\n",
      " \n",
      "   Fine-tuning      BART The      representati...\n",
      "      [ID: 93] chunk: Token      Deletion Text      Infilling - Chunk 6\n",
      "        Content: 3.1      Sequence      Classification      Tasks\n",
      "a...\n",
      "      [ID: 94] chunk: Token      Deletion Text      Infilling - Chunk 7\n",
      "        Content: This      approach      is      related      to   ...\n",
      "      [ID: 95] chunk: Token      Deletion Text      Infilling - Chunk 8\n",
      "        Content: Token      Classification      Tasks\n",
      "asks  \n",
      " \n",
      "  Fo...\n",
      "      [ID: 96] chunk: Token      Deletion Text      Infilling - Chunk 9\n",
      "        Content: This      representation      is      used      to...\n",
      "      [ID: 97] chunk: Token      Deletion Text      Infilling - Chunk 10\n",
      "        Content: In      both      of      these      tasks,      i...\n",
      "      [ID: 98] chunk: Token      Deletion Text      Infilling - Chunk 11\n",
      "        Content: 3.4      Machine      Translation\n",
      "tion  \n",
      " \n",
      "  We   ...\n",
      "      [ID: 99] chunk: Token      Deletion Text      Infilling - Chunk 12\n",
      "        Content: (2019)      has      shown      that      models  ...\n",
      "      [ID: 100] chunk: Token      Deletion Text      Infilling - Chunk 13\n",
      "        Content: We      show      that      it      is      possib...\n",
      "      [ID: 101] chunk: Token      Deletion Text      Infilling - Chunk 14\n",
      "        Content: More      precisely,      we      replace      BAR...\n",
      "      [ID: 102] chunk: Token      Deletion Text      Infilling - Chunk 15\n",
      "        Content: The      new      encoder      can      use   \n",
      " \n",
      "a...\n",
      "      [ID: 103] chunk: Token      Deletion Text      Infilling - Chunk 16\n",
      "        Content: In      the      first      step,      we      fre...\n",
      "      [ID: 104] chunk: Token      Deletion Text      Infilling - Chunk 17\n",
      "        Content: In      the      second      step,  \n",
      " \n",
      "  we      t...\n",
      "      [ID: 105] chunk: Token      Deletion Text      Infilling - Chunk 18\n",
      "        Content: We      compare   \n",
      " \n",
      "a       range      of      op...\n",
      "      [ID: 106] chunk: Token      Deletion Text      Infilling - Chunk 19\n",
      "        Content: 4.1      Comparison      Objectives\n",
      "ives  \n",
      " \n",
      "  Whi...\n",
      "      [ID: 107] chunk: Token      Deletion Text      Infilling - Chunk 20\n",
      "        Content: We  \n",
      " \n",
      "  ABCDE  \n",
      " \n",
      "  weee  \n",
      " \n",
      "  Pre-trained      P...\n",
      "      [ID: 108] chunk: Token      Deletion Text      Infilling - Chunk 21\n",
      "        Content: re-implement      strong      pre-training      ap...\n",
      "      [ID: 109] chunk: Token      Deletion Text      Infilling - Chunk 22\n",
      "        Content: However,      we      do  \n",
      " \n",
      "  make      minor    ...\n",
      "      [ID: 110] chunk: Token      Deletion Text      Infilling - Chunk 23\n",
      "        Content: For      refer-  \n",
      " \n",
      "  ence,      we      compare  ...\n",
      "      [ID: 111] chunk: Token      Deletion Text      Infilling - Chunk 24\n",
      "        Content: We      compare      the      following      appro...\n",
      "      [ID: 112] chunk: Token      Deletion Text      Infilling - Chunk 25\n",
      "        Content: Permuted      Language      Model      Based      ...\n",
      "      [ID: 113] chunk: Token      Deletion Text      Infilling - Chunk 26\n",
      "        Content: For      con-  \n",
      " \n",
      "  sistency      with      other ...\n",
      "      [ID: 114] chunk: Token      Deletion Text      Infilling - Chunk 27\n",
      "        Content: Masked      Language      Model      Following    ...\n",
      "      [ID: 115] chunk: Token      Deletion Text      Infilling - Chunk 28\n",
      "        Content: Multitask      Masked      Language      Model    ...\n",
      "      [ID: 116] chunk: Token      Deletion Text      Infilling - Chunk 29\n",
      "        Content: Self      at-  \n",
      " \n",
      "  tention      masks      are   ...\n",
      "      [ID: 117] chunk: Token      Deletion Text      Infilling - Chunk 30\n",
      "        Content: Masked      Seq-to-Seq      Inspired      by      ...\n",
      "      [ID: 118] chunk: Token      Deletion Text      Infilling - Chunk 31\n",
      "        Content: For      the      Permuted      LM,      Masked   ...\n",
      "      [ID: 119] chunk: Token      Deletion Text      Infilling - Chunk 32\n",
      "        Content: We      experiment      with      (1)      treatin...\n",
      "      [ID: 120] chunk: Token      Deletion Text      Infilling - Chunk 33\n",
      "        Content: We      find      the      former      works      ...\n",
      "      [ID: 121] chunk: Token      Deletion Text      Infilling - Chunk 34\n",
      "        Content: 4.2      Tasks\n",
      "asks  \n",
      " \n",
      "  SQuAD  \n",
      " \n",
      "  (Rajpurkar  ...\n",
      "      [ID: 122] chunk: Token      Deletion Text      Infilling - Chunk 35\n",
      "        Content: Similar      to      BERT      (Devlin      et    ...\n",
      "      [ID: 123] chunk: Token      Deletion Text      Infilling - Chunk 36\n",
      "        Content: MNLI      (Williams      et      al.,      2017), ...\n",
      "      [ID: 124] chunk: Token      Deletion Text      Infilling - Chunk 37\n",
      "        Content: In      contrast      to      BERT,  \n",
      " \n",
      "  the     ...\n",
      "      [ID: 125] chunk: Token      Deletion Text      Infilling - Chunk 38\n",
      "        Content: XSum_      (Narayan      et      al.,      2018), ...\n",
      "      [ID: 126] chunk: Token      Deletion Text      Infilling - Chunk 39\n",
      "        Content: Summaries      here      are      typically      c...\n",
      "      [ID: 127] chunk: Token      Deletion Text      Infilling - Chunk 40\n",
      "        Content: Several      trends      are      clear:\n",
      "Model    ...\n",
      "      [ID: 128] chunk: Token      Deletion Text      Infilling - Chunk 41\n",
      "        Content: Performance      of      pre-training      methods...\n",
      "      [ID: 129] chunk: Token      Deletion Text      Infilling - Chunk 42\n",
      "        Content: Token      masking      is      crucial      Pre-t...\n",
      "      [ID: 130] chunk: Token      Deletion Text      Infilling - Chunk 43\n",
      "        Content: Left-to-right      pre-training      improves     ...\n",
      "      [ID: 131] chunk: Token      Deletion Text      Infilling - Chunk 44\n",
      "        Content: Bidirectional      encoders      are      crucial ...\n",
      "      [ID: 132] chunk: Token      Deletion Text      Infilling - Chunk 45\n",
      "        Content: However,      BART      achieves      similar     ...\n",
      "      [ID: 133] chunk: Token      Deletion Text      Infilling - Chunk 46\n",
      "        Content: Some      of      this      dif-  \n",
      " \n",
      "  ference    ...\n",
      "      [ID: 134] chunk: Token      Deletion Text      Infilling - Chunk 47\n",
      "        Content: Pure      language      models      perform      b...\n",
      "      [ID: 135] chunk: Token      Deletion Text      Infilling - Chunk 48\n",
      "        Content: A      pure      lan-  \n",
      " \n",
      "  guage      model      ...\n",
      "      [ID: 136] chunk: Token      Deletion Text      Infilling - Chunk 49\n",
      "        Content: With      the      exception      of      ELI5,   ...\n",
      "      [ID: 137] chunk: Token      Deletion Text      Infilling - Chunk 50\n",
      "        Content: 5\n",
      " \n",
      "   Large-scale      Pre-training      Experime...\n",
      "      [ID: 138] chunk: Token      Deletion Text      Infilling - Chunk 51\n",
      "        Content: To      test      how      well      BART      per...\n",
      "      [ID: 139] chunk: Token      Deletion Text      Infilling - Chunk 52\n",
      "        Content: 5.1      Experimental      Setup\n",
      "etup  \n",
      " \n",
      "  We    ...\n",
      "      [ID: 140] chunk: Token      Deletion Text      Infilling - Chunk 53\n",
      "        Content: Docu-  \n",
      " \n",
      "  ments      are      tokenized      wit...\n",
      "      [ID: 141] chunk: Token      Deletion Text      Infilling - Chunk 54\n",
      "        Content: Although      sen-  \n",
      " \n",
      "  tence      permutation   ...\n",
      "      [ID: 142] chunk: Token      Deletion Text      Infilling - Chunk 55\n",
      "        Content: BART      performs      comparably      to      RO...\n",
      "      [ID: 143] chunk: Token      Deletion Text      Infilling - Chunk 56\n",
      "        Content: CNN/DailyMail      XSum       RI      R2      RL  ...\n",
      "      [ID: 144] chunk: Token      Deletion Text      Infilling - Chunk 57\n",
      "        Content: BART      outperforms      previous      work     ...\n",
      "      [ID: 145] chunk: Token      Deletion Text      Infilling - Chunk 58\n",
      "        Content: To      help      th      del      better      fit...\n",
      "      [ID: 146] chunk: Token      Deletion Text      Infilling - Chunk 59\n",
      "        Content: (2019),      consisting      of      160Gb      of...\n",
      "      [ID: 147] chunk: Token      Deletion Text      Infilling - Chunk 60\n",
      "        Content: BART      20.72      11.85 5.2      Discriminative...\n",
      "      [ID: 148] chunk: Token      Deletion Text      Infilling - Chunk 61\n",
      "        Content: Table      4:      BART      outperforms      prev...\n",
      "      [ID: 149] chunk: Token      Deletion Text      Infilling - Chunk 62\n",
      "        Content: Overall,      BART      performs      simi-  \n",
      " \n",
      "  ...\n",
      "      [ID: 150] chunk: Token      Deletion Text      Infilling - Chunk 63\n",
      "        Content: Summarization      To      provide   \n",
      " \n",
      "a      com...\n",
      "      [ID: 151] chunk: Token      Deletion Text      Infilling - Chunk 64\n",
      "        Content: Extractive      models      do      well      here...\n",
      "      [ID: 152] chunk: Token      Deletion Text      Infilling - Chunk 65\n",
      "        Content: BART      is      fine-tuned      as      a      s...\n",
      "      [ID: 153] chunk: Token      Deletion Text      Infilling - Chunk 66\n",
      "        Content: During      generation,      we      set      beam...\n",
      "      [ID: 154] chunk: Token      Deletion Text      Infilling - Chunk 67\n",
      "        Content: | 6.0      points      on      all      ROUGE     ...\n",
      "      [ID: 155] chunk: Token      Deletion Text      Infilling - Chunk 68\n",
      "        Content: BART      outperforms      the  \n",
      " \n",
      "  best      pre...\n",
      "      [ID: 156] chunk: Token      Deletion Text      Infilling - Chunk 69\n",
      "        Content: BART  \n",
      " \n",
      "  outperforms      previous      work    ...\n",
      "    [ID: 157] section: 3.1      Sequence      Classification      Tasks\n",
      "      [ID: 158] chunk: 3.1      Sequence      Classification      Tasks - Chunk 1\n",
      "        Content: 3.1      Sequence      Classification      Tasks\n",
      "a...\n",
      "      [ID: 159] chunk: 3.1      Sequence      Classification      Tasks - Chunk 2\n",
      "        Content: This      approach      is      related      to   ...\n",
      "    [ID: 160] section: 3.2.      Token      Classification      Tasks\n",
      "      [ID: 161] chunk: 3.2.      Token      Classification      Tasks - Chunk 1\n",
      "        Content: 3.2. Token      Classification      Tasks\n",
      "asks  \n",
      " ...\n",
      "      [ID: 162] chunk: 3.2.      Token      Classification      Tasks - Chunk 2\n",
      "        Content: This      representation      is      used      to...\n",
      "    [ID: 163] section: 3.3      Sequence      Generation      Tasks\n",
      "      [ID: 164] chunk: 3.3      Sequence      Generation      Tasks - Chunk 1\n",
      "        Content: 3.3      Sequence      Generation      Tasks\n",
      "asks ...\n",
      "      [ID: 165] chunk: 3.3      Sequence      Generation      Tasks - Chunk 2\n",
      "        Content: In      both      of      these      tasks,      i...\n",
      "    [ID: 166] section: 3.4      Machine      Translation\n",
      "      [ID: 167] chunk: 3.4      Machine      Translation - Chunk 1\n",
      "        Content: 3.4      Machine      Translation\n",
      "tion  \n",
      " \n",
      "  We   ...\n",
      "      [ID: 168] chunk: 3.4      Machine      Translation - Chunk 2\n",
      "        Content: (2019)      has      shown      that      models  ...\n",
      "      [ID: 169] chunk: 3.4      Machine      Translation - Chunk 3\n",
      "        Content: We      show      that      it      is      possib...\n",
      "      [ID: 170] chunk: 3.4      Machine      Translation - Chunk 4\n",
      "        Content: More      precisely,      we      replace      BAR...\n",
      "      [ID: 171] chunk: 3.4      Machine      Translation - Chunk 5\n",
      "        Content: The      new      encoder      can      use   \n",
      " \n",
      "a...\n",
      "      [ID: 172] chunk: 3.4      Machine      Translation - Chunk 6\n",
      "        Content: In      the      first      step,      we      fre...\n",
      "      [ID: 173] chunk: 3.4      Machine      Translation - Chunk 7\n",
      "        Content: In      the      second      step,  \n",
      " \n",
      "  we      t...\n",
      "      [ID: 174] chunk: 3.4      Machine      Translation - Chunk 8\n",
      "        Content: We      compare   \n",
      " \n",
      "a       range      of      op...\n",
      "      [ID: 175] chunk: 3.4      Machine      Translation - Chunk 9\n",
      "        Content: 4.1      Comparison      Objectives\n",
      "ives  \n",
      " \n",
      "  Whi...\n",
      "      [ID: 176] chunk: 3.4      Machine      Translation - Chunk 10\n",
      "        Content: We  \n",
      " \n",
      "  ABCDE  \n",
      " \n",
      "  weee  \n",
      " \n",
      "  Pre-trained      P...\n",
      "      [ID: 177] chunk: 3.4      Machine      Translation - Chunk 11\n",
      "        Content: re-implement      strong      pre-training      ap...\n",
      "      [ID: 178] chunk: 3.4      Machine      Translation - Chunk 12\n",
      "        Content: However,      we      do  \n",
      " \n",
      "  make      minor    ...\n",
      "      [ID: 179] chunk: 3.4      Machine      Translation - Chunk 13\n",
      "        Content: For      refer-  \n",
      " \n",
      "  ence,      we      compare  ...\n",
      "      [ID: 180] chunk: 3.4      Machine      Translation - Chunk 14\n",
      "        Content: We      compare      the      following      appro...\n",
      "      [ID: 181] chunk: 3.4      Machine      Translation - Chunk 15\n",
      "        Content: Permuted      Language      Model      Based      ...\n",
      "      [ID: 182] chunk: 3.4      Machine      Translation - Chunk 16\n",
      "        Content: For      con-  \n",
      " \n",
      "  sistency      with      other ...\n",
      "      [ID: 183] chunk: 3.4      Machine      Translation - Chunk 17\n",
      "        Content: Masked      Language      Model      Following    ...\n",
      "      [ID: 184] chunk: 3.4      Machine      Translation - Chunk 18\n",
      "        Content: Multitask      Masked      Language      Model    ...\n",
      "      [ID: 185] chunk: 3.4      Machine      Translation - Chunk 19\n",
      "        Content: Self      at-  \n",
      " \n",
      "  tention      masks      are   ...\n",
      "      [ID: 186] chunk: 3.4      Machine      Translation - Chunk 20\n",
      "        Content: Masked      Seq-to-Seq      Inspired      by      ...\n",
      "      [ID: 187] chunk: 3.4      Machine      Translation - Chunk 21\n",
      "        Content: For      the      Permuted      LM,      Masked   ...\n",
      "      [ID: 188] chunk: 3.4      Machine      Translation - Chunk 22\n",
      "        Content: We      experiment      with      (1)      treatin...\n",
      "      [ID: 189] chunk: 3.4      Machine      Translation - Chunk 23\n",
      "        Content: We      find      the      former      works      ...\n",
      "      [ID: 190] chunk: 3.4      Machine      Translation - Chunk 24\n",
      "        Content: 4.2      Tasks\n",
      "asks  \n",
      " \n",
      "  SQuAD  \n",
      " \n",
      "  (Rajpurkar  ...\n",
      "      [ID: 191] chunk: 3.4      Machine      Translation - Chunk 25\n",
      "        Content: Similar      to      BERT      (Devlin      et    ...\n",
      "      [ID: 192] chunk: 3.4      Machine      Translation - Chunk 26\n",
      "        Content: MNLI      (Williams      et      al.,      2017), ...\n",
      "      [ID: 193] chunk: 3.4      Machine      Translation - Chunk 27\n",
      "        Content: In      contrast      to      BERT,  \n",
      " \n",
      "  the     ...\n",
      "      [ID: 194] chunk: 3.4      Machine      Translation - Chunk 28\n",
      "        Content: XSum_      (Narayan      et      al.,      2018), ...\n",
      "      [ID: 195] chunk: 3.4      Machine      Translation - Chunk 29\n",
      "        Content: Summaries      here      are      typically      c...\n",
      "      [ID: 196] chunk: 3.4      Machine      Translation - Chunk 30\n",
      "        Content: Several      trends      are      clear:\n",
      "Model    ...\n",
      "      [ID: 197] chunk: 3.4      Machine      Translation - Chunk 31\n",
      "        Content: Performance      of      pre-training      methods...\n",
      "      [ID: 198] chunk: 3.4      Machine      Translation - Chunk 32\n",
      "        Content: Token      masking      is      crucial      Pre-t...\n",
      "      [ID: 199] chunk: 3.4      Machine      Translation - Chunk 33\n",
      "        Content: Left-to-right      pre-training      improves     ...\n",
      "      [ID: 200] chunk: 3.4      Machine      Translation - Chunk 34\n",
      "        Content: Bidirectional      encoders      are      crucial ...\n",
      "      [ID: 201] chunk: 3.4      Machine      Translation - Chunk 35\n",
      "        Content: However,      BART      achieves      similar     ...\n",
      "      [ID: 202] chunk: 3.4      Machine      Translation - Chunk 36\n",
      "        Content: Some      of      this      dif-  \n",
      " \n",
      "  ference    ...\n",
      "      [ID: 203] chunk: 3.4      Machine      Translation - Chunk 37\n",
      "        Content: Pure      language      models      perform      b...\n",
      "      [ID: 204] chunk: 3.4      Machine      Translation - Chunk 38\n",
      "        Content: A      pure      lan-  \n",
      " \n",
      "  guage      model      ...\n",
      "      [ID: 205] chunk: 3.4      Machine      Translation - Chunk 39\n",
      "        Content: With      the      exception      of      ELI5,   ...\n",
      "      [ID: 206] chunk: 3.4      Machine      Translation - Chunk 40\n",
      "        Content: 5\n",
      " \n",
      "   Large-scale      Pre-training      Experime...\n",
      "      [ID: 207] chunk: 3.4      Machine      Translation - Chunk 41\n",
      "        Content: To      test      how      well      BART      per...\n",
      "      [ID: 208] chunk: 3.4      Machine      Translation - Chunk 42\n",
      "        Content: 5.1      Experimental      Setup\n",
      "etup  \n",
      " \n",
      "  We    ...\n",
      "      [ID: 209] chunk: 3.4      Machine      Translation - Chunk 43\n",
      "        Content: Docu-  \n",
      " \n",
      "  ments      are      tokenized      wit...\n",
      "      [ID: 210] chunk: 3.4      Machine      Translation - Chunk 44\n",
      "        Content: Although      sen-  \n",
      " \n",
      "  tence      permutation   ...\n",
      "      [ID: 211] chunk: 3.4      Machine      Translation - Chunk 45\n",
      "        Content: BART      performs      comparably      to      RO...\n",
      "    [ID: 212] section: 4.1      Comparison      Objectives\n",
      "      [ID: 213] chunk: 4.1      Comparison      Objectives - Chunk 1\n",
      "        Content: 4.1      Comparison      Objectives\n",
      "ives  \n",
      " \n",
      "  Whi...\n",
      "      [ID: 214] chunk: 4.1      Comparison      Objectives - Chunk 2\n",
      "        Content: We  \n",
      " \n",
      "  ABCDE  \n",
      " \n",
      "  weee  \n",
      " \n",
      "  Pre-trained      P...\n",
      "      [ID: 215] chunk: 4.1      Comparison      Objectives - Chunk 3\n",
      "        Content: re-implement      strong      pre-training      ap...\n",
      "      [ID: 216] chunk: 4.1      Comparison      Objectives - Chunk 4\n",
      "        Content: However,      we      do  \n",
      " \n",
      "  make      minor    ...\n",
      "      [ID: 217] chunk: 4.1      Comparison      Objectives - Chunk 5\n",
      "        Content: For      refer-  \n",
      " \n",
      "  ence,      we      compare  ...\n",
      "      [ID: 218] chunk: 4.1      Comparison      Objectives - Chunk 6\n",
      "        Content: We      compare      the      following      appro...\n",
      "      [ID: 219] chunk: 4.1      Comparison      Objectives - Chunk 7\n",
      "        Content: Permuted      Language      Model      Based      ...\n",
      "      [ID: 220] chunk: 4.1      Comparison      Objectives - Chunk 8\n",
      "        Content: For      con-  \n",
      " \n",
      "  sistency      with      other ...\n",
      "      [ID: 221] chunk: 4.1      Comparison      Objectives - Chunk 9\n",
      "        Content: Masked      Language      Model      Following    ...\n",
      "      [ID: 222] chunk: 4.1      Comparison      Objectives - Chunk 10\n",
      "        Content: Multitask      Masked      Language      Model    ...\n",
      "      [ID: 223] chunk: 4.1      Comparison      Objectives - Chunk 11\n",
      "        Content: Self      at-  \n",
      " \n",
      "  tention      masks      are   ...\n",
      "      [ID: 224] chunk: 4.1      Comparison      Objectives - Chunk 12\n",
      "        Content: Masked      Seq-to-Seq      Inspired      by      ...\n",
      "      [ID: 225] chunk: 4.1      Comparison      Objectives - Chunk 13\n",
      "        Content: For      the      Permuted      LM,      Masked   ...\n",
      "      [ID: 226] chunk: 4.1      Comparison      Objectives - Chunk 14\n",
      "        Content: We      experiment      with      (1)      treatin...\n",
      "      [ID: 227] chunk: 4.1      Comparison      Objectives - Chunk 15\n",
      "        Content: We      find      the      former      works      ...\n",
      "    [ID: 228] section: 4.2      Tasks\n",
      "      [ID: 229] chunk: 4.2      Tasks - Chunk 1\n",
      "        Content: 4.2      Tasks\n",
      "asks  \n",
      " \n",
      "  SQuAD  \n",
      " \n",
      "  (Rajpurkar  ...\n",
      "      [ID: 230] chunk: 4.2      Tasks - Chunk 2\n",
      "        Content: Similar      to      BERT      (Devlin      et    ...\n",
      "      [ID: 231] chunk: 4.2      Tasks - Chunk 3\n",
      "        Content: MNLI      (Williams      et      al.,      2017), ...\n",
      "      [ID: 232] chunk: 4.2      Tasks - Chunk 4\n",
      "        Content: In      contrast      to      BERT,  \n",
      " \n",
      "  the     ...\n",
      "      [ID: 233] chunk: 4.2      Tasks - Chunk 5\n",
      "        Content: XSum_      (Narayan      et      al.,      2018), ...\n",
      "      [ID: 234] chunk: 4.2      Tasks - Chunk 6\n",
      "        Content: Summaries      here      are      typically      c...\n",
      "    [ID: 235] section: 4.3      Results\n",
      "      [ID: 236] chunk: 4.3      Results - Chunk 1\n",
      "        Content: 4.3      Results\n",
      "ults Results      are      shown ...\n",
      "      [ID: 237] chunk: 4.3      Results - Chunk 2\n",
      "        Content: Several      trends      are      clear:\n",
      "Model    ...\n",
      "      [ID: 238] chunk: 4.3      Results - Chunk 3\n",
      "        Content: Performance      of      pre-training      methods...\n",
      "      [ID: 239] chunk: 4.3      Results - Chunk 4\n",
      "        Content: Token      masking      is      crucial      Pre-t...\n",
      "      [ID: 240] chunk: 4.3      Results - Chunk 5\n",
      "        Content: Left-to-right      pre-training      improves     ...\n",
      "      [ID: 241] chunk: 4.3      Results - Chunk 6\n",
      "        Content: Bidirectional      encoders      are      crucial ...\n",
      "      [ID: 242] chunk: 4.3      Results - Chunk 7\n",
      "        Content: However,      BART      achieves      similar     ...\n",
      "      [ID: 243] chunk: 4.3      Results - Chunk 8\n",
      "        Content: Some      of      this      dif-  \n",
      " \n",
      "  ference    ...\n",
      "      [ID: 244] chunk: 4.3      Results - Chunk 9\n",
      "        Content: Pure      language      models      perform      b...\n",
      "      [ID: 245] chunk: 4.3      Results - Chunk 10\n",
      "        Content: A      pure      lan-  \n",
      " \n",
      "  guage      model      ...\n",
      "      [ID: 246] chunk: 4.3      Results - Chunk 11\n",
      "        Content: With      the      exception      of      ELI5,   ...\n",
      "      [ID: 247] chunk: 4.3      Results - Chunk 12\n",
      "        Content: 5\n",
      " \n",
      "   Large-scale      Pre-training      Experime...\n",
      "      [ID: 248] chunk: 4.3      Results - Chunk 13\n",
      "        Content: To      test      how      well      BART      per...\n",
      "    [ID: 249] section: 5.1      Experimental      Setup\n",
      "      [ID: 250] chunk: 5.1      Experimental      Setup - Chunk 1\n",
      "        Content: 5.1      Experimental      Setup\n",
      "etup  \n",
      " \n",
      "  We    ...\n",
      "      [ID: 251] chunk: 5.1      Experimental      Setup - Chunk 2\n",
      "        Content: Docu-  \n",
      " \n",
      "  ments      are      tokenized      wit...\n",
      "      [ID: 252] chunk: 5.1      Experimental      Setup - Chunk 3\n",
      "        Content: Although      sen-  \n",
      " \n",
      "  tence      permutation   ...\n",
      "      [ID: 253] chunk: 5.1      Experimental      Setup - Chunk 4\n",
      "        Content: BART      performs      comparably      to      RO...\n",
      "    [ID: 254] section: CNN/DailyMail      XSum       RI      R2      RL      RI      R2      RL\n",
      "      [ID: 255] chunk: CNN/DailyMail      XSum       RI      R2      RL      RI      R2      RL - Chunk 1\n",
      "        Content: CNN/DailyMail      XSum       RI      R2      RL  ...\n",
      "      [ID: 256] chunk: CNN/DailyMail      XSum       RI      R2      RL      RI      R2      RL - Chunk 2\n",
      "        Content: BART      outperforms      previous      work     ...\n",
      "      [ID: 257] chunk: CNN/DailyMail      XSum       RI      R2      RL      RI      R2      RL - Chunk 3\n",
      "        Content: To      help      th      del      better      fit...\n",
      "      [ID: 258] chunk: CNN/DailyMail      XSum       RI      R2      RL      RI      R2      RL - Chunk 4\n",
      "        Content: (2019),      consisting      of      160Gb      of...\n",
      "      [ID: 259] chunk: CNN/DailyMail      XSum       RI      R2      RL      RI      R2      RL - Chunk 5\n",
      "        Content: BART      20.72      11.85 5.2      Discriminative...\n",
      "      [ID: 260] chunk: CNN/DailyMail      XSum       RI      R2      RL      RI      R2      RL - Chunk 6\n",
      "        Content: Table      4:      BART      outperforms      prev...\n",
      "      [ID: 261] chunk: CNN/DailyMail      XSum       RI      R2      RL      RI      R2      RL - Chunk 7\n",
      "        Content: Overall,      BART      performs      simi-  \n",
      " \n",
      "  ...\n",
      "      [ID: 262] chunk: CNN/DailyMail      XSum       RI      R2      RL      RI      R2      RL - Chunk 8\n",
      "        Content: Summarization      To      provide   \n",
      " \n",
      "a      com...\n",
      "      [ID: 263] chunk: CNN/DailyMail      XSum       RI      R2      RL      RI      R2      RL - Chunk 9\n",
      "        Content: Extractive      models      do      well      here...\n",
      "      [ID: 264] chunk: CNN/DailyMail      XSum       RI      R2      RL      RI      R2      RL - Chunk 10\n",
      "        Content: BART      is      fine-tuned      as      a      s...\n",
      "      [ID: 265] chunk: CNN/DailyMail      XSum       RI      R2      RL      RI      R2      RL - Chunk 11\n",
      "        Content: During      generation,      we      set      beam...\n",
      "      [ID: 266] chunk: CNN/DailyMail      XSum       RI      R2      RL      RI      R2      RL - Chunk 12\n",
      "        Content: | 6.0      points      on      all      ROUGE     ...\n",
      "      [ID: 267] chunk: CNN/DailyMail      XSum       RI      R2      RL      RI      R2      RL - Chunk 13\n",
      "        Content: BART      outperforms      the  \n",
      " \n",
      "  best      pre...\n",
      "      [ID: 268] chunk: CNN/DailyMail      XSum       RI      R2      RL      RI      R2      RL - Chunk 14\n",
      "        Content: BART  \n",
      " \n",
      "  outperforms      previous      work    ...\n",
      "    [ID: 269] section: 5.3.      Generation      Tasks\n",
      "      [ID: 270] chunk: 5.3.      Generation      Tasks - Chunk 1\n",
      "        Content: 5.3. Generation      Tasks\n",
      "Generation      Tasks\n",
      " ...\n",
      "      [ID: 271] chunk: 5.3.      Generation      Tasks - Chunk 2\n",
      "        Content: During      fine-       tuning      we      use   ...\n",
      "      [ID: 272] chunk: 5.3.      Generation      Tasks - Chunk 3\n",
      "        Content: During      generation,      we      set      beam...\n",
      "      [ID: 273] chunk: 5.3.      Generation      Tasks - Chunk 4\n",
      "        Content: | 6.0      points      on      all      ROUGE     ...\n",
      "      [ID: 274] chunk: 5.3.      Generation      Tasks - Chunk 5\n",
      "        Content: BART      outperforms      the  \n",
      " \n",
      "  best      pre...\n",
      "      [ID: 275] chunk: 5.3.      Generation      Tasks - Chunk 6\n",
      "        Content: BART  \n",
      " \n",
      "  outperforms      previous      work    ...\n",
      "    [ID: 276] section: Rl      R2      RL\n",
      "      [ID: 277] chunk: Rl      R2      RL - Chunk 1\n",
      "        Content: Rl      R2      RL\n",
      " \n",
      " \n",
      "  Best      Extractive     ...\n",
      "      [ID: 278] chunk: Rl      R2      RL - Chunk 2\n",
      "        Content: Comparison      models      are      from      Fan...\n",
      "    [ID: 279] section: RO-EN\n",
      "      [ID: 280] chunk: RO-EN - Chunk 1\n",
      "        Content: RO-EN\n",
      "Baseline      36.80 Fixed      BART      36....\n",
      "      [ID: 281] chunk: RO-EN - Chunk 2\n",
      "        Content: Abstractive      QA      We      use      the     ...\n",
      "      [ID: 282] chunk: RO-EN - Chunk 3\n",
      "        Content: We      find      BART      outperforms      the  ...\n",
      "      [ID: 283] chunk: RO-EN - Chunk 4\n",
      "        Content: 5.4      Translation\n",
      "tion  \n",
      " \n",
      "  We      also      ...\n",
      "      [ID: 284] chunk: RO-EN - Chunk 5\n",
      "        Content: We      use   \n",
      " \n",
      "a      6-layer  \n",
      " \n",
      "  transformer ...\n",
      "      [ID: 285] chunk: RO-EN - Chunk 6\n",
      "        Content: We  \n",
      " \n",
      "  compare      our      results      agains...\n",
      "      [ID: 286] chunk: RO-EN - Chunk 7\n",
      "        Content: For      each      row      we  \n",
      " \n",
      "  experiment   ...\n",
      "      [ID: 287] chunk: RO-EN - Chunk 8\n",
      "        Content: Preliminary      results      suggested      that ...\n",
      "      [ID: 288] chunk: RO-EN - Chunk 9\n",
      "        Content: 6\n",
      " \n",
      "   Qualitative      Analysis  \n",
      " \n",
      "  BART      s...\n",
      "      [ID: 289] chunk: RO-EN - Chunk 10\n",
      "        Content: Examples      are      taken      from      WikiNe...\n",
      "      [ID: 290] chunk: RO-EN - Chunk 11\n",
      "        Content: (2018),      we      remove      the      first   ...\n",
      "      [ID: 291] chunk: RO-EN - Chunk 12\n",
      "        Content: However,      model      output      is      also ...\n",
      "      [ID: 292] chunk: RO-EN - Chunk 13\n",
      "        Content: The  \n",
      " \n",
      "  output      is      also      generally ...\n",
      "      [ID: 293] chunk: RO-EN - Chunk 14\n",
      "        Content: In      the      first      example,      inferrin...\n",
      "      [ID: 294] chunk: RO-EN - Chunk 15\n",
      "        Content: These      samples      demonstrate      that     ...\n",
      "      [ID: 295] chunk: RO-EN - Chunk 16\n",
      "        Content: GPT      (Radford      et      al.,      2018)    ...\n",
      "      [ID: 296] chunk: RO-EN - Chunk 17\n",
      "        Content: (2019)  \n",
      " \n",
      "  demonstrated      that      very     ...\n",
      "      [ID: 297] chunk: RO-EN - Chunk 18\n",
      "        Content: An      input      sequence      where   \n",
      " \n",
      "a     ...\n",
      "      [ID: 298] chunk: RO-EN - Chunk 19\n",
      "        Content: BERT      (Devlin      et      al.,      2019)    ...\n",
      "      [ID: 299] chunk: RO-EN - Chunk 20\n",
      "        Content: Re-  \n",
      " \n",
      "  cent      work      has      shown      ...\n",
      "      [ID: 300] chunk: RO-EN - Chunk 21\n",
      "        Content: Predictions      are      not      made      auto-...\n",
      "      [ID: 301] chunk: RO-EN - Chunk 22\n",
      "        Content: Like      BART,      this      allows      UniLM  ...\n",
      "      [ID: 302] chunk: RO-EN - Chunk 23\n",
      "        Content: BART      re-  \n",
      " \n",
      "  duces      the      mismatch  ...\n",
      "      [ID: 303] chunk: RO-EN - Chunk 24\n",
      "        Content: XL-Net      (Yang      et      al.,      2019)    ...\n",
      "      [ID: 304] chunk: RO-EN - Chunk 25\n",
      "        Content: The      researchers      sug-       gested      t...\n",
      "      [ID: 305] chunk: RO-EN - Chunk 26\n",
      "        Content: |       Fisheries      off      the      coast    ...\n",
      "      [ID: 306] chunk: RO-EN - Chunk 27\n",
      "        Content: |       Sacoolas,      who      has      immunity ...\n",
      "      [ID: 307] chunk: RO-EN - Chunk 28\n",
      "        Content: He said, “I hope that Anne Sacoolas will come back...\n",
      "      [ID: 308] chunk: RO-EN - Chunk 29\n",
      "        Content: |       According      to      Syrian      state  ...\n",
      "      [ID: 309] chunk: RO-EN - Chunk 30\n",
      "        Content: Then      both      na-       tions      issued   ...\n",
      "      [ID: 310] chunk: RO-EN - Chunk 31\n",
      "        Content: |       Syrian      government      forces      ha...\n",
      "      [ID: 311] chunk: RO-EN - Chunk 32\n",
      "        Content: |       This      is      the      first      time...\n",
      "      [ID: 312] chunk: RO-EN - Chunk 33\n",
      "        Content: His      time      was      1      hour      59   ...\n",
      "      [ID: 313] chunk: RO-EN - Chunk 34\n",
      "        Content: |       PG&E      stated      it      scheduled   ...\n",
      "      [ID: 314] chunk: RO-EN - Chunk 35\n",
      "        Content: Nearly      800      thousand      customers      ...\n",
      "      [ID: 315] chunk: RO-EN - Chunk 36\n",
      "        Content: dicting      masked      tokens      auto-regressi...\n",
      "      [ID: 316] chunk: RO-EN - Chunk 37\n",
      "        Content: Several      papers      have      explored      u...\n",
      "      [ID: 317] chunk: RO-EN - Chunk 38\n",
      "        Content: The  \n",
      " \n",
      "  largest      improvements      have     ...\n",
      "      [ID: 318] chunk: RO-EN - Chunk 39\n",
      "        Content: Other      work      has  \n",
      " \n",
      "  shown      that    ...\n",
      "      [ID: 319] chunk: RO-EN - Chunk 40\n",
      "        Content: 8\n",
      " \n",
      "   Conclusions  \n",
      " \n",
      "  We      introduced      B...\n",
      "      [ID: 320] chunk: RO-EN - Chunk 41\n",
      "        Content: Fu-  \n",
      " \n",
      "  ture      work      should      explore ...\n",
      "      [ID: 321] chunk: RO-EN - Chunk 42\n",
      "        Content: Association      for      Computational      Lingu...\n",
      "      [ID: 322] chunk: RO-EN - Chunk 43\n",
      "        Content: 177—  \n",
      " \n",
      "  190. Springer,      2006. Jacob      De...\n",
      "      [ID: 323] chunk: RO-EN - Chunk 44\n",
      "        Content: In      Proceedings      of      the      2019    ...\n",
      "      [ID: 324] chunk: RO-EN - Chunk 45\n",
      "        Content: doi:      10.18653/  \n",
      " \n",
      "  vI/N19-1423. URL      ht...\n",
      "      [ID: 325] chunk: RO-EN - Chunk 46\n",
      "        Content: arXiv      preprint  \n",
      " \n",
      "  arXiv:      1902.00098, ...\n",
      "      [ID: 326] chunk: RO-EN - Chunk 47\n",
      "        Content: Li      Dong,      Nan      Yang,      Wenhui     ...\n",
      "      [ID: 327] chunk: RO-EN - Chunk 48\n",
      "        Content: Pre-trained      language      model      represen...\n",
      "      [ID: 328] chunk: RO-EN - Chunk 49\n",
      "        Content: Angela      Fan,      David      Grangier,      an...\n",
      "      [ID: 329] chunk: RO-EN - Chunk 50\n",
      "        Content: Dan      Hendrycks      and      Kevin      Gimpel...\n",
      "      [ID: 330] chunk: RO-EN - Chunk 51\n",
      "        Content: In      Advances      in      neural      infor-  ...\n",
      "      [ID: 331] chunk: RO-EN - Chunk 52\n",
      "        Content: Guillaume      Lample      and      Alexis      Co...\n",
      "      [ID: 332] chunk: RO-EN - Chunk 53\n",
      "        Content: arXiv      preprint  \n",
      " \n",
      "  arXiv:      1909.11942, ...\n",
      "      [ID: 333] chunk: RO-EN - Chunk 54\n",
      "        Content: arXiv:      1908.08345,      2019. Text      summa...\n",
      "      [ID: 334] chunk: RO-EN - Chunk 55\n",
      "        Content: Tomas      Mikolov,      Kai      Chen,      Greg ...\n",
      "      [ID: 335] chunk: RO-EN - Chunk 56\n",
      "        Content: topic-  \n",
      " \n",
      "  aware      convolutional      neural ...\n",
      "      [ID: 336] chunk: RO-EN - Chunk 57\n",
      "        Content: Matthew      E      Peters,      Mark      Neumann...\n",
      "      [ID: 337] chunk: RO-EN - Chunk 58\n",
      "        Content: Improving      language      un-  \n",
      " \n",
      "  derstanding...\n",
      "      [ID: 338] chunk: RO-EN - Chunk 59\n",
      "        Content: OpenAI  \n",
      " \n",
      "  Blog,      1(8),      2019. Pranav   ...\n",
      "      [ID: 339] chunk: RO-EN - Chunk 60\n",
      "        Content: arXiv      preprint  \n",
      " \n",
      "  arXiv:      1704.04368, ...\n",
      "      [ID: 340] chunk: RO-EN - Chunk 61\n",
      "        Content: Richard      Socher,      Alex      Perelygin,    ...\n",
      "      [ID: 341] chunk: RO-EN - Chunk 62\n",
      "        Content: Kaitao      Song,      Xu      Tan,      Tao      ...\n",
      "      [ID: 342] chunk: RO-EN - Chunk 63\n",
      "        Content: Ashish      Vaswani,      Noam      Shazeer,      ...\n",
      "      [ID: 343] chunk: RO-EN - Chunk 64\n",
      "        Content: Alex      Wang,      Amanpreet      Singh,      Ju...\n",
      "      [ID: 344] chunk: RO-EN - Chunk 65\n",
      "        Content: arXiv      preprint      1805.12471,      2018. Ad...\n",
      "      [ID: 345] chunk: RO-EN - Chunk 66\n",
      "        Content: A      broad-coverage      challenge      corpus  ...\n",
      "      [ID: 346] chunk: RO-EN - Chunk 67\n",
      "        Content: arXiv      preprint  \n",
      " \n",
      "  arXiv:      1906.08237, ...\n",
      "    [ID: 347] section: Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96\n",
      "      [ID: 348] chunk: Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 1\n",
      "        Content: Baseline      36.80 Fixed      BART      36.29 Tun...\n",
      "      [ID: 349] chunk: Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 2\n",
      "        Content: Abstractive      QA      We      use      the     ...\n",
      "      [ID: 350] chunk: Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 3\n",
      "        Content: We      find      BART      outperforms      the  ...\n",
      "      [ID: 351] chunk: Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 4\n",
      "        Content: 5.4      Translation\n",
      "tion  \n",
      " \n",
      "  We      also      ...\n",
      "      [ID: 352] chunk: Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 5\n",
      "        Content: We      use   \n",
      " \n",
      "a      6-layer  \n",
      " \n",
      "  transformer ...\n",
      "      [ID: 353] chunk: Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 6\n",
      "        Content: We  \n",
      " \n",
      "  compare      our      results      agains...\n",
      "      [ID: 354] chunk: Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 7\n",
      "        Content: For      each      row      we  \n",
      " \n",
      "  experiment   ...\n",
      "      [ID: 355] chunk: Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 8\n",
      "        Content: Preliminary      results      suggested      that ...\n",
      "      [ID: 356] chunk: Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 9\n",
      "        Content: 6\n",
      " \n",
      "   Qualitative      Analysis  \n",
      " \n",
      "  BART      s...\n",
      "      [ID: 357] chunk: Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 10\n",
      "        Content: Examples      are      taken      from      WikiNe...\n",
      "      [ID: 358] chunk: Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 11\n",
      "        Content: (2018),      we      remove      the      first   ...\n",
      "      [ID: 359] chunk: Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 12\n",
      "        Content: However,      model      output      is      also ...\n",
      "      [ID: 360] chunk: Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 13\n",
      "        Content: The  \n",
      " \n",
      "  output      is      also      generally ...\n",
      "      [ID: 361] chunk: Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 14\n",
      "        Content: In      the      first      example,      inferrin...\n",
      "      [ID: 362] chunk: Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 15\n",
      "        Content: These      samples      demonstrate      that     ...\n",
      "      [ID: 363] chunk: Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 16\n",
      "        Content: GPT      (Radford      et      al.,      2018)    ...\n",
      "      [ID: 364] chunk: Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 17\n",
      "        Content: (2019)  \n",
      " \n",
      "  demonstrated      that      very     ...\n",
      "      [ID: 365] chunk: Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 18\n",
      "        Content: An      input      sequence      where   \n",
      " \n",
      "a     ...\n",
      "      [ID: 366] chunk: Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 19\n",
      "        Content: BERT      (Devlin      et      al.,      2019)    ...\n",
      "      [ID: 367] chunk: Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 20\n",
      "        Content: Re-  \n",
      " \n",
      "  cent      work      has      shown      ...\n",
      "      [ID: 368] chunk: Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 21\n",
      "        Content: Predictions      are      not      made      auto-...\n",
      "      [ID: 369] chunk: Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 22\n",
      "        Content: Like      BART,      this      allows      UniLM  ...\n",
      "      [ID: 370] chunk: Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 23\n",
      "        Content: BART      re-  \n",
      " \n",
      "  duces      the      mismatch  ...\n",
      "      [ID: 371] chunk: Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 24\n",
      "        Content: XL-Net      (Yang      et      al.,      2019)    ...\n",
      "      [ID: 372] chunk: Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 25\n",
      "        Content: The      researchers      sug-       gested      t...\n",
      "      [ID: 373] chunk: Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 26\n",
      "        Content: |       Fisheries      off      the      coast    ...\n",
      "      [ID: 374] chunk: Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 27\n",
      "        Content: |       Sacoolas,      who      has      immunity ...\n",
      "      [ID: 375] chunk: Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 28\n",
      "        Content: He said, “I hope that Anne Sacoolas will come back...\n",
      "      [ID: 376] chunk: Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 29\n",
      "        Content: |       According      to      Syrian      state  ...\n",
      "      [ID: 377] chunk: Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 30\n",
      "        Content: Then      both      na-       tions      issued   ...\n",
      "      [ID: 378] chunk: Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 31\n",
      "        Content: |       Syrian      government      forces      ha...\n",
      "      [ID: 379] chunk: Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 32\n",
      "        Content: |       This      is      the      first      time...\n",
      "      [ID: 380] chunk: Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 33\n",
      "        Content: His      time      was      1      hour      59   ...\n",
      "      [ID: 381] chunk: Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 34\n",
      "        Content: |       PG&E      stated      it      scheduled   ...\n",
      "      [ID: 382] chunk: Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 35\n",
      "        Content: Nearly      800      thousand      customers      ...\n",
      "      [ID: 383] chunk: Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 36\n",
      "        Content: dicting      masked      tokens      auto-regressi...\n",
      "      [ID: 384] chunk: Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 37\n",
      "        Content: Several      papers      have      explored      u...\n",
      "      [ID: 385] chunk: Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 38\n",
      "        Content: The  \n",
      " \n",
      "  largest      improvements      have     ...\n",
      "      [ID: 386] chunk: Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 39\n",
      "        Content: Other      work      has  \n",
      " \n",
      "  shown      that    ...\n",
      "      [ID: 387] chunk: Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 40\n",
      "        Content: 8\n",
      " \n",
      "   Conclusions  \n",
      " \n",
      "  We      introduced      B...\n",
      "      [ID: 388] chunk: Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96 - Chunk 41\n",
      "        Content: Fu-  \n",
      " \n",
      "  ture      work      should      explore ...\n",
      "    [ID: 389] section: 5.4      Translation\n",
      "      [ID: 390] chunk: 5.4      Translation - Chunk 1\n",
      "        Content: 5.4      Translation\n",
      "tion  \n",
      " \n",
      "  We      also      ...\n",
      "      [ID: 391] chunk: 5.4      Translation - Chunk 2\n",
      "        Content: We      use   \n",
      " \n",
      "a      6-layer  \n",
      " \n",
      "  transformer ...\n",
      "      [ID: 392] chunk: 5.4      Translation - Chunk 3\n",
      "        Content: We  \n",
      " \n",
      "  compare      our      results      agains...\n",
      "      [ID: 393] chunk: 5.4      Translation - Chunk 4\n",
      "        Content: For      each      row      we  \n",
      " \n",
      "  experiment   ...\n",
      "      [ID: 394] chunk: 5.4      Translation - Chunk 5\n",
      "        Content: Preliminary      results      suggested      that ...\n",
      "      [ID: 395] chunk: 5.4      Translation - Chunk 6\n",
      "        Content: 6\n",
      " \n",
      "   Qualitative      Analysis  \n",
      " \n",
      "  BART      s...\n",
      "      [ID: 396] chunk: 5.4      Translation - Chunk 7\n",
      "        Content: Examples      are      taken      from      WikiNe...\n",
      "      [ID: 397] chunk: 5.4      Translation - Chunk 8\n",
      "        Content: (2018),      we      remove      the      first   ...\n",
      "      [ID: 398] chunk: 5.4      Translation - Chunk 9\n",
      "        Content: However,      model      output      is      also ...\n",
      "      [ID: 399] chunk: 5.4      Translation - Chunk 10\n",
      "        Content: The  \n",
      " \n",
      "  output      is      also      generally ...\n",
      "      [ID: 400] chunk: 5.4      Translation - Chunk 11\n",
      "        Content: In      the      first      example,      inferrin...\n",
      "      [ID: 401] chunk: 5.4      Translation - Chunk 12\n",
      "        Content: These      samples      demonstrate      that     ...\n",
      "      [ID: 402] chunk: 5.4      Translation - Chunk 13\n",
      "        Content: GPT      (Radford      et      al.,      2018)    ...\n",
      "      [ID: 403] chunk: 5.4      Translation - Chunk 14\n",
      "        Content: (2019)  \n",
      " \n",
      "  demonstrated      that      very     ...\n",
      "      [ID: 404] chunk: 5.4      Translation - Chunk 15\n",
      "        Content: An      input      sequence      where   \n",
      " \n",
      "a     ...\n",
      "      [ID: 405] chunk: 5.4      Translation - Chunk 16\n",
      "        Content: BERT      (Devlin      et      al.,      2019)    ...\n",
      "      [ID: 406] chunk: 5.4      Translation - Chunk 17\n",
      "        Content: Re-  \n",
      " \n",
      "  cent      work      has      shown      ...\n",
      "      [ID: 407] chunk: 5.4      Translation - Chunk 18\n",
      "        Content: Predictions      are      not      made      auto-...\n",
      "      [ID: 408] chunk: 5.4      Translation - Chunk 19\n",
      "        Content: Like      BART,      this      allows      UniLM  ...\n",
      "      [ID: 409] chunk: 5.4      Translation - Chunk 20\n",
      "        Content: BART      re-  \n",
      " \n",
      "  duces      the      mismatch  ...\n",
      "      [ID: 410] chunk: 5.4      Translation - Chunk 21\n",
      "        Content: XL-Net      (Yang      et      al.,      2019)    ...\n",
      "      [ID: 411] chunk: 5.4      Translation - Chunk 22\n",
      "        Content: The      researchers      sug-       gested      t...\n",
      "      [ID: 412] chunk: 5.4      Translation - Chunk 23\n",
      "        Content: |       Fisheries      off      the      coast    ...\n",
      "      [ID: 413] chunk: 5.4      Translation - Chunk 24\n",
      "        Content: |       Sacoolas,      who      has      immunity ...\n",
      "      [ID: 414] chunk: 5.4      Translation - Chunk 25\n",
      "        Content: He said, “I hope that Anne Sacoolas will come back...\n",
      "      [ID: 415] chunk: 5.4      Translation - Chunk 26\n",
      "        Content: |       According      to      Syrian      state  ...\n",
      "      [ID: 416] chunk: 5.4      Translation - Chunk 27\n",
      "        Content: Then      both      na-       tions      issued   ...\n",
      "      [ID: 417] chunk: 5.4      Translation - Chunk 28\n",
      "        Content: |       Syrian      government      forces      ha...\n",
      "      [ID: 418] chunk: 5.4      Translation - Chunk 29\n",
      "        Content: |       This      is      the      first      time...\n",
      "      [ID: 419] chunk: 5.4      Translation - Chunk 30\n",
      "        Content: His      time      was      1      hour      59   ...\n",
      "      [ID: 420] chunk: 5.4      Translation - Chunk 31\n",
      "        Content: |       PG&E      stated      it      scheduled   ...\n",
      "      [ID: 421] chunk: 5.4      Translation - Chunk 32\n",
      "        Content: Nearly      800      thousand      customers      ...\n",
      "      [ID: 422] chunk: 5.4      Translation - Chunk 33\n",
      "        Content: dicting      masked      tokens      auto-regressi...\n",
      "      [ID: 423] chunk: 5.4      Translation - Chunk 34\n",
      "        Content: Several      papers      have      explored      u...\n",
      "      [ID: 424] chunk: 5.4      Translation - Chunk 35\n",
      "        Content: The  \n",
      " \n",
      "  largest      improvements      have     ...\n",
      "      [ID: 425] chunk: 5.4      Translation - Chunk 36\n",
      "        Content: Other      work      has  \n",
      " \n",
      "  shown      that    ...\n",
      "      [ID: 426] chunk: 5.4      Translation - Chunk 37\n",
      "        Content: 8\n",
      " \n",
      "   Conclusions  \n",
      " \n",
      "  We      introduced      B...\n",
      "      [ID: 427] chunk: 5.4      Translation - Chunk 38\n",
      "        Content: Fu-  \n",
      " \n",
      "  ture      work      should      explore ...\n",
      "    [ID: 428] section: References\n",
      "      [ID: 429] chunk: References - Chunk 1\n",
      "        Content: References\n",
      " \n",
      " \n",
      "  Eneko      Agirre,      Llu’is   ...\n",
      "      [ID: 430] chunk: References - Chunk 2\n",
      "        Content: The      PASCAL      recognising      textual     ...\n",
      "      [ID: 431] chunk: References - Chunk 3\n",
      "        Content: BERT:      Pre-training      of      deep  \n",
      " \n",
      "  bi...\n",
      "      [ID: 432] chunk: References - Chunk 4\n",
      "        Content: 4171-  \n",
      " \n",
      "  4186,      Minneapolis,      Minnesota...\n",
      "      [ID: 433] chunk: References - Chunk 5\n",
      "        Content: The      second      conversational      in-  \n",
      " \n",
      " ...\n",
      "      [ID: 434] chunk: References - Chunk 6\n",
      "        Content: Li      Dong,      Nan      Yang,      Wenhui     ...\n",
      "      [ID: 435] chunk: References - Chunk 7\n",
      "        Content: Pre-trained      language      model      represen...\n",
      "      [ID: 436] chunk: References - Chunk 8\n",
      "        Content: Angela      Fan,      David      Grangier,      an...\n",
      "      [ID: 437] chunk: References - Chunk 9\n",
      "        Content: Dan      Hendrycks      and      Kevin      Gimpel...\n",
      "      [ID: 438] chunk: References - Chunk 10\n",
      "        Content: In      Advances      in      neural      infor-  ...\n",
      "      [ID: 439] chunk: References - Chunk 11\n",
      "        Content: Guillaume      Lample      and      Alexis      Co...\n",
      "      [ID: 440] chunk: References - Chunk 12\n",
      "        Content: arXiv      preprint  \n",
      " \n",
      "  arXiv:      1909.11942, ...\n",
      "      [ID: 441] chunk: References - Chunk 13\n",
      "        Content: arXiv:      1908.08345,      2019. Text      summa...\n",
      "      [ID: 442] chunk: References - Chunk 14\n",
      "        Content: Tomas      Mikolov,      Kai      Chen,      Greg ...\n",
      "      [ID: 443] chunk: References - Chunk 15\n",
      "        Content: topic-  \n",
      " \n",
      "  aware      convolutional      neural ...\n",
      "      [ID: 444] chunk: References - Chunk 16\n",
      "        Content: Matthew      E      Peters,      Mark      Neumann...\n",
      "      [ID: 445] chunk: References - Chunk 17\n",
      "        Content: Improving      language      un-  \n",
      " \n",
      "  derstanding...\n",
      "      [ID: 446] chunk: References - Chunk 18\n",
      "        Content: OpenAI  \n",
      " \n",
      "  Blog,      1(8),      2019. Pranav   ...\n",
      "      [ID: 447] chunk: References - Chunk 19\n",
      "        Content: arXiv      preprint  \n",
      " \n",
      "  arXiv:      1704.04368, ...\n",
      "      [ID: 448] chunk: References - Chunk 20\n",
      "        Content: Richard      Socher,      Alex      Perelygin,    ...\n",
      "      [ID: 449] chunk: References - Chunk 21\n",
      "        Content: Kaitao      Song,      Xu      Tan,      Tao      ...\n",
      "      [ID: 450] chunk: References - Chunk 22\n",
      "        Content: Ashish      Vaswani,      Noam      Shazeer,      ...\n",
      "      [ID: 451] chunk: References - Chunk 23\n",
      "        Content: Alex      Wang,      Amanpreet      Singh,      Ju...\n",
      "      [ID: 452] chunk: References - Chunk 24\n",
      "        Content: arXiv      preprint      1805.12471,      2018. Ad...\n",
      "      [ID: 453] chunk: References - Chunk 25\n",
      "        Content: A      broad-coverage      challenge      corpus  ...\n",
      "      [ID: 454] chunk: References - Chunk 26\n",
      "        Content: arXiv      preprint  \n",
      " \n",
      "  arXiv:      1906.08237, ...\n",
      "  [ID: 455] book: Book 2\n",
      "    [ID: 456] section: Abstract\n",
      "      [ID: 457] chunk: Abstract - Chunk 1\n",
      "        Content: Abstract\n",
      " \n",
      " \n",
      "  We      study      how      to     ...\n",
      "      [ID: 458] chunk: Abstract - Chunk 2\n",
      "        Content: This      underex-  \n",
      " \n",
      "  plored      problem      ...\n",
      "      [ID: 459] chunk: Abstract - Chunk 3\n",
      "        Content: STORM      models      the      pre-writing      s...\n",
      "      [ID: 460] chunk: Abstract - Chunk 4\n",
      "        Content: For      evaluation,      we      curate      Fres...\n",
      "      [ID: 461] chunk: Abstract - Chunk 5\n",
      "        Content: Com-  \n",
      " \n",
      "  pared      to      articles      genera...\n",
      "      [ID: 462] chunk: Abstract - Chunk 6\n",
      "        Content: The      expert      feedback      also  \n",
      " \n",
      "  help...\n",
      "      [ID: 463] chunk: Abstract - Chunk 7\n",
      "        Content: 1\n",
      " \n",
      "   Introduction  \n",
      " \n",
      "  Large      language     ...\n",
      "      [ID: 464] chunk: Abstract - Chunk 8\n",
      "        Content: Such      expository      writing,      which  \n",
      " \n",
      "...\n",
      "      [ID: 465] chunk: Abstract - Chunk 9\n",
      "        Content: Automating      this      process      can      fa...\n",
      "      [ID: 466] chunk: Abstract - Chunk 10\n",
      "        Content: |  |       ee      Prewriting | \n",
      " | --- | --- | --...\n",
      "      [ID: 467] chunk: Abstract - Chunk 11\n",
      "        Content: When      was      the      opening      ceremony ...\n",
      "      [ID: 468] chunk: Abstract - Chunk 12\n",
      "        Content: Can      you      provide      any      informatio...\n",
      "      [ID: 469] chunk: Abstract - Chunk 13\n",
      "        Content: (C)      Conversational      Question      Asking ...\n",
      "      [ID: 470] chunk: Abstract - Chunk 14\n",
      "        Content: These LLM- included Athletes from over 90 countrie...\n",
      "      [ID: 471] chunk: Abstract - Chunk 15\n",
      "        Content: LLM-  \n",
      " \n",
      "  Role1  \n",
      " \n",
      "  Figure      1:      We     ...\n",
      "      [ID: 472] chunk: Abstract - Chunk 16\n",
      "        Content: In  \n",
      " \n",
      "  contrast,      STORM      researches     ...\n",
      "      [ID: 473] chunk: Abstract - Chunk 17\n",
      "        Content: However,      prior      work      on      generat...\n",
      "      [ID: 474] chunk: Abstract - Chunk 18\n",
      "        Content: (2018)      presume      reference  \n",
      " \n",
      "  documents...\n",
      "      [ID: 475] chunk: Abstract - Chunk 19\n",
      "        Content: These  \n",
      " \n",
      "  assumptions      do      not      hold...\n",
      "      [ID: 476] chunk: Abstract - Chunk 20\n",
      "        Content: We      decompose      this      problem      into...\n",
      "      [ID: 477] chunk: Abstract - Chunk 21\n",
      "        Content: Such   \n",
      " \n",
      "a      task      decomposition      mirr...\n",
      "      [ID: 478] chunk: Abstract - Chunk 22\n",
      "        Content: As      pre-trained      language      models     ...\n",
      "      [ID: 479] chunk: Abstract - Chunk 23\n",
      "        Content: How-  \n",
      " \n",
      "  ever,      this      approach      is  ...\n",
      "      [ID: 480] chunk: Abstract - Chunk 24\n",
      "        Content: This      underscores      the      importance    ...\n",
      "      [ID: 481] chunk: Abstract - Chunk 25\n",
      "        Content: Human      learning      theories      (Tawfik    ...\n",
      "      [ID: 482] chunk: Abstract - Chunk 26\n",
      "        Content: Although  \n",
      " \n",
      "  instruction-tuned      models      ...\n",
      "      [ID: 483] chunk: Abstract - Chunk 27\n",
      "        Content: To  \n",
      " \n",
      "  endow      LLMs      with      the      c...\n",
      "      [ID: 484] chunk: Abstract - Chunk 28\n",
      "        Content: The      design      of      STORM      is      ba...\n",
      "      [ID: 485] chunk: Abstract - Chunk 29\n",
      "        Content: It  \n",
      " \n",
      "  first      discovers      diverse      pe...\n",
      "      [ID: 486] chunk: Abstract - Chunk 30\n",
      "        Content: Next,  \n",
      " \n",
      "  to      elicit      follow-up      que...\n",
      "      [ID: 487] chunk: Abstract - Chunk 31\n",
      "        Content: Finally,      based  \n",
      " \n",
      "  on      the      LLM’s  ...\n",
      "      [ID: 488] chunk: Abstract - Chunk 32\n",
      "        Content: We      evaluate      STORM      using      our   ...\n",
      "      [ID: 489] chunk: Abstract - Chunk 33\n",
      "        Content: We      further      invited   \n",
      " \n",
      "a      group    ...\n",
      "      [ID: 490] chunk: Abstract - Chunk 34\n",
      "        Content: They      also      identified  \n",
      " \n",
      "  challenges   ...\n",
      "      [ID: 491] chunk: Abstract - Chunk 35\n",
      "        Content: Our      main      contributions      include:\n",
      "*\n",
      " ...\n",
      "      [ID: 492] chunk: Abstract - Chunk 36\n",
      "        Content: ¢      We      propose      STORM,   \n",
      " \n",
      "a      nov...\n",
      "      [ID: 493] chunk: Abstract - Chunk 37\n",
      "        Content: ¢      Both      automatic      and      human    ...\n",
      "      [ID: 494] chunk: Abstract - Chunk 38\n",
      "        Content: 2\n",
      " \n",
      "   FreshWiki  \n",
      " \n",
      "  We      study      generati...\n",
      "      [ID: 495] chunk: Abstract - Chunk 39\n",
      "        Content: This      models      the      human ‘Our      res...\n",
      "    [ID: 496] section: Domain      Scope      Given      Given       P      Outline?      Refs?\n",
      "      [ID: 497] chunk: Domain      Scope      Given      Given       P      Outline?      Refs? - Chunk 1\n",
      "        Content: Domain      Scope      Given      Given       P   ...\n",
      "      [ID: 498] chunk: Domain      Scope      Given      Given       P      Outline?      Refs? - Chunk 2\n",
      "        Content: /      Yes  \n",
      " \n",
      "  Sauper      and      Barzilay    ...\n",
      "      [ID: 499] chunk: Domain      Scope      Given      Given       P      Outline?      Refs? - Chunk 3\n",
      "        Content: writing      approach      which      has      pro...\n",
      "      [ID: 500] chunk: Domain      Scope      Given      Given       P      Outline?      Refs? - Chunk 4\n",
      "        Content: Existing      work  \n",
      " \n",
      "  has      generally      f...\n",
      "      [ID: 501] chunk: Domain      Scope      Given      Given       P      Outline?      Refs? - Chunk 5\n",
      "        Content: A      notable      example      is      WikiSum  ...\n",
      "      [ID: 502] chunk: Domain      Scope      Given      Given       P      Outline?      Refs? - Chunk 6\n",
      "        Content: Specifically,      given   \n",
      " \n",
      "a      topic      ¢,...\n",
      "      [ID: 503] chunk: Domain      Scope      Given      Given       P      Outline?      Refs? - Chunk 7\n",
      "        Content: As  \n",
      " \n",
      "  modern      LLMs      are      generally ...\n",
      "      [ID: 504] chunk: Domain      Scope      Given      Given       P      Outline?      Refs? - Chunk 8\n",
      "        Content: Our      process      can      be      repeated   ...\n",
      "      [ID: 505] chunk: Domain      Scope      Given      Given       P      Outline?      Refs? - Chunk 9\n",
      "        Content: To      ensure      high-quality      references, ...\n",
      "      [ID: 506] chunk: Domain      Scope      Given      Given       P      Outline?      Refs? - Chunk 10\n",
      "        Content: 3      Obtained      from      https:      //wikim...\n",
      "      [ID: 507] chunk: Domain      Scope      Given      Given       P      Outline?      Refs? - Chunk 11\n",
      "        Content: While      high-quality      Wikipedia      articl...\n",
      "      [ID: 508] chunk: Domain      Scope      Given      Given       P      Outline?      Refs? - Chunk 12\n",
      "        Content: 2.2      Outline      Creation      and      Evalu...\n",
      "      [ID: 509] chunk: Domain      Scope      Given      Given       P      Outline?      Refs? - Chunk 13\n",
      "        Content: When  \n",
      " \n",
      "  human      educators      teach      st...\n",
      "      [ID: 510] chunk: Domain      Scope      Given      Given       P      Outline?      Refs? - Chunk 14\n",
      "        Content: Inspired      by      this,      we      decompose...\n",
      "      [ID: 511] chunk: Domain      Scope      Given      Given       P      Outline?      Refs? - Chunk 15\n",
      "        Content: In      the      writing      stage,      the     ...\n",
      "      [ID: 512] chunk: Domain      Scope      Given      Given       P      Outline?      Refs? - Chunk 16\n",
      "        Content: These      metrics      compare      the      mult...\n",
      "      [ID: 513] chunk: Domain      Scope      Given      Given       P      Outline?      Refs? - Chunk 17\n",
      "        Content: Recog-  \n",
      " \n",
      "  nizing      that      an      exact  ...\n",
      "      [ID: 514] chunk: Domain      Scope      Given      Given       P      Outline?      Refs? - Chunk 18\n",
      "        Content: We      also      compute      the      heading   ...\n",
      "      [ID: 515] chunk: Domain      Scope      Given      Given       P      Outline?      Refs? - Chunk 19\n",
      "        Content: 3\n",
      " \n",
      "   Method  \n",
      " \n",
      "  We      present      STORM    ...\n",
      "      [ID: 516] chunk: Domain      Scope      Given      Given       P      Outline?      Refs? - Chunk 20\n",
      "        Content: The      outline      will      be      extended  ...\n",
      "    [ID: 517] section: 2.1      The      FreshWiki      Dataset\n",
      "      [ID: 518] chunk: 2.1      The      FreshWiki      Dataset - Chunk 1\n",
      "        Content: 2.1      The      FreshWiki      Dataset\n",
      "aset  \n",
      " \n",
      "...\n",
      "      [ID: 519] chunk: 2.1      The      FreshWiki      Dataset - Chunk 2\n",
      "        Content: As  \n",
      " \n",
      "  modern      LLMs      are      generally ...\n",
      "      [ID: 520] chunk: 2.1      The      FreshWiki      Dataset - Chunk 3\n",
      "        Content: Our      process      can      be      repeated   ...\n",
      "      [ID: 521] chunk: 2.1      The      FreshWiki      Dataset - Chunk 4\n",
      "        Content: To      ensure      high-quality      references, ...\n",
      "      [ID: 522] chunk: 2.1      The      FreshWiki      Dataset - Chunk 5\n",
      "        Content: 3      Obtained      from      https:      //wikim...\n",
      "      [ID: 523] chunk: 2.1      The      FreshWiki      Dataset - Chunk 6\n",
      "        Content: While      high-quality      Wikipedia      articl...\n",
      "    [ID: 524] section: 2.2      Outline      Creation      and      Evaluation\n",
      "      [ID: 525] chunk: 2.2      Outline      Creation      and      Evaluation - Chunk 1\n",
      "        Content: 2.2      Outline      Creation      and      Evalu...\n",
      "      [ID: 526] chunk: 2.2      Outline      Creation      and      Evaluation - Chunk 2\n",
      "        Content: When  \n",
      " \n",
      "  human      educators      teach      st...\n",
      "      [ID: 527] chunk: 2.2      Outline      Creation      and      Evaluation - Chunk 3\n",
      "        Content: Inspired      by      this,      we      decompose...\n",
      "      [ID: 528] chunk: 2.2      Outline      Creation      and      Evaluation - Chunk 4\n",
      "        Content: In      the      writing      stage,      the     ...\n",
      "      [ID: 529] chunk: 2.2      Outline      Creation      and      Evaluation - Chunk 5\n",
      "        Content: These      metrics      compare      the      mult...\n",
      "      [ID: 530] chunk: 2.2      Outline      Creation      and      Evaluation - Chunk 6\n",
      "        Content: Recog-  \n",
      " \n",
      "  nizing      that      an      exact  ...\n",
      "      [ID: 531] chunk: 2.2      Outline      Creation      and      Evaluation - Chunk 7\n",
      "        Content: We      also      compute      the      heading   ...\n",
      "      [ID: 532] chunk: 2.2      Outline      Creation      and      Evaluation - Chunk 8\n",
      "        Content: 3\n",
      " \n",
      "   Method  \n",
      " \n",
      "  We      present      STORM    ...\n",
      "      [ID: 533] chunk: 2.2      Outline      Creation      and      Evaluation - Chunk 9\n",
      "        Content: The      outline      will      be      extended  ...\n",
      "    [ID: 534] section: Topic      t       @      Identify       Perspectives       @      Survey       —      Writer       =o       —       Related      Articles      |\n",
      "      [ID: 535] chunk: Topic      t       @      Identify       Perspectives       @      Survey       —      Writer       =o       —       Related      Articles      | - Chunk 1\n",
      "        Content: Topic      t       @      Identify       Perspecti...\n",
      "    [ID: 536] section: References      R\n",
      "      [ID: 537] chunk: References      R - Chunk 1\n",
      "        Content: References      R\n",
      "(§3.4). Figure   \n",
      " \n",
      "2      gives...\n",
      "      [ID: 538] chunk: References      R - Chunk 2\n",
      "        Content: In      parallel  \n",
      " \n",
      "  with      stakeholder      ...\n",
      "      [ID: 539] chunk: References      R - Chunk 3\n",
      "        Content: Further,      the      specific      perspec-  \n",
      " \n",
      "...\n",
      "      [ID: 540] chunk: References      R - Chunk 4\n",
      "        Content: For      example,  \n",
      " \n",
      "  an      event      planner...\n",
      "      [ID: 541] chunk: References      R - Chunk 5\n",
      "        Content: Given      the      input      topic      t,      ...\n",
      "      [ID: 542] chunk: References      R - Chunk 6\n",
      "        Content: Specifically,      STORM  \n",
      " \n",
      "  prompts      an    ...\n",
      "      [ID: 543] chunk: References      R - Chunk 7\n",
      "        Content: These      tables      of      contents      are  ...\n",
      "      [ID: 544] chunk: References      R - Chunk 8\n",
      "        Content: Finally,      the      LLM      synthe-  \n",
      " \n",
      "  Thtt...\n",
      "      [ID: 545] chunk: References      R - Chunk 9\n",
      "        Content: To      ensure      that      the      basic  \n",
      " \n",
      " ...\n",
      "      [ID: 546] chunk: References      R - Chunk 10\n",
      "        Content: Each      perspec-  \n",
      " \n",
      "  tive   \n",
      " \n",
      "p   \n",
      " \n",
      "€   \n",
      " \n",
      "P...\n",
      "      [ID: 547] chunk: References      R - Chunk 11\n",
      "        Content: 3.2      Simulating      Conversations\n",
      "ions  \n",
      " \n",
      "  ...\n",
      "      [ID: 548] chunk: References      R - Chunk 12\n",
      "        Content: To      kick      off      this  \n",
      " \n",
      "  dynamic     ...\n",
      "      [ID: 549] chunk: References      R - Chunk 13\n",
      "        Content: In      the      z-th      round      of      the ...\n",
      "      [ID: 550] chunk: References      R - Chunk 14\n",
      "        Content: The      conversation      history  \n",
      " \n",
      "  enables  ...\n",
      "      [ID: 551] chunk: References      R - Chunk 15\n",
      "        Content: To      ensure      that      the      conversatio...\n",
      "      [ID: 552] chunk: References      R - Chunk 16\n",
      "        Content: q....\n",
      "      [ID: 553] chunk: References      R - Chunk 17\n",
      "        Content: Since      q;      can      be      complicated,  ...\n",
      "      [ID: 554] chunk: References      R - Chunk 18\n",
      "        Content: To      fully      leverage      the      inter-  ...\n",
      "      [ID: 555] chunk: References      R - Chunk 19\n",
      "        Content: Subsequently,      the      LLM  \n",
      " \n",
      "  is      prom...\n",
      "      [ID: 556] chunk: References      R - Chunk 20\n",
      "        Content: 3.4      Writing      the      Full-Length      Ar...\n",
      "      [ID: 557] chunk: References      R - Chunk 21\n",
      "        Content: Since      it      is      usually      impossible...\n",
      "      [ID: 558] chunk: References      R - Chunk 22\n",
      "        Content: With      the      relevant      in-  \n",
      " \n",
      "  formati...\n",
      "      [ID: 559] chunk: References      R - Chunk 23\n",
      "        Content: Since      the      sections      are      gen-  \n",
      "...\n",
      "      [ID: 560] chunk: References      R - Chunk 24\n",
      "        Content: Furthermore,      in      alignment  \n",
      " \n",
      "  with    ...\n",
      "      [ID: 561] chunk: References      R - Chunk 25\n",
      "        Content: 4\n",
      " \n",
      "   Experiments\n",
      "4.1      Article      Selection...\n",
      "      [ID: 562] chunk: References      R - Chunk 26\n",
      "        Content: For   \n",
      " \n",
      "a      meaningful      comparison,      w...\n",
      "      [ID: 563] chunk: References      R - Chunk 27\n",
      "        Content: 4.2      Automatic      Metrics\n",
      "rics  \n",
      " \n",
      "  As     ...\n",
      "      [ID: 564] chunk: References      R - Chunk 28\n",
      "        Content: To      assess      the      full-length      arti...\n",
      "      [ID: 565] chunk: References      R - Chunk 29\n",
      "        Content: Moreover,      based      on      Wikipedia      c...\n",
      "      [ID: 566] chunk: References      R - Chunk 30\n",
      "        Content: For      aspects      (1)-(4),      we      use   ...\n",
      "      [ID: 567] chunk: References      R - Chunk 31\n",
      "        Content: For      verifiability,      we      calculate    ...\n",
      "      [ID: 568] chunk: References      R - Chunk 32\n",
      "        Content: 4.3      Baselines\n",
      "ines  \n",
      " \n",
      "  As      prior      w...\n",
      "      [ID: 569] chunk: References      R - Chunk 33\n",
      "        Content: Direct      Gen,   \n",
      " \n",
      "a      baseline      that   ...\n",
      "      [ID: 570] chunk: References      R - Chunk 34\n",
      "        Content: RAG,   \n",
      " \n",
      "a      retrieval-augmented      generati...\n",
      "      [ID: 571] chunk: References      R - Chunk 35\n",
      "        Content: Outline-driven      RAG      (ORAG),      which   ...\n",
      "      [ID: 572] chunk: References      R - Chunk 36\n",
      "        Content: 4.4      STORM      Implementation\n",
      "tion  \n",
      " \n",
      "  We  ...\n",
      "    [ID: 573] section: 3.1      Perspective-Guided      Question      Asking\n",
      "      [ID: 574] chunk: 3.1      Perspective-Guided      Question      Asking - Chunk 1\n",
      "        Content: 3.1      Perspective-Guided      Question      Ask...\n",
      "      [ID: 575] chunk: 3.1      Perspective-Guided      Question      Asking - Chunk 2\n",
      "        Content: In      parallel  \n",
      " \n",
      "  with      stakeholder      ...\n",
      "      [ID: 576] chunk: 3.1      Perspective-Guided      Question      Asking - Chunk 3\n",
      "        Content: Further,      the      specific      perspec-  \n",
      " \n",
      "...\n",
      "      [ID: 577] chunk: 3.1      Perspective-Guided      Question      Asking - Chunk 4\n",
      "        Content: For      example,  \n",
      " \n",
      "  an      event      planner...\n",
      "      [ID: 578] chunk: 3.1      Perspective-Guided      Question      Asking - Chunk 5\n",
      "        Content: Given      the      input      topic      t,      ...\n",
      "      [ID: 579] chunk: 3.1      Perspective-Guided      Question      Asking - Chunk 6\n",
      "        Content: Specifically,      STORM  \n",
      " \n",
      "  prompts      an    ...\n",
      "      [ID: 580] chunk: 3.1      Perspective-Guided      Question      Asking - Chunk 7\n",
      "        Content: These      tables      of      contents      are  ...\n",
      "      [ID: 581] chunk: 3.1      Perspective-Guided      Question      Asking - Chunk 8\n",
      "        Content: Finally,      the      LLM      synthe-  \n",
      " \n",
      "  Thtt...\n",
      "      [ID: 582] chunk: 3.1      Perspective-Guided      Question      Asking - Chunk 9\n",
      "        Content: To      ensure      that      the      basic  \n",
      " \n",
      " ...\n",
      "      [ID: 583] chunk: 3.1      Perspective-Guided      Question      Asking - Chunk 10\n",
      "        Content: Each      perspec-  \n",
      " \n",
      "  tive   \n",
      " \n",
      "p   \n",
      " \n",
      "€   \n",
      " \n",
      "P...\n",
      "    [ID: 584] section: 3.2      Simulating      Conversations\n",
      "      [ID: 585] chunk: 3.2      Simulating      Conversations - Chunk 1\n",
      "        Content: 3.2      Simulating      Conversations\n",
      "ions  \n",
      " \n",
      "  ...\n",
      "      [ID: 586] chunk: 3.2      Simulating      Conversations - Chunk 2\n",
      "        Content: To      kick      off      this  \n",
      " \n",
      "  dynamic     ...\n",
      "      [ID: 587] chunk: 3.2      Simulating      Conversations - Chunk 3\n",
      "        Content: In      the      z-th      round      of      the ...\n",
      "      [ID: 588] chunk: 3.2      Simulating      Conversations - Chunk 4\n",
      "        Content: The      conversation      history  \n",
      " \n",
      "  enables  ...\n",
      "      [ID: 589] chunk: 3.2      Simulating      Conversations - Chunk 5\n",
      "        Content: To      ensure      that      the      conversatio...\n",
      "      [ID: 590] chunk: 3.2      Simulating      Conversations - Chunk 6\n",
      "        Content: q. Since      q;      can      be      complicated...\n",
      "    [ID: 591] section: 3.3      Creating      the      Article      Outline\n",
      "      [ID: 592] chunk: 3.3      Creating      the      Article      Outline - Chunk 1\n",
      "        Content: 3.3      Creating      the      Article      Outli...\n",
      "      [ID: 593] chunk: 3.3      Creating      the      Article      Outline - Chunk 2\n",
      "        Content: To      fully      leverage      the      inter-  ...\n",
      "      [ID: 594] chunk: 3.3      Creating      the      Article      Outline - Chunk 3\n",
      "        Content: Subsequently,      the      LLM  \n",
      " \n",
      "  is      prom...\n",
      "    [ID: 595] section: 3.4      Writing      the      Full-Length      Article\n",
      "      [ID: 596] chunk: 3.4      Writing      the      Full-Length      Article - Chunk 1\n",
      "        Content: 3.4      Writing      the      Full-Length      Ar...\n",
      "      [ID: 597] chunk: 3.4      Writing      the      Full-Length      Article - Chunk 2\n",
      "        Content: Since      it      is      usually      impossible...\n",
      "      [ID: 598] chunk: 3.4      Writing      the      Full-Length      Article - Chunk 3\n",
      "        Content: With      the      relevant      in-  \n",
      " \n",
      "  formati...\n",
      "      [ID: 599] chunk: 3.4      Writing      the      Full-Length      Article - Chunk 4\n",
      "        Content: Since      the      sections      are      gen-  \n",
      "...\n",
      "      [ID: 600] chunk: 3.4      Writing      the      Full-Length      Article - Chunk 5\n",
      "        Content: Furthermore,      in      alignment  \n",
      " \n",
      "  with    ...\n",
      "      [ID: 601] chunk: 3.4      Writing      the      Full-Length      Article - Chunk 6\n",
      "        Content: 4\n",
      " \n",
      "   Experiments\n",
      "4.1      Article      Selection...\n",
      "      [ID: 602] chunk: 3.4      Writing      the      Full-Length      Article - Chunk 7\n",
      "        Content: For   \n",
      " \n",
      "a      meaningful      comparison,      w...\n",
      "      [ID: 603] chunk: 3.4      Writing      the      Full-Length      Article - Chunk 8\n",
      "        Content: 4.2      Automatic      Metrics\n",
      "rics  \n",
      " \n",
      "  As     ...\n",
      "      [ID: 604] chunk: 3.4      Writing      the      Full-Length      Article - Chunk 9\n",
      "        Content: To      assess      the      full-length      arti...\n",
      "      [ID: 605] chunk: 3.4      Writing      the      Full-Length      Article - Chunk 10\n",
      "        Content: Moreover,      based      on      Wikipedia      c...\n",
      "      [ID: 606] chunk: 3.4      Writing      the      Full-Length      Article - Chunk 11\n",
      "        Content: For      aspects      (1)-(4),      we      use   ...\n",
      "      [ID: 607] chunk: 3.4      Writing      the      Full-Length      Article - Chunk 12\n",
      "        Content: For      verifiability,      we      calculate    ...\n",
      "      [ID: 608] chunk: 3.4      Writing      the      Full-Length      Article - Chunk 13\n",
      "        Content: 4.3      Baselines\n",
      "ines  \n",
      " \n",
      "  As      prior      w...\n",
      "      [ID: 609] chunk: 3.4      Writing      the      Full-Length      Article - Chunk 14\n",
      "        Content: Direct      Gen,   \n",
      " \n",
      "a      baseline      that   ...\n",
      "      [ID: 610] chunk: 3.4      Writing      the      Full-Length      Article - Chunk 15\n",
      "        Content: RAG,   \n",
      " \n",
      "a      retrieval-augmented      generati...\n",
      "      [ID: 611] chunk: 3.4      Writing      the      Full-Length      Article - Chunk 16\n",
      "        Content: Outline-driven      RAG      (ORAG),      which   ...\n",
      "      [ID: 612] chunk: 3.4      Writing      the      Full-Length      Article - Chunk 17\n",
      "        Content: 4.4      STORM      Implementation\n",
      "tion  \n",
      " \n",
      "  We  ...\n",
      "    [ID: 613] section: 4.1      Article      Selection\n",
      "      [ID: 614] chunk: 4.1      Article      Selection - Chunk 1\n",
      "        Content: 4.1      Article      Selection\n",
      "tion  \n",
      " \n",
      "  STORM  ...\n",
      "      [ID: 615] chunk: 4.1      Article      Selection - Chunk 2\n",
      "        Content: org/wiki/Wikipedia:  \n",
      " \n",
      "  Reliable_sources  \n",
      " \n",
      "  r...\n",
      "    [ID: 616] section: 4.2      Automatic      Metrics\n",
      "      [ID: 617] chunk: 4.2      Automatic      Metrics - Chunk 1\n",
      "        Content: 4.2      Automatic      Metrics\n",
      "rics  \n",
      " \n",
      "  As     ...\n",
      "      [ID: 618] chunk: 4.2      Automatic      Metrics - Chunk 2\n",
      "        Content: To      assess      the      full-length      arti...\n",
      "      [ID: 619] chunk: 4.2      Automatic      Metrics - Chunk 3\n",
      "        Content: Moreover,      based      on      Wikipedia      c...\n",
      "      [ID: 620] chunk: 4.2      Automatic      Metrics - Chunk 4\n",
      "        Content: For      aspects      (1)-(4),      we      use   ...\n",
      "      [ID: 621] chunk: 4.2      Automatic      Metrics - Chunk 5\n",
      "        Content: For      verifiability,      we      calculate    ...\n",
      "    [ID: 622] section: 4.3      Baselines\n",
      "      [ID: 623] chunk: 4.3      Baselines - Chunk 1\n",
      "        Content: 4.3      Baselines\n",
      "ines  \n",
      " \n",
      "  As      prior      w...\n",
      "      [ID: 624] chunk: 4.3      Baselines - Chunk 2\n",
      "        Content: Direct      Gen,   \n",
      " \n",
      "a      baseline      that   ...\n",
      "      [ID: 625] chunk: 4.3      Baselines - Chunk 3\n",
      "        Content: RAG,   \n",
      " \n",
      "a      retrieval-augmented      generati...\n",
      "      [ID: 626] chunk: 4.3      Baselines - Chunk 4\n",
      "        Content: Outline-driven      RAG      (ORAG),      which   ...\n",
      "    [ID: 627] section: 4.4      STORM      Implementation\n",
      "      [ID: 628] chunk: 4.4      STORM      Implementation - Chunk 1\n",
      "        Content: 4.4      STORM      Implementation\n",
      "tion  \n",
      " \n",
      "  We  ...\n",
      "    [ID: 629] section: Comparsion      with      Human-written      Articles      Rubric      Grading\n",
      "      [ID: 630] chunk: Comparsion      with      Human-written      Articles      Rubric      Grading - Chunk 1\n",
      "        Content: Comparsion      with      Human-written      Artic...\n",
      "    [ID: 631] section: ROUGE-1      ROUGE-L      Entity      Recall      Interest      Level      Organization      Relevance      Coverage\n",
      "      [ID: 632] chunk: ROUGE-1      ROUGE-L      Entity      Recall      Interest      Level      Organization      Relevance      Coverage - Chunk 1\n",
      "        Content: ROUGE-1      ROUGE-L      Entity      Recall      ...\n",
      "    [ID: 633] section: Heading      Heading       Soft      Recall      Entity      Recall\n",
      "      [ID: 634] chunk: Heading      Heading       Soft      Recall      Entity      Recall - Chunk 1\n",
      "        Content: Heading      Heading       Soft      Recall      E...\n",
      "      [ID: 635] chunk: Heading      Heading       Soft      Recall      Entity      Recall - Chunk 2\n",
      "        Content: +      de-  \n",
      " \n",
      "  notes      significant      diffe...\n",
      "      [ID: 636] chunk: Heading      Heading       Soft      Recall      Entity      Recall - Chunk 3\n",
      "        Content: We      also      experiment      with      using ...\n",
      "      [ID: 637] chunk: Heading      Heading       Soft      Recall      Entity      Recall - Chunk 4\n",
      "        Content: The      ground      truth      Wikipedia  \n",
      " \n",
      "  ar...\n",
      "      [ID: 638] chunk: Heading      Heading       Soft      Recall      Entity      Recall - Chunk 5\n",
      "        Content: We      set      temperature      as      1.0     ...\n",
      "      [ID: 639] chunk: Heading      Heading       Soft      Recall      Entity      Recall - Chunk 6\n",
      "        Content: Outlines      directly      gen-  \n",
      " \n",
      "  erated     ...\n",
      "      [ID: 640] chunk: Heading      Heading       Soft      Recall      Entity      Recall - Chunk 7\n",
      "        Content: However,      STORM,      by  \n",
      " \n",
      "  asking      eff...\n",
      "      [ID: 641] chunk: Heading      Heading       Soft      Recall      Entity      Recall - Chunk 8\n",
      "        Content: Notably,      although      RAG      leverages  \n",
      " ...\n",
      "      [ID: 642] chunk: Heading      Heading       Soft      Recall      Entity      Recall - Chunk 9\n",
      "        Content: To      test  \n",
      " \n",
      "  the      limit      of      the...\n",
      "      [ID: 643] chunk: Heading      Heading       Soft      Recall      Entity      Recall - Chunk 10\n",
      "        Content: This  \n",
      " \n",
      "  modified      approach      is      ref...\n",
      "      [ID: 644] chunk: Heading      Heading       Soft      Recall      Entity      Recall - Chunk 11\n",
      "        Content: We      further      evaluate      the      full-l...\n",
      "      [ID: 645] chunk: Heading      Heading       Soft      Recall      Entity      Recall - Chunk 12\n",
      "        Content: Despite      this      method’s      advantages   ...\n",
      "      [ID: 646] chunk: Heading      Heading       Soft      Recall      Entity      Recall - Chunk 13\n",
      "        Content: The      evaluator      LLM      also      rates  ...\n",
      "      [ID: 647] chunk: Heading      Heading       Soft      Recall      Entity      Recall - Chunk 14\n",
      "        Content: Our      careful      human      evaluation      (...\n",
      "      [ID: 648] chunk: Heading      Heading       Soft      Recall      Entity      Recall - Chunk 15\n",
      "        Content: As      reported Citation      Recall Citation    ...\n",
      "      [ID: 649] chunk: Heading      Heading       Soft      Recall      Entity      Recall - Chunk 16\n",
      "        Content: 84.83 85.18  \n",
      " \n",
      "  STORM\n",
      " |  |       STORM      _  ...\n",
      "      [ID: 650] chunk: Heading      Heading       Soft      Recall      Entity      Recall - Chunk 17\n",
      "        Content: |       in      Table      4,      Mistral      7B...\n",
      "      [ID: 651] chunk: Heading      Heading       Soft      Recall      Entity      Recall - Chunk 18\n",
      "        Content: Ap-       pendix      C.3      investigates      t...\n",
      "      [ID: 652] chunk: Heading      Heading       Soft      Recall      Entity      Recall - Chunk 19\n",
      "        Content: | 5.2      Ablation      Studies\n",
      " | 5.2      Ablat...\n",
      "      [ID: 653] chunk: Heading      Heading       Soft      Recall      Entity      Recall - Chunk 20\n",
      "        Content: We      conduct      the      ablation      study ...\n",
      "      [ID: 654] chunk: Heading      Heading       Soft      Recall      Entity      Recall - Chunk 21\n",
      "        Content: To       ensure      a      fair      comparison, ...\n",
      "      [ID: 655] chunk: Heading      Heading       Soft      Recall      Entity      Recall - Chunk 22\n",
      "        Content: Also,      “STORM      w/o      Conversation”     ...\n",
      "      [ID: 656] chunk: Heading      Heading       Soft      Recall      Entity      Recall - Chunk 23\n",
      "        Content: As      shown      in      Ta-       ble      5,  ...\n",
      "      [ID: 657] chunk: Heading      Heading       Soft      Recall      Entity      Recall - Chunk 24\n",
      "        Content: In      Table      2,      “STORM       w/o      O...\n",
      "      [ID: 658] chunk: Heading      Heading       Soft      Recall      Entity      Recall - Chunk 25\n",
      "        Content: | 6      Human      Evaluation\n",
      " |       To      be...\n",
      "      [ID: 659] chunk: Heading      Heading       Soft      Recall      Entity      Recall - Chunk 26\n",
      "        Content: Each      pair      of      articles       is     ...\n",
      "      [ID: 660] chunk: Heading      Heading       Soft      Recall      Entity      Recall - Chunk 27\n",
      "        Content: |       who      have      made      at      least...\n",
      "      [ID: 661] chunk: Heading      Heading       Soft      Recall      Entity      Recall - Chunk 28\n",
      "        Content: |       Each      pair      of      articles      ...\n",
      "      [ID: 662] chunk: Heading      Heading       Soft      Recall      Entity      Recall - Chunk 29\n",
      "        Content: While       our      automatic      evaluation    ...\n",
      "      [ID: 663] chunk: Heading      Heading       Soft      Recall      Entity      Recall - Chunk 30\n",
      "        Content: After      the      evalua-       tion      finish...\n",
      "      [ID: 664] chunk: Heading      Heading       Soft      Recall      Entity      Recall - Chunk 31\n",
      "        Content: More      human      evaluation      de-       tai...\n",
      "      [ID: 665] chunk: Heading      Heading       Soft      Recall      Entity      Recall - Chunk 32\n",
      "        Content: In      ac-       cord      with      the      fin...\n",
      "      [ID: 666] chunk: Heading      Heading       Soft      Recall      Entity      Recall - Chunk 33\n",
      "        Content: Specifically,      25%      more      articles    ...\n",
      "      [ID: 667] chunk: Heading      Heading       Soft      Recall      Entity      Recall - Chunk 34\n",
      "        Content: Even       in      comparison      with      human...\n",
      "    [ID: 668] section: 5.1      Main      Results\n",
      "      [ID: 669] chunk: 5.1      Main      Results - Chunk 1\n",
      "        Content: 5.1      Main      Results\n",
      "ults  \n",
      " \n",
      "  We      use ...\n",
      "      [ID: 670] chunk: 5.1      Main      Results - Chunk 2\n",
      "        Content: com/api-reference/  \n",
      " \n",
      "  search  \n",
      " \n",
      "  high      he...\n",
      "      [ID: 671] chunk: 5.1      Main      Results - Chunk 3\n",
      "        Content: However,      STORM,      by  \n",
      " \n",
      "  asking      eff...\n",
      "      [ID: 672] chunk: 5.1      Main      Results - Chunk 4\n",
      "        Content: Notably,      although      RAG      leverages  \n",
      " ...\n",
      "      [ID: 673] chunk: 5.1      Main      Results - Chunk 5\n",
      "        Content: To      test  \n",
      " \n",
      "  the      limit      of      the...\n",
      "      [ID: 674] chunk: 5.1      Main      Results - Chunk 6\n",
      "        Content: This  \n",
      " \n",
      "  modified      approach      is      ref...\n",
      "      [ID: 675] chunk: 5.1      Main      Results - Chunk 7\n",
      "        Content: We      further      evaluate      the      full-l...\n",
      "      [ID: 676] chunk: 5.1      Main      Results - Chunk 8\n",
      "        Content: Despite      this      method’s      advantages   ...\n",
      "      [ID: 677] chunk: 5.1      Main      Results - Chunk 9\n",
      "        Content: The      evaluator      LLM      also      rates  ...\n",
      "      [ID: 678] chunk: 5.1      Main      Results - Chunk 10\n",
      "        Content: Our      careful      human      evaluation      (...\n",
      "      [ID: 679] chunk: 5.1      Main      Results - Chunk 11\n",
      "        Content: As      reported Citation      Recall Citation    ...\n",
      "      [ID: 680] chunk: 5.1      Main      Results - Chunk 12\n",
      "        Content: 84.83 85.18  \n",
      " \n",
      "  STORM\n",
      " |  |       STORM      _  ...\n",
      "      [ID: 681] chunk: 5.1      Main      Results - Chunk 13\n",
      "        Content: |       in      Table      4,      Mistral      7B...\n",
      "      [ID: 682] chunk: 5.1      Main      Results - Chunk 14\n",
      "        Content: Ap-       pendix      C.3      investigates      t...\n",
      "      [ID: 683] chunk: 5.1      Main      Results - Chunk 15\n",
      "        Content: | 5.2      Ablation      Studies\n",
      " | 5.2      Ablat...\n",
      "      [ID: 684] chunk: 5.1      Main      Results - Chunk 16\n",
      "        Content: We      conduct      the      ablation      study ...\n",
      "      [ID: 685] chunk: 5.1      Main      Results - Chunk 17\n",
      "        Content: To       ensure      a      fair      comparison, ...\n",
      "      [ID: 686] chunk: 5.1      Main      Results - Chunk 18\n",
      "        Content: Also,      “STORM      w/o      Conversation”     ...\n",
      "      [ID: 687] chunk: 5.1      Main      Results - Chunk 19\n",
      "        Content: As      shown      in      Ta-       ble      5,  ...\n",
      "      [ID: 688] chunk: 5.1      Main      Results - Chunk 20\n",
      "        Content: In      Table      2,      “STORM       w/o      O...\n",
      "      [ID: 689] chunk: 5.1      Main      Results - Chunk 21\n",
      "        Content: | 6      Human      Evaluation\n",
      " |       To      be...\n",
      "      [ID: 690] chunk: 5.1      Main      Results - Chunk 22\n",
      "        Content: Each      pair      of      articles       is     ...\n",
      "      [ID: 691] chunk: 5.1      Main      Results - Chunk 23\n",
      "        Content: |       who      have      made      at      least...\n",
      "      [ID: 692] chunk: 5.1      Main      Results - Chunk 24\n",
      "        Content: |       Each      pair      of      articles      ...\n",
      "      [ID: 693] chunk: 5.1      Main      Results - Chunk 25\n",
      "        Content: While       our      automatic      evaluation    ...\n",
      "      [ID: 694] chunk: 5.1      Main      Results - Chunk 26\n",
      "        Content: After      the      evalua-       tion      finish...\n",
      "      [ID: 695] chunk: 5.1      Main      Results - Chunk 27\n",
      "        Content: More      human      evaluation      de-       tai...\n",
      "      [ID: 696] chunk: 5.1      Main      Results - Chunk 28\n",
      "        Content: In      ac-       cord      with      the      fin...\n",
      "      [ID: 697] chunk: 5.1      Main      Results - Chunk 29\n",
      "        Content: Specifically,      25%      more      articles    ...\n",
      "      [ID: 698] chunk: 5.1      Main      Results - Chunk 30\n",
      "        Content: Even       in      comparison      with      human...\n",
      "    [ID: 699] section: Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree\n",
      "      [ID: 700] chunk: Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree - Chunk 1\n",
      "        Content: Strongly      Disagree      Somewhat      Disagree...\n",
      "      [ID: 701] chunk: Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree - Chunk 2\n",
      "        Content: = I      think      it      can      be   \n",
      " \n",
      "a    ...\n",
      "      [ID: 702] chunk: Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree - Chunk 3\n",
      "        Content: background      information”      and      another...\n",
      "      [ID: 703] chunk: Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree - Chunk 4\n",
      "        Content: We      examine      14      pair-  \n",
      " \n",
      "  wise     ...\n",
      "      [ID: 704] chunk: Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree - Chunk 5\n",
      "        Content: Through      analyzing      the      articles     ...\n",
      "      [ID: 705] chunk: Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree - Chunk 6\n",
      "        Content: These      arise      when      the      generated...\n",
      "      [ID: 706] chunk: Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree - Chunk 7\n",
      "        Content: Compared      to      the      widely      discuss...\n",
      "      [ID: 707] chunk: Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree - Chunk 8\n",
      "        Content: While      STORM      outperforms      the  \n",
      " \n",
      "  o...\n",
      "      [ID: 708] chunk: Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree - Chunk 9\n",
      "        Content: Another      major      issue      identified     ...\n",
      "      [ID: 709] chunk: Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree - Chunk 10\n",
      "        Content: This      feedback      suggests      that  \n",
      " \n",
      "  r...\n",
      "      [ID: 710] chunk: Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree - Chunk 11\n",
      "        Content: It      is      gratifying      to      know      ...\n",
      "      [ID: 711] chunk: Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree - Chunk 12\n",
      "        Content: More      reservation      is      expressed      ...\n",
      "      [ID: 712] chunk: Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree - Chunk 13\n",
      "        Content: 7\n",
      " \n",
      "   Related      Works  \n",
      " \n",
      "  Retrieval-Augmente...\n",
      "      [ID: 713] chunk: Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree - Chunk 14\n",
      "        Content: While      some      works      use      retrieval...\n",
      "      [ID: 714] chunk: Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree - Chunk 15\n",
      "        Content: Lewis      et      al. (2020)      study      RAG ...\n",
      "      [ID: 715] chunk: Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree - Chunk 16\n",
      "        Content: Besides,      RAG      can      be      used      ...\n",
      "      [ID: 716] chunk: Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree - Chunk 17\n",
      "        Content: While      RAG      is      widely  \n",
      " \n",
      "  studied  ...\n",
      "      [ID: 717] chunk: Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree - Chunk 18\n",
      "        Content: The      retrieval      sources  \n",
      " \n",
      "  can      var...\n",
      "      [ID: 718] chunk: Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree - Chunk 19\n",
      "        Content: Regarding      the      time,      besides   \n",
      " \n",
      "a ...\n",
      "      [ID: 719] chunk: Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree - Chunk 20\n",
      "        Content: Automatic      Expository      Writing      Differ...\n",
      "      [ID: 720] chunk: Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree - Chunk 21\n",
      "        Content: (2023)      propose      the      Imitate-  \n",
      " \n",
      "  R...\n",
      "      [ID: 721] chunk: Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree - Chunk 22\n",
      "        Content: (2023)  \n",
      " \n",
      "  highlight      that      expository  ...\n",
      "      [ID: 722] chunk: Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree - Chunk 23\n",
      "        Content: Question      Asking      in      NLP      Questio...\n",
      "      [ID: 723] chunk: Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree - Chunk 24\n",
      "        Content: While      humans  \n",
      " \n",
      "  usually      ask      ques...\n",
      "      [ID: 724] chunk: Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree - Chunk 25\n",
      "        Content: (2020)      which  \n",
      " \n",
      "  defines      the      ques...\n",
      "      [ID: 725] chunk: Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree - Chunk 26\n",
      "        Content: 8\n",
      " \n",
      "   Conclusion  \n",
      " \n",
      "  We      propose      STORM...\n",
      "      [ID: 726] chunk: Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree - Chunk 27\n",
      "        Content: Experimental      results      demonstrate  \n",
      " \n",
      "  t...\n",
      "      [ID: 727] chunk: Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree - Chunk 28\n",
      "        Content: The      experienced  \n",
      " \n",
      "  Wikipedia      editors ...\n",
      "    [ID: 728] section: Limitations\n",
      "      [ID: 729] chunk: Limitations - Chunk 1\n",
      "        Content: Limitations\n",
      " \n",
      " \n",
      "  In      this      work,      we ...\n",
      "      [ID: 730] chunk: Limitations - Chunk 2\n",
      "        Content: While      our      approach      sig-  \n",
      " \n",
      "  nific...\n",
      "      [ID: 731] chunk: Limitations - Chunk 3\n",
      "        Content: Although  \n",
      " \n",
      "  STORM      discovers      different...\n",
      "      [ID: 732] chunk: Limitations - Chunk 4\n",
      "        Content: Moreover,      the      verifiability      issues ...\n",
      "      [ID: 733] chunk: Limitations - Chunk 5\n",
      "        Content: Another      limitation      of      this      wor...\n",
      "      [ID: 734] chunk: Limitations - Chunk 6\n",
      "        Content: Human-authored      high-quality      Wikipedia   ...\n",
      "    [ID: 735] section: Acknowledgements\n",
      "      [ID: 736] chunk: Acknowledgements - Chunk 1\n",
      "        Content: Acknowledgements\n",
      " \n",
      " \n",
      "  We      thank      You.com ...\n",
      "      [ID: 737] chunk: Acknowledgements - Chunk 2\n",
      "        Content: This  \n",
      " \n",
      "  work      is      supported      in    ...\n",
      "    [ID: 738] section: Ethics      Statement\n",
      "      [ID: 739] chunk: Ethics      Statement - Chunk 1\n",
      "        Content: Ethics      Statement\n",
      " \n",
      " \n",
      "  Different      from   ...\n",
      "      [ID: 740] chunk: Ethics      Statement - Chunk 2\n",
      "        Content: All      the      stud-  \n",
      " \n",
      "  ies      and      th...\n",
      "      [ID: 741] chunk: Ethics      Statement - Chunk 3\n",
      "        Content: We      avoid      any  \n",
      " \n",
      "  disruption      to   ...\n",
      "      [ID: 742] chunk: Ethics      Statement - Chunk 4\n",
      "        Content: Also,  \n",
      " \n",
      "  although      we      try      to     ...\n",
      "      [ID: 743] chunk: Ethics      Statement - Chunk 5\n",
      "        Content: The      primary      risk      of      our      w...\n",
      "      [ID: 744] chunk: Ethics      Statement - Chunk 6\n",
      "        Content: Currently,      our      system      relies      o...\n",
      "      [ID: 745] chunk: Ethics      Statement - Chunk 7\n",
      "        Content: We      believe      improv-  \n",
      " \n",
      "  ing      the   ...\n",
      "      [ID: 746] chunk: Ethics      Statement - Chunk 8\n",
      "        Content: Another      limitation      we      see      from...\n",
      "      [ID: 747] chunk: Ethics      Statement - Chunk 9\n",
      "        Content: Extending      the      cur-  \n",
      " \n",
      "  rent      syste...\n",
      "    [ID: 748] section: References\n",
      "      [ID: 749] chunk: References - Chunk 1\n",
      "        Content: References\n",
      " \n",
      " \n",
      "  Sweta      Agrawal,      Chunting...\n",
      "      [ID: 750] chunk: References - Chunk 2\n",
      "        Content: Alan      Akbik,      Tanja      Bergmann,      Du...\n",
      "      [ID: 751] chunk: References - Chunk 3\n",
      "        Content: In      Proceedings      of      the      2019    ...\n",
      "      [ID: 752] chunk: References - Chunk 4\n",
      "        Content: Asking      clari-  \n",
      " \n",
      "  fying      questions     ...\n",
      "      [ID: 753] chunk: References - Chunk 5\n",
      "        Content: Expository      text      generation:      Imitate...\n",
      "      [ID: 754] chunk: References - Chunk 6\n",
      "        Content: WikiKreator:      Improving      Wikipedia      st...\n",
      "      [ID: 755] chunk: References - Chunk 7\n",
      "        Content: Association  \n",
      " \n",
      "  for      Computational      Ling...\n",
      "      [ID: 756] chunk: References - Chunk 8\n",
      "        Content: Tran,      Pat      Verga,      Roee      Aha-  \n",
      " ...\n",
      "      [ID: 757] chunk: References - Chunk 9\n",
      "        Content: Cohen,      Michael      Collins,      Dipanjan  \n",
      "...\n",
      "      [ID: 758] chunk: References - Chunk 10\n",
      "        Content: Laura      Dietz      and      John      Foley. 20...\n",
      "      [ID: 759] chunk: References - Chunk 11\n",
      "        Content: Supervi-  \n",
      " \n",
      "  sion      at      the      outline ...\n",
      "      [ID: 760] chunk: References - Chunk 12\n",
      "        Content: Generating      bi-  \n",
      " \n",
      "  ographies      on      W...\n",
      "      [ID: 761] chunk: References - Chunk 13\n",
      "        Content: Association      for      Computational      Lingu...\n",
      "      [ID: 762] chunk: References - Chunk 14\n",
      "        Content: Artificial      intelligence      (ai)      tech- ...\n",
      "      [ID: 763] chunk: References - Chunk 15\n",
      "        Content: Pattern      Recognition      Letters,      167:11...\n",
      "      [ID: 764] chunk: References - Chunk 16\n",
      "        Content: Enabling      large      language      models     ...\n",
      "      [ID: 765] chunk: References - Chunk 17\n",
      "        Content: Lei      Huang,      Weijiang      Yu,      Weitao...\n",
      "      [ID: 766] chunk: References - Chunk 18\n",
      "        Content: Gautier      Izacard,      Patrick      Lewis,    ...\n",
      "      [ID: 767] chunk: References - Chunk 19\n",
      "        Content: Albert   \n",
      " \n",
      "Q      Jiang,      Alexandre      Sabl...\n",
      "      [ID: 768] chunk: References - Chunk 20\n",
      "        Content: Zhengbao      Jiang,      Frank      Xu,      Luyu...\n",
      "      [ID: 769] chunk: References - Chunk 21\n",
      "        Content: As-  \n",
      " \n",
      "  sociation      for      Computational   ...\n",
      "      [ID: 770] chunk: References - Chunk 22\n",
      "        Content: Omar      Khattab,      Keshav      Santhanam,    ...\n",
      "      [ID: 771] chunk: References - Chunk 23\n",
      "        Content: Omar      Khattab,      Arnav      Singhvi,      P...\n",
      "      [ID: 772] chunk: References - Chunk 24\n",
      "        Content: arXiv      preprint      arXiv:2310.03714. Seungon...\n",
      "      [ID: 773] chunk: References - Chunk 25\n",
      "        Content: Mojtaba      Komeili,      Kurt      Shuster,     ...\n",
      "      [ID: 774] chunk: References - Chunk 26\n",
      "        Content: Kalpesh      Krishna,      Erin      Bransom,     ...\n",
      "      [ID: 775] chunk: References - Chunk 27\n",
      "        Content: In      Proceed-  \n",
      " \n",
      "  ings      of      the      ...\n",
      "      [ID: 776] chunk: References - Chunk 28\n",
      "        Content: Patrick      Lewis,      Ethan      Perez,      Al...\n",
      "      [ID: 777] chunk: References - Chunk 29\n",
      "        Content: Xiaonan      Li,      Kai      Lv,      Hang      ...\n",
      "      [ID: 778] chunk: References - Chunk 30\n",
      "        Content: In      Proceedings      of      the      61st    ...\n",
      "      [ID: 779] chunk: References - Chunk 31\n",
      "        Content: In      Text      Summariza-  \n",
      " \n",
      "  tion      Branc...\n",
      "      [ID: 780] chunk: References - Chunk 32\n",
      "        Content: In  \n",
      " \n",
      "  Proceedings      of      Deep      Learni...\n",
      "      [ID: 781] chunk: References - Chunk 33\n",
      "        Content: Liu,      Mohammad      Saleh,      Etienne      P...\n",
      "      [ID: 782] chunk: References - Chunk 34\n",
      "        Content: Jacob      Menick,      Maja      Trebacz,      Vl...\n",
      "      [ID: 783] chunk: References - Chunk 35\n",
      "        Content: Sewon      Min,      Kalpesh      Krishna,      Xi...\n",
      "      [ID: 784] chunk: References - Chunk 36\n",
      "        Content: In      Proceedings      of      the  \n",
      " \n",
      "  2023   ...\n",
      "      [ID: 785] chunk: References - Chunk 37\n",
      "        Content: Semi-automatic      generation      of   \n",
      " \n",
      "a     ...\n",
      "      [ID: 786] chunk: References - Chunk 38\n",
      "        Content: Reiichiro      Nakano,      Jacob      Hilton,    ...\n",
      "      [ID: 787] chunk: References - Chunk 39\n",
      "        Content: Webgpt:      Browser-  \n",
      " \n",
      "  assisted      question...\n",
      "      [ID: 788] chunk: References - Chunk 40\n",
      "        Content: Advances      in      Neural  \n",
      " \n",
      "  Information    ...\n",
      "      [ID: 789] chunk: References - Chunk 41\n",
      "        Content: Journal-  \n",
      " \n",
      "  ism   \n",
      " \n",
      "&      Mass      Communica...\n",
      "      [ID: 790] chunk: References - Chunk 42\n",
      "        Content: Ofir      Press,      Muru      Zhang,      Sewon ...\n",
      "      [ID: 791] chunk: References - Chunk 43\n",
      "        Content: Peng      Qi,      Yuhao      Zhang,      and     ...\n",
      "      [ID: 792] chunk: References - Chunk 44\n",
      "        Content: Hongjing      Qian,      Yutao      Zhu,      Zhic...\n",
      "      [ID: 793] chunk: References - Chunk 45\n",
      "        Content: Rahmani,      Xi      Wang,      Yue      Feng,   ...\n",
      "      [ID: 794] chunk: References - Chunk 46\n",
      "        Content: In      Proceedings      of      the      61st    ...\n",
      "      [ID: 795] chunk: References - Chunk 47\n",
      "        Content: Journal      of      the      Learning      Scienc...\n",
      "      [ID: 796] chunk: References - Chunk 48\n",
      "        Content: Sentence-  \n",
      " \n",
      "  BERT:      Sentence      embedding...\n",
      "      [ID: 797] chunk: References - Chunk 49\n",
      "        Content: Association      for      Com-  \n",
      " \n",
      "  putational   ...\n",
      "      [ID: 798] chunk: References - Chunk 50\n",
      "        Content: In      Proceedings      of      the      Joint   ...\n",
      "      [ID: 799] chunk: References - Chunk 51\n",
      "        Content: WikiChat:      Stopping      the      hallucinatio...\n",
      "      [ID: 800] chunk: References - Chunk 52\n",
      "        Content: Jonathan      Bragg,      Jeff      Hammerbacher, ...\n",
      "      [ID: 801] chunk: References - Chunk 53\n",
      "        Content: In      Proceedings      of      the      2022    ...\n",
      "      [ID: 802] chunk: References - Chunk 54\n",
      "        Content: Language      models      that      seek      for ...\n",
      "      [ID: 803] chunk: References - Chunk 55\n",
      "        Content: Retrieval      augmentation  \n",
      " \n",
      "  reduces      hal...\n",
      "      [ID: 804] chunk: References - Chunk 56\n",
      "        Content: In  \n",
      " \n",
      "  English      teaching      forum,      vo...\n",
      "      [ID: 805] chunk: References - Chunk 57\n",
      "        Content: validating      how      openai’s      chatgpt    ...\n",
      "      [ID: 806] chunk: References - Chunk 58\n",
      "        Content: In      Proceedings      of      the  \n",
      " \n",
      "  61st   ...\n",
      "      [ID: 807] chunk: References - Chunk 59\n",
      "        Content: DOC:      Improving      long      story      cohe...\n",
      "      [ID: 808] chunk: References - Chunk 60\n",
      "        Content: Kevin      Yang,      Yuandong      Tian,      Nan...\n",
      "      [ID: 809] chunk: References - Chunk 61\n",
      "        Content: Association      for      Com-  \n",
      " \n",
      "  putational   ...\n",
      "      [ID: 810] chunk: References - Chunk 62\n",
      "        Content: Cyril      Zakka,      Akash      Chaurasia,      ...\n",
      "      [ID: 811] chunk: References - Chunk 63\n",
      "        Content: Xu,      Zhengbao      Jiang,  \n",
      " \n",
      "  and      Graha...\n",
      "    [ID: 812] section: Average      Numer      of      Sections      8.4\n",
      "      [ID: 813] chunk: Average      Numer      of      Sections      8.4 - Chunk 1\n",
      "        Content: Average      Numer      of      Sections      8.4...\n",
      "    [ID: 814] section: Average      Number      of      All-level      Headings      15.8 Average      Length      of      a      Section      327.8       Average      Length      of      Total      Article      2159.1\n",
      "      [ID: 815] chunk: Average      Number      of      All-level      Headings      15.8 Average      Length      of      a      Section      327.8       Average      Length      of      Total      Article      2159.1 - Chunk 1\n",
      "        Content: Average      Number      of      All-level      He...\n",
      "    [ID: 816] section: Average      Number      of      References      90.1\n",
      "      [ID: 817] chunk: Average      Number      of      References      90.1 - Chunk 1\n",
      "        Content: Average      Number      of      References      9...\n",
      "      [ID: 818] chunk: Average      Number      of      References      90.1 - Chunk 2\n",
      "        Content: —\n",
      " \n",
      "   Average Number      of      references  \n",
      " \n",
      "...\n",
      "    [ID: 819] section: A_      Dataset      Details\n",
      "      [ID: 820] chunk: A_      Dataset      Details - Chunk 1\n",
      "        Content: A_      Dataset      Details\n",
      " \n",
      " \n",
      "  As      discuss...\n",
      "      [ID: 821] chunk: A_      Dataset      Details - Chunk 2\n",
      "        Content: We      select      the      most-edited  \n",
      " \n",
      "  pag...\n",
      "      [ID: 822] chunk: A_      Dataset      Details - Chunk 3\n",
      "        Content: For      quality,      we      consider      artic...\n",
      "      [ID: 823] chunk: A_      Dataset      Details - Chunk 4\n",
      "        Content: As      LLMs      can      generate      reasonabl...\n",
      "      [ID: 824] chunk: A_      Dataset      Details - Chunk 5\n",
      "        Content: For      experiments      in      this      work, ...\n",
      "      [ID: 825] chunk: A_      Dataset      Details - Chunk 6\n",
      "        Content: Notably,      human-  \n",
      " \n",
      "  authored      articles ...\n",
      "      [ID: 826] chunk: A_      Dataset      Details - Chunk 7\n",
      "        Content: Fig-  \n",
      " \n",
      "  ure   \n",
      " \n",
      "4      illustrates      the   ...\n",
      "      [ID: 827] chunk: A_      Dataset      Details - Chunk 8\n",
      "        Content: count      (A;)   \n",
      " \n",
      "=       where      embed(-)  ...\n",
      "      [ID: 828] chunk: A_      Dataset      Details - Chunk 9\n",
      "        Content: org/wiki/Wikipedia:  \n",
      " \n",
      "  Content_assessment  \n",
      " \n",
      " ...\n",
      "    [ID: 829] section: B_      Pseudo      Code      of      STORM\n",
      "      [ID: 830] chunk: B_      Pseudo      Code      of      STORM - Chunk 1\n",
      "        Content: B_      Pseudo      Code      of      STORM\n",
      " \n",
      " \n",
      "  ...\n",
      "      [ID: 831] chunk: B_      Pseudo      Code      of      STORM - Chunk 2\n",
      "        Content: We      implement      STORM      with      zero-s...\n",
      "      [ID: 832] chunk: B_      Pseudo      Code      of      STORM - Chunk 3\n",
      "        Content: We      highlight      that      STORM  \n",
      " \n",
      "  offer...\n",
      "    [ID: 833] section: C      Automatic      Evaluation      Details\n",
      "      [ID: 834] chunk: C      Automatic      Evaluation      Details - Chunk 1\n",
      "        Content: C      Automatic      Evaluation      Details...\n",
      "    [ID: 835] section: C.1      Soft      Heading      Recall\n",
      "      [ID: 836] chunk: C.1      Soft      Heading      Recall - Chunk 1\n",
      "        Content: C.1      Soft      Heading      Recall\n",
      " \n",
      " \n",
      "  We   ...\n",
      "      [ID: 837] chunk: C.1      Soft      Heading      Recall - Chunk 2\n",
      "        Content: The      calculation      is      based      on   ...\n",
      "      [ID: 838] chunk: C.1      Soft      Heading      Recall - Chunk 3\n",
      "        Content: Given  \n",
      " \n",
      "  aset   \n",
      " \n",
      "A   \n",
      " \n",
      "=      {Ai}*.,,      ...\n",
      "      [ID: 839] chunk: C.1      Soft      Heading      Recall - Chunk 4\n",
      "        Content: Signature):\n",
      " \n",
      " \n",
      "  I\\\\\\'m      writing   \n",
      " \n",
      "a      ...\n",
      "      [ID: 840] chunk: C.1      Soft      Heading      Recall - Chunk 5\n",
      "        Content: I\\\\\\'m      looking      for  \n",
      " \n",
      "  examples      t...\n",
      "      [ID: 841] chunk: C.1      Soft      Heading      Recall - Chunk 6\n",
      "        Content: non topic   \n",
      " \n",
      "=      dspy.InputField(prefix=\"Topi...\n",
      "      [ID: 842] chunk: C.1      Soft      Heading      Recall - Chunk 7\n",
      "        Content: Each      of      them      represents   \n",
      " \n",
      "a     ...\n",
      "      [ID: 843] chunk: C.1      Soft      Heading      Recall - Chunk 8\n",
      "        Content: short      summary      of      editor      1:\n",
      "des...\n",
      "      [ID: 844] chunk: C.1      Soft      Heading      Recall - Chunk 9\n",
      "        Content: Signature):\n",
      " \n",
      " \n",
      "  You      are      an      experi...\n",
      "      [ID: 845] chunk: C.1      Soft      Heading      Recall - Chunk 10\n",
      "        Content: Ask      good      questions      to  \n",
      " \n",
      "  get    ...\n",
      "      [ID: 846] chunk: C.1      Soft      Heading      Recall - Chunk 11\n",
      "        Content: Your      questions      should      be      relat...\n",
      "      [ID: 847] chunk: C.1      Soft      Heading      Recall - Chunk 12\n",
      "        Content: non  \n",
      " \n",
      "  topic   \n",
      " \n",
      "=      dspy.InputField(prefix...\n",
      "      [ID: 848] chunk: C.1      Soft      Heading      Recall - Chunk 13\n",
      "        Content: Signature):  \n",
      " \n",
      "  nnn  \n",
      " \n",
      "  You      want      to ...\n",
      "      [ID: 849] chunk: C.1      Soft      Heading      Recall - Chunk 14\n",
      "        Content: Write the queries you will use in the following fo...\n",
      "      [ID: 850] chunk: C.1      Soft      Heading      Recall - Chunk 15\n",
      "        Content: wow      Ne\n",
      " \n",
      " \n",
      "  20  \n",
      " \n",
      "  21  \n",
      " \n",
      "  22  \n",
      " \n",
      "  23  \n",
      "...\n",
      "      [ID: 851] chunk: C.1      Soft      Heading      Recall - Chunk 16\n",
      "        Content: You  \n",
      " \n",
      "  have      gathered      the      related...\n",
      "      [ID: 852] chunk: C.1      Soft      Heading      Recall - Chunk 17\n",
      "        Content: non  \n",
      " \n",
      "  topic   \n",
      " \n",
      "=      dspy.InputField(prefix...\n",
      "      [ID: 853] chunk: C.1      Soft      Heading      Recall - Chunk 18\n",
      "        Content: Signature):\n",
      "Write      an      outline      for   ...\n",
      "      [ID: 854] chunk: C.1      Soft      Heading      Recall - Chunk 19\n",
      "        Content: topic   \n",
      " \n",
      "=      dspy.InputField(prefix=\"Topic   ...\n",
      "      [ID: 855] chunk: C.1      Soft      Heading      Recall - Chunk 20\n",
      "        Content: Now      you      want      to      improve      i...\n",
      "      [ID: 856] chunk: C.1      Soft      Heading      Recall - Chunk 21\n",
      "        Content: Use      \"#\"      Title”      to      indicate    ...\n",
      "      [ID: 857] chunk: C.1      Soft      Heading      Recall - Chunk 22\n",
      "        Content: topic   \n",
      " \n",
      "=      dspy.InputField(prefix=\"Topic   ...\n",
      "      [ID: 858] chunk: C.1      Soft      Heading      Recall - Chunk 23\n",
      "        Content: yay      aA      uu      &}      WwW      YY      ...\n",
      "      [ID: 859] chunk: C.1      Soft      Heading      Recall - Chunk 24\n",
      "        Content: R-[]  \n",
      " \n",
      "  //      Discover      perspectives     ...\n",
      "      [ID: 860] chunk: C.1      Soft      Heading      Recall - Chunk 25\n",
      "        Content: convos   \n",
      " \n",
      "<      [|  \n",
      " \n",
      "  foreach   \n",
      " \n",
      "p      in...\n",
      "      [ID: 861] chunk: C.1      Soft      Heading      Recall - Chunk 26\n",
      "        Content: queries   \n",
      " \n",
      "<      gen_queries(t,      q)  \n",
      " \n",
      "  s...\n",
      "      [ID: 862] chunk: C.1      Soft      Heading      Recall - Chunk 27\n",
      "        Content: Op   \n",
      " \n",
      "<      direct_gen_outline(t)  \n",
      " \n",
      "  O      ...\n",
      "    [ID: 863] section: K       card(A)      =      S-      count      (A;)      (2)\n",
      "      [ID: 864] chunk: K       card(A)      =      S-      count      (A;)      (2) - Chunk 1\n",
      "        Content: K       card(A)      =      S-      count      (A;...\n",
      "    [ID: 865] section: C.2.      LLM      Evaluator\n",
      "      [ID: 866] chunk: C.2.      LLM      Evaluator - Chunk 1\n",
      "        Content: C.2. LLM      Evaluator\n",
      "We      use      Prometheu...\n",
      "      [ID: 867] chunk: C.2.      LLM      Evaluator - Chunk 2\n",
      "        Content: *      (Kim      et      al.,      2023),   \n",
      " \n",
      "a  ...\n",
      "      [ID: 868] chunk: C.2.      LLM      Evaluator - Chunk 3\n",
      "        Content: Table   \n",
      " \n",
      "8      gives      our      grading     ...\n",
      "      [ID: 869] chunk: C.2.      LLM      Evaluator - Chunk 4\n",
      "        Content: (2023)      show      Prometheus      ratings     ...\n",
      "    [ID: 870] section: C.3      More      Discussion      of      the      Citation      Quality\n",
      "      [ID: 871] chunk: C.3      More      Discussion      of      the      Citation      Quality - Chunk 1\n",
      "        Content: C.3      More      Discussion      of      the    ...\n",
      "    [ID: 872] section: False      Negative       15%\n",
      "      [ID: 873] chunk: False      Negative       15% - Chunk 1\n",
      "        Content: False      Negative       15%\n",
      "Figure      6:      ...\n",
      "      [ID: 874] chunk: False      Negative       15% - Chunk 2\n",
      "        Content: https:      //huggingface.co/kaist-ai/  \n",
      " \n",
      "  Crite...\n",
      "      [ID: 875] chunk: False      Negative       15% - Chunk 3\n",
      "        Content: Fairly      engaging      with   \n",
      " \n",
      "a      basic  ...\n",
      "      [ID: 876] chunk: False      Negative       15% - Chunk 4\n",
      "        Content: Exceptionally      engaging      throughout,      ...\n",
      "      [ID: 877] chunk: False      Negative       15% - Chunk 5\n",
      "        Content: Disorganized;      lacks      logical      structu...\n",
      "      [ID: 878] chunk: False      Negative       15% - Chunk 6\n",
      "        Content: Excellently      organized;      the      article ...\n",
      "      [ID: 879] chunk: False      Negative       15% - Chunk 7\n",
      "        Content: Criteria      Description  \n",
      " \n",
      "  Score   \n",
      " \n",
      "|      ...\n",
      "      [ID: 880] chunk: False      Negative       15% - Chunk 8\n",
      "        Content: Somewhat      on      topic      but      with    ...\n",
      "      [ID: 881] chunk: False      Negative       15% - Chunk 9\n",
      "        Content: Exceptionally      focused      and      entirely ...\n",
      "      [ID: 882] chunk: False      Negative       15% - Chunk 10\n",
      "        Content: Criteria      Description  \n",
      " \n",
      "  Score   \n",
      " \n",
      "|      ...\n",
      "      [ID: 883] chunk: False      Negative       15% - Chunk 11\n",
      "        Content: Severely      lacking;      offers      little    ...\n",
      "      [ID: 884] chunk: False      Negative       15% - Chunk 12\n",
      "        Content: Acceptable      breadth;      covers      most    ...\n",
      "      [ID: 885] chunk: False      Negative       15% - Chunk 13\n",
      "        Content: Exemplary      in      breadth;      delivers     ...\n",
      "    [ID: 886] section: Error      Type       Topic      Unsupported      Sentence      Source\n",
      "      [ID: 887] chunk: Error      Type       Topic      Unsupported      Sentence      Source - Chunk 1\n",
      "        Content: Error      Type       Topic      Unsupported      ...\n",
      "      [ID: 888] chunk: Error      Type       Topic      Unsupported      Sentence      Source - Chunk 2\n",
      "        Content: |       [5]      “Religion,      Beliefs      &   ...\n",
      "      [ID: 889] chunk: Error      Type       Topic      Unsupported      Sentence      Source - Chunk 3\n",
      "        Content: | 2022      Crimean       Bridge      explosion\n",
      " |...\n",
      "      [ID: 890] chunk: Error      Type       Topic      Unsupported      Sentence      Source - Chunk 4\n",
      "        Content: |       [22]      “Battlefield      2042      PC  ...\n",
      "      [ID: 891] chunk: Error      Type       Topic      Unsupported      Sentence      Source - Chunk 5\n",
      "        Content: We      use      Mistral      7B-Instruct!>      (...\n",
      "      [ID: 892] chunk: Error      Type       Topic      Unsupported      Sentence      Source - Chunk 6\n",
      "        Content: We      further      investi-  \n",
      " \n",
      "  gate      the ...\n",
      "      [ID: 893] chunk: Error      Type       Topic      Unsupported      Sentence      Source - Chunk 7\n",
      "        Content: Besides  \n",
      " \n",
      "  sentences      that      are      in...\n",
      "      [ID: 894] chunk: Error      Type       Topic      Unsupported      Sentence      Source - Chunk 8\n",
      "        Content: We      show      the      error      distribution...\n",
      "      [ID: 895] chunk: Error      Type       Topic      Unsupported      Sentence      Source - Chunk 9\n",
      "        Content: Our      analysis      of      citation  \n",
      " \n",
      "  qual...\n",
      "      [ID: 896] chunk: Error      Type       Topic      Unsupported      Sentence      Source - Chunk 10\n",
      "        Content: D      Human      Evaluation      Details\n",
      " \n",
      " \n",
      "  We...\n",
      "      [ID: 897] chunk: Error      Type       Topic      Unsupported      Sentence      Source - Chunk 11\n",
      "        Content: The      study      was      approved      by     ...\n",
      "      [ID: 898] chunk: Error      Type       Topic      Unsupported      Sentence      Source - Chunk 12\n",
      "        Content: To      streamline      the      evaluation      o...\n",
      "      [ID: 899] chunk: Error      Type       Topic      Unsupported      Sentence      Source - Chunk 13\n",
      "        Content: (2023),      we      check      citation      qual...\n",
      "      [ID: 900] chunk: Error      Type       Topic      Unsupported      Sentence      Source - Chunk 14\n",
      "        Content: \"https      ://meta.wikimedia.org  \n",
      " \n",
      "  \\\\\\'8Since...\n",
      "      [ID: 901] chunk: Error      Type       Topic      Unsupported      Sentence      Source - Chunk 15\n",
      "        Content: Figure   \n",
      " \n",
      "7      shows      the      screenshot ...\n",
      "      [ID: 902] chunk: Error      Type       Topic      Unsupported      Sentence      Source - Chunk 16\n",
      "        Content: We      collected      the      pairwise      pref...\n",
      "      [ID: 903] chunk: Error      Type       Topic      Unsupported      Sentence      Source - Chunk 17\n",
      "        Content: Specifically,      for      the      perceived    ...\n",
      "      [ID: 904] chunk: Error      Type       Topic      Unsupported      Sentence      Source - Chunk 18\n",
      "        Content: ”,      “I      think      it      will      help ...\n",
      "      [ID: 905] chunk: Error      Type       Topic      Unsupported      Sentence      Source - Chunk 19\n",
      "        Content: E_      Error      Analysis\n",
      " \n",
      " \n",
      "  While      artic...\n",
      "      [ID: 906] chunk: Error      Type       Topic      Unsupported      Sentence      Source - Chunk 20\n",
      "        Content: The      primary      issue      raised      is   ...\n",
      "      [ID: 907] chunk: Error      Type       Topic      Unsupported      Sentence      Source - Chunk 21\n",
      "        Content: Addressing      this      bias      in      the   ...\n",
      "      [ID: 908] chunk: Error      Type       Topic      Unsupported      Sentence      Source - Chunk 22\n",
      "        Content: Interest      Level  \n",
      " \n",
      "  Not      engaging      a...\n",
      "      [ID: 909] chunk: Error      Type       Topic      Unsupported      Sentence      Source - Chunk 23\n",
      "        Content: Quite      engaging      with   \n",
      " \n",
      "a      well-str...\n",
      "      [ID: 910] chunk: Error      Type       Topic      Unsupported      Sentence      Source - Chunk 24\n",
      "        Content: MOawWPYWNr\n",
      " \n",
      " \n",
      "  Coherence      and      Organizat...\n",
      "      [ID: 911] chunk: Error      Type       Topic      Unsupported      Sentence      Source - Chunk 25\n",
      "        Content: Good      organization;   \n",
      " \n",
      "a      clear      str...\n",
      "      [ID: 912] chunk: Error      Type       Topic      Unsupported      Sentence      Source - Chunk 26\n",
      "        Content: aw:\n",
      " \n",
      " \n",
      "  Relevance      and      Focus  \n",
      " \n",
      "  1:  ...\n",
      "      [ID: 913] chunk: Error      Type       Topic      Unsupported      Sentence      Source - Chunk 27\n",
      "        Content: 4:      Generally      on      topic,      despite...\n",
      "      [ID: 914] chunk: Error      Type       Topic      Unsupported      Sentence      Source - Chunk 28\n",
      "        Content: 7:      Exceptionally      focused      and      e...\n",
      "      [ID: 915] chunk: Error      Type       Topic      Unsupported      Sentence      Source - Chunk 29\n",
      "        Content: Broad      Coverage  \n",
      " \n",
      "  Severely      lacking;  ...\n",
      "      [ID: 916] chunk: Error      Type       Topic      Unsupported      Sentence      Source - Chunk 30\n",
      "        Content: Partial      coverage;      includes      some    ...\n",
      "      [ID: 917] chunk: Error      Type       Topic      Unsupported      Sentence      Source - Chunk 31\n",
      "        Content: Good      coverage;      achieves      broad      ...\n",
      "      [ID: 918] chunk: Error      Type       Topic      Unsupported      Sentence      Source - Chunk 32\n",
      "        Content: Exemplary      in      breadth;      delivers     ...\n",
      "    [ID: 919] section: We      use      Mistral      7B-Instruct!>      (Jiang      et      al.,      2023a)\n",
      "      [ID: 920] chunk: We      use      Mistral      7B-Instruct!>      (Jiang      et      al.,      2023a) - Chunk 1\n",
      "        Content: We      use      Mistral      7B-Instruct!>      (...\n",
      "      [ID: 921] chunk: We      use      Mistral      7B-Instruct!>      (Jiang      et      al.,      2023a) - Chunk 2\n",
      "        Content: We      further      investi-  \n",
      " \n",
      "  gate      the ...\n",
      "      [ID: 922] chunk: We      use      Mistral      7B-Instruct!>      (Jiang      et      al.,      2023a) - Chunk 3\n",
      "        Content: Besides  \n",
      " \n",
      "  sentences      that      are      in...\n",
      "      [ID: 923] chunk: We      use      Mistral      7B-Instruct!>      (Jiang      et      al.,      2023a) - Chunk 4\n",
      "        Content: We      show      the      error      distribution...\n",
      "      [ID: 924] chunk: We      use      Mistral      7B-Instruct!>      (Jiang      et      al.,      2023a) - Chunk 5\n",
      "        Content: Our      analysis      of      citation  \n",
      " \n",
      "  qual...\n",
      "    [ID: 925] section: D      Human      Evaluation      Details\n",
      "      [ID: 926] chunk: D      Human      Evaluation      Details - Chunk 1\n",
      "        Content: D      Human      Evaluation      Details\n",
      " \n",
      " \n",
      "  We...\n",
      "      [ID: 927] chunk: D      Human      Evaluation      Details - Chunk 2\n",
      "        Content: The      study      was      approved      by     ...\n",
      "      [ID: 928] chunk: D      Human      Evaluation      Details - Chunk 3\n",
      "        Content: To      streamline      the      evaluation      o...\n",
      "      [ID: 929] chunk: D      Human      Evaluation      Details - Chunk 4\n",
      "        Content: (2023),      we      check      citation      qual...\n",
      "      [ID: 930] chunk: D      Human      Evaluation      Details - Chunk 5\n",
      "        Content: \"https      ://meta.wikimedia.org  \n",
      " \n",
      "  \\\\\\'8Since...\n",
      "      [ID: 931] chunk: D      Human      Evaluation      Details - Chunk 6\n",
      "        Content: Figure   \n",
      " \n",
      "7      shows      the      screenshot ...\n",
      "      [ID: 932] chunk: D      Human      Evaluation      Details - Chunk 7\n",
      "        Content: We      collected      the      pairwise      pref...\n",
      "      [ID: 933] chunk: D      Human      Evaluation      Details - Chunk 8\n",
      "        Content: Specifically,      for      the      perceived    ...\n",
      "      [ID: 934] chunk: D      Human      Evaluation      Details - Chunk 9\n",
      "        Content: ”,      “I      think      it      will      help ...\n",
      "    [ID: 935] section: E_      Error      Analysis\n",
      "      [ID: 936] chunk: E_      Error      Analysis - Chunk 1\n",
      "        Content: E_      Error      Analysis\n",
      " \n",
      " \n",
      "  While      artic...\n",
      "      [ID: 937] chunk: E_      Error      Analysis - Chunk 2\n",
      "        Content: The      primary      issue      raised      is   ...\n",
      "      [ID: 938] chunk: E_      Error      Analysis - Chunk 3\n",
      "        Content: Addressing      this      bias      in      the   ...\n",
      "      [ID: 939] chunk: E_      Error      Analysis - Chunk 4\n",
      "        Content: Interest      Level  \n",
      " \n",
      "  Not      engaging      a...\n",
      "      [ID: 940] chunk: E_      Error      Analysis - Chunk 5\n",
      "        Content: Quite      engaging      with   \n",
      " \n",
      "a      well-str...\n",
      "      [ID: 941] chunk: E_      Error      Analysis - Chunk 6\n",
      "        Content: MOawWPYWNr\n",
      " \n",
      " \n",
      "  Coherence      and      Organizat...\n",
      "      [ID: 942] chunk: E_      Error      Analysis - Chunk 7\n",
      "        Content: Good      organization;   \n",
      " \n",
      "a      clear      str...\n",
      "      [ID: 943] chunk: E_      Error      Analysis - Chunk 8\n",
      "        Content: aw:\n",
      " \n",
      " \n",
      "  Relevance      and      Focus  \n",
      " \n",
      "  1:  ...\n",
      "      [ID: 944] chunk: E_      Error      Analysis - Chunk 9\n",
      "        Content: 4:      Generally      on      topic,      despite...\n",
      "      [ID: 945] chunk: E_      Error      Analysis - Chunk 10\n",
      "        Content: 7:      Exceptionally      focused      and      e...\n",
      "      [ID: 946] chunk: E_      Error      Analysis - Chunk 11\n",
      "        Content: Broad      Coverage  \n",
      " \n",
      "  Severely      lacking;  ...\n",
      "      [ID: 947] chunk: E_      Error      Analysis - Chunk 12\n",
      "        Content: Partial      coverage;      includes      some    ...\n",
      "      [ID: 948] chunk: E_      Error      Analysis - Chunk 13\n",
      "        Content: Good      coverage;      achieves      broad      ...\n",
      "      [ID: 949] chunk: E_      Error      Analysis - Chunk 14\n",
      "        Content: Exemplary      in      breadth;      delivers     ...\n",
      "    [ID: 950] section: TMAWP      YN\n",
      "      [ID: 951] chunk: TMAWP      YN - Chunk 1\n",
      "        Content: TMAWP      YN\n",
      "Verifiability\n",
      " \n",
      " \n",
      "  1:      No      ...\n",
      "      [ID: 952] chunk: TMAWP      YN - Chunk 2\n",
      "        Content: 4:      Generally      verified;      claims      ...\n",
      "      [ID: 953] chunk: TMAWP      YN - Chunk 3\n",
      "        Content: 6:      Very      well-supported;      almost     ...\n",
      "      [ID: 954] chunk: TMAWP      YN - Chunk 4\n",
      "        Content: Table      10:      Scoring      rubrics      on  ...\n",
      "      [ID: 955] chunk: TMAWP      YN - Chunk 5\n",
      "        Content: Use      of      emotional      words,  \n",
      " \n",
      "  (comm...\n",
      "      [ID: 956] chunk: TMAWP      YN - Chunk 6\n",
      "        Content: unneutral      12      (comment      on      artic...\n",
      "      [ID: 957] chunk: TMAWP      YN - Chunk 7\n",
      "        Content: (comment      on      article      Gehraiyaan)  \n",
      " ...\n",
      "      [ID: 958] chunk: TMAWP      YN - Chunk 8\n",
      "        Content: (comment      on      article      Typhoon      Hi...\n",
      "      [ID: 959] chunk: TMAWP      YN - Chunk 9\n",
      "        Content: associating      unrelated      sources      (comm...\n",
      "      [ID: 960] chunk: TMAWP      YN - Chunk 10\n",
      "        Content: (comment      on      article      2022      AFL  ...\n",
      "      [ID: 961] chunk: TMAWP      YN - Chunk 11\n",
      "        Content: . article. (comment      on      article      LK-9...\n",
      "      [ID: 962] chunk: TMAWP      YN - Chunk 12\n",
      "        Content: (comment      on      article      2022      West ...\n",
      "      [ID: 963] chunk: TMAWP      YN - Chunk 13\n",
      "        Content: Improper      handling      of   \n",
      " \n",
      "5      (commen...\n",
      "      [ID: 964] chunk: TMAWP      YN - Chunk 14\n",
      "        Content: Other      than      that,      the      AI      d...\n",
      "      [ID: 965] chunk: TMAWP      YN - Chunk 15\n",
      "        Content: |       Select      a      key       8      v     ...\n",
      "      [ID: 966] chunk: TMAWP      YN - Chunk 16\n",
      "        Content: It      was      during      this      tour      t...\n",
      "      [ID: 967] chunk: TMAWP      YN - Chunk 17\n",
      "        Content: Dave      initially      thought      that      Ta...\n",
      "      [ID: 968] chunk: TMAWP      YN - Chunk 18\n",
      "        Content: It      was      from       that      point      t...\n",
      "      [ID: 969] chunk: TMAWP      YN - Chunk 19\n",
      "        Content: Url:      https://marshall.com/live-for-       mus...\n",
      "      [ID: 970] chunk: TMAWP      YN - Chunk 20\n",
      "        Content: Born      in      Fort      Walton,      Texas,   ...\n",
      "      [ID: 971] chunk: TMAWP      YN - Chunk 21\n",
      "        Content: He      kick-started      his      professional   ...\n",
      "      [ID: 972] chunk: TMAWP      YN - Chunk 22\n",
      "        Content: His      talents      were      recognized      by...\n",
      "      [ID: 973] chunk: TMAWP      YN - Chunk 23\n",
      "        Content: Hawkins      was      celebrated      for      his...\n",
      "      [ID: 974] chunk: TMAWP      YN - Chunk 24\n",
      "        Content: His  \n",
      " \n",
      "  performances,      marked      by   \n",
      " \n",
      "a...\n",
      "      [ID: 975] chunk: TMAWP      YN - Chunk 25\n",
      "        Content: Apart      from      his      role      in      th...\n",
      "      [ID: 976] chunk: TMAWP      YN - Chunk 26\n",
      "        Content: Outside      of      his      professional      li...\n",
      "      [ID: 977] chunk: TMAWP      YN - Chunk 27\n",
      "        Content: His      legacy      continues      to      inspir...\n",
      "      [ID: 978] chunk: TMAWP      YN - Chunk 28\n",
      "        Content: Tributes      poured      in      from      around...\n",
      "      [ID: 979] chunk: TMAWP      YN - Chunk 29\n",
      "        Content: | Early      Life      and      Background\n",
      " |     ...\n",
      "      [ID: 980] chunk: TMAWP      YN - Chunk 30\n",
      "        Content: Asa      child,      Hawkins      wa:      influen...\n",
      "      [ID: 981] chunk: TMAWP      YN - Chunk 31\n",
      "        Content: |       During      his      high      school     ...\n",
      "      [ID: 982] chunk: TMAWP      YN - Chunk 32\n",
      "        Content: His      interest      in       music      was    ...\n",
      "      [ID: 983] chunk: TMAWP      YN - Chunk 33\n",
      "        Content: He      noted      that      music      was      a...\n",
      "      [ID: 984] chunk: TMAWP      YN - Chunk 34\n",
      "        Content: Born      in      Fort      Walton,      Texas,   ...\n",
      "      [ID: 985] chunk: TMAWP      YN - Chunk 35\n",
      "        Content: He      kick-started      his      professional   ...\n",
      "      [ID: 986] chunk: TMAWP      YN - Chunk 36\n",
      "        Content: His      talents      were      recognized      by...\n",
      "      [ID: 987] chunk: TMAWP      YN - Chunk 37\n",
      "        Content: Hawkins      was      celebrated      for      his...\n",
      "      [ID: 988] chunk: TMAWP      YN - Chunk 38\n",
      "        Content: His      performances,  \n",
      " \n",
      "  marked      by   \n",
      " \n",
      "a...\n",
      "      [ID: 989] chunk: TMAWP      YN - Chunk 39\n",
      "        Content: Apart      from      his      role      in      th...\n",
      "      [ID: 990] chunk: TMAWP      YN - Chunk 40\n",
      "        Content: Outside      of      his      professional      li...\n",
      "      [ID: 991] chunk: TMAWP      YN - Chunk 41\n",
      "        Content: His      legacy      continues      to      inspir...\n",
      "      [ID: 992] chunk: TMAWP      YN - Chunk 42\n",
      "        Content: Hawkins’      sudden      death      in      2022 ...\n",
      "      [ID: 993] chunk: TMAWP      YN - Chunk 43\n",
      "        Content: His      life      and      career      were      ...\n",
      "      [ID: 994] chunk: TMAWP      YN - Chunk 44\n",
      "        Content: His  \n",
      " \n",
      "  family      moved      to      Laguna   ...\n",
      "      [ID: 995] chunk: TMAWP      YN - Chunk 45\n",
      "        Content: As   \n",
      " \n",
      "a      child,      Hawkins      was      p...\n",
      "      [ID: 996] chunk: TMAWP      YN - Chunk 46\n",
      "        Content: During      his      high      school      days   ...\n",
      "      [ID: 997] chunk: TMAWP      YN - Chunk 47\n",
      "        Content: His      interest      in  \n",
      " \n",
      "  music      was    ...\n",
      "      [ID: 998] chunk: TMAWP      YN - Chunk 48\n",
      "        Content: Despite      facing      certain      hardships   ...\n",
      "      [ID: 999] chunk: TMAWP      YN - Chunk 49\n",
      "        Content: His      first      major      musical      experi...\n",
      "      [ID: 1000] chunk: TMAWP      YN - Chunk 50\n",
      "        Content: #\n",
      " \n",
      "   Career  \n",
      " \n",
      "  Taylor      Hawkins      began...\n",
      "      [ID: 1001] chunk: TMAWP      YN - Chunk 51\n",
      "        Content: His  \n",
      " \n",
      "  performances      not      only      in ...\n",
      "      [ID: 1002] chunk: TMAWP      YN - Chunk 52\n",
      "        Content: Throughout      this      time,      Hawkins      ...\n",
      "      [ID: 1003] chunk: TMAWP      YN - Chunk 53\n",
      "        Content: In      1997,      Hawkins      was      asked    ...\n",
      "      [ID: 1004] chunk: TMAWP      YN - Chunk 54\n",
      "        Content: At      the      time,      Grohl      thought    ...\n",
      "      [ID: 1005] chunk: TMAWP      YN - Chunk 55\n",
      "        Content: This  \n",
      " \n",
      "  marked      the      beginning      of ...\n",
      "      [ID: 1006] chunk: TMAWP      YN - Chunk 56\n",
      "        Content: He      drummed      for      Sass      Jordan    ...\n",
      "      [ID: 1007] chunk: TMAWP      YN - Chunk 57\n",
      "        Content: In      addition,      Hawkins      formed      hi...\n",
      "      [ID: 1008] chunk: TMAWP      YN - Chunk 58\n",
      "        Content: His      son,      Shane      Hawkins,  \n",
      " \n",
      "  has  ...\n",
      "      [ID: 1009] chunk: TMAWP      YN - Chunk 59\n",
      "        Content: #\n",
      " \n",
      "   Musical      Style      and      Influences...\n",
      "      [ID: 1010] chunk: TMAWP      YN - Chunk 60\n",
      "        Content: Known      for      his      passionate      fando...\n",
      "      [ID: 1011] chunk: TMAWP      YN - Chunk 61\n",
      "        Content: He      was      heavily      influenced      by  ...\n",
      "      [ID: 1012] chunk: TMAWP      YN - Chunk 62\n",
      "        Content: Hawkins      drew      influences      from   \n",
      " \n",
      "a...\n",
      "      [ID: 1013] chunk: TMAWP      YN - Chunk 63\n",
      "        Content: This  \n",
      " \n",
      "  distinctive      style      and      in...\n",
      "      [ID: 1014] chunk: TMAWP      YN - Chunk 64\n",
      "        Content: His      performances  \n",
      " \n",
      "  were      recognized  ...\n",
      "      [ID: 1015] chunk: TMAWP      YN - Chunk 65\n",
      "        Content: Through      his      career,      Hawkins      le...\n",
      "      [ID: 1016] chunk: TMAWP      YN - Chunk 66\n",
      "        Content: #\n",
      " \n",
      "   Personal      Life  \n",
      " \n",
      "  Taylor      Hawkin...\n",
      "      [ID: 1017] chunk: TMAWP      YN - Chunk 67\n",
      "        Content: In      his      personal      life,      Hawkins ...\n",
      "      [ID: 1018] chunk: TMAWP      YN - Chunk 68\n",
      "        Content: However,      he      managed      to      overcom...\n",
      "      [ID: 1019] chunk: TMAWP      YN - Chunk 69\n",
      "        Content: Outside      of      his      main      role      ...\n",
      "      [ID: 1020] chunk: TMAWP      YN - Chunk 70\n",
      "        Content: Hawkins      was      also      known      for    ...\n",
      "      [ID: 1021] chunk: TMAWP      YN - Chunk 71\n",
      "        Content: His      work      with      the      Foo      Fig...\n",
      "      [ID: 1022] chunk: TMAWP      YN - Chunk 72\n",
      "        Content: Notable      tributes      came      from      roc...\n",
      "      [ID: 1023] chunk: TMAWP      YN - Chunk 73\n",
      "        Content: Similarly,      Led      Zeppelin’s      Jimmy    ...\n",
      "      [ID: 1024] chunk: TMAWP      YN - Chunk 74\n",
      "        Content: Singers      like      Miley      Cyrus      and  ...\n",
      "      [ID: 1025] chunk: TMAWP      YN - Chunk 75\n",
      "        Content: He      had      received      numerous      accol...\n",
      "      [ID: 1026] chunk: TMAWP      YN - Chunk 76\n",
      "        Content: #\n",
      " \n",
      "   Discography  \n",
      " \n",
      "  Taylor      Hawkins      ...\n",
      "      [ID: 1027] chunk: TMAWP      YN - Chunk 77\n",
      "        Content: Aside      from      his      work      with      ...\n",
      "      [ID: 1028] chunk: TMAWP      YN - Chunk 78\n",
      "        Content: ###      Taylor      Hawkins   \n",
      " \n",
      "&      The      ...\n",
      "      [ID: 1029] chunk: TMAWP      YN - Chunk 79\n",
      "        Content: The      band      grew      from  \n",
      " \n",
      "  an      in...\n",
      "      [ID: 1030] chunk: TMAWP      YN - Chunk 80\n",
      "        Content: Notably,      these      albums      featured     ...\n",
      "      [ID: 1031] chunk: TMAWP      YN - Chunk 81\n",
      "        Content: ###      Red      Light      Fever  \n",
      " \n",
      "  Red      ...\n",
      "      [ID: 1032] chunk: TMAWP      YN - Chunk 82\n",
      "        Content: Prior      to      its      release,  \n",
      " \n",
      "  Hawkins...\n",
      "      [ID: 1033] chunk: TMAWP      YN - Chunk 83\n",
      "        Content: Red      Light      Fever      was      recorded  ...\n",
      "      [ID: 1034] chunk: TMAWP      YN - Chunk 84\n",
      "        Content: ##      Get      the      Money  \n",
      " \n",
      "  Get      the...\n",
      "      [ID: 1035] chunk: TMAWP      YN - Chunk 85\n",
      "        Content: The      music      video      for      the      s...\n",
      "      [ID: 1036] chunk: TMAWP      YN - Chunk 86\n",
      "        Content: The  \n",
      " \n",
      "  Coattail      Riders’      albums      n...\n",
      "      [ID: 1037] chunk: TMAWP      YN - Chunk 87\n",
      "        Content: Hawkins      also      fronted      another      g...\n",
      "      [ID: 1038] chunk: TMAWP      YN - Chunk 88\n",
      "        Content: #\n",
      " \n",
      "   Tragic      Passing  \n",
      " \n",
      "  Taylor      Hawki...\n",
      "      [ID: 1039] chunk: TMAWP      YN - Chunk 89\n",
      "        Content: The      official      cause      of      death   ...\n",
      "      [ID: 1040] chunk: TMAWP      YN - Chunk 90\n",
      "        Content: On      the      night      of      his      passi...\n",
      "      [ID: 1041] chunk: TMAWP      YN - Chunk 91\n",
      "        Content: The      news      of      Hawkins’      sudden   ...\n",
      "      [ID: 1042] chunk: TMAWP      YN - Chunk 92\n",
      "        Content: The      band      confirmed      the      news   ...\n",
      "      [ID: 1043] chunk: TMAWP      YN - Chunk 93\n",
      "        Content: The  \n",
      " \n",
      "  festival      stage      at      the    ...\n",
      "      [ID: 1044] chunk: TMAWP      YN - Chunk 94\n",
      "        Content: ##      Tributes      and      Remembrances  \n",
      " \n",
      "  ...\n",
      "      [ID: 1045] chunk: TMAWP      YN - Chunk 95\n",
      "        Content: Among      the      many      paying      their   ...\n",
      "      [ID: 1046] chunk: TMAWP      YN - Chunk 96\n",
      "        Content: In      heartfelt      social      media      post...\n",
      "      [ID: 1047] chunk: TMAWP      YN - Chunk 97\n",
      "        Content: There      were      also      numerous      onsta...\n",
      "      [ID: 1048] chunk: TMAWP      YN - Chunk 98\n",
      "        Content: Similarly,      Liam      Gallagher      of      O...\n",
      "      [ID: 1049] chunk: TMAWP      YN - Chunk 99\n",
      "        Content: Hawkins’      life      and      career      were ...\n",
      "      [ID: 1050] chunk: TMAWP      YN - Chunk 100\n",
      "        Content: “#’,      “##”      indicate      the      section...\n",
      "    [ID: 1051] section: Verifiability\n",
      "      [ID: 1052] chunk: Verifiability - Chunk 1\n",
      "        Content: Verifiability\n",
      " \n",
      " \n",
      "  1:      No      supporting    ...\n",
      "      [ID: 1053] chunk: Verifiability - Chunk 2\n",
      "        Content: 4:      Generally      verified;      claims      ...\n",
      "      [ID: 1054] chunk: Verifiability - Chunk 3\n",
      "        Content: 6:      Very      well-supported;      almost     ...\n",
      "      [ID: 1055] chunk: Verifiability - Chunk 4\n",
      "        Content: Table      10:      Scoring      rubrics      on  ...\n",
      "    [ID: 1056] section: Issue       Mentioned      Time       Example      Comments\n",
      "      [ID: 1057] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 1\n",
      "        Content: Issue       Mentioned      Time       Example     ...\n",
      "      [ID: 1058] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 2\n",
      "        Content: Use      of      emotional      words,  \n",
      " \n",
      "  (comm...\n",
      "      [ID: 1059] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 3\n",
      "        Content: unneutral      12      (comment      on      artic...\n",
      "      [ID: 1060] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 4\n",
      "        Content: (comment      on      article      Gehraiyaan)  \n",
      " ...\n",
      "      [ID: 1061] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 5\n",
      "        Content: (comment      on      article      Typhoon      Hi...\n",
      "      [ID: 1062] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 6\n",
      "        Content: associating      unrelated      sources      (comm...\n",
      "      [ID: 1063] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 7\n",
      "        Content: (comment      on      article      2022      AFL  ...\n",
      "      [ID: 1064] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 8\n",
      "        Content: . article. (comment      on      article      LK-9...\n",
      "      [ID: 1065] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 9\n",
      "        Content: (comment      on      article      2022      West ...\n",
      "      [ID: 1066] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 10\n",
      "        Content: Improper      handling      of   \n",
      " \n",
      "5      (commen...\n",
      "      [ID: 1067] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 11\n",
      "        Content: Other      than      that,      the      AI      d...\n",
      "      [ID: 1068] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 12\n",
      "        Content: |       Select      a      key       8      v     ...\n",
      "      [ID: 1069] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 13\n",
      "        Content: It      was      during      this      tour      t...\n",
      "      [ID: 1070] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 14\n",
      "        Content: Dave      initially      thought      that      Ta...\n",
      "      [ID: 1071] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 15\n",
      "        Content: It      was      from       that      point      t...\n",
      "      [ID: 1072] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 16\n",
      "        Content: Url:      https://marshall.com/live-for-       mus...\n",
      "      [ID: 1073] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 17\n",
      "        Content: Born      in      Fort      Walton,      Texas,   ...\n",
      "      [ID: 1074] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 18\n",
      "        Content: He      kick-started      his      professional   ...\n",
      "      [ID: 1075] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 19\n",
      "        Content: His      talents      were      recognized      by...\n",
      "      [ID: 1076] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 20\n",
      "        Content: Hawkins      was      celebrated      for      his...\n",
      "      [ID: 1077] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 21\n",
      "        Content: His  \n",
      " \n",
      "  performances,      marked      by   \n",
      " \n",
      "a...\n",
      "      [ID: 1078] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 22\n",
      "        Content: Apart      from      his      role      in      th...\n",
      "      [ID: 1079] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 23\n",
      "        Content: Outside      of      his      professional      li...\n",
      "      [ID: 1080] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 24\n",
      "        Content: His      legacy      continues      to      inspir...\n",
      "      [ID: 1081] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 25\n",
      "        Content: Tributes      poured      in      from      around...\n",
      "      [ID: 1082] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 26\n",
      "        Content: | Early      Life      and      Background\n",
      " |     ...\n",
      "      [ID: 1083] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 27\n",
      "        Content: Asa      child,      Hawkins      wa:      influen...\n",
      "      [ID: 1084] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 28\n",
      "        Content: |       During      his      high      school     ...\n",
      "      [ID: 1085] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 29\n",
      "        Content: His      interest      in       music      was    ...\n",
      "      [ID: 1086] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 30\n",
      "        Content: He      noted      that      music      was      a...\n",
      "      [ID: 1087] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 31\n",
      "        Content: Born      in      Fort      Walton,      Texas,   ...\n",
      "      [ID: 1088] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 32\n",
      "        Content: He      kick-started      his      professional   ...\n",
      "      [ID: 1089] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 33\n",
      "        Content: His      talents      were      recognized      by...\n",
      "      [ID: 1090] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 34\n",
      "        Content: Hawkins      was      celebrated      for      his...\n",
      "      [ID: 1091] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 35\n",
      "        Content: His      performances,  \n",
      " \n",
      "  marked      by   \n",
      " \n",
      "a...\n",
      "      [ID: 1092] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 36\n",
      "        Content: Apart      from      his      role      in      th...\n",
      "      [ID: 1093] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 37\n",
      "        Content: Outside      of      his      professional      li...\n",
      "      [ID: 1094] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 38\n",
      "        Content: His      legacy      continues      to      inspir...\n",
      "      [ID: 1095] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 39\n",
      "        Content: Hawkins’      sudden      death      in      2022 ...\n",
      "      [ID: 1096] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 40\n",
      "        Content: His      life      and      career      were      ...\n",
      "      [ID: 1097] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 41\n",
      "        Content: His  \n",
      " \n",
      "  family      moved      to      Laguna   ...\n",
      "      [ID: 1098] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 42\n",
      "        Content: As   \n",
      " \n",
      "a      child,      Hawkins      was      p...\n",
      "      [ID: 1099] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 43\n",
      "        Content: During      his      high      school      days   ...\n",
      "      [ID: 1100] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 44\n",
      "        Content: His      interest      in  \n",
      " \n",
      "  music      was    ...\n",
      "      [ID: 1101] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 45\n",
      "        Content: Despite      facing      certain      hardships   ...\n",
      "      [ID: 1102] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 46\n",
      "        Content: His      first      major      musical      experi...\n",
      "      [ID: 1103] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 47\n",
      "        Content: #\n",
      " \n",
      "   Career  \n",
      " \n",
      "  Taylor      Hawkins      began...\n",
      "      [ID: 1104] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 48\n",
      "        Content: His  \n",
      " \n",
      "  performances      not      only      in ...\n",
      "      [ID: 1105] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 49\n",
      "        Content: Throughout      this      time,      Hawkins      ...\n",
      "      [ID: 1106] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 50\n",
      "        Content: In      1997,      Hawkins      was      asked    ...\n",
      "      [ID: 1107] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 51\n",
      "        Content: At      the      time,      Grohl      thought    ...\n",
      "      [ID: 1108] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 52\n",
      "        Content: This  \n",
      " \n",
      "  marked      the      beginning      of ...\n",
      "      [ID: 1109] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 53\n",
      "        Content: He      drummed      for      Sass      Jordan    ...\n",
      "      [ID: 1110] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 54\n",
      "        Content: In      addition,      Hawkins      formed      hi...\n",
      "      [ID: 1111] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 55\n",
      "        Content: His      son,      Shane      Hawkins,  \n",
      " \n",
      "  has  ...\n",
      "      [ID: 1112] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 56\n",
      "        Content: #\n",
      " \n",
      "   Musical      Style      and      Influences...\n",
      "      [ID: 1113] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 57\n",
      "        Content: Known      for      his      passionate      fando...\n",
      "      [ID: 1114] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 58\n",
      "        Content: He      was      heavily      influenced      by  ...\n",
      "      [ID: 1115] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 59\n",
      "        Content: Hawkins      drew      influences      from   \n",
      " \n",
      "a...\n",
      "      [ID: 1116] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 60\n",
      "        Content: This  \n",
      " \n",
      "  distinctive      style      and      in...\n",
      "      [ID: 1117] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 61\n",
      "        Content: His      performances  \n",
      " \n",
      "  were      recognized  ...\n",
      "      [ID: 1118] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 62\n",
      "        Content: Through      his      career,      Hawkins      le...\n",
      "      [ID: 1119] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 63\n",
      "        Content: #\n",
      " \n",
      "   Personal      Life  \n",
      " \n",
      "  Taylor      Hawkin...\n",
      "      [ID: 1120] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 64\n",
      "        Content: In      his      personal      life,      Hawkins ...\n",
      "      [ID: 1121] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 65\n",
      "        Content: However,      he      managed      to      overcom...\n",
      "      [ID: 1122] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 66\n",
      "        Content: Outside      of      his      main      role      ...\n",
      "      [ID: 1123] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 67\n",
      "        Content: Hawkins      was      also      known      for    ...\n",
      "      [ID: 1124] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 68\n",
      "        Content: His      work      with      the      Foo      Fig...\n",
      "      [ID: 1125] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 69\n",
      "        Content: Notable      tributes      came      from      roc...\n",
      "      [ID: 1126] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 70\n",
      "        Content: Similarly,      Led      Zeppelin’s      Jimmy    ...\n",
      "      [ID: 1127] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 71\n",
      "        Content: Singers      like      Miley      Cyrus      and  ...\n",
      "      [ID: 1128] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 72\n",
      "        Content: He      had      received      numerous      accol...\n",
      "      [ID: 1129] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 73\n",
      "        Content: #\n",
      " \n",
      "   Discography  \n",
      " \n",
      "  Taylor      Hawkins      ...\n",
      "      [ID: 1130] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 74\n",
      "        Content: Aside      from      his      work      with      ...\n",
      "      [ID: 1131] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 75\n",
      "        Content: ###      Taylor      Hawkins   \n",
      " \n",
      "&      The      ...\n",
      "      [ID: 1132] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 76\n",
      "        Content: The      band      grew      from  \n",
      " \n",
      "  an      in...\n",
      "      [ID: 1133] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 77\n",
      "        Content: Notably,      these      albums      featured     ...\n",
      "      [ID: 1134] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 78\n",
      "        Content: ###      Red      Light      Fever  \n",
      " \n",
      "  Red      ...\n",
      "      [ID: 1135] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 79\n",
      "        Content: Prior      to      its      release,  \n",
      " \n",
      "  Hawkins...\n",
      "      [ID: 1136] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 80\n",
      "        Content: Red      Light      Fever      was      recorded  ...\n",
      "      [ID: 1137] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 81\n",
      "        Content: ##      Get      the      Money  \n",
      " \n",
      "  Get      the...\n",
      "      [ID: 1138] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 82\n",
      "        Content: The      music      video      for      the      s...\n",
      "      [ID: 1139] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 83\n",
      "        Content: The  \n",
      " \n",
      "  Coattail      Riders’      albums      n...\n",
      "      [ID: 1140] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 84\n",
      "        Content: Hawkins      also      fronted      another      g...\n",
      "      [ID: 1141] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 85\n",
      "        Content: #\n",
      " \n",
      "   Tragic      Passing  \n",
      " \n",
      "  Taylor      Hawki...\n",
      "      [ID: 1142] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 86\n",
      "        Content: The      official      cause      of      death   ...\n",
      "      [ID: 1143] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 87\n",
      "        Content: On      the      night      of      his      passi...\n",
      "      [ID: 1144] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 88\n",
      "        Content: The      news      of      Hawkins’      sudden   ...\n",
      "      [ID: 1145] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 89\n",
      "        Content: The      band      confirmed      the      news   ...\n",
      "      [ID: 1146] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 90\n",
      "        Content: The  \n",
      " \n",
      "  festival      stage      at      the    ...\n",
      "      [ID: 1147] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 91\n",
      "        Content: ##      Tributes      and      Remembrances  \n",
      " \n",
      "  ...\n",
      "      [ID: 1148] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 92\n",
      "        Content: Among      the      many      paying      their   ...\n",
      "      [ID: 1149] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 93\n",
      "        Content: In      heartfelt      social      media      post...\n",
      "      [ID: 1150] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 94\n",
      "        Content: There      were      also      numerous      onsta...\n",
      "      [ID: 1151] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 95\n",
      "        Content: Similarly,      Liam      Gallagher      of      O...\n",
      "      [ID: 1152] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 96\n",
      "        Content: Hawkins’      life      and      career      were ...\n",
      "      [ID: 1153] chunk: Issue       Mentioned      Time       Example      Comments - Chunk 97\n",
      "        Content: “#’,      “##”      indicate      the      section...\n",
      "    [ID: 1154] section:       (comment      on      article      2022      Crimean      Bridge      explosion)\n",
      "      [ID: 1155] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 1\n",
      "        Content: (comment      on      article      2022      Crime...\n",
      "      [ID: 1156] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 2\n",
      "        Content: |       Select      a      key       8      v     ...\n",
      "      [ID: 1157] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 3\n",
      "        Content: It      was      during      this      tour      t...\n",
      "      [ID: 1158] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 4\n",
      "        Content: Dave      initially      thought      that      Ta...\n",
      "      [ID: 1159] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 5\n",
      "        Content: It      was      from       that      point      t...\n",
      "      [ID: 1160] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 6\n",
      "        Content: Url:      https://marshall.com/live-for-       mus...\n",
      "      [ID: 1161] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 7\n",
      "        Content: Born      in      Fort      Walton,      Texas,   ...\n",
      "      [ID: 1162] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 8\n",
      "        Content: He      kick-started      his      professional   ...\n",
      "      [ID: 1163] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 9\n",
      "        Content: His      talents      were      recognized      by...\n",
      "      [ID: 1164] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 10\n",
      "        Content: Hawkins      was      celebrated      for      his...\n",
      "      [ID: 1165] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 11\n",
      "        Content: His  \n",
      " \n",
      "  performances,      marked      by   \n",
      " \n",
      "a...\n",
      "      [ID: 1166] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 12\n",
      "        Content: Apart      from      his      role      in      th...\n",
      "      [ID: 1167] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 13\n",
      "        Content: Outside      of      his      professional      li...\n",
      "      [ID: 1168] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 14\n",
      "        Content: His      legacy      continues      to      inspir...\n",
      "      [ID: 1169] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 15\n",
      "        Content: Tributes      poured      in      from      around...\n",
      "      [ID: 1170] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 16\n",
      "        Content: | Early      Life      and      Background\n",
      " |     ...\n",
      "      [ID: 1171] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 17\n",
      "        Content: Asa      child,      Hawkins      wa:      influen...\n",
      "      [ID: 1172] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 18\n",
      "        Content: |       During      his      high      school     ...\n",
      "      [ID: 1173] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 19\n",
      "        Content: His      interest      in       music      was    ...\n",
      "      [ID: 1174] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 20\n",
      "        Content: He      noted      that      music      was      a...\n",
      "      [ID: 1175] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 21\n",
      "        Content: Born      in      Fort      Walton,      Texas,   ...\n",
      "      [ID: 1176] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 22\n",
      "        Content: He      kick-started      his      professional   ...\n",
      "      [ID: 1177] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 23\n",
      "        Content: His      talents      were      recognized      by...\n",
      "      [ID: 1178] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 24\n",
      "        Content: Hawkins      was      celebrated      for      his...\n",
      "      [ID: 1179] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 25\n",
      "        Content: His      performances,  \n",
      " \n",
      "  marked      by   \n",
      " \n",
      "a...\n",
      "      [ID: 1180] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 26\n",
      "        Content: Apart      from      his      role      in      th...\n",
      "      [ID: 1181] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 27\n",
      "        Content: Outside      of      his      professional      li...\n",
      "      [ID: 1182] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 28\n",
      "        Content: His      legacy      continues      to      inspir...\n",
      "      [ID: 1183] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 29\n",
      "        Content: Hawkins’      sudden      death      in      2022 ...\n",
      "      [ID: 1184] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 30\n",
      "        Content: His      life      and      career      were      ...\n",
      "      [ID: 1185] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 31\n",
      "        Content: His  \n",
      " \n",
      "  family      moved      to      Laguna   ...\n",
      "      [ID: 1186] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 32\n",
      "        Content: As   \n",
      " \n",
      "a      child,      Hawkins      was      p...\n",
      "      [ID: 1187] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 33\n",
      "        Content: During      his      high      school      days   ...\n",
      "      [ID: 1188] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 34\n",
      "        Content: His      interest      in  \n",
      " \n",
      "  music      was    ...\n",
      "      [ID: 1189] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 35\n",
      "        Content: Despite      facing      certain      hardships   ...\n",
      "      [ID: 1190] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 36\n",
      "        Content: His      first      major      musical      experi...\n",
      "      [ID: 1191] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 37\n",
      "        Content: #\n",
      " \n",
      "   Career  \n",
      " \n",
      "  Taylor      Hawkins      began...\n",
      "      [ID: 1192] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 38\n",
      "        Content: His  \n",
      " \n",
      "  performances      not      only      in ...\n",
      "      [ID: 1193] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 39\n",
      "        Content: Throughout      this      time,      Hawkins      ...\n",
      "      [ID: 1194] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 40\n",
      "        Content: In      1997,      Hawkins      was      asked    ...\n",
      "      [ID: 1195] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 41\n",
      "        Content: At      the      time,      Grohl      thought    ...\n",
      "      [ID: 1196] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 42\n",
      "        Content: This  \n",
      " \n",
      "  marked      the      beginning      of ...\n",
      "      [ID: 1197] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 43\n",
      "        Content: He      drummed      for      Sass      Jordan    ...\n",
      "      [ID: 1198] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 44\n",
      "        Content: In      addition,      Hawkins      formed      hi...\n",
      "      [ID: 1199] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 45\n",
      "        Content: His      son,      Shane      Hawkins,  \n",
      " \n",
      "  has  ...\n",
      "      [ID: 1200] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 46\n",
      "        Content: #\n",
      " \n",
      "   Musical      Style      and      Influences...\n",
      "      [ID: 1201] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 47\n",
      "        Content: Known      for      his      passionate      fando...\n",
      "      [ID: 1202] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 48\n",
      "        Content: He      was      heavily      influenced      by  ...\n",
      "      [ID: 1203] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 49\n",
      "        Content: Hawkins      drew      influences      from   \n",
      " \n",
      "a...\n",
      "      [ID: 1204] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 50\n",
      "        Content: This  \n",
      " \n",
      "  distinctive      style      and      in...\n",
      "      [ID: 1205] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 51\n",
      "        Content: His      performances  \n",
      " \n",
      "  were      recognized  ...\n",
      "      [ID: 1206] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 52\n",
      "        Content: Through      his      career,      Hawkins      le...\n",
      "      [ID: 1207] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 53\n",
      "        Content: #\n",
      " \n",
      "   Personal      Life  \n",
      " \n",
      "  Taylor      Hawkin...\n",
      "      [ID: 1208] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 54\n",
      "        Content: In      his      personal      life,      Hawkins ...\n",
      "      [ID: 1209] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 55\n",
      "        Content: However,      he      managed      to      overcom...\n",
      "      [ID: 1210] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 56\n",
      "        Content: Outside      of      his      main      role      ...\n",
      "      [ID: 1211] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 57\n",
      "        Content: Hawkins      was      also      known      for    ...\n",
      "      [ID: 1212] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 58\n",
      "        Content: His      work      with      the      Foo      Fig...\n",
      "      [ID: 1213] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 59\n",
      "        Content: Notable      tributes      came      from      roc...\n",
      "      [ID: 1214] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 60\n",
      "        Content: Similarly,      Led      Zeppelin’s      Jimmy    ...\n",
      "      [ID: 1215] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 61\n",
      "        Content: Singers      like      Miley      Cyrus      and  ...\n",
      "      [ID: 1216] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 62\n",
      "        Content: He      had      received      numerous      accol...\n",
      "      [ID: 1217] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 63\n",
      "        Content: #\n",
      " \n",
      "   Discography  \n",
      " \n",
      "  Taylor      Hawkins      ...\n",
      "      [ID: 1218] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 64\n",
      "        Content: Aside      from      his      work      with      ...\n",
      "      [ID: 1219] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 65\n",
      "        Content: ###      Taylor      Hawkins   \n",
      " \n",
      "&      The      ...\n",
      "      [ID: 1220] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 66\n",
      "        Content: The      band      grew      from  \n",
      " \n",
      "  an      in...\n",
      "      [ID: 1221] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 67\n",
      "        Content: Notably,      these      albums      featured     ...\n",
      "      [ID: 1222] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 68\n",
      "        Content: ###      Red      Light      Fever  \n",
      " \n",
      "  Red      ...\n",
      "      [ID: 1223] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 69\n",
      "        Content: Prior      to      its      release,  \n",
      " \n",
      "  Hawkins...\n",
      "      [ID: 1224] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 70\n",
      "        Content: Red      Light      Fever      was      recorded  ...\n",
      "      [ID: 1225] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 71\n",
      "        Content: ##      Get      the      Money  \n",
      " \n",
      "  Get      the...\n",
      "      [ID: 1226] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 72\n",
      "        Content: The      music      video      for      the      s...\n",
      "      [ID: 1227] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 73\n",
      "        Content: The  \n",
      " \n",
      "  Coattail      Riders’      albums      n...\n",
      "      [ID: 1228] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 74\n",
      "        Content: Hawkins      also      fronted      another      g...\n",
      "      [ID: 1229] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 75\n",
      "        Content: #\n",
      " \n",
      "   Tragic      Passing  \n",
      " \n",
      "  Taylor      Hawki...\n",
      "      [ID: 1230] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 76\n",
      "        Content: The      official      cause      of      death   ...\n",
      "      [ID: 1231] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 77\n",
      "        Content: On      the      night      of      his      passi...\n",
      "      [ID: 1232] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 78\n",
      "        Content: The      news      of      Hawkins’      sudden   ...\n",
      "      [ID: 1233] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 79\n",
      "        Content: The      band      confirmed      the      news   ...\n",
      "      [ID: 1234] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 80\n",
      "        Content: The  \n",
      " \n",
      "  festival      stage      at      the    ...\n",
      "      [ID: 1235] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 81\n",
      "        Content: ##      Tributes      and      Remembrances  \n",
      " \n",
      "  ...\n",
      "      [ID: 1236] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 82\n",
      "        Content: Among      the      many      paying      their   ...\n",
      "      [ID: 1237] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 83\n",
      "        Content: In      heartfelt      social      media      post...\n",
      "      [ID: 1238] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 84\n",
      "        Content: There      were      also      numerous      onsta...\n",
      "      [ID: 1239] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 85\n",
      "        Content: Similarly,      Liam      Gallagher      of      O...\n",
      "      [ID: 1240] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 86\n",
      "        Content: Hawkins’      life      and      career      were ...\n",
      "      [ID: 1241] chunk:       (comment      on      article      2022      Crimean      Bridge      explosion) - Chunk 87\n",
      "        Content: “#’,      “##”      indicate      the      section...\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from typing import List, Dict\n",
    "import re\n",
    "\n",
    "class IDGenerator:\n",
    "    def __init__(self):\n",
    "        self.current_id = 0\n",
    "\n",
    "    def get_next_id(self):\n",
    "        self.current_id += 1\n",
    "        return self.current_id\n",
    "\n",
    "id_generator = IDGenerator()\n",
    "\n",
    "def create_node(title, node_type, content=None):\n",
    "    return {\n",
    "        \"id\": id_generator.get_next_id(),\n",
    "        \"title\": title,\n",
    "        \"type\": node_type,\n",
    "        \"children\": [],\n",
    "        \"content\": content if content else \"\"\n",
    "    }\n",
    "\n",
    "def chunk_content(content: str, max_chunk_size: int = 1000) -> List[str]:\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', content)\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        if len(current_chunk) + len(sentence) > max_chunk_size and current_chunk:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence\n",
    "        else:\n",
    "            current_chunk += \" \" + sentence\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def convert_to_textbook_structure(books: List[List[Dict]], max_chunk_size: int = 1000):\n",
    "    library = create_node(\"Library\", \"library\")\n",
    "\n",
    "    for book_index, book_documents in enumerate(books):\n",
    "        book = create_node(f\"Book {book_index + 1}\", \"book\")\n",
    "        library[\"children\"].append(book)\n",
    "\n",
    "        # Sort documents by section number\n",
    "        sorted_documents = sorted(book_documents, key=lambda x: x['metadata']['section_number'])\n",
    "\n",
    "        for doc in sorted_documents:\n",
    "            section_title = doc['metadata']['section_title']\n",
    "            content = doc['page_content']\n",
    "\n",
    "            # Create a new section node\n",
    "            new_section = create_node(section_title, \"section\")\n",
    "\n",
    "            # Chunk the content\n",
    "            content_chunks = chunk_content(content, max_chunk_size)\n",
    "\n",
    "            # Create chunk nodes\n",
    "            for i, chunk in enumerate(content_chunks):\n",
    "                chunk_node = create_node(f\"{section_title} - Chunk {i+1}\", \"chunk\")\n",
    "                chunk_node[\"content\"] = chunk\n",
    "                new_section[\"children\"].append(chunk_node)\n",
    "\n",
    "            # Add the new section to the book\n",
    "            book[\"children\"].append(new_section)\n",
    "\n",
    "    return library\n",
    "\n",
    "def save_structure_to_json(structure, file_path):\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(structure, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def load_structure_from_json(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def print_structure(node, indent=0):\n",
    "    print(\"  \" * indent + f\"[ID: {node['id']}] {node['type']}: {node['title']}\")\n",
    "    if node['content']:\n",
    "        print(\"  \" * (indent + 1) + f\"Content: {node['content'][:50]}...\")\n",
    "    for child in node['children']:\n",
    "        print_structure(child, indent + 1)\n",
    "\n",
    "# Example usage\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# List of books\n",
    "books = [section_documents_dicts, section_documents_dicts_2]\n",
    "\n",
    "# Convert to library structure\n",
    "library_structure = convert_to_textbook_structure(books, max_chunk_size=500)\n",
    "\n",
    "# Save the structure to a JSON file\n",
    "json_file_path = \"library_structure.json\"\n",
    "save_structure_to_json(library_structure, json_file_path)\n",
    "\n",
    "print(f\"Library structure saved to {json_file_path}\")\n",
    "\n",
    "# Load the structure from the JSON file\n",
    "loaded_structure = load_structure_from_json(json_file_path)\n",
    "\n",
    "print(\"\\nLoaded structure:\")\n",
    "print_structure(loaded_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.llmsherpa import LLMSherpaFileLoader\n",
    "loader = LLMSherpaFileLoader(\n",
    "    file_path=\"https://arxiv.org/pdf/2402.14207.pdf\",\n",
    "    new_indent_parser=True,\n",
    "    apply_ocr=True,\n",
    "    strategy=\"sections\",\n",
    "    llmsherpa_api_url=\"http://localhost:5010/api/parseDocument?renderFormat=all\",\n",
    ")\n",
    "section_documents_2 = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 0, 'section_title': 'Abstract'}\n",
      "{'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 1, 'section_title': 'Domain      Scope      Given      Given       P      Outline?      Refs?'}\n",
      "{'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 2, 'section_title': '2.1      The      FreshWiki      Dataset'}\n",
      "{'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 3, 'section_title': '2.2      Outline      Creation      and      Evaluation'}\n",
      "{'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 4, 'section_title': 'Topic      t       @      Identify       Perspectives       @      Survey       —      Writer       =o       —       Related      Articles      |'}\n",
      "{'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 5, 'section_title': 'References      R'}\n",
      "{'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 6, 'section_title': '3.1      Perspective-Guided      Question      Asking'}\n",
      "{'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 7, 'section_title': '3.2      Simulating      Conversations'}\n",
      "{'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 8, 'section_title': '3.3      Creating      the      Article      Outline'}\n",
      "{'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 9, 'section_title': '3.4      Writing      the      Full-Length      Article'}\n",
      "{'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 10, 'section_title': '4.1      Article      Selection'}\n",
      "{'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 11, 'section_title': '4.2      Automatic      Metrics'}\n",
      "{'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 12, 'section_title': '4.3      Baselines'}\n",
      "{'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 13, 'section_title': '4.4      STORM      Implementation'}\n",
      "{'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 14, 'section_title': 'Comparsion      with      Human-written      Articles      Rubric      Grading'}\n",
      "{'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 15, 'section_title': 'ROUGE-1      ROUGE-L      Entity      Recall      Interest      Level      Organization      Relevance      Coverage'}\n",
      "{'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 16, 'section_title': 'Heading      Heading       Soft      Recall      Entity      Recall'}\n",
      "{'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 17, 'section_title': '5.1      Main      Results'}\n",
      "{'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 18, 'section_title': 'Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree'}\n",
      "{'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 19, 'section_title': 'Limitations'}\n",
      "{'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 20, 'section_title': 'Acknowledgements'}\n",
      "{'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 21, 'section_title': 'Ethics      Statement'}\n",
      "{'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 22, 'section_title': 'References'}\n",
      "{'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 23, 'section_title': 'Average      Numer      of      Sections      8.4'}\n",
      "{'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 24, 'section_title': 'Average      Number      of      All-level      Headings      15.8 Average      Length      of      a      Section      327.8       Average      Length      of      Total      Article      2159.1'}\n",
      "{'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 25, 'section_title': 'Average      Number      of      References      90.1'}\n",
      "{'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 26, 'section_title': 'A_      Dataset      Details'}\n",
      "{'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 27, 'section_title': 'B_      Pseudo      Code      of      STORM'}\n",
      "{'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 28, 'section_title': 'C      Automatic      Evaluation      Details'}\n",
      "{'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 29, 'section_title': 'C.1      Soft      Heading      Recall'}\n",
      "{'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 30, 'section_title': 'K       card(A)      =      S-      count      (A;)      (2)'}\n",
      "{'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 31, 'section_title': 'C.2.      LLM      Evaluator'}\n",
      "{'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 32, 'section_title': 'C.3      More      Discussion      of      the      Citation      Quality'}\n",
      "{'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 33, 'section_title': 'False      Negative       15%'}\n",
      "{'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 34, 'section_title': 'Error      Type       Topic      Unsupported      Sentence      Source'}\n",
      "{'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 35, 'section_title': 'We      use      Mistral      7B-Instruct!>      (Jiang      et      al.,      2023a)'}\n",
      "{'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 36, 'section_title': 'D      Human      Evaluation      Details'}\n",
      "{'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 37, 'section_title': 'E_      Error      Analysis'}\n",
      "{'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 38, 'section_title': 'TMAWP      YN'}\n",
      "{'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 39, 'section_title': 'Verifiability'}\n",
      "{'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 40, 'section_title': 'Issue       Mentioned      Time       Example      Comments'}\n",
      "{'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 41, 'section_title': '      (comment      on      article      2022      Crimean      Bridge      explosion)'}\n"
     ]
    }
   ],
   "source": [
    "for doc in section_documents_2:\n",
    "    print(doc.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 0, 'section_title': 'Abstract'}, 'page_content': 'Abstract\\n \\n \\n  We      study      how      to      apply      large      language      models  \\n \\n  to      write      grounded      and      organized      long-form      ar-  \\n \\n  ticles      from      scratch,      with      comparable      breadth  \\n \\n  and      depth      to      Wikipedia      pages.\\nThis      underex-  \\n \\n  plored      problem      poses      new      challenges      at      the  \\n \\n  pre-writing      stage,      including      how      to      research  \\n \\n  the      topic      and      prepare      an      outline      prior      to      writ-  \\n \\n  ing.\\nWe      propose      STORM,   \\n \\na      writing      system  \\n \\n  for      the      Synthesis      of      Topic      Outlines      through  \\n \\n  Retrieval      and      Multi-perspective      Question      Ask-  \\n \\n  ing.\\nSTORM      models      the      pre-writing      stage      by\\n(1)      discovering      diverse      perspectives      in      research-  \\n \\n  ing      the      given      topic,      (2)      simulating      conversa-  \\n \\n  tions      where      writers      carrying      different      perspec-  \\n \\n  tives      pose      questions      to   \\n \\na      topic      expert      grounded  \\n \\n  on      trusted      Internet      sources,      (3)      curating      the      col-  \\n \\n  lected      information      to      create      an      outline.\\n \\n \\n  For      evaluation,      we      curate      FreshWiki,   \\n \\na      dataset  \\n \\n  of      recent      high-quality      Wikipedia      articles,      and  \\n \\n  formulate      outline      assessments      to      evaluate      the  \\n \\n  pre-writing      stage.\\nWe      further      gather      feedback  \\n \\n  from      experienced      Wikipedia      editors.\\nCom-  \\n \\n  pared      to      articles      generated      by      an      outline-  \\n \\n  driven      retrieval-augmented      baseline,      more      of  \\n \\n  STORM’;      articles      are      deemed      to      be      organized  \\n \\n  (by   \\n \\na      25%      absolute      increase)      and      broad      in      cov-  \\n \\n  erage      (by      10%).\\nThe      expert      feedback      also  \\n \\n  helps      identify      new      challenges      for      generating  \\n \\n  grounded      long      articles,      such      as      source      bias  \\n \\n  transfer      and      over-association      of      unrelated      facts.\\n1\\n \\n   Introduction  \\n \\n  Large      language      models      (LLMs)      have      demonstrated  \\n \\n  impressive      writing      capabilities      (Yang      et      al.,      2023;  \\n \\n  Pavlik,      2023;      Wenzlaff      and      Spaeth,      2022;      Fitria,  \\n \\n  2023),      but      it      is      unclear      how      we      can      use      them      to  \\n \\n  write      grounded,      long-form      articles,      like      full-length  \\n \\n  Wikipedia      pages.\\nSuch      expository      writing,      which  \\n \\n  seeks      to      inform      the      reader      on   \\n \\na      topic      in      an      or-  \\n \\n  ganized      manner      (Weaver      II      and      Kintsch,      1991;  \\n \\n  Balepur      et      al.,      2023),      requires      thorough      research  \\n \\n  and      planning      in      the      pre-writing      stage      (Rohman,  \\n \\n  Writing  \\n \\n  tify,      evaluate,      and      organize      external      sources   \\n \\n-   \\n \\na      task  \\n \\n  that      is      challenging      even      for      experienced      writers.\\n \\n \\n  Automating      this      process      can      facilitate      individuals  \\n \\n  in      initiating      in-depth      learning      about   \\n \\na      topic      and  \\n \\n  greatly      reduce      the      expensive      expert      hours      neces-  \\n \\n  sary      for      their      expository      writing.\\n |  |       ee      Prewriting | \\n | --- | --- | ---\\n |  | =\\n \\n   Full-length  \\n \\n  5      Article |       =      Full-length       5      Article\\n |       arXiv:2402.14207v2      [cs.CL]      8      Apr      2024 | 2022      Winter      Olympics      [=      Outline   \\n \\n|       Opening      Ceremony  \\n \\n  Research      via      Question      Asking  \\n \\n  (A)      Direct      Prompting  \\n \\n  -y      Prompt:      Ask      30      questions      about      the      given      topic.\\n \\n \\n  1.\\nWhen      was      the      opening      ceremony      held?\\n \\n \\n  {22}      2.\\nWhere      was      the      opening      ceremony      held?\\n \\n \\n  LLM      3.\\nHow      many      countries      participated      in      the      opening      ceremony?\\n \\n \\n  (B)      Perspective-Guided      Question      Asking  \\n \\n  Prompt:      You      are      an      event      planner      who      focuses      on      the B® preparation of the opening ceremony.\\n \\n \\n  1.\\nCan      you      provide      any      information      about      the      transportation  \\n \\n  arrangements      for      the      opening      ceremony?\\n \\n \\n  Lim      2.\\nCan      you      provide      any      information      about      the      budget      for      the  \\n \\n  2022      Winter      Olympics      opening      ceremony?\\n \\n \\n  (C)      Conversational      Question      Asking  \\n \\n  Can      you      provide      me      with   \\n \\na      list      of      the      participating      countries  \\n \\n  tim-      in      the      2022      Winter      Olympics      opening      ceremony?\\n \\n \\n  Role1  \\n \\n  The      2022      Winter      Olympics      featured   \\n \\na      diverse      group      of  \\n \\n  countries      participating      in      the      opening      ceremony.\\nThese LLM- included Athletes from over 90 countries will enter the  \\n \\n  Role2      stadium      ina      specific      order.\\n \\n \\n  How      is      the      order      of      participating      countries      in      the      2022  \\n \\n  Winter      Olympics      opening      ceremony      determined?\\n \\n \\n  LLM-  \\n \\n  Role1  \\n \\n  Figure      1:      We      explore      writing      Wikipedia-like      articles  \\n \\n  from      scratch,      which      demands   \\n \\na      pre-writing      stage      before  \\n \\n  producing      the      article.\\nIn      this      stage,      simpler      approaches  \\n \\n  like      Direct      Prompting      have      limited      planning      capacity.\\nIn  \\n \\n  contrast,      STORM      researches      the      topic      via      perspective-  \\n \\n  guided      question      asking      in      simulated      conversations.\\n \\n \\n  1965),      even      before      the      actual      writing      process      can  \\n \\n  start.\\nHowever,      prior      work      on      generating      Wikipedia  \\n \\n  articles      (Banerjee      and      Mitra,      2015;      Minguillén  \\n \\n  et      al.,      2017;      Liu      et      al.,      2018;      Fan      and      Gardent,  \\n \\n  2022)      has      generally      bypassed      the      pre-writing      stage:  \\n \\n  for      instance,      Liu      et      al.\\n(2018)      presume      reference  \\n \\n  documents      are      provided      in      advance,      while      Fan      and  \\n \\n  Gardent      (2022)      assume      an      article      outline      is      avail-  \\n \\n  able      and      focus      on      expanding      each      section.\\nThese  \\n \\n  assumptions      do      not      hold      in      general,      as      collecting  \\n \\n  references      and      crafting      outlines      demand      advanced  \\n \\n  information      literacy      skills      (Doyle,      1994)      to      iden- | \\n\\n \\n \\n  We      explore      these      challenges      by      focusing      on      how  \\n \\n  to      generate      Wikipedia-like      articles      from      scratch.\\n \\n \\n  We      decompose      this      problem      into      two      tasks.\\nThe  \\n \\n  first      is      to      conduct      research      to      generate      an      outline,  \\n \\n  i.e.,   \\n \\na      list      of      multi-level      sections,      and      collect   \\n \\na      set      of  \\n \\n  reference      documents.\\nThe      second      uses      the      outline  \\n \\n  and      the      references      to      produce      the      full-length      arti-  \\n \\n  cle.\\nSuch   \\n \\na      task      decomposition      mirrors      the      human  \\n \\n  writing      process      which      usually      includes      phases      of  \\n \\n  pre-writing,      drafting,      and      revising      (Rohman,      1965;  \\n \\n  Munoz-Luna,      2015).\\n \\n \\n  As      pre-trained      language      models      inherently      pos-  \\n \\n  sess   \\n \\na      wealth      of      knowledge,   \\n \\na      direct      approach      is      to  \\n \\n  rely      on      their      parametric      knowledge      for      generating  \\n \\n  outlines      or      even      entire      articles      (Direct      Gen).\\nHow-  \\n \\n  ever,      this      approach      is      limited      by   \\n \\na      lack      of      details  \\n \\n  and      hallucinations      (Xu      et      al.,      2023),      particularly      in  \\n \\n  addressing      long-tail      topics      (Kandpal      et      al.,      2023).\\n \\n \\n  This      underscores      the      importance      of      leveraging      ex-  \\n \\n  ternal      sources,      and      current      strategies      often      involve  \\n \\n  retrieval-augmented      generation      (RAG),      which      cir-  \\n \\n  cles      back      to      the      problem      of      researching      the      topic      in  \\n \\n  the      pre-writing      stage,      as      much      information      cannot  \\n \\n  be      surfaced      through      simple      topic      searches.\\n \\n \\n  Human      learning      theories      (Tawfik      et      al.,      2020;  \\n \\n  Booth      et      al.,      2003)      highlight      asking      effective  \\n \\n  questions      in      information      acquisition.\\nAlthough  \\n \\n  instruction-tuned      models      (Ouyang      et      al.,      2022)      can  \\n \\n  be      prompted      directly      to      generate      questions,      we      find  \\n \\n  that      they      typically      produce      basic      “What”,      “When”,  \\n \\n  and      “Where”      questions      (Figure   \\n \\n1      (A))      which      often  \\n \\n  only      address      surface-level      facts      about      the      topic.\\nTo  \\n \\n  endow      LLMs      with      the      capacity      to      conduct      better  \\n \\n  research,      we      propose      the      STORM      paradigm      for  \\n \\n  the      Synthesis      of      Topic      Outlines      through      Retrieval  \\n \\n  and      Multi-perspective      Question      Asking.\\n \\n \\n  The      design      of      STORM      is      based      on      two      hypothe-  \\n \\n  ses:      (1)      diverse      perspectives      lead      to      varied      ques-  \\n \\n  tions;      (2)      formulating      in-depth      questions      requires  \\n \\n  iterative      research.\\nBuilding      upon      these      hypotheses,  \\n \\n  STORM      employs   \\n \\na      novel      multi-stage      approach.\\nIt  \\n \\n  first      discovers      diverse      perspectives      by      retrieving  \\n \\n  and      analyzing      Wikipedia      articles      from      similar      top-  \\n \\n  ics      and      then      personifies      the      LLM      with      specific      per-  \\n \\n  spectives      for      question      asking      (Figure   \\n \\n1      (B)).\\nNext,  \\n \\n  to      elicit      follow-up      questions      for      iterative      research  \\n \\n  (Figure   \\n \\n1      (C)),      STORM      simulates      multi-turn      con-  \\n \\n  versations      where      the      answers      to      the      generated      ques-  \\n \\n  tions      are      grounded      on      the      Internet.\\nFinally,      based  \\n \\n  on      the      LLM’s      internal      knowledge      and      the      collected  \\n \\n  information,      STORM      creates      an      outline      that      can  \\n \\n  be      expanded      section      by      section      to      develop   \\n \\na      full-  \\n \\n  length      Wikipedia-like      article.\\n \\n \\n  We      evaluate      STORM      using      our      FreshWiki  \\n \\n  dataset      (§2.1)      which      curates      recent,      high-quality  \\n \\n  Wikipedia      articles      to      avoid      data      leakage      during      pre-  \\n \\n  training.!\\nTo      facilitate      the      study      of      the      pre-writing  \\n \\n  stage,      we      define      metrics      for      evaluating      the      outline  \\n \\n  quality      against      human-written      articles.\\n \\n \\n  We      further      invited   \\n \\na      group      of      experienced  \\n \\n  Wikipedia      editors      for      expert      evaluation.\\nThe      ed-  \\n \\n  itors      found      STORM      outperforms      an      outline-driven  \\n \\n  RAG      baseline,      especially      regarding      the      breadth      and  \\n \\n  organization      of      the      articles.\\nThey      also      identified  \\n \\n  challenges      for      future      research,      including      address-  \\n \\n  ing      cases      where:      (1)      the      bias      on      the      Internet      affects  \\n \\n  the      generated      articles;      (2)      LLMs      fabricate      connec-  \\n \\n  tions      between      unrelated      facts.\\nThese      challenges  \\n \\n  present      new      frontiers      to      grounded      writing      systems.\\n \\n \\n  Our      main      contributions      include:\\n*\\n \\n   To      evaluate      the      capacity      of      LLM      systems      at  \\n \\n  generating      long-form      grounded      articles      from  \\n \\n  scratch,      and      the      pre-writing      challenge      in      par-  \\n \\n  ticular,      we      curate      the      FreshWiki      dataset      and  \\n \\n  establish      evaluation      criteria      for      both      outline  \\n \\n  and      final      article      quality.\\n \\n \\n \\n¢      We      propose      STORM,   \\n \\na      novel      system      that      au-  \\n \\n  tomates      the      pre-writing      stage.\\nSTORM      re-  \\n \\n  searches      the      topic      and      creates      an      outline      by  \\n \\n  using      LLMs      to      ask      incisive      questions      and      re-  \\n \\n  trieving      trusted      information      from      the      Internet.\\n \\n \\n \\n¢      Both      automatic      and      human      evaluation      demon-  \\n \\n  strate      the      effectiveness      of      our      approach.\\nEx-  \\n \\n  pert      feedback      further      reveals      new      challenges  \\n \\n  in      generating      grounded      long-form      articles.\\n2\\n \\n   FreshWiki  \\n \\n  We      study      generating      Wikipedia-like      articles      from  \\n \\n  scratch,      placing      emphasis      on      the      pre-writing  \\n \\n  stage      (Rohman,      1965),      which      involves      the      demand-  \\n \\n  ing      sub-tasks      of      gathering      and      curating      relevant  \\n \\n  information      (“‘research’’).      This      models      the      human ‘Our      resources      and      code      are      released      at      https:      //github.\\n \\n \\n  com/stanford-oval/storm.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 1, 'section_title': 'Domain      Scope      Given      Given       P      Outline?      Refs?'}, 'page_content': 'Domain      Scope      Given      Given       P      Outline?      Refs?\\n \\n \\n  Balepur      et      al.\\n(2023)      One      One      para.\\n \\n \\n/      Yes  \\n \\n  Qian      et      al.\\n(2023)      All      One      para.\\n \\n \\n/      No  \\n \\n  Fan      and      Gardent      (2022)      One      Full      article      Yes      No  \\n \\n  Liu      et      al.\\n(2018)      All      One      para.\\n \\n \\n/      Yes  \\n \\n  Sauper      and      Barzilay      (2009)      Two      Full      article      No      No  \\n \\n  Ours      All      Full      article      No      No  \\n \\n  Table      1:      Comparison      of      different      Wikipedia      generation  \\n \\n  setups      in      existing      literature.\\nGenerating      one      paragraph  \\n \\n  does      not      need      an      article      outline.\\n \\n \\n  writing      approach      which      has      prompted      some      educa-  \\n \\n  tors      to      view      Wikipedia      article      writing      as      an      educa-  \\n \\n  tional      exercise      for      academic      training      (Tardy,      2010).\\n \\n \\n  Table   \\n \\n1      compares      our      work      against      prior      bench-  \\n \\n  marks      for      Wikipedia      generation.\\nExisting      work  \\n \\n  has      generally      focused      on      evaluating      the      generation  \\n \\n  of      shorter      snippets      (e.g.,      one      paragraph),      within   \\n \\na       narrower      scope      (e.g.,   \\n \\na      specific      domain      or      two),      or  \\n \\n  when      an      explicit      outline      or      reference      documents  \\n \\n  are      supplied.\\n \\n \\nA      notable      example      is      WikiSum      (Liu  \\n \\n  et      al.,      2018),      which      treats      generating      Wikipedia      ar-  \\n \\n  ticles      as   \\n \\na      multi-document      summarization      problem,  \\n \\n  with      respect      to      the      reference      documents.\\n \\n \\n  Our      setup      emphasizes      the      capability      of      long-  \\n \\n  form      grounded      writing      systems      to      research      and  \\n \\n  curate      content.\\nSpecifically,      given   \\n \\na      topic      ¢,      the  \\n \\n  task      is      to      find   \\n \\na      set      of      references   \\n \\n®      and      generate a full-length article S = s1598,, where each  \\n \\n  sentence      s;      cites   \\n \\na      list      of      documents      in      R.7\\n2.1      The      FreshWiki      Dataset\\naset  \\n \\n  Creating   \\n \\na      new      Wikipedia-like      article      demands      not  \\n \\n  only      fluent      writing      but      also      good      research      skills.\\nAs  \\n \\n  modern      LLMs      are      generally      trained      on      Wikipedia  \\n \\n  text,      we      mitigate      data      leakage      by      explicitly      seeking  \\n \\n  out      recent      Wikipedia      articles      that      were      created      (or  \\n \\n  very      heavily      edited)      after      the      training      cutoff      of      the  \\n \\n  LLMs      we      test.\\nOur      process      can      be      repeated      at  \\n \\n  future      dates      when      new      LLMs      emerge.\\n \\n \\n  To      apply      our      date      criteria,      we      focus      on      the      top  \\n \\n  100      most-edited      pages,      based      on      edit      counts,      for  \\n \\n  each      month      from      February      2022      to      September  \\n \\n  2023.      To      ensure      high-quality      references,      we      filter these      articles      to      keep      only      those      having      B-class  \\n \\n  quality      or      above      assessed      by      ORES*.\\nWe      also      ex-  \\n \\n  \"In      practice,   \\n \\nS      also      includes      organizational      elements      such  \\n \\n  as      section      and      subsection      titles,      which      do      not      require      citations.\\n \\n \\n  3      Obtained      from      https:      //wikimedia.\\n \\n \\n  org/api/rest_v1/metrics/edited-pages/  \\n \\n  top-by-edits/en.wikipedia/all-editor-types/  \\n \\n  content/      {year      }/{month}/all-days  \\n \\n  ‘https:      //www.mediawiki.org/wiki/ORES  \\n \\n  clude      list      articles      and      articles      that      have      no      sub-  \\n \\n  sections.\\nWhile      high-quality      Wikipedia      articles  \\n \\n  usually      contain      structured      data      (e.g.,      tables)      and      are  \\n \\n  multi-modal,      we      only      consider      the      plain      text      com-  \\n \\n  ponent      in      constructing      the      dataset      to      simplify      our  \\n \\n  task.\\nMore      details      of      the      dataset      are      in      Appendix      A.\\n2.2      Outline      Creation      and      Evaluation\\ntion  \\n \\n  A      full-length      article      is      hard      to      generate      or      evalu-  \\n \\n  ate      (Xu      et      al.,      2023;      Krishna      et      al.,      2023).\\nWhen  \\n \\n  human      educators      teach      students      academic      writing,  \\n \\n  they      sometimes      supervise      students      at      the      outline  \\n \\n  stage      (Eriksson      and      Makitalo,      2015)      because      an  \\n \\n  extensive      outline      indicates   \\n \\na      comprehensive      under-  \\n \\n  standing      of      the      topic      and      provides   \\n \\na      solid      founda-  \\n \\n  tion      for      writing      the      full-length      article      (Dietz      and  \\n \\n  Foley,      2019).\\nInspired      by      this,      we      decompose      the  \\n \\n  generation      of   \\n \\nS      into      two      stages.\\nIn      the      pre-writing  \\n \\n  stage,      we      require      the      system      to      create      an      outline  \\n \\n  O,      which      is      defined      as   \\n \\na      list      of      multi-level      section  \\n \\n  headings®.\\nIn      the      writing      stage,      the      system      uses  \\n \\n  the      topic      t,      the      references      R,      and      an      outline   \\n \\nO      to  \\n \\n  produce      the      full-length      article      S.\\n \\n \\n  To      evaluate      the      outline      coverage,      we      introduce  \\n \\n  two      metrics:      heading      soft      recall      and      heading      en-  \\n \\n  tity      recall.\\nThese      metrics      compare      the      multi-level  \\n \\n  section      headings      of      the      human-written      article,      con-  \\n \\n  sidered      as      ground      truth,      and      those      in      O.      Recog-  \\n \\n  nizing      that      an      exact      match      between      elements      in  \\n \\n  these      two      sets      of      headings      is      unnecessary,      we      cal-  \\n \\n  culate      the      heading      soft      recall      (Franti      and      Mariescu-  \\n \\n  Istodor,      2023)      using      cosine      similarity      derived      from  \\n \\n  Sentence-BERT      (Reimers      and      Gurevych,      2019)      em-  \\n \\n  beddings      of      the      headings      (details      in      Appendix      C.1).\\n \\n \\n  We      also      compute      the      heading      entity      recall      which  \\n \\n  is      quantified      as      the      percentage      of      named      entities      in  \\n \\n  human-written      article      headings      covered      by      O.      We  \\n \\n  extract      entities      with      FLAIR      named      entity      recogni-  \\n \\n  tion      (NER)      (Akbik      et      al.,      2019).\\n3\\n \\n   Method  \\n \\n  We      present      STORM      to      automate      the      pre-writing  \\n \\n  stage      by      researching   \\n \\na      given      topic      via      effective  \\n \\n  question      asking      (§3.1,      §3.2)      and      creating      an      out-  \\n \\n  line      (§3.3).\\nThe      outline      will      be      extended      to   \\n \\na      full-  \\n \\n  length      article      grounded      on      the      collected      references  \\n \\n  Shttps://en.wikipedia.org/wiki/Wikipedia:  \\n \\n  Stand-alone_lists  \\n \\n  ®Since      language      models      process      and      produce      sequences,  \\n \\n  we      can      linearize   \\n \\nO      by      adding      “#”      to      indicate      section      titles,  \\n \\n  “#4?”\\nto      indicate      subsection      titles,      etc.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 2, 'section_title': '2.1      The      FreshWiki      Dataset'}, 'page_content': '2.1      The      FreshWiki      Dataset\\naset  \\n \\n  Creating   \\n \\na      new      Wikipedia-like      article      demands      not  \\n \\n  only      fluent      writing      but      also      good      research      skills.\\nAs  \\n \\n  modern      LLMs      are      generally      trained      on      Wikipedia  \\n \\n  text,      we      mitigate      data      leakage      by      explicitly      seeking  \\n \\n  out      recent      Wikipedia      articles      that      were      created      (or  \\n \\n  very      heavily      edited)      after      the      training      cutoff      of      the  \\n \\n  LLMs      we      test.\\nOur      process      can      be      repeated      at  \\n \\n  future      dates      when      new      LLMs      emerge.\\n \\n \\n  To      apply      our      date      criteria,      we      focus      on      the      top  \\n \\n  100      most-edited      pages,      based      on      edit      counts,      for  \\n \\n  each      month      from      February      2022      to      September  \\n \\n  2023.      To      ensure      high-quality      references,      we      filter these      articles      to      keep      only      those      having      B-class  \\n \\n  quality      or      above      assessed      by      ORES*.\\nWe      also      ex-  \\n \\n  \"In      practice,   \\n \\nS      also      includes      organizational      elements      such  \\n \\n  as      section      and      subsection      titles,      which      do      not      require      citations.\\n \\n \\n  3      Obtained      from      https:      //wikimedia.\\n \\n \\n  org/api/rest_v1/metrics/edited-pages/  \\n \\n  top-by-edits/en.wikipedia/all-editor-types/  \\n \\n  content/      {year      }/{month}/all-days  \\n \\n  ‘https:      //www.mediawiki.org/wiki/ORES  \\n \\n  clude      list      articles      and      articles      that      have      no      sub-  \\n \\n  sections.\\nWhile      high-quality      Wikipedia      articles  \\n \\n  usually      contain      structured      data      (e.g.,      tables)      and      are  \\n \\n  multi-modal,      we      only      consider      the      plain      text      com-  \\n \\n  ponent      in      constructing      the      dataset      to      simplify      our  \\n \\n  task.\\nMore      details      of      the      dataset      are      in      Appendix      A.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 3, 'section_title': '2.2      Outline      Creation      and      Evaluation'}, 'page_content': '2.2      Outline      Creation      and      Evaluation\\ntion  \\n \\n  A      full-length      article      is      hard      to      generate      or      evalu-  \\n \\n  ate      (Xu      et      al.,      2023;      Krishna      et      al.,      2023).\\nWhen  \\n \\n  human      educators      teach      students      academic      writing,  \\n \\n  they      sometimes      supervise      students      at      the      outline  \\n \\n  stage      (Eriksson      and      Makitalo,      2015)      because      an  \\n \\n  extensive      outline      indicates   \\n \\na      comprehensive      under-  \\n \\n  standing      of      the      topic      and      provides   \\n \\na      solid      founda-  \\n \\n  tion      for      writing      the      full-length      article      (Dietz      and  \\n \\n  Foley,      2019).\\nInspired      by      this,      we      decompose      the  \\n \\n  generation      of   \\n \\nS      into      two      stages.\\nIn      the      pre-writing  \\n \\n  stage,      we      require      the      system      to      create      an      outline  \\n \\n  O,      which      is      defined      as   \\n \\na      list      of      multi-level      section  \\n \\n  headings®.\\nIn      the      writing      stage,      the      system      uses  \\n \\n  the      topic      t,      the      references      R,      and      an      outline   \\n \\nO      to  \\n \\n  produce      the      full-length      article      S.\\n \\n \\n  To      evaluate      the      outline      coverage,      we      introduce  \\n \\n  two      metrics:      heading      soft      recall      and      heading      en-  \\n \\n  tity      recall.\\nThese      metrics      compare      the      multi-level  \\n \\n  section      headings      of      the      human-written      article,      con-  \\n \\n  sidered      as      ground      truth,      and      those      in      O.      Recog-  \\n \\n  nizing      that      an      exact      match      between      elements      in  \\n \\n  these      two      sets      of      headings      is      unnecessary,      we      cal-  \\n \\n  culate      the      heading      soft      recall      (Franti      and      Mariescu-  \\n \\n  Istodor,      2023)      using      cosine      similarity      derived      from  \\n \\n  Sentence-BERT      (Reimers      and      Gurevych,      2019)      em-  \\n \\n  beddings      of      the      headings      (details      in      Appendix      C.1).\\n \\n \\n  We      also      compute      the      heading      entity      recall      which  \\n \\n  is      quantified      as      the      percentage      of      named      entities      in  \\n \\n  human-written      article      headings      covered      by      O.      We  \\n \\n  extract      entities      with      FLAIR      named      entity      recogni-  \\n \\n  tion      (NER)      (Akbik      et      al.,      2019).\\n3\\n \\n   Method  \\n \\n  We      present      STORM      to      automate      the      pre-writing  \\n \\n  stage      by      researching   \\n \\na      given      topic      via      effective  \\n \\n  question      asking      (§3.1,      §3.2)      and      creating      an      out-  \\n \\n  line      (§3.3).\\nThe      outline      will      be      extended      to   \\n \\na      full-  \\n \\n  length      article      grounded      on      the      collected      references  \\n \\n  Shttps://en.wikipedia.org/wiki/Wikipedia:  \\n \\n  Stand-alone_lists  \\n \\n  ®Since      language      models      process      and      produce      sequences,  \\n \\n  we      can      linearize   \\n \\nO      by      adding      “#”      to      indicate      section      titles,  \\n \\n  “#4?”\\nto      indicate      subsection      titles,      etc.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 4, 'section_title': 'Topic      t       @      Identify       Perspectives       @      Survey       —      Writer       =o       —       Related      Articles      |'}, 'page_content': 'Topic      t       @      Identify       Perspectives       @      Survey       —      Writer       =o       —       Related      Articles      |\\n@\\n \\n   Direct      Generate Question   \\n \\nq    \\n \\n@      Split      Queries  \\n \\n  ©      Search   \\n \\n&      Sift  \\n \\n  ©      Synthesize |\\n \\n   Answer   \\n \\na      \\\\\\\\\\\\\\\\\\n |       \\\\\\\\\\\\\\\\       y      Gather       ‘\\\\\\\\\\\\\\\\      Add      Trusted | \\n | ¥       ,      Cy}\\n\\n |       Draft      Outline      Op | Conversations {Cg,       Refine |  | \\n | --- | --- | --- | ---\\n | \\\\\\\\\\\\\\\\      Sources       Ns\\n'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 5, 'section_title': 'References      R'}, 'page_content': 'References      R\\n(§3.4).\\nFigure   \\n \\n2      gives      an      overview      of      STORM      and  \\n \\n  we      include      the      pseudo      code      in      Appendix      B.\\n3.1      Perspective-Guided      Question      Asking\\nking  \\n \\n  Rohman      (1965)      defines      pre-writing      as      the      stage  \\n \\n  of      discovery      in      the      writing      process.\\nIn      parallel  \\n \\n  with      stakeholder      theory      in      business      (Freeman      et      al.,  \\n \\n  2010),      where      diverse      stakeholders      prioritize      vary-  \\n \\n  ing      facets      of   \\n \\na      company,      individuals      with      distinct  \\n \\n  perspectives      may      concentrate      on      different      aspects  \\n \\n  when      researching      the      same      topic      and      discover      mul-  \\n \\n  tifaceted      information.\\nFurther,      the      specific      perspec-  \\n \\n  tives      can      serve      as      prior      knowledge,      guiding      individ-  \\n \\n  uals      to      ask      more      in-depth      questions.\\nFor      example,  \\n \\n  an      event      planner      might      ask      about      the      “‘transporta-  \\n \\n  tion      arrangements”      and      “budget”      for      “the      2022  \\n \\n  Winter      Olympics      opening      ceremony”,      whereas   \\n \\na       layperson      might      ask      more      general      questions      about  \\n \\n  the      event’s      basic      information      (Figure   \\n \\n1      (A)).\\n \\n \\n  Given      the      input      topic      t,      STORM      discovers      differ-  \\n \\n  ent      perspectives      by      surveying      existing      articles      from  \\n \\n  similar      topics      and      uses      these      perspectives      to      control  \\n \\n  the      question      asking      process.\\nSpecifically,      STORM  \\n \\n  prompts      an      LLM      to      generate   \\n \\na      list      of      related      top-  \\n \\n  ics      and      subsequently      extracts      the      tables      of      contents  \\n \\n  from      their      corresponding      Wikipedia      articles,      if      such  \\n \\n  articles      can      be      obtained      through      Wikipedia      API’  \\n \\n  (Figure   \\n \\n2      (1).\\nThese      tables      of      contents      are      con- catenated      to      create   \\n \\na      context      to      prompt      the      LLM to identify N perspectives P = {p1,, pn} that  \\n \\n  be      evaluated      using   \\n \\na      rule-based      filter      according      to  \\n \\n  the      Wikipedia      guideline®      to      exclude      untrustworthy  \\n \\n  sources      (Figure   \\n \\n2      (5)).\\nFinally,      the      LLM      synthe-  \\n \\n  Thttps://pypi.org/project/Wikipedia-API/  \\n \\n  can      collectively      contribute      to   \\n \\na      comprehensive      ar-  \\n \\n  ticle      on   \\n \\n¢      (Figure   \\n \\n2      (2)).\\nTo      ensure      that      the      basic  \\n \\n  information      about   \\n \\n¢      is      also      covered,      we      add      pg      as  \\n \\n  “basic      fact      writer      focusing      on      broadly      covering      the  \\n \\n  basic      facts      about      the      topic”      into      P.      Each      perspec-  \\n \\n  tive   \\n \\np   \\n \\n€   \\n \\nP      will      be      utilized      to      guide      the      LLM      in      the  \\n \\n  process      of      question      asking      in      parallel.\\n3.2      Simulating      Conversations\\nions  \\n \\n  The      theory      of      questions      and      question      asking      (Ram,  \\n \\n  1991)      highlights      that      while      answers      to      existing  \\n \\n  questions      contribute      to   \\n \\na      more      comprehensive  \\n \\n  understanding      of   \\n \\na      topic,      they      often      simultane-  \\n \\n  ously      give      rise      to      new      questions.\\nTo      kick      off      this  \\n \\n  dynamic      process,      STORM      simulates   \\n \\na      conversa-  \\n \\n  tion      between   \\n \\na      Wikipedia      writer      and   \\n \\na      topic      ex-  \\n \\n  pert.\\nIn      the      z-th      round      of      the      conversation,      the  \\n \\n  LLM-powered      Wikipedia      writer      generates   \\n \\na      sin-  \\n \\n  gle      question      q;      based      on      the      topic      1,      its      assigned  \\n \\n  perspective   \\n \\np   \\n \\n€      P,      and      the      conversation      history  \\n \\n  {q1,      41,      ---,      Gi-1,      41-1}      where      a;      denotes      the      sim-  \\n \\n  ulated      expert’s      answer.\\nThe      conversation      history  \\n \\n  enables      the      LLM      to      update      its      understanding      of      the  \\n \\n  topic      and      ask      follow-up      questions.\\nIn      practice,      we  \\n \\n  limit      the      conversation      to      at      most   \\n \\n/      rounds.\\n \\n \\n  To      ensure      that      the      conversation      history      provides  \\n \\n  factual      information,      we      use      trusted      sources      from  \\n \\n  the      Internet      to      ground      the      answer      a;      to      each      query  \\n \\n  sizes      the      trustworthy      sources      to      generate      the      answer  \\n \\n  a;,      and      these      sources      will      also      be      added      to      R      for  \\n \\n  full      article      generation      (§3.4).\\nq.      Since      q;      can      be      complicated,      we      first      prompt  \\n \\n  the      LLM      to      break      down      q;      into   \\n \\na      set      of      search  \\n \\n  queries      (Figure   \\n \\n2      (4))      and      the      searched      results      will\\n3.3      Creating      the      Article      Outline\\nline  \\n \\n  After      thoroughly      researching      the      topic      through  \\n \\n  N   \\n \\n+   \\n \\n1      simulated      conversations,      denoted      as {Co, Ci, -,; Cw }, STORM creates an outline before  \\n \\n  the      actual      writing      starts.\\nTo      fully      leverage      the      inter-  \\n \\n  nal      knowledge      of      LLMs,      we      first      prompt      the      model  \\n \\n  to      generate   \\n \\na      draft      outline      Op      given      only      the      topic  \\n \\n  t      (Figure   \\n \\n2      (7)).\\nOp      typically      provides   \\n \\na      general  \\n \\n  but      organized      framework.\\nSubsequently,      the      LLM  \\n \\n  is      prompted      with      the      topic      ¢,      the      draft      outline      Op, and the simulated conversations {Co, Cj,,Cw}  \\n \\n  to      refine      the      outline      (Figure   \\n \\n2      (8)).\\nThis      results in      an      improved      outline   \\n \\nO      which      will      be      used      for  \\n \\n  producing      the      full-length      article.\\n3.4      Writing      the      Full-Length      Article\\nicle  \\n \\n  Building      upon      the      references      R      collected      and      the  \\n \\n  outline   \\n \\nO      developed      during      the      pre-writing      stage,  \\n \\n  the      full-length      article      can      be      composed      section      by  \\n \\n  section.\\nSince      it      is      usually      impossible      to      fit      the  \\n \\n  entire   \\n \\n7      within      the      context      window      of      the      LLM,  \\n \\n  we      use      the      section      title      and      headings      of      its      all-level  \\n \\n  subsections      to      retrieve      relevant      documents      from  \\n \\n  R      based      on      semantic      similarity      calculated      from  \\n \\n  Sentence-BERT      embeddings.\\nWith      the      relevant      in-  \\n \\n  formation      at      hand,      the      LLM      is      then      prompted      to  \\n \\n  generate      the      section      with      citations.\\nOnce      all      sec-  \\n \\n  tions      are      generated,      they      are      concatenated      to      form  \\n \\n  the      full-length      article.\\nSince      the      sections      are      gen-  \\n \\n  erated      in      parallel,      we      prompt      the      LLM      with      the  \\n \\n  concatenated      article      to      delete      repeated      information  \\n \\n  to      improve      coherence.\\nFurthermore,      in      alignment  \\n \\n  with      Wikipedia’s      stylistic      norms,      the      LLM      is      also  \\n \\n  utilized      to      synthesize   \\n \\na      summary      of      the      entire      arti-  \\n \\n  cle,      forming      the      lead      section      at      the      beginning.\\n4\\n \\n   Experiments\\n4.1      Article      Selection\\ntion  \\n \\n  STORM      is      capable      of      researching      complicated      top-  \\n \\n  ics      and      writing      long      articles      from      detailed      outlines.\\n \\n \\n  However,      in      this      controlled      experiment,      we      limit  \\n \\n  the      final      output      to      at      most      4000      tokens      (roughly  \\n \\n  3000      words).\\nFor   \\n \\na      meaningful      comparison,      we  \\n \\n  Shttps://en.wikipedia.\\norg/wiki/Wikipedia:  \\n \\n  Reliable_sources  \\n \\n  randomly      select      100      samples      from      the      Fresh      Wiki  \\n \\n  dataset      (see      §2.1)      that      have      human-written      articles  \\n \\n  not      exceeding      3000      words.\\n4.2      Automatic      Metrics\\nrics  \\n \\n  As      discussed      in      §2.2,      we      evaluate      the      outline      qual-  \\n \\n  ity      to      assess      the      pre-writing      stage      by      calculating  \\n \\n  the      heading      soft      recall      and      heading      entity      recall.\\n \\n \\nA       higher      recall      score      signifies   \\n \\na      more      comprehensive  \\n \\n  outline      relative      to      the      human-written      article.\\n \\n \\n  To      assess      the      full-length      article      quality,      we      adopt  \\n \\n  ROUGE      scores      (Lin,      2004)      and      compute      the      entity  \\n \\n  recall      in      the      article      level      based      on      FLAIR      NER  \\n \\n  results.\\nMoreover,      based      on      Wikipedia      criteria’,  \\n \\n  we      evaluate      the      article      from      the      aspects      of      (1)      In-  \\n \\n  terest      Level,      (2)      Coherence      and      Organization,      (3)  \\n \\n  Relevance      and      Focus,      (4)      Coverage,      and      (5)      Verifia-  \\n \\n  bility.\\nFor      aspects      (1)-(4),      we      use      Prometheus      (Kim  \\n \\n  et      al.,      2023),   \\n \\na      13B      evaluator      LLM      to      score      the      arti-  \\n \\n  cle      based      on   \\n \\na      5-point      rubric      collaboratively      devel-  \\n \\n  oped      with      two      experienced      Wikipedia      editors      (see  \\n \\n  Appendix      C.2).\\nFor      verifiability,      we      calculate      the  \\n \\n  citation      recall      and      citation      precision      based      on      the  \\n \\n  definition      in      Gao      et      al.\\n(2023).\\nWe      use      Mistral      7B-  \\n \\n  Instruct      (Jiang      et      al.,      2023a)      to      examine      whether  \\n \\n  the      cited      passages      entail      the      generated      sentence.\\n4.3      Baselines\\nines  \\n \\n  As      prior      works      use      different      setups      and      do      not      use  \\n \\n  LLMs,      they      are      hard      to      compare      directly.\\nInstead,  \\n \\n  we      use      the      following      three      LLM-based      baselines.\\n1. Direct      Gen,   \\n \\na      baseline      that      directly      prompts  \\n \\n  the      LLM      to      generate      an      outline,      which      is      then  \\n \\n  used      to      generate      the      full-length      article.\\n2. RAG,   \\n \\na      retrieval-augmented      generation      base-  \\n \\n  line      that      searches      with      the      topic      and      uses      the  \\n \\n  searched      results      together      with      the      topic   \\n \\n¢      to  \\n \\n  generate      an      outline      or      the      entire      article.\\n3. Outline-driven      RAG      (ORAG),      which      is      iden-  \\n \\n  tical      to      RAG      in      outline      creation,      but      further  \\n \\n  searches      additional      information      with      section  \\n \\n  titles      to      generate      the      article      section      by      section.\\n4.4      STORM      Implementation\\ntion  \\n \\n  We      build      STORM      with      zero-shot      prompting      us-  \\n \\n  ing      the      DSPy      framework      (Khattab      et      al.,      2023).\\n \\n \\n  Appendix   \\n \\nB      includes      the      pseudo      code      and      corre-  \\n \\n  sponding      prompts.\\nThe      hyperparameters      N      and      MZ °https://en.wikipedia.org/wiki/Wikipedia:  \\n \\n  Good_article_criteria'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 6, 'section_title': '3.1      Perspective-Guided      Question      Asking'}, 'page_content': '3.1      Perspective-Guided      Question      Asking\\nking  \\n \\n  Rohman      (1965)      defines      pre-writing      as      the      stage  \\n \\n  of      discovery      in      the      writing      process.\\nIn      parallel  \\n \\n  with      stakeholder      theory      in      business      (Freeman      et      al.,  \\n \\n  2010),      where      diverse      stakeholders      prioritize      vary-  \\n \\n  ing      facets      of   \\n \\na      company,      individuals      with      distinct  \\n \\n  perspectives      may      concentrate      on      different      aspects  \\n \\n  when      researching      the      same      topic      and      discover      mul-  \\n \\n  tifaceted      information.\\nFurther,      the      specific      perspec-  \\n \\n  tives      can      serve      as      prior      knowledge,      guiding      individ-  \\n \\n  uals      to      ask      more      in-depth      questions.\\nFor      example,  \\n \\n  an      event      planner      might      ask      about      the      “‘transporta-  \\n \\n  tion      arrangements”      and      “budget”      for      “the      2022  \\n \\n  Winter      Olympics      opening      ceremony”,      whereas   \\n \\na       layperson      might      ask      more      general      questions      about  \\n \\n  the      event’s      basic      information      (Figure   \\n \\n1      (A)).\\n \\n \\n  Given      the      input      topic      t,      STORM      discovers      differ-  \\n \\n  ent      perspectives      by      surveying      existing      articles      from  \\n \\n  similar      topics      and      uses      these      perspectives      to      control  \\n \\n  the      question      asking      process.\\nSpecifically,      STORM  \\n \\n  prompts      an      LLM      to      generate   \\n \\na      list      of      related      top-  \\n \\n  ics      and      subsequently      extracts      the      tables      of      contents  \\n \\n  from      their      corresponding      Wikipedia      articles,      if      such  \\n \\n  articles      can      be      obtained      through      Wikipedia      API’  \\n \\n  (Figure   \\n \\n2      (1).\\nThese      tables      of      contents      are      con- catenated      to      create   \\n \\na      context      to      prompt      the      LLM to identify N perspectives P = {p1,, pn} that  \\n \\n  be      evaluated      using   \\n \\na      rule-based      filter      according      to  \\n \\n  the      Wikipedia      guideline®      to      exclude      untrustworthy  \\n \\n  sources      (Figure   \\n \\n2      (5)).\\nFinally,      the      LLM      synthe-  \\n \\n  Thttps://pypi.org/project/Wikipedia-API/  \\n \\n  can      collectively      contribute      to   \\n \\na      comprehensive      ar-  \\n \\n  ticle      on   \\n \\n¢      (Figure   \\n \\n2      (2)).\\nTo      ensure      that      the      basic  \\n \\n  information      about   \\n \\n¢      is      also      covered,      we      add      pg      as  \\n \\n  “basic      fact      writer      focusing      on      broadly      covering      the  \\n \\n  basic      facts      about      the      topic”      into      P.      Each      perspec-  \\n \\n  tive   \\n \\np   \\n \\n€   \\n \\nP      will      be      utilized      to      guide      the      LLM      in      the  \\n \\n  process      of      question      asking      in      parallel.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 7, 'section_title': '3.2      Simulating      Conversations'}, 'page_content': '3.2      Simulating      Conversations\\nions  \\n \\n  The      theory      of      questions      and      question      asking      (Ram,  \\n \\n  1991)      highlights      that      while      answers      to      existing  \\n \\n  questions      contribute      to   \\n \\na      more      comprehensive  \\n \\n  understanding      of   \\n \\na      topic,      they      often      simultane-  \\n \\n  ously      give      rise      to      new      questions.\\nTo      kick      off      this  \\n \\n  dynamic      process,      STORM      simulates   \\n \\na      conversa-  \\n \\n  tion      between   \\n \\na      Wikipedia      writer      and   \\n \\na      topic      ex-  \\n \\n  pert.\\nIn      the      z-th      round      of      the      conversation,      the  \\n \\n  LLM-powered      Wikipedia      writer      generates   \\n \\na      sin-  \\n \\n  gle      question      q;      based      on      the      topic      1,      its      assigned  \\n \\n  perspective   \\n \\np   \\n \\n€      P,      and      the      conversation      history  \\n \\n  {q1,      41,      ---,      Gi-1,      41-1}      where      a;      denotes      the      sim-  \\n \\n  ulated      expert’s      answer.\\nThe      conversation      history  \\n \\n  enables      the      LLM      to      update      its      understanding      of      the  \\n \\n  topic      and      ask      follow-up      questions.\\nIn      practice,      we  \\n \\n  limit      the      conversation      to      at      most   \\n \\n/      rounds.\\n \\n \\n  To      ensure      that      the      conversation      history      provides  \\n \\n  factual      information,      we      use      trusted      sources      from  \\n \\n  the      Internet      to      ground      the      answer      a;      to      each      query  \\n \\n  sizes      the      trustworthy      sources      to      generate      the      answer  \\n \\n  a;,      and      these      sources      will      also      be      added      to      R      for  \\n \\n  full      article      generation      (§3.4).\\nq.      Since      q;      can      be      complicated,      we      first      prompt  \\n \\n  the      LLM      to      break      down      q;      into   \\n \\na      set      of      search  \\n \\n  queries      (Figure   \\n \\n2      (4))      and      the      searched      results      will'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 8, 'section_title': '3.3      Creating      the      Article      Outline'}, 'page_content': '3.3      Creating      the      Article      Outline\\nline  \\n \\n  After      thoroughly      researching      the      topic      through  \\n \\n  N   \\n \\n+   \\n \\n1      simulated      conversations,      denoted      as {Co, Ci, -,; Cw }, STORM creates an outline before  \\n \\n  the      actual      writing      starts.\\nTo      fully      leverage      the      inter-  \\n \\n  nal      knowledge      of      LLMs,      we      first      prompt      the      model  \\n \\n  to      generate   \\n \\na      draft      outline      Op      given      only      the      topic  \\n \\n  t      (Figure   \\n \\n2      (7)).\\nOp      typically      provides   \\n \\na      general  \\n \\n  but      organized      framework.\\nSubsequently,      the      LLM  \\n \\n  is      prompted      with      the      topic      ¢,      the      draft      outline      Op, and the simulated conversations {Co, Cj,,Cw}  \\n \\n  to      refine      the      outline      (Figure   \\n \\n2      (8)).\\nThis      results in      an      improved      outline   \\n \\nO      which      will      be      used      for  \\n \\n  producing      the      full-length      article.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 9, 'section_title': '3.4      Writing      the      Full-Length      Article'}, 'page_content': '3.4      Writing      the      Full-Length      Article\\nicle  \\n \\n  Building      upon      the      references      R      collected      and      the  \\n \\n  outline   \\n \\nO      developed      during      the      pre-writing      stage,  \\n \\n  the      full-length      article      can      be      composed      section      by  \\n \\n  section.\\nSince      it      is      usually      impossible      to      fit      the  \\n \\n  entire   \\n \\n7      within      the      context      window      of      the      LLM,  \\n \\n  we      use      the      section      title      and      headings      of      its      all-level  \\n \\n  subsections      to      retrieve      relevant      documents      from  \\n \\n  R      based      on      semantic      similarity      calculated      from  \\n \\n  Sentence-BERT      embeddings.\\nWith      the      relevant      in-  \\n \\n  formation      at      hand,      the      LLM      is      then      prompted      to  \\n \\n  generate      the      section      with      citations.\\nOnce      all      sec-  \\n \\n  tions      are      generated,      they      are      concatenated      to      form  \\n \\n  the      full-length      article.\\nSince      the      sections      are      gen-  \\n \\n  erated      in      parallel,      we      prompt      the      LLM      with      the  \\n \\n  concatenated      article      to      delete      repeated      information  \\n \\n  to      improve      coherence.\\nFurthermore,      in      alignment  \\n \\n  with      Wikipedia’s      stylistic      norms,      the      LLM      is      also  \\n \\n  utilized      to      synthesize   \\n \\na      summary      of      the      entire      arti-  \\n \\n  cle,      forming      the      lead      section      at      the      beginning.\\n4\\n \\n   Experiments\\n4.1      Article      Selection\\ntion  \\n \\n  STORM      is      capable      of      researching      complicated      top-  \\n \\n  ics      and      writing      long      articles      from      detailed      outlines.\\n \\n \\n  However,      in      this      controlled      experiment,      we      limit  \\n \\n  the      final      output      to      at      most      4000      tokens      (roughly  \\n \\n  3000      words).\\nFor   \\n \\na      meaningful      comparison,      we  \\n \\n  Shttps://en.wikipedia.\\norg/wiki/Wikipedia:  \\n \\n  Reliable_sources  \\n \\n  randomly      select      100      samples      from      the      Fresh      Wiki  \\n \\n  dataset      (see      §2.1)      that      have      human-written      articles  \\n \\n  not      exceeding      3000      words.\\n4.2      Automatic      Metrics\\nrics  \\n \\n  As      discussed      in      §2.2,      we      evaluate      the      outline      qual-  \\n \\n  ity      to      assess      the      pre-writing      stage      by      calculating  \\n \\n  the      heading      soft      recall      and      heading      entity      recall.\\n \\n \\nA       higher      recall      score      signifies   \\n \\na      more      comprehensive  \\n \\n  outline      relative      to      the      human-written      article.\\n \\n \\n  To      assess      the      full-length      article      quality,      we      adopt  \\n \\n  ROUGE      scores      (Lin,      2004)      and      compute      the      entity  \\n \\n  recall      in      the      article      level      based      on      FLAIR      NER  \\n \\n  results.\\nMoreover,      based      on      Wikipedia      criteria’,  \\n \\n  we      evaluate      the      article      from      the      aspects      of      (1)      In-  \\n \\n  terest      Level,      (2)      Coherence      and      Organization,      (3)  \\n \\n  Relevance      and      Focus,      (4)      Coverage,      and      (5)      Verifia-  \\n \\n  bility.\\nFor      aspects      (1)-(4),      we      use      Prometheus      (Kim  \\n \\n  et      al.,      2023),   \\n \\na      13B      evaluator      LLM      to      score      the      arti-  \\n \\n  cle      based      on   \\n \\na      5-point      rubric      collaboratively      devel-  \\n \\n  oped      with      two      experienced      Wikipedia      editors      (see  \\n \\n  Appendix      C.2).\\nFor      verifiability,      we      calculate      the  \\n \\n  citation      recall      and      citation      precision      based      on      the  \\n \\n  definition      in      Gao      et      al.\\n(2023).\\nWe      use      Mistral      7B-  \\n \\n  Instruct      (Jiang      et      al.,      2023a)      to      examine      whether  \\n \\n  the      cited      passages      entail      the      generated      sentence.\\n4.3      Baselines\\nines  \\n \\n  As      prior      works      use      different      setups      and      do      not      use  \\n \\n  LLMs,      they      are      hard      to      compare      directly.\\nInstead,  \\n \\n  we      use      the      following      three      LLM-based      baselines.\\n1. Direct      Gen,   \\n \\na      baseline      that      directly      prompts  \\n \\n  the      LLM      to      generate      an      outline,      which      is      then  \\n \\n  used      to      generate      the      full-length      article.\\n2. RAG,   \\n \\na      retrieval-augmented      generation      base-  \\n \\n  line      that      searches      with      the      topic      and      uses      the  \\n \\n  searched      results      together      with      the      topic   \\n \\n¢      to  \\n \\n  generate      an      outline      or      the      entire      article.\\n3. Outline-driven      RAG      (ORAG),      which      is      iden-  \\n \\n  tical      to      RAG      in      outline      creation,      but      further  \\n \\n  searches      additional      information      with      section  \\n \\n  titles      to      generate      the      article      section      by      section.\\n4.4      STORM      Implementation\\ntion  \\n \\n  We      build      STORM      with      zero-shot      prompting      us-  \\n \\n  ing      the      DSPy      framework      (Khattab      et      al.,      2023).\\n \\n \\n  Appendix   \\n \\nB      includes      the      pseudo      code      and      corre-  \\n \\n  sponding      prompts.\\nThe      hyperparameters      N      and      MZ °https://en.wikipedia.org/wiki/Wikipedia:  \\n \\n  Good_article_criteria'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 10, 'section_title': '4.1      Article      Selection'}, 'page_content': '4.1      Article      Selection\\ntion  \\n \\n  STORM      is      capable      of      researching      complicated      top-  \\n \\n  ics      and      writing      long      articles      from      detailed      outlines.\\n \\n \\n  However,      in      this      controlled      experiment,      we      limit  \\n \\n  the      final      output      to      at      most      4000      tokens      (roughly  \\n \\n  3000      words).\\nFor   \\n \\na      meaningful      comparison,      we  \\n \\n  Shttps://en.wikipedia.\\norg/wiki/Wikipedia:  \\n \\n  Reliable_sources  \\n \\n  randomly      select      100      samples      from      the      Fresh      Wiki  \\n \\n  dataset      (see      §2.1)      that      have      human-written      articles  \\n \\n  not      exceeding      3000      words.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 11, 'section_title': '4.2      Automatic      Metrics'}, 'page_content': '4.2      Automatic      Metrics\\nrics  \\n \\n  As      discussed      in      §2.2,      we      evaluate      the      outline      qual-  \\n \\n  ity      to      assess      the      pre-writing      stage      by      calculating  \\n \\n  the      heading      soft      recall      and      heading      entity      recall.\\n \\n \\nA       higher      recall      score      signifies   \\n \\na      more      comprehensive  \\n \\n  outline      relative      to      the      human-written      article.\\n \\n \\n  To      assess      the      full-length      article      quality,      we      adopt  \\n \\n  ROUGE      scores      (Lin,      2004)      and      compute      the      entity  \\n \\n  recall      in      the      article      level      based      on      FLAIR      NER  \\n \\n  results.\\nMoreover,      based      on      Wikipedia      criteria’,  \\n \\n  we      evaluate      the      article      from      the      aspects      of      (1)      In-  \\n \\n  terest      Level,      (2)      Coherence      and      Organization,      (3)  \\n \\n  Relevance      and      Focus,      (4)      Coverage,      and      (5)      Verifia-  \\n \\n  bility.\\nFor      aspects      (1)-(4),      we      use      Prometheus      (Kim  \\n \\n  et      al.,      2023),   \\n \\na      13B      evaluator      LLM      to      score      the      arti-  \\n \\n  cle      based      on   \\n \\na      5-point      rubric      collaboratively      devel-  \\n \\n  oped      with      two      experienced      Wikipedia      editors      (see  \\n \\n  Appendix      C.2).\\nFor      verifiability,      we      calculate      the  \\n \\n  citation      recall      and      citation      precision      based      on      the  \\n \\n  definition      in      Gao      et      al.\\n(2023).\\nWe      use      Mistral      7B-  \\n \\n  Instruct      (Jiang      et      al.,      2023a)      to      examine      whether  \\n \\n  the      cited      passages      entail      the      generated      sentence.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 12, 'section_title': '4.3      Baselines'}, 'page_content': '4.3      Baselines\\nines  \\n \\n  As      prior      works      use      different      setups      and      do      not      use  \\n \\n  LLMs,      they      are      hard      to      compare      directly.\\nInstead,  \\n \\n  we      use      the      following      three      LLM-based      baselines.\\n1. Direct      Gen,   \\n \\na      baseline      that      directly      prompts  \\n \\n  the      LLM      to      generate      an      outline,      which      is      then  \\n \\n  used      to      generate      the      full-length      article.\\n2. RAG,   \\n \\na      retrieval-augmented      generation      base-  \\n \\n  line      that      searches      with      the      topic      and      uses      the  \\n \\n  searched      results      together      with      the      topic   \\n \\n¢      to  \\n \\n  generate      an      outline      or      the      entire      article.\\n3. Outline-driven      RAG      (ORAG),      which      is      iden-  \\n \\n  tical      to      RAG      in      outline      creation,      but      further  \\n \\n  searches      additional      information      with      section  \\n \\n  titles      to      generate      the      article      section      by      section.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 13, 'section_title': '4.4      STORM      Implementation'}, 'page_content': '4.4      STORM      Implementation\\ntion  \\n \\n  We      build      STORM      with      zero-shot      prompting      us-  \\n \\n  ing      the      DSPy      framework      (Khattab      et      al.,      2023).\\n \\n \\n  Appendix   \\n \\nB      includes      the      pseudo      code      and      corre-  \\n \\n  sponding      prompts.\\nThe      hyperparameters      N      and      MZ °https://en.wikipedia.org/wiki/Wikipedia:  \\n \\n  Good_article_criteria'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 14, 'section_title': 'Comparsion      with      Human-written      Articles      Rubric      Grading'}, 'page_content': 'Comparsion      with      Human-written      Articles      Rubric      Grading'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 15, 'section_title': 'ROUGE-1      ROUGE-L      Entity      Recall      Interest      Level      Organization      Relevance      Coverage'}, 'page_content': 'ROUGE-1      ROUGE-L      Entity      Recall      Interest      Level      Organization      Relevance      Coverage\\n \\n \\n  Direct      Gen      25.62      12.63      5.08      2.87      4.60      3.10      4.16  \\n \\n  RAG      28.52      13.18      7.57      3.14      4.22      3.05      4.08  \\n \\n  oRAG      44.26      16.51      12.57      3.90      4.79      4.09      4.70 STORM      45.82      16.70      14.107      3.997      4.82      4.457      4.887  \\n \\n  w/o      Outline      Stage      26.77      12.77      7.39      3.33      4.87      3.35      4.37'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 16, 'section_title': 'Heading      Heading       Soft      Recall      Entity      Recall'}, 'page_content': 'Heading      Heading       Soft      Recall      Entity      Recall\\n \\n \\n  Direct      Gen      80.23      32.39  \\n \\n  RAG/oRAG      73.59      33.85  \\n \\n  GPT-3.5   \\n \\n=      RAG-expand      74.40      33.85  \\n \\n  STORM      86.267      40.527  \\n \\n  w/o      Perspective      84.49      40.12  \\n \\n  w/o      Conversation      77.97      31.98  \\n \\n  Direct      Gen      87.66      34.78  \\n \\n  RAG/oRAG      89.55      42.38  \\n \\n  GPT-4      RAG-expand      91.36      43.53  \\n \\n  STORM      92.737      45.91  \\n \\n  w/o      Perspective      92.39      42.70  \\n \\n  w/o      Conversation      88.75      39.30  \\n \\n  Table      3:      Results      of      outline      quality      evaluation      (%).\\n \\n \\n+      de-  \\n \\n  notes      significant      differences      (p   \\n \\n<      0.05)      from   \\n \\na      paired  \\n \\n  t-test      between      STORM      and      baselines.\\n \\n \\n  in      STORM      are      both      set      as      5.      We      use      the      chat  \\n \\n  model      gpt-3.5-turbo      for      question      asking      and  \\n \\n  use      gpt-3.5-turbo-instruct      for      other      parts      of  \\n \\n  STORM.\\nWe      also      experiment      with      using      gpt-4      for  \\n \\n  drafting      and      refining      the      outline      (Figure   \\n \\n2      ()8)).\\nFor      reported      results,      the      simulated      topic      expert      in  \\n \\n  STORM      is      grounded      on      the      You.com      search      API!°,  \\n \\n  although      the      proposed      pipeline      is      compatible      with  \\n \\n  other      search      engines.\\nThe      ground      truth      Wikipedia  \\n \\n  article      is      excluded      from      the      search      results.\\n \\n \\n  For      final      article      generation,      we      only      report      the  \\n \\n  results      using      gpt-4      as      gpt-3.5      is      not      faithful      to  \\n \\n  sources      when      generating      text      with      citations      (Gao  \\n \\n  et      al.,      2023).\\nWe      set      temperature      as      1.0      and      top_p  \\n \\n  as      0.9      for      all      experiments.\\n5\\n \\n   Results      and      Analysis\\n5.1      Main      Results\\nults  \\n \\n  We      use      outline      coverage      as   \\n \\na      proxy      to      assess      the      pre-  \\n \\n  writing      stage      (see      §2.2).\\nTable   \\n \\n3      shows      the      heading  \\n \\n  soft      recall      and      entity      recall.\\nOutlines      directly      gen-  \\n \\n  erated      by      LLMs      (Direct      Gen)      already      demonstrate  \\n \\n  https:      //documentation.\\nyou.\\ncom/api-reference/  \\n \\n  search  \\n \\n  high      heading      soft      recall,      indicating      LLMs’      ability  \\n \\n  to      grasp      high-level      aspects      of   \\n \\na      topic      through      their  \\n \\n  rich      parametric      knowledge.\\nHowever,      STORM,      by  \\n \\n  asking      effective      questions      to      research      the      topic,      can  \\n \\n  create      higher      recall      outlines      that      cover      more      topic-  \\n \\n  specific      aspects.\\nNotably,      although      RAG      leverages  \\n \\n  additional      information,      presenting      unorganized      in-  \\n \\n  formation      in      the      context      window      makes      outline  \\n \\n  generation      more      challenging      for      the      weaker      model,  \\n \\n  i.e.,      GPT-3.5,      leading      to      worse      performance.\\nTo      test  \\n \\n  the      limit      of      the      RAG      baseline,      we      further      expand  \\n \\n  the      retrieved      sources      by      starting      with      the      outline  \\n \\n  produced      by      RAG,      using      its      section      titles      as      search  \\n \\n  queries      to      collect      more      sources,      and      inputting      the  \\n \\n  newly      collected      sources      together      with      the      initial  \\n \\n  outline      to      LLM      to      generate   \\n \\na      polished      outline.\\nThis  \\n \\n  modified      approach      is      referred      to      as      “RAG-expand”  \\n \\n  in      Table      3.\\nThe      experiment      results      indicate      that  \\n \\n  even      though      having      an      additional      round      of      search  \\n \\n  and      refinement      can      improve      the      outline      produced  \\n \\n  by      RAG,      our      proposed      STORM      still      surpasses      its  \\n \\n  performance.\\n \\n \\n  We      further      evaluate      the      full-length      article      quality.\\n \\n \\n  As      shown      in      Table      2,      oRAG      significantly      outper-  \\n \\n  forms      RAG,      highlighting      the      effectiveness      of      using  \\n \\n  outlines      for      structuring      full-length      article      genera-  \\n \\n  tion.\\nDespite      this      method’s      advantages      in      leverag-  \\n \\n  ing      retrieval      and      outlining,      our      approach      still      out-  \\n \\n  performs      it.\\nThe      effective      question      asking      mecha-  \\n \\n  nism      enhances      the      articles      with      greater      entity      recall.\\n \\n \\n  The      evaluator      LLM      also      rates      these      articles      with      sig-  \\n \\n  nificantly      higher      scores      in      the      aspects      of      “Interest  \\n \\n  Level’,      “Relevance      and      Focus’,      and      “Coverage”.\\n \\n \\n  Nonetheless,      we      acknowledge      the      possibility      of  \\n \\n  the      evaluator      LLM      overrating      machine-generated  \\n \\n  text.\\nOur      careful      human      evaluation      (§6)      reveals  \\n \\n  that      STORM      still      has      much      room      for      improvement.\\n \\n \\n  Although      this      work      primarily      focuses      on      the      pre-  \\n \\n  writing      stage      and      does      not      optimize      generating      text  \\n \\n  with      citations,      we      still      examine      the      citation      quality  \\n \\n  of      articles      produced      by      our      approach.\\nAs      reported Citation      Recall Citation      Precision oRAG      STORM      value  \\n \\n  Avg.      >4Rates      Av.g.\\n \\n \\n>   \\n \\n4      Rates      peval Table      4:      Citation      quality      judged      by      Mistral      7B-Instruct.\\n84.83 85.18  \\n \\n  STORM\\n |  |       STORM      _      w/o      Perspective       w/o      Conversation | \\n | --- | --- | ---\\n |       IR|      99.83      54.36 |       39.56 |       Interest      Level      3.63      57.5%      4.03      70.0%      0.077       Organization      3.25      45.0%      4.00      70.0%      0.005       Relevance      3.93      62.5%      4.15      65.0%      0.347       Coverage      3.58      57.5%      4.00      67.5%      0.084       Verifiability      3.85      67.5%      3.80      67.5%      0.843       #Preferred      14      26\\n | Table      5:      Average      number      of      unique      references      (|R|)       collected      using      different      methods.\\n |       in      Table      4,      Mistral      7B-Instruct      judges      84.83%      of       the      sentences      are      supported      by      their      citations.      Ap-       pendix      C.3      investigates      the      unsupported      sentences       and      reveals      that      the      primary      issues      stem      from      draw-       ing      improper      inferences      and      inaccurate      paraphras-       ing,      rather      than      hallucinating      non-existent      contents.\\n | 5.2      Ablation      Studies\\n | 5.2      Ablation      Studies\\n |       As      introduced      in      §3,      STORM      prompts      LLMs      to       ask      effective      questions      by      discovering      specific       perspectives      and      simulating      multi-turn      conversa-       tions.      We      conduct      the      ablation      study      on      outline       creation      by      comparing      STORM      with      two      variants:\\n | (1)      “STORM      w/o      Perspective”,      which      omits      per-       spective      in      the      question      generation      prompt;      (2)       “STORM      w/o      Conversation”,      which      prompts      LLMs       to      generate      a      set      number      of      questions      altogether.      To       ensure      a      fair      comparison,      we      control      an      equal      total       number      of      generated      questions      across      all      variants.       Table      3      shows      the      ablation      results      and      full      STORM       pipeline      produces      outlines      with      the      highest      recall.       Also,      “STORM      w/o      Conversation”      gives      much       worse      results,      indicating      reading      relevant      informa-       tion      is      crucial      to      generating      effective      questions.      We       further      examine      how      many      unique      sources      are      col-       lected      in      ?      via      different      variants.      As      shown      in      Ta-       ble      5,      the      full      pipeline      discovers      more      different       sources      and      the      trend      is      in      accord      with      the      auto-       matic      metrics      for      outline      quality.       We      also      verify      whether      having      an      outline      stage       is      necessary      with      STORM.      In      Table      2,      “STORM       w/o      Outline      Stage”      denotes      the      results      of      generat-       ing      the      entire      article      given      the      topic      and      the      sim-       ulated      conversations.      Removing      the      outline      stage       significantly      deteriorates      the      performance      across       all      metrics.\\n | 6      Human      Evaluation\\n |       To      better      understand      the      strengths      and      weaknesses       of      STORM,      we      conduct      human      evaluation      by      col-       laborating      with      10      experienced      Wikipedia      editors       Table      6:      Human      evaluation      results      on      20      pairs      of      articles       generated      by      STORM      and      oRAG.      Each      pair      of      articles       is      evaluated      by      two      Wikipedia      editors.      The      ratings      are       given      on      a      scale      between      |      and      7,      with      values      >      4       indicating      good      quality      (see      Table      10).      We      conduct       paired      t-test      and      report      the      p-value.\\n |       who      have      made      at      least      500      edits      on      Wikipedia      and       have      more      than      |      year      of      experience.      We      randomly       sample      20      topics      from      our      dataset      and      evaluate      the       articles      generated      by      our      method      and      oRAG,      the       best      baseline      according      to      the      automatic      evaluation.\\n |       Each      pair      of      articles      is      assigned      to      2      editors.\\n |       We      request      editors      to      judge      each      article      from      the       same      five      aspects      defined      in      $4.2,      but      using      a      |      to       7      scale      for      more      fine-grained      evaluation.      While       our      automatic      evaluation      uses      citation      quality      as       a      proxy      to      evaluate      Verifiability,      we      stick      to      the       Wikipedia      standard      of      “verifiable      with      no      original       research”      in      human      evaluation.      Besides      rating      the       articles,      editors      are      asked      to      provide      open-ended       feedback      and      pairwise      preference.      After      the      evalua-       tion      finishes,      they      are      further      requested      to      compare       an      article      produced      by      our      method,      which      they      have       just      reviewed,      with      its      human-written      counterpart,       and      report      their      perceived      usefulness      of      STORM       using      a      1-5      Likert      scale.      More      human      evaluation      de-       tails      are      included      in      Appendix      D.      Table      6      presents       the      rating      and      pairwise      comparison      results.!!\\n |       Articles      produced      by      STORM      exhibit      greater       breadth      and      depth      than      oRAG      outputs.      In      ac-       cord      with      the      finding      in      §5.1,      editors      judge      articles       produced      by      STORM      as      more      interesting,      orga-       nized,      and      having      broader      coverage      compared      to       oRAG      outputs.      Specifically,      25%      more      articles      pro-       duced      by      STORM      are      considered      organized      (Orga-       nization      rating      >      4),      and      10%      more      are      deemed      to       have      good      coverage      (Coverage      rating      >      4).      Even       in      comparison      with      human-written      articles,      one       editor      praises      our      result      as      providing      “a      bit      more\\n |       \"For      the      1-7      scale      rating      results      on      each      criterion,      we      cal-       culate      the      Krippendorff’s      Alpha      to      measure      the      inter      annotator       agreement      (IAA),      and      the      results      are      as      follows:      Interest      Level       (0.349),      Organization      (0.221),      Relevance      (0.256),      Coverage       (0.346),      Verifiability      (0.388).\\n'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 17, 'section_title': '5.1      Main      Results'}, 'page_content': '5.1      Main      Results\\nults  \\n \\n  We      use      outline      coverage      as   \\n \\na      proxy      to      assess      the      pre-  \\n \\n  writing      stage      (see      §2.2).\\nTable   \\n \\n3      shows      the      heading  \\n \\n  soft      recall      and      entity      recall.\\nOutlines      directly      gen-  \\n \\n  erated      by      LLMs      (Direct      Gen)      already      demonstrate  \\n \\n  https:      //documentation.\\nyou.\\ncom/api-reference/  \\n \\n  search  \\n \\n  high      heading      soft      recall,      indicating      LLMs’      ability  \\n \\n  to      grasp      high-level      aspects      of   \\n \\na      topic      through      their  \\n \\n  rich      parametric      knowledge.\\nHowever,      STORM,      by  \\n \\n  asking      effective      questions      to      research      the      topic,      can  \\n \\n  create      higher      recall      outlines      that      cover      more      topic-  \\n \\n  specific      aspects.\\nNotably,      although      RAG      leverages  \\n \\n  additional      information,      presenting      unorganized      in-  \\n \\n  formation      in      the      context      window      makes      outline  \\n \\n  generation      more      challenging      for      the      weaker      model,  \\n \\n  i.e.,      GPT-3.5,      leading      to      worse      performance.\\nTo      test  \\n \\n  the      limit      of      the      RAG      baseline,      we      further      expand  \\n \\n  the      retrieved      sources      by      starting      with      the      outline  \\n \\n  produced      by      RAG,      using      its      section      titles      as      search  \\n \\n  queries      to      collect      more      sources,      and      inputting      the  \\n \\n  newly      collected      sources      together      with      the      initial  \\n \\n  outline      to      LLM      to      generate   \\n \\na      polished      outline.\\nThis  \\n \\n  modified      approach      is      referred      to      as      “RAG-expand”  \\n \\n  in      Table      3.\\nThe      experiment      results      indicate      that  \\n \\n  even      though      having      an      additional      round      of      search  \\n \\n  and      refinement      can      improve      the      outline      produced  \\n \\n  by      RAG,      our      proposed      STORM      still      surpasses      its  \\n \\n  performance.\\n \\n \\n  We      further      evaluate      the      full-length      article      quality.\\n \\n \\n  As      shown      in      Table      2,      oRAG      significantly      outper-  \\n \\n  forms      RAG,      highlighting      the      effectiveness      of      using  \\n \\n  outlines      for      structuring      full-length      article      genera-  \\n \\n  tion.\\nDespite      this      method’s      advantages      in      leverag-  \\n \\n  ing      retrieval      and      outlining,      our      approach      still      out-  \\n \\n  performs      it.\\nThe      effective      question      asking      mecha-  \\n \\n  nism      enhances      the      articles      with      greater      entity      recall.\\n \\n \\n  The      evaluator      LLM      also      rates      these      articles      with      sig-  \\n \\n  nificantly      higher      scores      in      the      aspects      of      “Interest  \\n \\n  Level’,      “Relevance      and      Focus’,      and      “Coverage”.\\n \\n \\n  Nonetheless,      we      acknowledge      the      possibility      of  \\n \\n  the      evaluator      LLM      overrating      machine-generated  \\n \\n  text.\\nOur      careful      human      evaluation      (§6)      reveals  \\n \\n  that      STORM      still      has      much      room      for      improvement.\\n \\n \\n  Although      this      work      primarily      focuses      on      the      pre-  \\n \\n  writing      stage      and      does      not      optimize      generating      text  \\n \\n  with      citations,      we      still      examine      the      citation      quality  \\n \\n  of      articles      produced      by      our      approach.\\nAs      reported Citation      Recall Citation      Precision oRAG      STORM      value  \\n \\n  Avg.      >4Rates      Av.g.\\n \\n \\n>   \\n \\n4      Rates      peval Table      4:      Citation      quality      judged      by      Mistral      7B-Instruct.\\n84.83 85.18  \\n \\n  STORM\\n |  |       STORM      _      w/o      Perspective       w/o      Conversation | \\n | --- | --- | ---\\n |       IR|      99.83      54.36 |       39.56 |       Interest      Level      3.63      57.5%      4.03      70.0%      0.077       Organization      3.25      45.0%      4.00      70.0%      0.005       Relevance      3.93      62.5%      4.15      65.0%      0.347       Coverage      3.58      57.5%      4.00      67.5%      0.084       Verifiability      3.85      67.5%      3.80      67.5%      0.843       #Preferred      14      26\\n | Table      5:      Average      number      of      unique      references      (|R|)       collected      using      different      methods.\\n |       in      Table      4,      Mistral      7B-Instruct      judges      84.83%      of       the      sentences      are      supported      by      their      citations.      Ap-       pendix      C.3      investigates      the      unsupported      sentences       and      reveals      that      the      primary      issues      stem      from      draw-       ing      improper      inferences      and      inaccurate      paraphras-       ing,      rather      than      hallucinating      non-existent      contents.\\n | 5.2      Ablation      Studies\\n | 5.2      Ablation      Studies\\n |       As      introduced      in      §3,      STORM      prompts      LLMs      to       ask      effective      questions      by      discovering      specific       perspectives      and      simulating      multi-turn      conversa-       tions.      We      conduct      the      ablation      study      on      outline       creation      by      comparing      STORM      with      two      variants:\\n | (1)      “STORM      w/o      Perspective”,      which      omits      per-       spective      in      the      question      generation      prompt;      (2)       “STORM      w/o      Conversation”,      which      prompts      LLMs       to      generate      a      set      number      of      questions      altogether.      To       ensure      a      fair      comparison,      we      control      an      equal      total       number      of      generated      questions      across      all      variants.       Table      3      shows      the      ablation      results      and      full      STORM       pipeline      produces      outlines      with      the      highest      recall.       Also,      “STORM      w/o      Conversation”      gives      much       worse      results,      indicating      reading      relevant      informa-       tion      is      crucial      to      generating      effective      questions.      We       further      examine      how      many      unique      sources      are      col-       lected      in      ?      via      different      variants.      As      shown      in      Ta-       ble      5,      the      full      pipeline      discovers      more      different       sources      and      the      trend      is      in      accord      with      the      auto-       matic      metrics      for      outline      quality.       We      also      verify      whether      having      an      outline      stage       is      necessary      with      STORM.      In      Table      2,      “STORM       w/o      Outline      Stage”      denotes      the      results      of      generat-       ing      the      entire      article      given      the      topic      and      the      sim-       ulated      conversations.      Removing      the      outline      stage       significantly      deteriorates      the      performance      across       all      metrics.\\n | 6      Human      Evaluation\\n |       To      better      understand      the      strengths      and      weaknesses       of      STORM,      we      conduct      human      evaluation      by      col-       laborating      with      10      experienced      Wikipedia      editors       Table      6:      Human      evaluation      results      on      20      pairs      of      articles       generated      by      STORM      and      oRAG.      Each      pair      of      articles       is      evaluated      by      two      Wikipedia      editors.      The      ratings      are       given      on      a      scale      between      |      and      7,      with      values      >      4       indicating      good      quality      (see      Table      10).      We      conduct       paired      t-test      and      report      the      p-value.\\n |       who      have      made      at      least      500      edits      on      Wikipedia      and       have      more      than      |      year      of      experience.      We      randomly       sample      20      topics      from      our      dataset      and      evaluate      the       articles      generated      by      our      method      and      oRAG,      the       best      baseline      according      to      the      automatic      evaluation.\\n |       Each      pair      of      articles      is      assigned      to      2      editors.\\n |       We      request      editors      to      judge      each      article      from      the       same      five      aspects      defined      in      $4.2,      but      using      a      |      to       7      scale      for      more      fine-grained      evaluation.      While       our      automatic      evaluation      uses      citation      quality      as       a      proxy      to      evaluate      Verifiability,      we      stick      to      the       Wikipedia      standard      of      “verifiable      with      no      original       research”      in      human      evaluation.      Besides      rating      the       articles,      editors      are      asked      to      provide      open-ended       feedback      and      pairwise      preference.      After      the      evalua-       tion      finishes,      they      are      further      requested      to      compare       an      article      produced      by      our      method,      which      they      have       just      reviewed,      with      its      human-written      counterpart,       and      report      their      perceived      usefulness      of      STORM       using      a      1-5      Likert      scale.      More      human      evaluation      de-       tails      are      included      in      Appendix      D.      Table      6      presents       the      rating      and      pairwise      comparison      results.!!\\n |       Articles      produced      by      STORM      exhibit      greater       breadth      and      depth      than      oRAG      outputs.      In      ac-       cord      with      the      finding      in      §5.1,      editors      judge      articles       produced      by      STORM      as      more      interesting,      orga-       nized,      and      having      broader      coverage      compared      to       oRAG      outputs.      Specifically,      25%      more      articles      pro-       duced      by      STORM      are      considered      organized      (Orga-       nization      rating      >      4),      and      10%      more      are      deemed      to       have      good      coverage      (Coverage      rating      >      4).      Even       in      comparison      with      human-written      articles,      one       editor      praises      our      result      as      providing      “a      bit      more\\n |       \"For      the      1-7      scale      rating      results      on      each      criterion,      we      cal-       culate      the      Krippendorff’s      Alpha      to      measure      the      inter      annotator       agreement      (IAA),      and      the      results      are      as      follows:      Interest      Level       (0.349),      Organization      (0.221),      Relevance      (0.256),      Coverage       (0.346),      Verifiability      (0.388).\\n'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 18, 'section_title': 'Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree'}, 'page_content': 'Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree\\n \\n \\n \\nI      think      it      can      be      specifically      helpful  \\n \\n  wae      70%      30%  \\n \\n  for      my      pre-writing      stage.\\nI\\n \\n   think      it      will      help      me      edit   \\n \\na      Wikipedia      anes   \\n \\n3      oars      30%  \\n \\n  article      for   \\n \\na      new      topic.\\n \\n \\n= I      think      it      can      be   \\n \\na      potentially      useful      10%      20%   \\n \\n:      60%      10%  \\n \\n  tool      for      the      Wikipedia      community.\\nFigure      3:      Survey      results      of      the      perceived      usefulness      of  \\n \\n  STORM      (n   \\n \\n=      10).\\n \\n \\n  background      information”      and      another      notes      that      “I  \\n \\n  found      that      the      AI      articles      had      more      depth      compared  \\n \\n  to      the      Wikipedia      articles”.\\nSTORM      also      outper-  \\n \\n  forms      the      best      baseline      in      pairwise      comparison.\\n \\n \\n  More      information      in      |R|      poses      challenges      be-  \\n \\n  yond      factual      hallucination.\\nWe      examine      14      pair-  \\n \\n  wise      comparison      responses      where      editors      prefer  \\n \\n  oORAG      outputs      over      STORM.\\nExcluding   \\n \\n3      cases  \\n \\n  where      pairwise      preferences      do      not      align      with      their  \\n \\n  ratings,      editors      assign      lower      Verifiability      scores      to  \\n \\n  articles      from      our      approach      in      over      50%      of      the      cases.\\n \\n \\n  Through      analyzing      the      articles      and      editors’      free-  \\n \\n  form      feedback,      we      discover      that      low      Verifiability  \\n \\n  scores      stem      from      red      herring      fallacy      or      overspec-  \\n \\n  ulation      issues.\\nThese      arise      when      the      generated  \\n \\n  articles      introduce      unverifiable      connections      between  \\n \\n  different      pieces      of      information      in      |7?|      or      between  \\n \\n  the      information      and      the      topic      (examples      included  \\n \\n  in      Table      11).\\nCompared      to      the      widely      discussed  \\n \\n  factual      hallucination      (Shuster      et      al.,      2021;      Huang  \\n \\n  et      al.,      2023),      addressing      such      verifiability      issues      is  \\n \\n  more      nuanced,      surpassing      basic      fact-checking      (Min  \\n \\n  et      al.,      2023).\\n \\n \\n  Generated      articles      trail      behind      well-revised      hu-  \\n \\n  man      works.\\nWhile      STORM      outperforms      the  \\n \\n  oRAG      baseline,      editors      comment      that      the      generated  \\n \\n  articles      are      less      informative      than      actual      Wikipedia  \\n \\n  pages.\\nAnother      major      issue      identified      is      the      trans-  \\n \\n  fer      of      bias      and      tone      from      Internet      sources      to      the  \\n \\n  generated      article,      with   \\n \\n7      out      of      10      editors      men-  \\n \\n  tioning      that      the      STORM-generated      articles      sound  \\n \\n  “emotional”      or      “unneutral”.\\nMore      analysis      is      dis-  \\n \\n  cussed      in      Appendix      E.      This      feedback      suggests      that  \\n \\n  reducing      the      retrieval      bias      in      the      pre-writing      stage  \\n \\n  is   \\n \\na      worthwhile      direction      for      future      work.\\n \\n \\n  Generated      articles      are   \\n \\na      good      starting      point.\\nAs  \\n \\n  shown      in      Figure      3,      editors      are      unanimous      in      agree-  \\n \\n  ing      that      STORM      can      aid      them      in      their      pre-writing  \\n \\n  stage.\\nIt      is      gratifying      to      know      that      the      tool      is      help-  \\n \\n  ful      to      experienced      editors.\\n80%      of      the      editors      think  \\n \\n  that      STORM      can      help      them      edit   \\n \\na      Wikipedia      article  \\n \\n  for   \\n \\na      new      topic.\\nMore      reservation      is      expressed      to  \\n \\n  the      usefulness      of      STORM      for      the      Wikipedia      com-  \\n \\n  munity      at      large;      nonetheless,      70%      of      the      editors  \\n \\n  think      it      is      useful,      with      only      10%      disagreeing.\\n7\\n \\n   Related      Works  \\n \\n  Retrieval-Augmented      Generation      (RAG)      Aug-  \\n \\n  menting      language      models      (LMs)      with      retrieval      at  \\n \\n  inference      time      is   \\n \\na      typical      way      to      leverage      exter-  \\n \\n  nal      knowledge      stores      (Ram      et      al.,      2023;      Izacard  \\n \\n  et      al.,      2023).\\nWhile      some      works      use      retrieval  \\n \\n  to      construct      demonstrations      for      in-context      learn-  \\n \\n  ing      (Li      et      al.,      2023;      Liu      et      al.,      2022;      Agrawal      et      al.,  \\n \\n  2023;      Poesia      et      al.,      2022;      Shi      et      al.,      2022;      Khattab  \\n \\n  et      al.,      2022),      another      line      of      works      uses      retrieval      to  \\n \\n  provide      additional      information      for      LMs      to      ground  \\n \\n  on.\\nLewis      et      al.\\n(2020)      study      RAG      on      knowledge-  \\n \\n  intensive      NLP      tasks      and      find      it      improves      diver-  \\n \\n  sity      and      factuality.\\nSemnani      et      al.\\n(2023)      de-  \\n \\n  signs   \\n \\na      RAG-based      chatbot      grounded      on      English  \\n \\n  Wikipedia      to      stop      LLM-based      chatbots      from      hal-  \\n \\n  lucination.\\nBesides,      RAG      can      be      used      to      generate  \\n \\n  text      with      citations      (Menick      et      al.,      2022;      Gao      et      al.,  \\n \\n  2023)      and      build      attributed      question      answering      sys-  \\n \\n  tems      (Bohnet      et      al.,      2023).\\nWhile      RAG      is      widely  \\n \\n  studied      in      question      answering,      how      to      use      it      for  \\n \\n  long-form      article      generation      is      less      investigated.\\n \\n \\n  As   \\n \\na      general      framework,      RAG      is      flexible      in      both  \\n \\n  the      retrieval      source      and      time.\\nThe      retrieval      sources  \\n \\n  can      vary      from      domain      databases      (Zakka      et      al.,  \\n \\n  2023),      code      documentation      (Zhou      et      al.,      2023),  \\n \\n  to      the      whole      Internet      (Nakano      et      al.,      2022;      Komeili  \\n \\n  et      al.,      2022).\\nRegarding      the      time,      besides   \\n \\na      one-  \\n \\n  time      retrieval      before      generation,      the      system      can      be  \\n \\n  designed      to      self-decide      when      to      retrieve      across      the  \\n \\n  course      of      the      generation      (Jiang      et      al.,      2023b;      Parisi  \\n \\n  et      al.,      2022;      Shuster      et      al.,      2022;      Yao      et      al.,      2023).\\n \\n \\n  Automatic      Expository      Writing      Different      from  \\n \\n  other      types      of      long-form      generation      (Yang      et      al.,  \\n \\n  2022;      Feng      et      al.,      2018),      automatic      expository      writ-  \\n \\n  ing      requires      grounding      on      external      documents      and  \\n \\n  leveraging      the      interplay      between      reading      and      writ-  \\n \\n  ing.\\nBalepur      et      al.\\n(2023)      propose      the      Imitate-  \\n \\n  Retrieve-Paraphrase      framework      for      expository      writ-  \\n \\n  ing      at      the      paragraph      level      to      address      the      challenges  \\n \\n  in      synthesizing      information      from      multiple      sources.\\n \\n \\n  Beyond      summarizing      sources,      Shen      et      al.\\n(2023)  \\n \\n  highlight      that      expository      writing      requires      the      au-  \\n \\n  thor’s      sensemaking      process      over      source      documents  \\n \\n  and      good      outline      planning.\\nWe      tackle      these      chal-  \\n \\n  lenges      by      focusing      on      the      pre-writing      stage.\\n \\n \\n  Question      Asking      in      NLP      Question      asking      capa-  \\n \\n  bilities      in      NLP      systems      have      expanded      across      sev-  \\n \\n  eral      fronts,      including      generating      clarification      ques-  \\n \\n  tions      to      understand      user      intents      (Aliannejadi      et      al.,  \\n \\n  2019;      Rahmani      et      al.,      2023),      and      breaking      large  \\n \\n  questions      into      smaller      ones      to      improve      composi-  \\n \\n  tional      reasoning      (Press      et      al.,      2023).\\nWhile      humans  \\n \\n  usually      ask      questions      to      learn      new      knowledge      (Taw-  \\n \\n  fik      et      al.,      2020;      Booth      et      al.,      2003),      how      to      opti-  \\n \\n  mize      question      informativeness      and      specificity      in  \\n \\n  information-seeking      conversations      remains      less      ex-  \\n \\n  plored.\\nThe      closest      work      is      Qi      et      al.\\n(2020)      which  \\n \\n  defines      the      question      informativeness      using      the      un-  \\n \\n  igram      precision      function      and      uses      reinforcement  \\n \\n  learning      to      increase      the      question      informativeness.\\n8\\n \\n   Conclusion  \\n \\n  We      propose      STORM,      an      LLM-based      writing      sys-  \\n \\n  tem      that      automates      the      pre-writing      stage      for      creat-  \\n \\n  ing      Wikipedia-like      articles      from      scratch.\\nWe      cu-  \\n \\n  rate      the      FreshWiki      dataset      and      establish      evaluation  \\n \\n  criteria      to      study      the      generation      of      grounded      long-  \\n \\n  form      articles.\\nExperimental      results      demonstrate  \\n \\n  that      the      question      asking      mechanism      in      STORM  \\n \\n  improves      both      the      outline      and      article      quality.\\nWith  \\n \\n  the      improved      breadth      and      depth,      STORM      helps  \\n \\n  surface      new      challenges      for      grounded      writing      sys-  \\n \\n  tems      through      expert      evaluation.\\nThe      experienced  \\n \\n  Wikipedia      editors      in      our      study      unanimously      agree  \\n \\n  that      STORM      is      helpful      for      their      pre-writing      stage.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 19, 'section_title': 'Limitations'}, 'page_content': 'Limitations\\n \\n \\n  In      this      work,      we      explore      generating      Wikipedia-  \\n \\n  like      articles      from      scratch      as   \\n \\na      way      to      push      the  \\n \\n  frontier      of      automatic      expository      writing      and      long-  \\n \\n  form      article      generation.\\nWhile      our      approach      sig-  \\n \\n  nificantly      outperforms      baseline      methods      in      both  \\n \\n  automatic      and      human      evaluations,      the      quality      of  \\n \\n  machine-written      articles      still      lags      behind      well-  \\n \\n  revised      human-authored      articles,      specifically      in  \\n \\n  aspects      of      neutrality      and      verifiability.\\nAlthough  \\n \\n  STORM      discovers      different      perspectives      in      re-  \\n \\n  searching      the      given      topic,      the      collected      information  \\n \\n  may      still      be      biased      towards      dominant      sources      on  \\n \\n  the      Internet      and      may      contain      promotional      content.\\n \\n \\n  Moreover,      the      verifiability      issues      identified      in      this  \\n \\n  work      go      beyond      factual      hallucination,      which      high-  \\n \\n  lights      new      challenges      to      grounded      writing      systems.\\n \\n \\n  Another      limitation      of      this      work      is      that      although  \\n \\n  we      focus      on      the      task      of      generating      Wikipedia-like  \\n \\n  articles      from      scratch,      our      task      setup      is      still      simpli-  \\n \\n  fied      to      only      consider      the      generation      of      free-form  \\n \\n  text.\\nHuman-authored      high-quality      Wikipedia      ar-  \\n \\n  ticles      usually      contain      structured      data      and      multi-  \\n \\n  modal      information.\\nWe      leave      the      exploration      of  \\n \\n  generating      multi-modal      grounded      articles      for      fu-  \\n \\n  ture      work.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 20, 'section_title': 'Acknowledgements'}, 'page_content': 'Acknowledgements\\n \\n \\n  We      thank      You.com      for      generously      providing      the  \\n \\n  search      API      that      supported      our      experiments.\\nWe  \\n \\n  also      thank      Sina      J.      Semnani,      Shicheng      Liu,      Eric      Ze-  \\n \\n  likman      for      providing      helpful      feedback      and      the      ACL  \\n \\n  ARR      reviewers      for      their      valuable      comments.\\nThis  \\n \\n  work      is      supported      in      part      by      the      Verdant      Founda-  \\n \\n  tion      and      Microsoft      Azure      AI      credits.\\nYijia      Shao  \\n \\n  is      supported      by   \\n \\na      Stanford      School      of      Engineering  \\n \\n  Fellowship.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 21, 'section_title': 'Ethics      Statement'}, 'page_content': 'Ethics      Statement\\n \\n \\n  Different      from      the      creative      generation,      grounded      ar-  \\n \\n  ticle      generation      may      impact      how      people      learn      about  \\n \\n  topics      or      consume      source      information.\\nAll      the      stud-  \\n \\n  ies      and      the      evaluation      in      this      work      are      designed  \\n \\n  to      prevent      the      dissemination      of      misinformation      by  \\n \\n  not      publishing      generated      content      online      and      im-  \\n \\n  plementing      strict      accuracy      checks.\\nWe      avoid      any  \\n \\n  disruption      to      Wikipedia      or      related      communities,      as  \\n \\n  our      system      does      not      interact      with      live      pages.\\nAlso,  \\n \\n  although      we      try      to      generate      grounded      articles,      we  \\n \\n  believe      there      is      no      privacy      issue      related      to      this      work  \\n \\n  as      we      only      use      information      publicly      available      on  \\n \\n  the      Internet.\\n \\n \\n  The      primary      risk      of      our      work      is      that      the  \\n \\n  Wikipedia      articles      written      by      our      system      are  \\n \\n  grounded      on      information      on      the      Internet      which  \\n \\n  contains      some      biased      or      discriminative      content      on  \\n \\n  its      own.\\nCurrently,      our      system      relies      on      the      search  \\n \\n  engine      to      retrieve      information      but      does      not      include  \\n \\n  any      post-processing      module.\\nWe      believe      improv-  \\n \\n  ing      the      retrieval      module      to      have      good      coverage      of  \\n \\n  different      viewpoints      and      adding   \\n \\na      content      sifting  \\n \\n  module      to      the      current      system      will      be   \\n \\na      critical      next  \\n \\n  step      to      achieve      better      neutrality      and      balance      in      the  \\n \\n  generated      articles.\\n \\n \\n  Another      limitation      we      see      from      an      ethical      point  \\n \\n  of      view      is      that      we      only      consider      writing      English  \\n \\n  Wikipedia      articles      in      this      work.\\nExtending      the      cur-  \\n \\n  rent      system      to   \\n \\na      multilingual      setup      is   \\n \\na      meaningful  \\n \\n  direction      for      future      work      as      more      topics      do      not      have  \\n \\n  Wikipedia      pages      in      non-English      languages.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 22, 'section_title': 'References'}, 'page_content': 'References\\n \\n \\n  Sweta      Agrawal,      Chunting      Zhou,      Mike      Lewis,      Luke  \\n \\n  Zettlemoyer,      and      Marjan      Ghazvininejad.\\n2023.\\nIn-  \\n \\n  context      examples      selection      for      machine      translation.\\n \\n \\n  In      Findings      of      the      Association      for      Computational  \\n \\n  Linguistics:      ACL      2023,      pages      8857-8873,      Toronto,  \\n \\n  Canada.\\nAssociation      for      Computational      Linguistics.\\n \\n \\n  Alan      Akbik,      Tanja      Bergmann,      Duncan      Blythe,      Kashif  \\n \\n  Rasul,      Stefan      Schweter,      and      Roland      Vollgraf.\\n2019.  \\n \\n  FLAIR:      An      easy-to-use      framework      for      state-of-the-  \\n \\n  art      NLP.\\nIn      Proceedings      of      the      2019      Conference      of  \\n \\n  the      North      American      Chapter      of      the      Association      for  \\n \\n  Computational      Linguistics      (Demonstrations),      pages  \\n \\n  54-59,      Minneapolis,      Minnesota.\\nAssociation      for  \\n \\n  Computational      Linguistics.\\n \\n \\n  Mohammad      Aliannejadi,      Hamed      Zamani,      Fabio  \\n \\n  Crestani,      and      W      Bruce      Croft.\\n2019.\\nAsking      clari-  \\n \\n  fying      questions      in      open-domain      information-seeking  \\n \\n  conversations.\\nIn      Proceedings      of      the      42nd      interna-  \\n \\n  tional      acm      sigir      conference      on      research      and      develop-  \\n \\n  ment      in      information      retrieval,      pages      475-484.\\n \\n \\n  Nishant      Balepur,      Jie      Huang,      and      Kevin      Chang.\\n2023.  \\n \\n  Expository      text      generation:      Imitate,      retrieve,      para-  \\n \\n  phrase.\\nIn      Proceedings      of      the      2023      Conference      on  \\n \\n  Empirical      Methods      in      Natural      Language      Process-  \\n \\n  ing,      pages      11896-11919,      Singapore.\\nAssociation      for  \\n \\n  Computational      Linguistics.\\n \\n \\n  Siddhartha      Banerjee      and      Prasenjit      Mitra.\\n2015.  \\n \\n  WikiKreator:      Improving      Wikipedia      stubs      automat-  \\n \\n  ically.\\nIn      Proceedings      of      the      53rd      Annual      Meet-  \\n \\n  ing      of      the      Association      for      Computational      Linguis-  \\n \\n  tics      and      the      7th      International      Joint      Conference      on  \\n \\n  Natural      Language      Processing      (Volume      1:      Long      Pa-  \\n \\n  pers),      pages      867-877,      Beijing,      China.\\nAssociation  \\n \\n  for      Computational      Linguistics.\\n \\n \\n  Bernd      Bohnet,      Vinh      Q.      Tran,      Pat      Verga,      Roee      Aha-  \\n \\n  roni,      Daniel      Andor,      Livio      Baldini      Soares,      Massimil-  \\n \\n  iano      Ciaramita,      Jacob      Eisenstein,      Kuzman      Ganchev,  \\n \\n  Jonathan      Herzig,      Kai      Hui,      Tom      Kwiatkowski,      Ji      Ma,  \\n \\n  Jianmo      Ni,      Lierni      Sestorain      Saralegui,      Tal      Schus-  \\n \\n  ter,      William      W.      Cohen,      Michael      Collins,      Dipanjan  \\n \\n  Das,      Donald      Metzler,      Slav      Petrov,      and      Kellie      Webster.\\n \\n \\n  2023.\\nAttributed      question      answering:      Evaluation      and  \\n \\n  modeling      for      attributed      large      language      models.\\n \\n \\n  Wayne      C      Booth,      Gregory      G      Colomb,      and      Joseph   \\n \\nM       Williams.\\n2003.\\nThe      craft      of      research.\\nUniversity      of  \\n \\n  Chicago      press.\\n \\n \\n  Laura      Dietz      and      John      Foley.\\n2019.\\nTrec      car      y3:      Com-  \\n \\n  plex      answer      retrieval      overview.\\nIn      Proceedings      of  \\n \\n  Text      REtrieval      Conference      (TREC).\\n \\n \\n  Christina   \\n \\nS      Doyle.\\n1994.\\nInformation      literacy      in      an  \\n \\n  information      society:   \\n \\nA      concept      for      the      information  \\n \\n  age.\\nDiane      Publishing.\\n \\n \\n  Ann-Marie      Eriksson      and      Asa      Mikitalo.\\n2015.\\nSupervi-  \\n \\n  sion      at      the      outline      stage:      Introducing      and      encounter-  \\n \\n  ing      issues      of      sustainable      development      through      aca-  \\n \\n  demic      writing      assignments.\\nText   \\n \\n&      Talk,      35(2):123-  \\n \\n  153.\\n \\n \\n  Angela      Fan      and      Claire      Gardent.\\n2022.\\nGenerating      bi-  \\n \\n  ographies      on      Wikipedia:      The      impact      of      gender      bias  \\n \\n  on      the      retrieval-based      generation      of      women      biogra-  \\n \\n  phies.\\nIn      Proceedings      of      the      60th      Annual      Meeting      of  \\n \\n  the      Association      for      Computational      Linguistics      (Vol-  \\n \\n  ume      I:      Long      Papers),      pages      8561-8576,      Dublin,  \\n \\n  Ireland.\\nAssociation      for      Computational      Linguistics.\\n \\n \\n  Xiaocheng      Feng,      Ming      Liu,      Jiahao      Liu,      Bing      Qin,      Yibo  \\n \\n  Sun,      and      Ting      Liu.\\n2018.\\nTopic-to-essay      generation  \\n \\n  with      neural      networks.\\nIn      JJCAI,      pages      4078-4084.\\n \\n \\n  Tira      Nur      Fitria.\\n2023.\\nArtificial      intelligence      (ai)      tech-  \\n \\n  nology      in      openai      chatgpt      application:   \\n \\nA      review      of  \\n \\n  chatgpt      in      writing      english      essay.\\nIn      ELT      Forum:      Jour-  \\n \\n  nal      of      English      Language      Teaching,      volume      12,      pages  \\n \\n  44-58.\\n \\n \\n  Pasi      Franti      and      Radu      Mariescu-Istodor.\\n2023.\\nSoft      preci-  \\n \\n  sion      and      recall.\\nPattern      Recognition      Letters,      167:115—  \\n \\n  121.\\n \\n \\n  R      Edward      Freeman,      Jeffrey   \\n \\nS      Harrison,      Andrew      C       Wicks,      Bidhan   \\n \\nL      Parmar,      and      Simone      De      Colle.\\n2010.\\n \\n \\n  Stakeholder      theory:      The      state      of      the      art.\\n \\n \\n  Tianyu      Gao,      Howard      Yen,      Jiatong      Yu,      and      Danqi      Chen.\\n \\n \\n  2023.\\nEnabling      large      language      models      to      generate  \\n \\n  text      with      citations.\\nIn      Proceedings      of      the      2023      Con-  \\n \\n  ference      on      Empirical      Methods      in      Natural      Language  \\n \\n  Processing,      pages      6465-6488,      Singapore.\\nAssocia-  \\n \\n  tion      for      Computational      Linguistics.\\n \\n \\n  Lei      Huang,      Weijiang      Yu,      Weitao      Ma,      Weihong      Zhong,  \\n \\n  Zhangyin      Feng,      Haotian      Wang,      Qianglong      Chen,  \\n \\n  Weihua      Peng,      Xiaocheng      Feng,      Bing      Qin,      and      Ting  \\n \\n  Liu.\\n2023.   \\n \\nA      survey      on      hallucination      in      large      lan-  \\n \\n  guage      models:      Principles,      taxonomy,      challenges,      and  \\n \\n  open      questions.\\n \\n \\n  Gautier      Izacard,      Patrick      Lewis,      Maria      Lomeli,      Lucas  \\n \\n  Hosseini,      Fabio      Petroni,      Timo      Schick,      Jane      Dwivedi-  \\n \\n  Yu,      Armand      Joulin,      Sebastian      Riedel,      and      Edouard  \\n \\n  Grave.\\n2023.\\nAtlas:      Few-shot      learning      with      retrieval  \\n \\n  augmented      language      models.\\nJournal      of      Machine  \\n \\n  Learning      Research,      24(251):1-43.\\n \\n \\n  Albert   \\n \\nQ      Jiang,      Alexandre      Sablayrolles,      Arthur      Men-  \\n \\n  sch,      Chris      Bamford,      Devendra      Singh      Chaplot,      Diego  \\n \\n  de      las      Casas,      Florian      Bressand,      Gianna      Lengyel,      Guil-  \\n \\n  laume      Lample,      Lucile      Saulnier,      et      al.\\n2023a.\\nMistral  \\n \\n  7b.\\narXiv      preprint      arXiv:2310.06825.\\n \\n \\n  Zhengbao      Jiang,      Frank      Xu,      Luyu      Gao,      Zhiqing      Sun,  \\n \\n  Qian      Liu,      Jane      Dwivedi-Yu,      Yiming      Yang,      Jamie  \\n \\n  Callan,      and      Graham      Neubig.\\n2023b.\\nActive      retrieval  \\n \\n  augmented      generation.\\nIn      Proceedings      of      the      2023  \\n \\n  Conference      on      Empirical      Methods      in      Natural      Lan-  \\n \\n  guage      Processing,      pages      7969-7992,      Singapore.\\nAs-  \\n \\n  sociation      for      Computational      Linguistics.\\n \\n \\n  Nikhil      Kandpal,      Haikang      Deng,      Adam      Roberts,      Eric  \\n \\n  Wallace,      and      Colin      Raffel.\\n2023.\\nLarge      language  \\n \\n  models      struggle      to      learn      long-tail      knowledge.\\nIn      In-  \\n \\n  ternational      Conference      on      Machine      Learning,      pages  \\n \\n  15696-15707.\\nPMLR.\\n \\n \\n  Omar      Khattab,      Keshav      Santhanam,      Xiang      Lisa  \\n \\n  Li,      David      Hall,      Percy      Liang,      Christopher      Potts,  \\n \\n  and      Matei      Zaharia.\\n2022.\\nDemonstrate-search-  \\n \\n  predict:      Composing      retrieval      and      language      mod-  \\n \\n  els      for      knowledge-intensive      NLP.\\narXiv      preprint  \\n \\n  arXiv:2212.14024.\\n \\n \\n  Omar      Khattab,      Arnav      Singhvi,      Paridhi      Maheshwari,  \\n \\n  Zhiyuan      Zhang,      Keshav      Santhanam,      Sri      Vard-  \\n \\n  hamanan,      Saiful      Haq,      Ashutosh      Sharma,      Thomas      T.  \\n \\n  Joshi,      Hanna      Moazam,      Heather      Miller,      Matei      Za-  \\n \\n  haria,      and      Christopher      Potts.\\n2023.\\nDspy:      Compiling  \\n \\n  declarative      language      model      calls      into      self-improving  \\n \\n  pipelines.\\narXiv      preprint      arXiv:2310.03714.\\n \\n \\n  Seungone      Kim,      Jamin      Shin,      Yejin      Cho,      Joel      Jang,  \\n \\n  Shayne      Longpre,      Hwaran      Lee,      Sangdoo      Yun,  \\n \\n  Seongjin      Shin,      Sungdong      Kim,      James      Thorne,      et      al.\\n \\n \\n  2023.\\nPrometheus:      Inducing      fine-grained      evalua-  \\n \\n  tion      capability      in      language      models.\\narXiv      preprint  \\n \\n  arXiv:2310.08491.\\n \\n \\n  Mojtaba      Komeili,      Kurt      Shuster,      and      Jason      Weston.\\n2022.  \\n \\n  Internet-augmented      dialogue      generation.\\nIn      Proceed-  \\n \\n  ings      of      the      60th      Annual      Meeting      of      the      Association  \\n \\n  for      Computational      Linguistics      (Volume      1:      Long      Pa-  \\n \\n  pers),      pages      8460-8478,      Dublin,      Ireland.\\nAssociation  \\n \\n  for      Computational      Linguistics.\\n \\n \\n  Kalpesh      Krishna,      Erin      Bransom,      Bailey      Kuehl,      Mohit  \\n \\n  Iyyer,      Pradeep      Dasigi,      Arman      Cohan,      and      Kyle      Lo.\\n \\n \\n  2023.\\nLongEval:      Guidelines      for      human      evaluation      of  \\n \\n  faithfulness      in      long-form      summarization.\\nIn      Proceed-  \\n \\n  ings      of      the      17th      Conference      of      the      European      Chap-  \\n \\n  ter      of      the      Association      for      Computational      Linguistics,  \\n \\n  pages      1650-1669,      Dubrovnik,      Croatia.\\nAssociation  \\n \\n  for      Computational      Linguistics.\\n \\n \\n  Patrick      Lewis,      Ethan      Perez,      Aleksandra      Piktus,      Fabio  \\n \\n  Petroni,      Vladimir      Karpukhin,      Naman      Goyal,      Hein-  \\n \\n  rich      Kiittler,      Mike      Lewis,      Wen-tau      Yih,      Tim      Rock-  \\n \\n  taschel,      et      al.\\n2020.\\nRetrieval-augmented      generation  \\n \\n  for      knowledge-intensive      nlp      tasks.\\nAdvances      in      Neu-  \\n \\n  ral      Information      Processing      Systems,      33:9459-9474.\\n \\n \\n  Xiaonan      Li,      Kai      Lv,      Hang      Yan,      Tianyang      Lin,      Wei      Zhu,  \\n \\n  Yuan      Ni,      Guotong      Xie,      Xiaoling      Wang,      and      Xipeng  \\n \\n  Qiu.\\n2023.\\nUnified      demonstration      retriever      for      in-  \\n \\n  context      learning.\\nIn      Proceedings      of      the      61st      Annual  \\n \\n  Meeting      of      the      Association      for      Computational      Lin-  \\n \\n  guistics      (Volume      1:      Long      Papers),      pages      4644-4668,  \\n \\n  Toronto,      Canada.\\nAssociation      for      Computational      Lin-  \\n \\n  guistics.\\n \\n \\n  Chin-Yew      Lin.\\n2004.\\nROUGE:   \\n \\nA      package      for      auto-  \\n \\n  matic      evaluation      of      summaries.\\nIn      Text      Summariza-  \\n \\n  tion      Branches      Out,      pages      74-81,      Barcelona,      Spain.\\n \\n \\n  Association      for      Computational      Linguistics.\\n \\n \\n  Jiachang      Liu,      Dinghan      Shen,      Yizhe      Zhang,      Bill      Dolan,  \\n \\n  Lawrence      Carin,      and      Weizhu      Chen.\\n2022.\\nWhat  \\n \\n  makes      good      in-context      examples      for      GPT-3?\\nIn  \\n \\n  Proceedings      of      Deep      Learning      Inside      Out      (DeeLIO  \\n \\n  2022):      The      3rd      Workshop      on      Knowledge      Extrac-  \\n \\n  tion      and      Integration      for      Deep      Learning      Architectures,  \\n \\n  pages      100-114,      Dublin,      Ireland      and      Online.\\nAssocia-  \\n \\n  tion      for      Computational      Linguistics.\\n \\n \\n  Peter      J.      Liu,      Mohammad      Saleh,      Etienne      Pot,      Ben  \\n \\n  Goodrich,      Ryan      Sepassi,      Lukasz      Kaiser,      and      Noam  \\n \\n  Shazeer.\\n2018.\\nGenerating      wikipedia      by      summariz-  \\n \\n  ing      long      sequences.\\nIn      International      Conference      on  \\n \\n  Learning      Representations.\\n \\n \\n  Jacob      Menick,      Maja      Trebacz,      Vladimir      Mikulik,  \\n \\n  John      Aslanides,      Francis      Song,      Martin      Chadwick,  \\n \\n  Mia      Glaese,      Susannah      Young,      Lucy      Campbell-  \\n \\n  Gillingham,      Geoffrey      Irving,      and      Nat      McAleese.\\n \\n \\n  2022.\\nTeaching      language      models      to      support      answers  \\n \\n  with      verified      quotes.\\n \\n \\n  Sewon      Min,      Kalpesh      Krishna,      Xinxi      Lyu,      Mike      Lewis,  \\n \\n  Wen-tau      Yih,      Pang      Koh,      Mohit      Iyyer,      Luke      Zettle-  \\n \\n  moyer,      and      Hannaneh      Hajishirzi.\\n2023.\\nFActScore:\\n \\n \\n  Fine-grained      atomic      evaluation      of      factual      precision  \\n \\n  in      long      form      text      generation.\\nIn      Proceedings      of      the  \\n \\n  2023      Conference      on      Empirical      Methods      in      Natural  \\n \\n  Language      Processing,      pages      12076-12100,      Singa-  \\n \\n  pore.\\nAssociation      for      Computational      Linguistics.\\n \\n \\n  Julia      Minguill6n,      Maura      Lerga,      Eduard      Aibar,      Josep  \\n \\n  Lladés-Masllorens,      and      Antoni      Meseguer-Artola.\\n \\n \\n  2017.\\nSemi-automatic      generation      of   \\n \\na      corpus      of  \\n \\n  wikipedia      articles      on      science      and      technology.\\nProfe-  \\n \\n  sional      de      la      Informacion,      26(5):995—1005.\\n \\n \\n  Rosa      Munoz-Luna.\\n2015.\\nMain      ingredients      for      suc-  \\n \\n  cess      in      12      academic      writing:      Outlining,      drafting      and  \\n \\n  proofreading.\\nPloS      one,      10(6):e0128309.\\n \\n \\n  Reiichiro      Nakano,      Jacob      Hilton,      Suchir      Balaji,      Jeff      Wu,  \\n \\n  Long      Ouyang,      Christina      Kim,      Christopher      Hesse,  \\n \\n  Shantanu      Jain,      Vineet      Kosaraju,      William      Saunders,  \\n \\n  Xu      Jiang,      Karl      Cobbe,      Tyna      Eloundou,      Gretchen  \\n \\n  Krueger,      Kevin      Button,      Matthew      Knight,      Benjamin  \\n \\n  Chess,      and      John      Schulman.\\n2022.\\nWebgpt:      Browser-  \\n \\n  assisted      question-answering      with      human      feedback.\\n \\n \\n  Long      Ouyang,      Jeffrey      Wu,      Xu      Jiang,      Diogo      Almeida,  \\n \\n  Carroll      Wainwright,      Pamela      Mishkin,      Chong      Zhang,  \\n \\n  Sandhini      Agarwal,      Katarina      Slama,      Alex      Ray,      et      al.\\n \\n \\n  2022.\\nTraining      language      models      to      follow      instruc-  \\n \\n  tions      with      human      feedback.\\nAdvances      in      Neural  \\n \\n  Information      Processing      Systems,      35:27730—27744.\\nAaron      Parisi,      Yao      Zhao,      and      Noah      Fiedel.\\n2022.\\nTalm:  \\n \\n  Tool      augmented      language      models.\\n \\n \\n  John   \\n \\nV      Pavlik.\\n2023.\\nCollaborating      with      chatgpt:      Con-  \\n \\n  sidering      the      implications      of      generative      artificial      intel-  \\n \\n  ligence      for      journalism      and      media      education.\\nJournal-  \\n \\n  ism   \\n \\n&      Mass      Communication      Educator,      78(1):84—93.\\n \\n \\n  Gabriel      Poesia,      Alex      Polozov,      Vu      Le,      Ashish      Tiwari,  \\n \\n  Gustavo      Soares,      Christopher      Meek,      and      Sumit      Gul-  \\n \\n  wani.\\n2022.\\nSynchromesh:      Reliable      code      generation  \\n \\n  from      pre-trained      language      models.\\nIn      International  \\n \\n  Conference      on      Learning      Representations.\\n \\n \\n  Ofir      Press,      Muru      Zhang,      Sewon      Min,      Ludwig      Schmidt,  \\n \\n  Noah      Smith,      and      Mike      Lewis.\\n2023.\\nMeasuring      and  \\n \\n  narrowing      the      compositionality      gap      in      language      mod-  \\n \\n  els.\\nIn      Findings      of      the      Association      for      Computational  \\n \\n  Linguistics:      EMNLP      2023,      pages      5687-5711,      Singa-  \\n \\n  pore.\\nAssociation      for      Computational      Linguistics.\\n \\n \\n  Peng      Qi,      Yuhao      Zhang,      and      Christopher      D.      Manning.\\n \\n \\n  2020.      Stay      hungry,      stay      focused:      Generating      infor-  \\n \\n  mative      and      specific      questions      in      information-seeking  \\n \\n  conversations.\\nIn      Findings      of      the      Association      for  \\n \\n  Computational      Linguistics:      EMNLP      2020,      pages      25—  \\n \\n  40,      Online.\\nAssociation      for      Computational      Linguis-  \\n \\n  tics.\\n \\n \\n  Hongjing      Qian,      Yutao      Zhu,      Zhicheng      Dou,      Haoqi      Gu,  \\n \\n  Xinyu      Zhang,      Zheng      Liu,      Ruofei      Lai,      Zhao      Cao,  \\n \\n  Jian-Yun      Nie,      and      Ji-Rong      Wen.\\n2023.\\nWebbrain:\\n \\n \\n  Learning      to      generate      factually      correct      articles      for  \\n \\n  queries      by      grounding      on      large      web      corpus.\\n \\n \\n  Hossein      A.      Rahmani,      Xi      Wang,      Yue      Feng,      Qiang      Zhang,  \\n \\n  Emine      Yilmaz,      and      Aldo      Lipani.\\n2023.   \\n \\nA      survey      on  \\n \\n  asking      clarification      questions      datasets      in      conversa-  \\n \\n  tional      systems.\\nIn      Proceedings      of      the      61st      Annual  \\n \\n  Meeting      of      the      Association      for      Computational      Lin-  \\n \\n  guistics      (Volume      1:      Long      Papers),      pages      2698-2716,  \\n \\n  Toronto,      Canada.\\nAssociation      for      Computational      Lin-  \\n \\n  guistics.\\n \\n \\n  Ashwin      Ram.\\n1991.   \\n \\nA      theory      of      questions      and      question  \\n \\n  asking.\\nJournal      of      the      Learning      Sciences,      1(3-4):273-  \\n \\n  318.\\n \\n \\n  Ori      Ram,      Yoav      Levine,      Itay      Dalmedigos,      Dor      Muhlgay,  \\n \\n  Amnon      Shashua,      Kevin      Leyton-Brown,      and      Yoav  \\n \\n  Shoham.\\n2023.\\nIn-context      retrieval-augmented      lan-  \\n \\n  guage      models.\\nTransactions      of      the      Association      for  \\n \\n  Computational      Linguistics.\\n \\n \\n  Nils      Reimers      and      Iryna      Gurevych.\\n2019.\\nSentence-  \\n \\n  BERT:      Sentence      embeddings      using      Siamese      BERT-  \\n \\n  networks.\\nIn      Proceedings      of      the      2019      Conference      on  \\n \\n  Empirical      Methods      in      Natural      Language      Processing  \\n \\n  and      the      9th      International      Joint      Conference      on      Natu-  \\n \\n  ral      Language      Processing      (EMNLP-IJCNLP),      pages  \\n \\n  3982-3992,      Hong      Kong,      China.\\nAssociation      for      Com-  \\n \\n  putational      Linguistics.\\n \\n \\n \\nD      Gordon      Rohman.\\n1965.\\nPre-writing      the      stage      of      dis-  \\n \\n  covery      in      the      writing      process.\\nCollege      composition  \\n \\n  and      communication,      16(2):106—112.\\n \\n \\n  Christina      Sauper      and      Regina      Barzilay.\\n2009.\\nAuto-  \\n \\n  matically      generating      Wikipedia      articles:   \\n \\nA      structure-  \\n \\n  aware      approach.\\nIn      Proceedings      of      the      Joint      Con-  \\n \\n  ference      of      the      47th      Annual      Meeting      of      the      ACL      and  \\n \\n  the      4th      International      Joint      Conference      on      Natural  \\n \\n  Language      Processing      of      the      AFNLP,      pages      208-216,  \\n \\n  Suntec,      Singapore.\\nAssociation      for      Computational  \\n \\n  Linguistics.\\n \\n \\n  Lam.\\n2023.\\nWikiChat:      Stopping      the      hallucination      of  \\n \\n  large      language      model      chatbots      by      few-shot      ground-  \\n \\n  ing      on      Wikipedia.\\nIn      Findings      of      the      Association  \\n \\n  for      Computational      Linguistics:      EMNLP      2023,      pages  \\n \\n  2387-2413,      Singapore.\\nAssociation      for      Computa-  \\n \\n  tional      Linguistics.\\n \\n \\n  Jonathan      Bragg,      Jeff      Hammerbacher,      Doug      Downey,  \\n \\n  Joseph      Chee      Chang,      and      David      Sontag.\\n2023.\\nBe-  \\n \\n  yond      summarization:      Designing      ai      support      for      real-  \\n \\n  world      expository      writing      tasks.\\n \\n \\n  Luke      Zettlemoyer.\\n2022.\\nNearest      neighbor      zero-shot  \\n \\n  inference.\\nIn      Proceedings      of      the      2022      Conference      on  \\n \\n  Empirical      Methods      in      Natural      Language      Processing,  \\n \\n  pages      3254-3265,      Abu      Dhabi,      United      Arab      Emirates.\\n \\n \\n  Association      for      Computational      Linguistics.\\n \\n \\n  Stephen      Roller,      Arthur      Szlam,      and      Jason      Weston.\\n \\n \\n  2022.\\nLanguage      models      that      seek      for      knowledge:  \\n \\n  Modular      search   \\n \\n&      generation      for      dialogue      and  \\n \\n  prompt      completion.\\nIn      Findings      of      the      Association  \\n \\n  for      Computational      Linguistics:      EMNLP      2022,      pages  \\n \\n  373-393,      Abu      Dhabi,      United      Arab      Emirates.\\nAssoci-  \\n \\n  ation      for      Computational      Linguistics.\\n \\n \\n  and      Jason      Weston.\\n2021.\\nRetrieval      augmentation  \\n \\n  reduces      hallucination      in      conversation.\\nIn      Findings  \\n \\n  of      the      Association      for      Computational      Linguistics:  \\n \\n  EMNLP      2021,      pages      3784-3803,      Punta      Cana,      Do-  \\n \\n  minican      Republic.\\nAssociation      for      Computational  \\n \\n  Linguistics.\\nWikipedia      as      an      introduction      to      academic      writing.\\nIn  \\n \\n  English      teaching      forum,      volume      48,      page      12.\\nERIC.\\n \\n \\n  and      Jaclyn      Gishbaugher.\\n2020.\\nRole      of      questions      in  \\n \\n  inquiry-based      instruction:      towards   \\n \\na      design      taxon-  \\n \\n  omy      for      question-asking      and      implications      for      design.\\n \\n \\n  Educational      Technology      Research      and      Development,  \\n \\n  68:653-678.\\nitory      text.\\n \\n \\n  than      humans?\\nvalidating      how      openai’s      chatgpt      model  \\n \\n  explains      crowdfunding,      alternative      finance      and      com-  \\n \\n  munity      finance.\\nValidating      how      OpenAlI’s      ChatGPT  \\n \\n  model      explains      Crowdfunding,      Alternative      Finance  \\n \\n  and      Community      Finance.(December      22,      2022).\\n \\n \\n  Choi.\\n2023.   \\n \\nA      critical      evaluation      of      evaluations      for  \\n \\n  long-form      question      answering.\\nIn      Proceedings      of      the  \\n \\n  61st      Annual      Meeting      of      the      Association      for      Compu-  \\n \\n  tational      Linguistics      (Volume      1:      Long      Papers),      pages  \\n \\n  3225-3245,      Toronto,      Canada.\\nAssociation      for      Com-  \\n \\n  putational      Linguistics.\\n \\n \\n  Kevin      Yang,      Dan      Klein,      Nanyun      Peng,      and      Yuandong  \\n \\n  Tian.\\n2023.      DOC:      Improving      long      story      coherence  \\n \\n  with      detailed      outline      control.\\nIn      Proceedings      of      the  \\n \\n  61st      Annual      Meeting      of      the      Association      for      Compu-  \\n \\n  tational      Linguistics      (Volume      I:      Long      Papers),      pages  \\n \\n  3378-3465,      Toronto,      Canada.\\nAssociation      for      Com-  \\n \\n  putational      Linguistics.\\n \\n \\n  Kevin      Yang,      Yuandong      Tian,      Nanyun      Peng,      and      Dan  \\n \\n  Klein.\\n2022.\\nRe3:      Generating      longer      stories      with  \\n \\n  recursive      reprompting      and      revision.\\nIn      Proceedings  \\n \\n  of      the      2022      Conference      on      Empirical      Methods      in      Nat-  \\n \\n  ural      Language      Processing,      pages      4393-4479,      Abu  \\n \\n  Dhabi,      United      Arab      Emirates.\\nAssociation      for      Com-  \\n \\n  putational      Linguistics.\\n \\n \\n  Shunyu      Yao,      Jeffrey      Zhao,      Dian      Yu,      Nan      Du,      Izhak  \\n \\n  Shafran,      Karthik      R      Narasimhan,      and      Yuan      Cao.\\n2023.  \\n \\n  React:      Synergizing      reasoning      and      acting      in      language  \\n \\n  models.\\nIn      The      Eleventh      International      Conference  \\n \\n  on      Learning      Representations.\\n \\n \\n  Cyril      Zakka,      Akash      Chaurasia,      Rohan      Shad,      Alex      R       Dalal,      Jennifer   \\n \\nL      Kim,      Michael      Moor,      Kevin      Alexan-  \\n \\n  der,      Euan      Ashley,      Jack      Boyd,      Kathleen      Boyd,      et      al.\\n \\n \\n  2023.\\nAlmanac:      Retrieval-augmented      language      mod-  \\n \\n  els      for      clinical      medicine.\\nResearch      Square.\\n \\n \\n  Shuyan      Zhou,      Uri      Alon,      Frank      F.      Xu,      Zhengbao      Jiang,  \\n \\n  and      Graham      Neubig.\\n2023.\\nDocprompting:      Gener-  \\n \\n  ating      code      by      retrieving      the      docs.\\nIn      The      Eleventh  \\n \\n  International      Conference      on      Learning      Representa-  \\n \\n  tions.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 23, 'section_title': 'Average      Numer      of      Sections      8.4'}, 'page_content': 'Average      Numer      of      Sections      8.4'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 24, 'section_title': 'Average      Number      of      All-level      Headings      15.8 Average      Length      of      a      Section      327.8       Average      Length      of      Total      Article      2159.1'}, 'page_content': 'Average      Number      of      All-level      Headings      15.8 Average      Length      of      a      Section      327.8       Average      Length      of      Total      Article      2159.1'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 25, 'section_title': 'Average      Number      of      References      90.1'}, 'page_content': 'Average      Number      of      References      90.1\\nTable      7:      Statistics      of      the      dataset      used      in      our      experiments.\\n—\\n \\n   Average Number      of      references  \\n \\n  a      BR  \\n \\n  N      Py      oa      feo}      Oo      N       Oo      Oo      oO      Oo      oO      Oo  \\n \\n  fo)  \\n \\n  1 0\\n \\n   20      40      60      80      100  \\n \\n  Edit      progress      (%      of      total      edits) Figure      4:      Evolution      of      reference      count      in      the      Wikipedia  \\n \\n  article      editing      process.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 26, 'section_title': 'A_      Dataset      Details'}, 'page_content': 'A_      Dataset      Details\\n \\n \\n  As      discussed      in      §2.1,      we      curate      the      FreshWiki  \\n \\n  dataset      by      collecting      recent      and      high-quality      En-  \\n \\n  glish      Wikipedia      articles.\\nWe      select      the      most-edited  \\n \\n  pages      over   \\n \\na      specific      period      rather      than      using      cre-  \\n \\n  ation      dates      as   \\n \\na      cutoff      because      most      of      Wikipedia  \\n \\n  articles      are      “stubs”      or      are      of      low      quality      when      they  \\n \\n  were      created.\\nFor      quality,      we      consider      articles      pre-  \\n \\n  dicted      to      be      of      B-class      quality      or      above.\\nAccording  \\n \\n  to      Wikipedia      statistics!\\n*,      only      around      3%      of      ex-  \\n \\n  isting      Wikipedia      pages      meet      this      quality      standard.\\n \\n \\n  As      LLMs      can      generate      reasonably      good      outputs,  \\n \\n  we      think      it      is      important      to      use      high-quality      human-  \\n \\n  written      articles      as      references      for      further      research.\\n \\n \\n  For      experiments      in      this      work,      we      randomly      se-  \\n \\n  lect      100      samples      with      human-written      articles      un-  \\n \\n  der      3000      words      to      have   \\n \\na      meaningful      comparison.\\n \\n \\n  Table   \\n \\n7      gives      the      data      statistics.\\nNotably,      human-  \\n \\n  authored      articles      have   \\n \\na      large      number      of      references  \\n \\n  but      they      require      numerous      edits      to      achieve      this.\\nFig-  \\n \\n  ure   \\n \\n4      illustrates      the      evolution      of      the      reference      count  \\n \\n  in      the      article      edit      process      and      Figure   \\n \\n5      gives      the      dis-  \\n \\n  tribution      of      edit      counts      for      human-authored      articles  \\n \\n  used      in      our      experiments.\\ncount      (A;)   \\n \\n=       where      embed(-)      in      Equation      (1)      is      parameterized  \\n \\n  by      paraphrase-MiniLM-L6-v2      provided      in      the  \\n \\n  Sentence-Transformers      library!*.\\nThe      cardinality  \\n \\n  https://en.wikipedia.\\norg/wiki/Wikipedia:  \\n \\n  Content_assessment  \\n \\n  100) Bhttps://huggingface.co/sentence-transformers/  \\n \\n  paraphrase-MiniLM-L6-v2 Percentage      of      articles      (n\\n |       T      T      T      T       0      500      1000      1500 |       y      1      7      1      7      7      7       2000      2500      3000      3500      4000      4500      5000       Number      of      edits\\n | Figure      5:      Distribution      of      edit      counts      for      Wikipedia      arti-       cles      in      our      experiments      (n      =      100).\\n'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 27, 'section_title': 'B_      Pseudo      Code      of      STORM'}, 'page_content': 'B_      Pseudo      Code      of      STORM\\n \\n \\n  In      §3,      we      introduce      STORM,   \\n \\na      framework      that      au-  \\n \\n  tomates      the      pre-writing      stage      by      discovering      differ-  \\n \\n  ent      perspectives,      simulating      information-seeking  \\n \\n  conversations,      and      creating   \\n \\na      comprehensive      out-  \\n \\n  line.\\nAlgorithm   \\n \\n1      displays      the      skeleton      of      STORM.\\n \\n \\n  We      implement      STORM      with      zero-shot      prompt-  \\n \\n  ing      using      the      DSPy      framework      (Khattab      et      al.,  \\n \\n  2023).\\nListing   \\n \\n1      and   \\n \\n2      show      the      prompts      used  \\n \\n  in      our      implementation.\\nWe      highlight      that      STORM  \\n \\n  offers   \\n \\na      general      framework      designed      to      assist      the  \\n \\n  creation      of      grounded,      long-form      articles,      without  \\n \\n  depending      extensively      on      prompt      engineering      for   \\n \\na       single      domain.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 28, 'section_title': 'C      Automatic      Evaluation      Details'}, 'page_content': 'C      Automatic      Evaluation      Details'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 29, 'section_title': 'C.1      Soft      Heading      Recall'}, 'page_content': 'C.1      Soft      Heading      Recall\\n \\n \\n  We      calculate      the      soft      heading      recall      between      the  \\n \\n  multi-level      headings      in      the      generated      outline,      con-  \\n \\n  sidered      as      the      prediction      P,      and      those      in      the      human-  \\n \\n  written      article,      considered      as      the      ground      truth      G.  \\n \\n  The      calculation      is      based      on      the      soft      recall      defini-  \\n \\n  tion      in      Franti      and      Mariescu-Istodor      (2023).\\nGiven  \\n \\n  aset   \\n \\nA   \\n \\n=      {Ai}*.,,      soft      count      of      an      item      is      defined as      the      inverse      of      the      sum      of      its      similarity      to      other  \\n \\n  items      in      the      set:\\n1\\n \\n \\n  Dj      Sim      (Aj,      Aj)      (1) Sim      (A;,      A;)   \\n \\n=      cos      (embed(A;),      embed(A;))   \\n \\n,       28  \\n \\n  29  \\n \\n  class      GenRelatedTopicsPrompt      (dspy.      Signature):\\n \\n \\n  I\\\\\\\\\\\\\\'m      writing   \\n \\na      Wikipedia      page      for   \\n \\na      topic      mentioned      below.\\nPlease      identify      and  \\n \\n  recommend      some      Wikipedia      pages      on      closely      related      subjects.\\nI\\\\\\\\\\\\\\'m      looking      for  \\n \\n  examples      that      provide      insights      into      interesting      aspects      commonly      associated  \\n \\n  with      this      topic,      or      examples      that      help      me      understand      the      typical      content      and  \\n \\n  structure      included      in      Wikipedia      pages      for      similar      topics.\\n \\n \\n  Please      list      the      urls      in      separate      lines.\\n \\n \\n  non topic   \\n \\n=      dspy.InputField(prefix=\"Topic      of      interest:”,      format=str)  \\n \\n  related_topics   \\n \\n=      dspy.OutputField() class      GenPerspectivesPrompt      (dspy.Signature):\\n \\n \\n  You      need      to      select   \\n \\na      group      of      Wikipedia      editors      who      will      work      together      to      create  \\n \\n  a      comprehensive      article      on      the      topic.\\nEach      of      them      represents   \\n \\na      different  \\n \\n  perspective,      role,      or      affiliation      related      to      this      topic.\\nYou      can      use      other  \\n \\n  Wikipedia      pages      of      related      topics      for      inspiration.\\nFor      each      editor,      add  \\n \\n  description      of      what      they      will      focus      on.\\n \\n \\n  Give      your      answer      in      the      following      format:      1.      short      summary      of      editor      1:\\ndescription\\\\\\\\2.\\nshort summary of editor 2: description\\\\\\\\  \\n \\n  non  \\n \\n  topic   \\n \\n=      dspy.InputField(prefix=\\\\\\\\\\\\\\'Topic      of      interest:\\\\\\\\\\\\\\',      format=str)  \\n \\n  examples   \\n \\n=      dspy.InputField(prefix=\\\\\\\\\\\\\\'Wiki      page      outlines      of      related      topics      for  \\n \\n  inspiration:\\\\\\\\\\\\\\\\\\\\\\',      format=str)  \\n \\n  perspectives   \\n \\n=      dspy.OutputField() class      GenQnPrompt(dspy.      Signature):\\n \\n \\n  You      are      an      experienced      Wikipedia      writer      and      want      to      edit   \\n \\na      specific      page.\\n \\n \\n  Besides      your      identity      as   \\n \\na      Wikipedia      writer,      you      have   \\n \\na      specific      focus      when  \\n \\n  researching      the      topic.\\n \\n \\n  Now,      you      are      chatting      with      an      expert      to      get      information.\\nAsk      good      questions      to  \\n \\n  get      more      useful      information.\\n \\n \\n  When      you      have      no      more      question      to      ask,      say      \"Thank      you      so      much      for      your      help!”\\nto  \\n \\n  end      the      conversation.\\n \\n \\n  Please      only      ask      one      question      at   \\n \\na      time      and      don\\\\\\\\\\\\\\'t      ask      what      you      have      asked      before.\\n \\n \\n  Your      questions      should      be      related      to      the      topic      you      want      to      write.\\n \\n \\n  non  \\n \\n  topic   \\n \\n=      dspy.InputField(prefix=\\\\\\\\\\\\\\'Topic      you      want      to      write:      \\\\\\\\\\\\\\',      format=str)  \\n \\n  persona   \\n \\n=      dspy.InputField(prefix=\\\\\\\\\\\\\\'Your      specific      perspective:      \\\\\\\\\\\\\\',      format=str)  \\n \\n  conv   \\n \\n=      dspy.InputField(prefix=\\\\\\\\\\\\\\'Conversation      history:\\\\\\\\\\\\\\\\\\\\\\',      format=str)  \\n \\n  question   \\n \\n=      dspy.OutputField()  \\n \\n  class      GenQueriesPrompt      (dspy.      Signature):  \\n \\n  nnn  \\n \\n  You      want      to      answer      the      question      using      Google      search.\\nWhat      do      you      type      in      the  \\n \\n  search      box?\\nWrite the queries you will use in the following format:- query 1\\\\\\\\- query 2\\\\\\\\\\n \\n \\n  topic   \\n \\n=      dspy.InputField(prefix=\\\\\\\\\\\\\\'Topic      you      are      discussing      about:      \\\\\\\\\\\\\\',      format=str)  \\n \\n  question   \\n \\n=      dspy.InputField(prefix=\\\\\\\\\\\\\\'Question      you      want      to      answer:      \\\\\\\\\\\\\\',      format=str)  \\n \\n  queries   \\n \\n=      dspy.OutputField()\\nListing      1:      Prompts      used      in      STORM,      corresponding      to      Line      4,      11,      19,      22      in      Algorithm      1.\\nwow      Ne\\n \\n \\n  20  \\n \\n  21  \\n \\n  22  \\n \\n  23  \\n \\n  24  \\n \\n  25  \\n \\n  26  \\n \\n  27  \\n \\n  28\\n29  \\n \\n  30 class      GenAnswerPrompt(dspy.      Signature):\\n \\n \\n  You      are      an      expert      who      can      use      information      effectively.\\nYou      are      chatting      with   \\n \\na       Wikipedia      writer      who      wants      to      write   \\n \\na      Wikipedia      page      on      topic      you      know.\\nYou  \\n \\n  have      gathered      the      related      information      and      will      now      use      the      information      to  \\n \\n  form   \\n \\na      response.\\n \\n \\n  Make      your      response      as      informative      as      possible      and      make      sure      every      sentence      is  \\n \\n  supported      by      the      gathered      information.\\n \\n \\n  non  \\n \\n  topic   \\n \\n=      dspy.InputField(prefix=\\\\\\\\\\\\\\'Topic      you      are      discussing      about:\\\\\\\\\\\\\\',      format=str)  \\n \\n  conv      dspy.InputField(prefix=\\\\\\\\\\\\\\'Question:\\\\\\\\\\\\\\\\\\\\\\',      format=str)  \\n \\n  info   \\n \\n=      dspy.InputField(      prefix=\\\\\\\\\\\\\\'Gathered      information:\\\\\\\\\\\\\\\\\\\\\\',      format=str)  \\n \\n  answer   \\n \\n=      dspy.OutputField(prefix=\\\\\\\\\\\\\\'Now      give      your      response:\\\\\\\\\\\\\\\\\\\\\\')\\nclass      DirectGenOutlinePrompt      (dspy.      Signature):\\nWrite      an      outline      for   \\n \\na      Wikipedia      page.\\n \\n \\n  Here      is      the      format      of      your      writing:\\n2.      Do      not      include      other      information.\\n \\n \\n  non\\n1. Use      \"#\"      Title”      to      indicate      section      title,      \"##\"      Title”      to      indicate  \\n \\n  subsection      title,      \"###\"”      Title”      to      indicate      subsubsection      title,      and      so  \\n \\n  on.\\ntopic   \\n \\n=      dspy.InputField(prefix=\"Topic      you      want      to      write:      ”\",      format=str)  \\n \\n  outline   \\n \\n=      dspy.OutputField(prefix=\"Write      the      Wikipedia      page      outline:\\\\\\\\\"”)”\\nclass      RefineOutlinePrompt(dspy.      Signature):\\n \\n \\n  Improve      an      outline      for   \\n \\na      Wikipedia      page.\\nYou      already      have   \\n \\na      draft      outline      that  \\n \\n  covers      the      general      information.\\nNow      you      want      to      improve      it      based      on      the  \\n \\n  information      learned      from      an      information-seeking      conversation      to      make      it      more  \\n \\n  comprehensive.\\n \\n \\n  Here      is      the      format      of      your      writing:\\n2.      Do      not      include      other      information.\\n \\n \\n  non\\n1. Use      \"#\"      Title”      to      indicate      section      title,      \"##\"      Title”      to      indicate  \\n \\n  subsection      title,      \"###\"      Title”      to      indicate      subsubsection      title,      and      so  \\n \\n  on.\\n \\n \\n  topic   \\n \\n=      dspy.InputField(prefix=\"Topic      you      want      to      write:      \",      format=str)  \\n \\n  conv   \\n \\n=      dspy.InputField(prefix=\"Conversation      history:\\\\\\\\\",      format=str)  \\n \\n  old_outline   \\n \\n=      dspy.OutputField(prefix=\"Current      outline:\\\\\\\\”,      format=str)  \\n \\n  outline   \\n \\n=      dspy.OutputField(      prefix=\\\\\\\\\\\\\\'Write      the      Wikipedia      page      outline:\\\\\\\\\\\\\\\\\\\\\\')”\\nListing      2:      Prompts      used      in      STORM      (continue),      corresponding      to      Line      24,      31,      32      in      Algorithm      1.\\nyay      aA      uu      &}      WwW      YY      —\\n \\n \\n  11  \\n \\n  12  \\n \\n  13  \\n \\n  14  \\n \\n  15  \\n \\n  16  \\n \\n  17  \\n \\n  18  \\n \\n  19  \\n \\n  20  \\n \\n  21  \\n \\n  22  \\n \\n  23\\n \\n \\n  24  \\n \\n  25  \\n \\n  26  \\n \\n  27  \\n \\n  28  \\n \\n  29  \\n \\n  30  \\n \\n  31  \\n \\n  32  \\n \\n  33  \\n \\n  Input      :Topic      t,      maximum      perspective      N,  \\n \\n  maximum      conversation      round      MJ  \\n \\n  Output   \\n \\n:      Outline      O,      references   \\n \\nR PO = \"basic fact writer \" // Constant.\\n \\n \\n  R-[]  \\n \\n  //      Discover      perspectives      P.  \\n \\n  related_topics   \\n \\n+      gen_related_topics(t)  \\n \\n  tocs   \\n \\n+   \\n \\n|   \\n \\n|       foreach      related_t      in      related_topics      do  \\n \\n  article   \\n \\n<      get_wiki_article(related_t)  \\n \\n  if      article      then  \\n \\n  |      tocs.append(extract_toc(article))  \\n \\n  end  \\n \\n  end  \\n \\n  P   \\n \\n<      gen_perspectives(t,      tocs)  \\n \\n  P<      [PO]   \\n \\n+      P[:N]  \\n \\n  //      Simulate      conversations.\\n \\n \\n  convos   \\n \\n<      [|  \\n \\n  foreach   \\n \\np      in   \\n \\nP      do  \\n \\n  convo_history   \\n \\n<   \\n \\n|   \\n \\n]       for:      =1to   \\n \\nM      do  \\n \\n  //      Question      asking.\\n \\n \\n  q+      gen_qn(t,      p,      dlg_history)  \\n \\n  convo_history.append(q)  \\n \\n  //      Question      answering.\\n \\n \\n  queries   \\n \\n<      gen_queries(t,      q)  \\n \\n  sources      <—  \\n \\n  search_and_sift(queries)  \\n \\n  a   \\n \\n+      gen_ans(t,      q,      sources)  \\n \\n  convo_history.append(a)  \\n \\n  R.append(sources)  \\n \\n  end  \\n \\n  convos.append(convo_history)  \\n \\n  end  \\n \\n  //      Create      the      outline.\\n \\n \\n  Op   \\n \\n<      direct_gen_outline(t)  \\n \\n  O      «<      refine_outline(t,      Op,      convos)  \\n \\n  return      O,      R       of   \\n \\nA      is      the      sum      of      the      counts      of      its      individual      items:'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 30, 'section_title': 'K       card(A)      =      S-      count      (A;)      (2)'}, 'page_content': 'K       card(A)      =      S-      count      (A;)      (2)\\ni=1 The      soft      heading      recall      is      calculated      as card(Gn      P)  \\n \\n  card(G)   \\n \\n”      @) soft      heading      recall      =\\nwhere      the      cardinality      of      intersection      is      defined      via  \\n \\n  the      union      as      follows:\\ncard(Gn      P)   \\n \\n=       card(G)   \\n \\n+      card(P)   \\n \\n—      card(G   \\n \\nU      P).\\n®'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 31, 'section_title': 'C.2.      LLM      Evaluator'}, 'page_content': 'C.2.      LLM      Evaluator\\nWe      use      Prometheus!\\n*      (Kim      et      al.,      2023),   \\n \\na      13B  \\n \\n  open-source      evaluator      LLM      that      can      assess      long-  \\n \\n  form      text      based      on      customized      1-5      scale      rubric,      to  \\n \\n  grade      the      article      from      the      aspects      of      Interest      level,  \\n \\n  Coherence      and      Organization,      Relevance      and      Fo-  \\n \\n  cus,      and      Coverage.\\nTable   \\n \\n8      gives      our      grading      rubric.\\n \\n \\n  While      Prometheus      is      best      used      with   \\n \\na      score   \\n \\n5      ref-  \\n \\n  erence      answer,      we      find      adding      the      reference      will  \\n \\n  exceed      the      context      length      limit      of      the      model.\\nSince  \\n \\n  Kim      et      al.\\n(2023)      show      Prometheus      ratings      without  \\n \\n  reference      also      correlate      well      with      human      prefer-  \\n \\n  ences,      we      omit      the      reference      and      trim      the      input  \\n \\n  article      to      be      within      2000      words      by      iteratively      re-  \\n \\n  moving      contents      from      the      shortest      section      to      ensure  \\n \\n  the      input      can      fit      into      the      model’s      context      window.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 32, 'section_title': 'C.3      More      Discussion      of      the      Citation      Quality'}, 'page_content': 'C.3      More      Discussion      of      the      Citation      Quality\\n \\n \\n  Irrelevant  \\n \\n  Source  \\n \\n  Inaccurate  \\n \\n  Othe      Paraphrasing  \\n \\n  1%      4%  \\n \\n  7%  \\n \\n  Improper  \\n \\n  Inferential      Linking  \\n \\n  Lack      Citation      14%  \\n \\n  47%  \\n \\n  Incorrectly      Split  \\n \\n  12%'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 33, 'section_title': 'False      Negative       15%'}, 'page_content': 'False      Negative       15%\\nFigure      6:      Error      analysis      of      unsupported      sentences      in      10  \\n \\n  sampled      articles.\\n \\n \\n  https:      //huggingface.co/kaist-ai/  \\n \\n  Criteria      Description  \\n \\n  Score   \\n \\n|      Description  \\n \\n  Score   \\n \\n2      Description  \\n \\n  Score   \\n \\n3      Description  \\n \\n  Score   \\n \\n4      Description  \\n \\n  Score   \\n \\n5      Description\\n \\n \\n  Interest      Level:      How      engaging      and      thought-provoking      is      the      article?\\n \\n \\n  Not      engaging      at      all;      no      attempt      to      capture      the      reader’s      attention.\\n \\n \\n  Fairly      engaging      with   \\n \\na      basic      narrative      but      lacking      depth.\\n \\n \\n  Moderately      engaging      with      several      interesting      points.\\n \\n \\n  Quite      engaging      with   \\n \\na      well-structured      narrative      and      noteworthy      points      that      frequently      capture      and      retain      attention.\\n \\n \\n  Exceptionally      engaging      throughout,      with   \\n \\na      compelling      narrative      that      consistently      stimulates      interest.\\n \\n \\n  Criteria      Description  \\n \\n  Score   \\n \\n|      Description  \\n \\n  Score   \\n \\n2      Description  \\n \\n  Score   \\n \\n3      Description  \\n \\n  Score   \\n \\n4      Description  \\n \\n  Score   \\n \\n5      Description  \\n \\n  Coherence      and      Organization:      Is      the      article      well-organized      and      logically      structured?\\n \\n \\n  Disorganized;      lacks      logical      structure      and      coherence.\\n \\n \\n  Fairly      organized;   \\n \\na      basic      structure      is      present      but      not      consistently      followed.\\n \\n \\n  Organized;   \\n \\na      clear      structure      is      mostly      followed      with      some      lapses      in      coherence.\\n \\n \\n  Good      organization;   \\n \\na      clear      structure      with      minor      lapses      in      coherence.\\n \\n \\n  Excellently      organized;      the      article      is      logically      structured      with      seamless      transitions      and   \\n \\na      clear      argument.\\n \\n \\n  Criteria      Description  \\n \\n  Score   \\n \\n|      Description  \\n \\n  Score   \\n \\n2      Description  \\n \\n  Score   \\n \\n3      Description  \\n \\n  Score   \\n \\n4      Description  \\n \\n  Score   \\n \\n5      Description  \\n \\n  Relevance      and      Focus:      Does      the      article      stay      on      topic      and      maintain   \\n \\na      clear      focus?\\n \\n \\n  Off-topic;      the      content      does      not      align      with      the      headline      or      core      subject.\\n \\n \\n  Somewhat      on      topic      but      with      several      digressions;      the      core      subject      is      evident      but      not      consistently      adhered      to.\\n \\n \\n  Generally      on      topic,      despite   \\n \\na      few      unrelated      details.\\n \\n \\n  Mostly      on      topic      and      focused;      the      narrative      has   \\n \\na      consistent      relevance      to      the      core      subject      with      infrequent      digressions.\\n \\n \\n  Exceptionally      focused      and      entirely      on      topic;      the      article      is      tightly      centered      on      the      subject,      with      every      piece      of      information      contributing  \\n \\n  to   \\n \\na      comprehensive      understanding      of      the      topic.\\n \\n \\n  Criteria      Description  \\n \\n  Score   \\n \\n|      Description  \\n \\n  Score   \\n \\n2      Description  \\n \\n  Score   \\n \\n3      Description  \\n \\n  Score   \\n \\n4      Description  \\n \\n  Score   \\n \\n5      Description  \\n \\n  Broad      Coverage:      Does      the      article      provide      an      in-depth      exploration      of      the      topic      and      have      good      coverage?\\n \\n \\n  Severely      lacking;      offers      little      to      no      coverage      of      the      topic’s      primary      aspects,      resulting      in   \\n \\na      very      narrow      perspective.\\n \\n \\n  Partial      coverage;      includes      some      of      the      topic’s      main      aspects      but      misses      others,      resulting      in      an      incomplete      portrayal.\\n \\n \\n  Acceptable      breadth;      covers      most      main      aspects,      though      it      may      stray      into      minor      unnecessary      details      or      overlook      some      relevant      points.\\n \\n \\n  Good      coverage;      achieves      broad      coverage      of      the      topic,      hitting      on      all      major      points      with      minimal      extraneous      information.\\n \\n \\n  Exemplary      in      breadth;      delivers      outstanding      coverage,      thoroughly      detailing      all      crucial      aspects      of      the      topic      without      including      irrelevant  \\n \\n  information.\\nTable      8:      Scoring      rubrics      on   \\n \\na      1-5      scale      for      the      evaluator      LLM.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 34, 'section_title': 'Error      Type       Topic      Unsupported      Sentence      Source'}, 'page_content': 'Error      Type       Topic      Unsupported      Sentence      Source\\n\\n |       Throughout      its      history,      religion      has      remained      the       paramount      aspect      of      Hawaiian      life      in      Lahaina      ,       permeating      every      daily      activity      and      significant      event[5]. |       [5]      “Religion,      Beliefs      &      Spirituality”       (The      source      discusses      religion      as      part      of      Hawaiian      life       but      does      not      mention      Lahania      .)\\n |       Lahaina,      Hawaii\\n |       [2]      “Crimean      Bridge      -      Wikipedia”       (The      source      says      “The      first      scheduled      passenger      train       crossed      the      bridge      on      25      December      2019,      while      the       bridge      was      opened      for      freight      trains      on      30      June      2020      ”.)       Completed      in      June      2020      ,      the      bridge      serves      as      a       major      supply      route      for      Russian      forces      in      the      region       and      is      significant      to      Russia’s      claim      over      the      disputed       territory[2][11].\\n | 2022      Crimean       Bridge      explosion\\n |       For      example,      comparisons      have      been      drawn      between       the      performance      of      LK-9      and      the      dynamic      resolution       capabilities      of      video      games      such      as      Battlefield      2042[22]. |       [22]      “Battlefield      2042      PC      performance      guide:      The      best       settings      for      a      high      frame      rate”       (      The      source      is      irrelevant      to      LK-99.      )\\n |       LK-99\\n\\nTable      9:      Examples      of      different      error      types      of      unsupported      sentences.\\nWe      use      Mistral      7B-Instruct!>      (Jiang      et      al.,      2023a)\\n \\n \\n  to      examine      whether      the      cited      passages      entail      the  \\n \\n  generated      sentence.\\nTable   \\n \\n4      reports      the      citation  \\n \\n  quality      of      articles      produced      by      our      approach,      show-  \\n \\n  ing      that      around      15%      sentences      in      generated      articles  \\n \\n  are      unsupported      by      citations.\\nWe      further      investi-  \\n \\n  gate      the      failure      cases      by      randomly      sampling      10  \\n \\n  articles      and      an      author      manually      examines      all      the  \\n \\n  unsupported      sentences      in      these      articles.\\nBesides  \\n \\n  sentences      that      are      incorrectly      split!®,      lack      citations,  \\n \\n  or      are      deemed      supported      by      the      author’s      judgment,  \\n \\n  our      analysis      identifies      three      main      error      categories  \\n \\n  (examples      are      given      in      Table      9):      improper      inferen-  \\n \\n  tial      linking,      inaccurate      paraphrasing,      and      citing  \\n \\n  irrelevant      sources.\\n \\n \\n  We      show      the      error      distribution      in      Figure      6.      No-  \\n \\n  tably,      the      most      common      errors      stem      from      the      ten-  \\n \\n  dency      of      LLMs      to      form      improper      inferential      links  \\n \\n  between      different      pieces      of      information      presented  \\n \\n  in      the      context      window.\\nOur      analysis      of      citation  \\n \\n  quality      suggests      that,      in      addition      to      avoiding      hallu-  \\n \\n  cinations,      future      research      in      grounded      text      gener-  \\n \\n  ation      should      also      focus      on      preventing      LLMs      from  \\n \\n  making      overly      inferential      leaps      based      on      the      pro-  \\n \\n  vided      information.\\nD      Human      Evaluation      Details\\n \\n \\n  We      recruited      10      experienced      Wikipedia      editors  \\n \\n  to      participate      in      our      study      by      creating   \\n \\na      research  \\n \\n  page      on      Meta-Wiki!”\\nand      reaching      out      to      active editors      who      have      recently      approved      articles      for  \\n \\n  Wikipedia.\\\\\\\\\\\\\\'®      Our      participation      group      includes   \\n \\n3       editors      with      1-5      years      of      experience,   \\n \\n4      with      6-10  \\n \\n  years,      and   \\n \\n3      with      over      15      years      of      contribution.\\n \\n \\n  The      study      was      approved      by      the      Institutional      Re-  \\n \\n  view      Board      of      our      institution      and      the      participants  \\n \\n  signed      the      consent      form      through      Qualtrics      ques-  \\n \\n  tionnaires      before      the      study      started.\\n \\n \\n  To      streamline      the      evaluation      of      grounded      articles,  \\n \\n  we      developed   \\n \\na      web      application,      which      features   \\n \\na       side-by-side      display      of      the      article      and      its      citation  \\n \\n  snippets,      to      gather      ratings      and      open-ended      feedback  \\n \\n  Shttps      ://huggingface.co/mistralai/  \\n \\n  Mistral-7B-Instruct-vQ.1  \\n \\n  \\\\\\\\\\\\\\'6Rollowing      Gao      et      al.\\n(2023),      we      check      citation      quality      in  \\n \\n  the      sentence      level      and      split      articles      into      sentences      using      NLTK  \\n \\n  sent_tokenize.\\nsent_tokenize      sometimes      fails      to      split      sen-  \\n \\n  tences      correctly      when      the      article      contains      special      words      like  \\n \\n  “No.12847”,      “Bhatia      et      al.\\n”,      etc.\\n \\n \\n  \"https      ://meta.wikimedia.org  \\n \\n  \\\\\\\\\\\\\\'8Since      evaluating      Wikipedia-like      articles      is      time-  \\n \\n  consuming      and      requires      expertise,      we      paid      each      participant  \\n \\n  50$      for      our      study.\\n \\n \\n  for      each      article.\\nFigure   \\n \\n7      shows      the      screenshot      of  \\n \\n  our      web      application      and      the      full      article      produced  \\n \\n  by      STORM      is      included      in      Table      12.\\nFor      human  \\n \\n  evaluation,      we      use   \\n \\na   \\n \\n|      to   \\n \\n7      scale      for      more      fine-  \\n \\n  grained      evaluation.\\nThe      grading      rubric      is      included  \\n \\n  in      Table      10.\\n \\n \\n  We      collected      the      pairwise      preferences      and      the  \\n \\n  perceived      usefulness      of      STORM      via      an      online      ques-  \\n \\n  tionnaire.\\nSpecifically,      for      the      perceived      usefulness,  \\n \\n  we      request      editors      to      rate      their      agreement      with      state-  \\n \\n  ments      “I      think      it      can      be      specifically      helpful      for      my  \\n \\n  pre-writing      stage      (e.g.,      collecting      relevant      sources,  \\n \\n  outlining,      drafting).\\n”,      “I      think      it      will      help      me      edit  \\n \\n  a      Wikipedia      article      for   \\n \\na      new      topic”,      “I      think      it  \\n \\n  can      be   \\n \\na      potentially      useful      tool      for      the      Wikipedia  \\n \\n  community”      on   \\n \\na      Likert      scale      of      1-5,      correspond-  \\n \\n  ing      to      Strongly      disagree,      Somewhat      disagree,      Nei-  \\n \\n  ther      agree      nor      disagree,      Somewhat      agree,      Strongly agree.\\nE_      Error      Analysis\\n \\n \\n  While      articles      produced      by      STORM      are      preferred  \\n \\n  by      both      automatic      metrics      and      human      evaluation,  \\n \\n  experienced      editors      still      identified      multiple      prob-  \\n \\n  lems      with      the      machine-generated      articles.\\nWe      an-  \\n \\n  alyze      the      free-form      comments      and      summarize      the  \\n \\n  major      issues      in      Table      11.\\n \\n \\n  The      primary      issue      raised      is      that      the      generated  \\n \\n  articles      often      contain      emotional      language      and      lack  \\n \\n  neutrality,      primarily      due      to      the      source      material.\\n \\n \\n  STORM      currently      retrieves      grounding      sources  \\n \\n  from      the      Internet      which      is      not      neutral      and      con-  \\n \\n  tains      considerable      promotional      content      on      its      own.\\n \\n \\n  Addressing      this      bias      in      the      pre-writing      stage      repre-  \\n \\n  sents   \\n \\na      valuable      direction      for      future      research.\\nAn-  \\n \\n  other      major      issue      is      the      red      herring      fallacy      or      the  \\n \\n  over-association      of      unrelated      facts.\\nAddressing      this  \\n \\n  challenge      calls      for      high-level      sensemaking      rather  \\n \\n  than      mere      fact-level      verification.\\n \\n \\n  Interest      Level  \\n \\n  Not      engaging      at      all;      no      attempt      to      capture      the      reader’s      attention.\\n \\n \\n  Slightly      engaging      with      rare      moments      that      capture      attention.\\n \\n \\n  Fairly      engaging      with   \\n \\na      basic      narrative      but      lacking      depth.\\n \\n \\n  Moderately      engaging      with      several      interesting      points.\\n \\n \\n  Quite      engaging      with   \\n \\na      well-structured      narrative      and      noteworthy      points      that      frequently      capture      and      retain      attention.\\n \\n \\n  Very      engaging      with   \\n \\na      compelling      narrative      that      captures      and      mostly      retains      attention.\\n \\n \\n  Exceptionally      engaging      throughout,      with   \\n \\na      compelling      narrative      that      consistently      stimulates      interest.\\nMOawWPYWNr\\n \\n \\n  Coherence      and      Organization  \\n \\n  Disorganized;      lacks      logical      structure      and      coherence.\\n \\n \\n  Poor      organization;      some      structure      is      evident      but      very      weak.\\n \\n \\n  Fairly      organized;   \\n \\na      basic      structure      is      present      but      not      consistently      followed.\\n \\n \\n  Organized;   \\n \\na      clear      structure      is      mostly      followed      with      some      lapses      in      coherence.\\n \\n \\n  Good      organization;   \\n \\na      clear      structure      with      minor      lapses      in      coherence.\\n \\n \\n  Very      well-organized;   \\n \\na      logical      structure      with      transitions      that      effectively      guide      the      reader.\\n \\n \\n  Excellently      organized;      the      article      is      logically      structured      with      seamless      transitions      and   \\n \\na      clear      argument.\\naw:\\n \\n \\n  Relevance      and      Focus  \\n \\n  1:      Off-topic;      the      content      does      not      align      with      the      headline      or      core      subject.\\n \\n \\n  2:      Mostly      off-topic      with      some      relevant      points.\\n \\n \\n  3:      Somewhat      on      topic      but      with      several      digressions;      the      core      subject      is      evident      but      not      consistently      adhered      to.\\n \\n \\n  4:      Generally      on      topic,      despite   \\n \\na      few      unrelated      details.\\n \\n \\n  5:      Mostly      on      topic      and      focused;      the      narrative      has   \\n \\na      consistent      relevance      to      the      core      subject      with      infrequent      digressions.\\n \\n \\n  6:      Highly      relevant      with   \\n \\na      focused      narrative      and      purpose.\\n \\n \\n  7:      Exceptionally      focused      and      entirely      on      topic;      the      article      is      tightly      centered      on      the      subject,      with      every      piece      of      information      contributing      to   \\n \\na       comprehensive      understanding      of      the      topic.\\n \\n \\n  Broad      Coverage  \\n \\n  Severely      lacking;      offers      little      to      no      coverage      of      the      topic’s      primary      aspects,      resulting      in   \\n \\na      very      narrow      perspective.\\n \\n \\n  Minimal      coverage;      addresses      only   \\n \\na      small      selection      of      the      topic’s      main      aspects,      with      significant      omissions.\\n \\n \\n  Partial      coverage;      includes      some      of      the      topic’s      main      aspects      but      misses      others,      resulting      in      an      incomplete      portrayal.\\n \\n \\n  Acceptable      breadth;      covers      most      main      aspects,      though      it      may      stray      into      minor      unnecessary      details      or      overlook      some      relevant      points.\\n \\n \\n  Good      coverage;      achieves      broad      coverage      of      the      topic,      hitting      on      all      major      points      with      minimal      extraneous      information.\\n \\n \\n  Comprehensive;      provides      thorough      coverage      of      all      significant      aspects      of      the      topic,      with   \\n \\na      well-balanced      focus.\\n \\n \\n  Exemplary      in      breadth;      delivers      outstanding      coverage,      thoroughly      detailing      all      crucial      aspects      of      the      topic      without      including      irrelevant      information.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 35, 'section_title': 'We      use      Mistral      7B-Instruct!>      (Jiang      et      al.,      2023a)'}, 'page_content': 'We      use      Mistral      7B-Instruct!>      (Jiang      et      al.,      2023a)\\n \\n \\n  to      examine      whether      the      cited      passages      entail      the  \\n \\n  generated      sentence.\\nTable   \\n \\n4      reports      the      citation  \\n \\n  quality      of      articles      produced      by      our      approach,      show-  \\n \\n  ing      that      around      15%      sentences      in      generated      articles  \\n \\n  are      unsupported      by      citations.\\nWe      further      investi-  \\n \\n  gate      the      failure      cases      by      randomly      sampling      10  \\n \\n  articles      and      an      author      manually      examines      all      the  \\n \\n  unsupported      sentences      in      these      articles.\\nBesides  \\n \\n  sentences      that      are      incorrectly      split!®,      lack      citations,  \\n \\n  or      are      deemed      supported      by      the      author’s      judgment,  \\n \\n  our      analysis      identifies      three      main      error      categories  \\n \\n  (examples      are      given      in      Table      9):      improper      inferen-  \\n \\n  tial      linking,      inaccurate      paraphrasing,      and      citing  \\n \\n  irrelevant      sources.\\n \\n \\n  We      show      the      error      distribution      in      Figure      6.      No-  \\n \\n  tably,      the      most      common      errors      stem      from      the      ten-  \\n \\n  dency      of      LLMs      to      form      improper      inferential      links  \\n \\n  between      different      pieces      of      information      presented  \\n \\n  in      the      context      window.\\nOur      analysis      of      citation  \\n \\n  quality      suggests      that,      in      addition      to      avoiding      hallu-  \\n \\n  cinations,      future      research      in      grounded      text      gener-  \\n \\n  ation      should      also      focus      on      preventing      LLMs      from  \\n \\n  making      overly      inferential      leaps      based      on      the      pro-  \\n \\n  vided      information.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 36, 'section_title': 'D      Human      Evaluation      Details'}, 'page_content': 'D      Human      Evaluation      Details\\n \\n \\n  We      recruited      10      experienced      Wikipedia      editors  \\n \\n  to      participate      in      our      study      by      creating   \\n \\na      research  \\n \\n  page      on      Meta-Wiki!”\\nand      reaching      out      to      active editors      who      have      recently      approved      articles      for  \\n \\n  Wikipedia.\\\\\\\\\\\\\\'®      Our      participation      group      includes   \\n \\n3       editors      with      1-5      years      of      experience,   \\n \\n4      with      6-10  \\n \\n  years,      and   \\n \\n3      with      over      15      years      of      contribution.\\n \\n \\n  The      study      was      approved      by      the      Institutional      Re-  \\n \\n  view      Board      of      our      institution      and      the      participants  \\n \\n  signed      the      consent      form      through      Qualtrics      ques-  \\n \\n  tionnaires      before      the      study      started.\\n \\n \\n  To      streamline      the      evaluation      of      grounded      articles,  \\n \\n  we      developed   \\n \\na      web      application,      which      features   \\n \\na       side-by-side      display      of      the      article      and      its      citation  \\n \\n  snippets,      to      gather      ratings      and      open-ended      feedback  \\n \\n  Shttps      ://huggingface.co/mistralai/  \\n \\n  Mistral-7B-Instruct-vQ.1  \\n \\n  \\\\\\\\\\\\\\'6Rollowing      Gao      et      al.\\n(2023),      we      check      citation      quality      in  \\n \\n  the      sentence      level      and      split      articles      into      sentences      using      NLTK  \\n \\n  sent_tokenize.\\nsent_tokenize      sometimes      fails      to      split      sen-  \\n \\n  tences      correctly      when      the      article      contains      special      words      like  \\n \\n  “No.12847”,      “Bhatia      et      al.\\n”,      etc.\\n \\n \\n  \"https      ://meta.wikimedia.org  \\n \\n  \\\\\\\\\\\\\\'8Since      evaluating      Wikipedia-like      articles      is      time-  \\n \\n  consuming      and      requires      expertise,      we      paid      each      participant  \\n \\n  50$      for      our      study.\\n \\n \\n  for      each      article.\\nFigure   \\n \\n7      shows      the      screenshot      of  \\n \\n  our      web      application      and      the      full      article      produced  \\n \\n  by      STORM      is      included      in      Table      12.\\nFor      human  \\n \\n  evaluation,      we      use   \\n \\na   \\n \\n|      to   \\n \\n7      scale      for      more      fine-  \\n \\n  grained      evaluation.\\nThe      grading      rubric      is      included  \\n \\n  in      Table      10.\\n \\n \\n  We      collected      the      pairwise      preferences      and      the  \\n \\n  perceived      usefulness      of      STORM      via      an      online      ques-  \\n \\n  tionnaire.\\nSpecifically,      for      the      perceived      usefulness,  \\n \\n  we      request      editors      to      rate      their      agreement      with      state-  \\n \\n  ments      “I      think      it      can      be      specifically      helpful      for      my  \\n \\n  pre-writing      stage      (e.g.,      collecting      relevant      sources,  \\n \\n  outlining,      drafting).\\n”,      “I      think      it      will      help      me      edit  \\n \\n  a      Wikipedia      article      for   \\n \\na      new      topic”,      “I      think      it  \\n \\n  can      be   \\n \\na      potentially      useful      tool      for      the      Wikipedia  \\n \\n  community”      on   \\n \\na      Likert      scale      of      1-5,      correspond-  \\n \\n  ing      to      Strongly      disagree,      Somewhat      disagree,      Nei-  \\n \\n  ther      agree      nor      disagree,      Somewhat      agree,      Strongly agree.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 37, 'section_title': 'E_      Error      Analysis'}, 'page_content': 'E_      Error      Analysis\\n \\n \\n  While      articles      produced      by      STORM      are      preferred  \\n \\n  by      both      automatic      metrics      and      human      evaluation,  \\n \\n  experienced      editors      still      identified      multiple      prob-  \\n \\n  lems      with      the      machine-generated      articles.\\nWe      an-  \\n \\n  alyze      the      free-form      comments      and      summarize      the  \\n \\n  major      issues      in      Table      11.\\n \\n \\n  The      primary      issue      raised      is      that      the      generated  \\n \\n  articles      often      contain      emotional      language      and      lack  \\n \\n  neutrality,      primarily      due      to      the      source      material.\\n \\n \\n  STORM      currently      retrieves      grounding      sources  \\n \\n  from      the      Internet      which      is      not      neutral      and      con-  \\n \\n  tains      considerable      promotional      content      on      its      own.\\n \\n \\n  Addressing      this      bias      in      the      pre-writing      stage      repre-  \\n \\n  sents   \\n \\na      valuable      direction      for      future      research.\\nAn-  \\n \\n  other      major      issue      is      the      red      herring      fallacy      or      the  \\n \\n  over-association      of      unrelated      facts.\\nAddressing      this  \\n \\n  challenge      calls      for      high-level      sensemaking      rather  \\n \\n  than      mere      fact-level      verification.\\n \\n \\n  Interest      Level  \\n \\n  Not      engaging      at      all;      no      attempt      to      capture      the      reader’s      attention.\\n \\n \\n  Slightly      engaging      with      rare      moments      that      capture      attention.\\n \\n \\n  Fairly      engaging      with   \\n \\na      basic      narrative      but      lacking      depth.\\n \\n \\n  Moderately      engaging      with      several      interesting      points.\\n \\n \\n  Quite      engaging      with   \\n \\na      well-structured      narrative      and      noteworthy      points      that      frequently      capture      and      retain      attention.\\n \\n \\n  Very      engaging      with   \\n \\na      compelling      narrative      that      captures      and      mostly      retains      attention.\\n \\n \\n  Exceptionally      engaging      throughout,      with   \\n \\na      compelling      narrative      that      consistently      stimulates      interest.\\nMOawWPYWNr\\n \\n \\n  Coherence      and      Organization  \\n \\n  Disorganized;      lacks      logical      structure      and      coherence.\\n \\n \\n  Poor      organization;      some      structure      is      evident      but      very      weak.\\n \\n \\n  Fairly      organized;   \\n \\na      basic      structure      is      present      but      not      consistently      followed.\\n \\n \\n  Organized;   \\n \\na      clear      structure      is      mostly      followed      with      some      lapses      in      coherence.\\n \\n \\n  Good      organization;   \\n \\na      clear      structure      with      minor      lapses      in      coherence.\\n \\n \\n  Very      well-organized;   \\n \\na      logical      structure      with      transitions      that      effectively      guide      the      reader.\\n \\n \\n  Excellently      organized;      the      article      is      logically      structured      with      seamless      transitions      and   \\n \\na      clear      argument.\\naw:\\n \\n \\n  Relevance      and      Focus  \\n \\n  1:      Off-topic;      the      content      does      not      align      with      the      headline      or      core      subject.\\n \\n \\n  2:      Mostly      off-topic      with      some      relevant      points.\\n \\n \\n  3:      Somewhat      on      topic      but      with      several      digressions;      the      core      subject      is      evident      but      not      consistently      adhered      to.\\n \\n \\n  4:      Generally      on      topic,      despite   \\n \\na      few      unrelated      details.\\n \\n \\n  5:      Mostly      on      topic      and      focused;      the      narrative      has   \\n \\na      consistent      relevance      to      the      core      subject      with      infrequent      digressions.\\n \\n \\n  6:      Highly      relevant      with   \\n \\na      focused      narrative      and      purpose.\\n \\n \\n  7:      Exceptionally      focused      and      entirely      on      topic;      the      article      is      tightly      centered      on      the      subject,      with      every      piece      of      information      contributing      to   \\n \\na       comprehensive      understanding      of      the      topic.\\n \\n \\n  Broad      Coverage  \\n \\n  Severely      lacking;      offers      little      to      no      coverage      of      the      topic’s      primary      aspects,      resulting      in   \\n \\na      very      narrow      perspective.\\n \\n \\n  Minimal      coverage;      addresses      only   \\n \\na      small      selection      of      the      topic’s      main      aspects,      with      significant      omissions.\\n \\n \\n  Partial      coverage;      includes      some      of      the      topic’s      main      aspects      but      misses      others,      resulting      in      an      incomplete      portrayal.\\n \\n \\n  Acceptable      breadth;      covers      most      main      aspects,      though      it      may      stray      into      minor      unnecessary      details      or      overlook      some      relevant      points.\\n \\n \\n  Good      coverage;      achieves      broad      coverage      of      the      topic,      hitting      on      all      major      points      with      minimal      extraneous      information.\\n \\n \\n  Comprehensive;      provides      thorough      coverage      of      all      significant      aspects      of      the      topic,      with   \\n \\na      well-balanced      focus.\\n \\n \\n  Exemplary      in      breadth;      delivers      outstanding      coverage,      thoroughly      detailing      all      crucial      aspects      of      the      topic      without      including      irrelevant      information.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 38, 'section_title': 'TMAWP      YN'}, 'page_content': 'TMAWP      YN\\nVerifiability\\n \\n \\n  1:      No      supporting      evidence;      claims      are      unsubstantiated.\\n \\n \\n  2:      Rarely      supported      with      evidence;      many      claims      are      unsubstantiated.\\n \\n \\n  3:      Inconsistently      verified;      some      claims      are      supported;      evidence      is      occasionally      provided.\\n \\n \\n  4:      Generally      verified;      claims      are      usually      supported      with      evidence;      however,      there      might      be   \\n \\na      few      instances      where      verification      is      lacking  \\n \\n  5:      Well-supported;      claims      are      very      well      supported      with      credible      evidence,      and      instances      of      unsupported      claims      are      rare.\\n \\n \\n  6:      Very      well-supported;      almost      every      claim      is      substantiated      with      credible      evidence,      showing   \\n \\na      high      level      of      thorough      verification.\\n \\n \\n  7:      Exemplary      verification;      each      claim      is      supported      by      robust,      credible      evidence      from      authoritative      sources,      reflecting      strict      adherence      to      the      no  \\n \\n  original      research      policy.\\nTable      10:      Scoring      rubrics      on   \\n \\na      1-7      scale      for      human      evaluation.\\nIssue       Mentioned      Time       Example      Comments\\n \\n \\n  The      word      “significant”      is      used      17      times      in      this      article.\\nVague      and      unsupported      claims      are  \\n \\n  made      about      broader      political      importance      and      “pivotal      role[s]”,      and      is      unencyclopedic.\\nUse      of      emotional      words,  \\n \\n  (comment      on      article      Lahaina,      Hawaii) [] but they still have not fixed the issue of neutral point of view.\\nIt is also evident in this  \\n \\n  article      that      the      writer’s      standpoint      is      biased      towards      Taylor      Swift.\\nOther      than      that,      it      did  \\n \\n  a      good      job      at      summarizing      key      points      and      putting      depth      into      this.\\n \\n \\n  unneutral      12      (comment      on      article      Speak      Now      (Taylor’s      Version))  \\n \\n  “The      film      was      also      featured      in      an      art      and      film      festival      hosted      by      The      California      Endowment,  \\n \\n  highlighting      the      power      of      stories      in      reshaping      narratives      about      communities.”\\nYes,      technically  \\n \\n  the      source      says      that,      but      it’s   \\n \\na      stretch      to      say      in      Wikipedia      voice      and      just      sounds      like  \\n \\n  non-neutral,      promotional      prose.\\n(comment      on      article      Gehraiyaan)  \\n \\n  Polling      from      America      shouldn’t      be      included      and      links      to      climate      change      shouldn’t      be  \\n \\n  made      unless      explicitly      connected      by      the      source.\\n(comment      on      article      Typhoon      Hinnamnor)  \\n \\n  Red      herring      fallacy,   \\n \\nu      Sourcing      seems      mostly      fine,      though      some      aren’t      directly      related      (Ex.      39,40).\\n \\n \\n  associating      unrelated      sources      (comment      on      article      Gehraiyaan)  \\n \\n  Here      is   \\n \\na      lengthy      digression      about      KISS,      not      necessary      because      the      article      on      the      band  \\n \\n  should      be      linked      to.\\n(comment      on      article      2022      AFL      Grand      Final)  \\n \\n  “One      study,      conducted      by      Sinéad      Griffin,   \\n \\na      physicist      at      the      Lawrence      Berkeley      National  \\n \\n  Laboratory,      provided      some      analysis      of      LK-99’s      abilities      using      supercomputer      simulations[20].”\\n \\n \\n  This      is      not      enough      information      about      the      analysis,      which      would      have      been      very      useful      in      the  \\n \\n  rr  \\n \\n.    \\n.\\n     article.\\n(comment      on      article      LK-99)  \\n \\n  Missing      important      information   \\n \\n6       Although      the      earthquake’s      immediate      aftermath      and      response      are      adequately      covered,      there  \\n \\n  could      be      more      about      the      long-term      socioeconomic      impact      and      recovery      processes.\\n \\n \\n  (comment      on      article      2022      West      Java      earthquake)  \\n \\n  Words      like      “now”      should      be      avoided      in      Wikipedia      articles      to      prevent      them      from      becoming  \\n \\n  dated      and      phrases      such      as,      “as      of      December      2023”      should      be      used      instead.\\n \\n \\n  Improper      handling      of   \\n \\n5      (comment      on      article      Cyclone      Batsirai)  \\n \\n  time-sensitive      information      “as      of      December      13”      doesn’t      specify   \\n \\na      year,      and      is      old      information  \\n \\n  (comment      on      article      2022      West      Java      earthquake) too      many      subsections      in      the      “Recovery      and      Rehabilitation”      section  \\n \\n  (comment      on      article      2022      West      Java      earthquake)\\n      (comment      on      article      2022      Crimean      Bridge      explosion)\\n \\n \\n  Section      organization      problem   \\n \\n5   \\n \\nI      do      not      like      how      the      article      is      organized,      with      too      many      headers      cluttering      the      article,  \\n \\n  making      it      not      as      readable.\\nOther      than      that,      the      AI      did      great      work      on      the      piece.\\n |       Select      a      key       8      v       Selected      Key:      8       Title:      Taylor      Hawkins      -      marshall.com       Snippets:       In      1995,      off      the      back      of      her      hit      album       ‘Jagged      Little      Pill’,      Canadian      American       superstar      Alanis      Morissette      recruited      him       to      be      her      touring      drummer      for      her      18       month      album      tour,      along      with      him       featuring      in      the      music      videos      for      “You       Oughta      Know’,      “All|      Really      Want”      and       “You      Learn’.      It      was      during      this      tour      that      he       met      his      musical      soul      mate,      Dave      Grohl.       The      tour      with      Alanis      ended      and      he\\\\\\\\\\\\\\'d      heard       that      Dave      and      Foo      Fighters      were      looking       for      anew      drummer,      so      Taylor      enquired.       Dave      initially      thought      that      Taylor      wouldn’t       be      interested      in      joining      as      Alanis       Morissette      was      much      bigger      than      the      Foo       Fighters      at      that      time,      but      Taylor      jumped      at       the      chance      due      to      his      overwhelming       desire      to      be      in      a      rock      band.      It      was      from       that      point      that      the      bromance      between       them      began      and      on      March      18th      1997       Taylor      was      announced      as      their      new       drummer.       Url:      https://marshall.com/live-for-       music/drum-icons/taylor-hawkins | Logout  \\n \\n  Select      an      option:  \\n \\n  (11,      ‘Taylor      Hawkins      [Quite      Richard}’)   \\n \\n¥       You      selected:      Taylor      Hawkins      [Quite      Richard]  \\n \\n  Taylor      Hawkins  \\n \\n  Oliver      Taylor      Hawkins      (1972-2022)      was      an      American      musician,      best      known      as      the      drummer      for      the      rock      band      Foo      Fighters[1].\\nBorn      in      Fort      Walton,      Texas,      Hawkins’      love      for      music      was      ignited      at   \\n \\na      young  \\n \\n  age,      particularly      after      watching   \\n \\na      Queen      concert      in      1982[2][3][5].\\nHe      kick-started      his      professional      career      as      the      drummer      for      Alanis      Morissette’s      band      during      her      world      tour      for      the      hit      album      ‘Jagged      Little  \\n \\n  Pill\\\\\\\\\\\\\\'[8][9].\\nHis      talents      were      recognized      by      Dave      Grohl,      who      invited      him      to      join      the      Foo      Fighters      in      1997,      marking      the      beginning      of      his      impactful      tenure      with      the      band{7][8].\\n \\n \\n  Hawkins      was      celebrated      for      his      versatile      drumming      style,      drawing      influence      from      renowned      drummers      like      Roger      Taylor,      Neil      Peart,      Phil      Collins,      Alex      Van      Halen,      and      Stewart      Copeland[14).\\nHis  \\n \\n  performances,      marked      by   \\n \\na      unique      energy      and      aggressive      style      of      play,      earned      him      recognition      as      one      of      the      top      rock      drummers      of      his      era[15].\\nApart      from      his      role      in      the      Foo      Fighters,      Hawkins\\\\\\\\\\\\\\'      passion  \\n \\n  for      music      saw      him      involved      in      numerous      side      projects      and      collaborations,      cementing      his      place      in      the      world      of      rock      music(10].\\n \\n \\n  Outside      of      his      professional      life,      Hawkins      was      known      for      his      resilience      and      dedication      to      his      family.\\nDespite      personal      struggles,      including   \\n \\na      near-fatal      drug      overdose      in      2001,      Hawkins      remained      committed  \\n \\n  to      his      musical      career{4][9].\\nHis      legacy      continues      to      inspire      musicians      and      fans      alike,      as      his      contributions      to      rock      music,      coupled      with      his   \\n \\ni       industry(13}.\\n \\n \\n  spirit,      made      him      an      icon      in      the      music  \\n \\n  Hawkins’      sudden      death      in      2022      while      on      tour      in      Bogotd,      Colombia,      sent      shockwaves      through      the      music      world[34].\\nTributes      poured      in      from      around      the      globe,      reflecting      the      respect      and      admiration  \\n \\n  Hawkins      had      garnered      during      his      lifetime[21][31].\\nHis      life      and      career      were      honored      at   \\n \\na      star-studded      tribute      concert      in      Los      Angeles,      attesting      to      the      enduring      impact      of      his      music[22].\\n | Early      Life      and      Background\\n |       Oliver      Taylor      Hawkins,      known      as      Taylor      Hawkins,      was      born      and      raised      in      Fort      Walton,      Texas[3].      His      family      moved      to      Laguna      Beach,      California      when      he      was      four      years      old[3].      He      has      two      younger      siblings,       a      brother      named      Jason,      and      a      sister      named      Heather[3].      Asa      child,      Hawkins      wa:      influenced      by      his      paternal      Josie      Hawkins,      who      had      grown      up      during      the      Great      Depression      and       lived      in      Jackson,      Mississippi(2].\\n |       During      his      high      school      days      at      Laguna      Beach      High      School,      from      where      he      graduated      in      1990,      he      became      friends      with      Jon      Davison,      who      later      became      the      lead      vocalist      of      the      band      Yes(2][3].      His      interest      in       music      was      nurtured      from      an      early      age,      particularly      after      watching      a      Queen      concert      in      1982      which      inspired      him      to      learn      to      play      the      drums(2][5].      He      noted      that      music      was      a      constant      presence      in      his      family       Taylor      Hawkins\\n\\n \\n \\n  Oliver      Taylor      Hawkins      (1972-2022)      was      an      American      musician,      best      known      as      the      drummer      for      the      rock  \\n \\n  band      Foo      Fighters[1].\\nBorn      in      Fort      Walton,      Texas,      Hawkins’      love      for      music      was      ignited      at   \\n \\na      young      age,  \\n \\n  particularly      after      watching   \\n \\na      Queen      concert      in      1982[2][3][5].\\nHe      kick-started      his      professional      career  \\n \\n  as      the      drummer      for      Alanis      Morissette’s      band      during      her      world      tour      for      the      hit      album      ’Jagged      Little  \\n \\n  Pill’[8]L[9].\\nHis      talents      were      recognized      by      Dave      Grohl,      who      invited      him      to      join      the      Foo      Fighters      in  \\n \\n  1997,      marking      the      beginning      of      his      impactful      tenure      with      the      band[7][8].\\n \\n \\n  Hawkins      was      celebrated      for      his      versatile      drumming      style,      drawing      influence      from      renowned      drummers      like  \\n \\n  Roger      Taylor,      Neil      Peart,      Phil      Collins,      Alex      Van      Halen,      and      Stewart      Copeland[14].\\nHis      performances,  \\n \\n  marked      by   \\n \\na      unique      energy      and      aggressive      style      of      play,      earned      him      recognition      as      one      of      the      top      rock  \\n \\n  drummers      of      his      era[15].\\nApart      from      his      role      in      the      Foo      Fighters,      Hawkins’      passion      for      music      saw  \\n \\n  him      involved      in      numerous      side      projects      and      collaborations,      cementing      his      place      in      the      world      of      rock  \\n \\n  music[10].\\n \\n \\n  Outside      of      his      professional      life,      Hawkins      was      known      for      his      resilience      and      dedication      to      his      family.\\n \\n \\n  Despite      personal      struggles,      including   \\n \\na      near-fatal      drug      overdose      in      2001,      Hawkins      remained      committed      to  \\n \\n  his      musical      career[4][9].\\nHis      legacy      continues      to      inspire      musicians      and      fans      alike,      as      his      contributions  \\n \\n  to      rock      music,      coupled      with      his      indomitable      spirit,      made      him      an      unforgettable      icon      in      the      music  \\n \\n  industry[13].\\n \\n \\n  Hawkins’      sudden      death      in      2022      while      on      tour      in      Bogota,      Colombia,      sent      shockwaves      through      the      music  \\n \\n  world[34].\\nTributes      poured      in      from      around      the      globe,      reflecting      the      respect      and      admiration      Hawkins  \\n \\n  had      garnered      during      his      lifetime[21][31].\\nHis      life      and      career      were      honored      at   \\n \\na      star-studded      tribute  \\n \\n  concert      in      Los      Angeles,      attesting      to      the      enduring      impact      of      his      music[22].\\n#\\n \\n   Early      Life      and      Background  \\n \\n  Oliver      Taylor      Hawkins,      known      as      Taylor      Hawkins,      was      born      and      raised      in      Fort      Walton,      Texas[3].\\nHis  \\n \\n  family      moved      to      Laguna      Beach,      California      when      he      was      four      years      old[3].\\nHe      has      two      younger      siblings,  \\n \\n  a      brother      named      Jason,      and   \\n \\na      sister      named      Heather[3].\\nAs   \\n \\na      child,      Hawkins      was      particularly      influenced  \\n \\n  by      his      paternal      grandmother,      Josie      Hawkins,      who      had      grown      up      during      the      Great      Depression      and      lived      in  \\n \\n  Jackson,      Mississippi[1].\\n \\n \\n  During      his      high      school      days      at      Laguna      Beach      High      School,      from      where      he      graduated      in      1990,      he      became  \\n \\n  friends      with      Jon      Davison,      who      later      became      the      lead      vocalist      of      the      band      Yes[2][3].\\nHis      interest      in  \\n \\n  music      was      nurtured      from      an      early      age,      particularly      after      watching   \\n \\na      Queen      concert      in      1982      which      inspired  \\n \\n  him      to      learn      to      play      the      drums[2][5].\\nHe      noted      that      music      was   \\n \\na      constant      presence      in      his      family      home[5].\\n \\n \\n  Despite      facing      certain      hardships      during      his      upbringing,      including      his      mother’s      struggles      with      \"demons\",  \\n \\n  Hawkins      pursued      his      musical      ambitions[4].\\nHe      credits      his      older      sister      Heather      for      taking      care      of      the  \\n \\n  family      during      difficult      times[4].\\n \\n \\n  His      first      major      musical      experience      came      from      playing      drums      for      Alanis      Morissette’s      album,      Jagged      Little  \\n \\n  Pill,      and      accompanying      her      on      the      subsequent      tour[3].\\nThis      marked      the      beginning      of      his      professional  \\n \\n  career      in      the      music      industry.\\n#\\n \\n   Career  \\n \\n  Taylor      Hawkins      began      his      professional      music      career      playing      in      Alanis      Morissette’s      band      during      her  \\n \\n  18-month      world      tour      in      support      of      the      hit      album      ’Jagged      Little      Pill’      from      1995      to      1997[8][9].\\nHis  \\n \\n  performances      not      only      in      the      tour      but      also      in      the      music      videos      for      “You      Oughta      Know”,      “All   \\n \\nI      Really      Want”  \\n \\n  and      “You      Learn”      introduced      him      to      the      world      of      rock      music      and      ultimately      led      to      his      meeting      with      Dave  \\n \\n  Grohl[8].\\nThroughout      this      time,      Hawkins      contributed      significantly      to      the      band’s      sound      and      performance,  \\n \\n  transforming      the      songs      from      their      original      drum      loop      format      to   \\n \\na      rock-band      vibe      that      resonated      with  \\n \\n  audiences[1][7].\\n \\n \\n  In      1997,      Hawkins      was      asked      by      Grohl      to      join      the      Foo      Fighters,      an      invitation      that      he      readily      accepted[7][8].\\n \\n \\n  At      the      time,      Grohl      thought      it      was   \\n \\na      long      shot      to      recruit      Hawkins      given      that      Morissette      was      at      the      height  \\n \\n  of      her      career,      but      Hawkins’      desire      to      be   \\n \\na      part      of   \\n \\na      rock      band      compelled      him      to      make      the      move[7].\\nThis  \\n \\n  marked      the      beginning      of      Hawkins’      tenure      as      the      drummer      of      the      Foo      Fighters,   \\n \\na      role      that      he      would      play  \\n \\n  until      his      passing[6][9].\\n \\n \\n  Apart      from      his      work      with      Morissette      and      the      Foo      Fighters,      Hawkins      had      an      array      of      other      musical  \\n \\n  experiences[10].\\nHe      drummed      for      Sass      Jordan      before      joining      Morissette’s      touring      band[10].\\nHe      was      part  \\n \\n  of      an      ad      hoc      drum      supergroup      called      SOS      Allstars      and      filled      the      void      for      Coheed      and      Cambria’s      2007  \\n \\n  album      after      their      drummer      Josh      Eppard      left      the      group[10].\\nIn      addition,      Hawkins      formed      his      own      side  \\n \\n  project,      the      Coattail      Riders,      in      2005,      through      which      he      recorded      his      own      music      and      took      the      project      on  \\n \\n  the      road,      performing      in      small      clubs      despite      the      Foo      Fighters’      arena-status[7].\\nHis      son,      Shane      Hawkins,  \\n \\n  has      since      taken      on      his      father’s      legacy,      joining      the      Foo      Fighters      for   \\n \\na      performance      during      the      Boston  \\n \\n  Calling      Music      Festival      in      2023[6].\\n#\\n \\n   Musical      Style      and      Influences  \\n \\n  Taylor      Hawkins      was   \\n \\na      profound      drummer,      with      his      musical      style      and      influences      spreading      across   \\n \\na      wide  \\n \\n  array      of      rock      genres[11].\\nKnown      for      his      passionate      fandom      of      groups      that      came      before      him,      Hawkins  \\n \\n  regularly      expressed      his      admiration      for      bands      like      Rush,      Genesis,      and      the      Police,      all      of      which      featured  \\n \\n  some      of      the      greatest      drummers      in      rock      history      like      Neil      Peart,      Phil      Collins,      and      Stewart      Copeland[11].\\nHe      was      heavily      influenced      by      his      love      for      classic      rock,      as      evidenced      by      his      performances,      where      he  \\n \\n  covered      songs      from      bands      like      Van      Halen[11].\\n \\n \\n  Hawkins      drew      influences      from   \\n \\na      variety      of      drumming      styles,      developing   \\n \\na      signature      style      inspired      by  \\n \\n  greats      like      Roger      Taylor,      Neil      Peart,      Phil      Collins,      Alex      Van      Halen,      and      Stewart      Copeland[14].\\nThis  \\n \\n  distinctive      style      and      influence      extended      to      his      drum      kit,      which      incorporated      elements      like      rototoms  \\n \\n  and      concert      toms[14].\\n \\n \\n  Beyond      his      influences,      Hawkins      had   \\n \\na      unique      energy      that      made      him      stand      out      as   \\n \\na      drummer.\\nHis      performances  \\n \\n  were      recognized      for      their      power,      and      he      was      known      for      his      enthusiastic      and      aggressive      style      of      play[15].\\n \\n \\n  This      earned      him      recognition      as      one      of      the      top      rock      drummers      of      his      time,      with      his      passion      for      music  \\n \\n  living      on      through      his      performances[14].\\n \\n \\n  Through      his      career,      Hawkins      left      an      indelible      mark      on      rock      music,      through      his      distinct      style,      passion,  \\n \\n  and      contributions      to      the      music      industry[13].\\nHis      love      for      music      and      dedication      to      his      craft      made      him       an      unforgettable      icon      in      the      world      of      rock      music[13].\\n#\\n \\n   Personal      Life  \\n \\n  Taylor      Hawkins      married      Alison      Hawkins,      an      American      celebrity      and      entrepreneur,      in      2005[18].\\nThe      couple  \\n \\n  had      three      children,      Oliver,      Annabelle,      and      Everleigh[19].\\nHawkins’      commitment      to      his      family      was      evident;\\n \\n \\n  in      fact,      he      even      wrote   \\n \\na      song      for      his      middle      child,      Annabelle[9].\\n \\n \\n  In      his      personal      life,      Hawkins      had      also      struggled      with      drug      use,      which      nearly      claimed      his      life      in   \\n \\na      2001  \\n \\n  overdose[9][7][4].\\nHowever,      he      managed      to      overcome      this      challenge,      and      later      expressed      gratitude      for  \\n \\n  the      experience      as   \\n \\na      lesson      that      allowed      him      to      realize      the      destructive      path      he      was      on[7].\\n \\n \\n  Outside      of      his      main      role      in      the      Foo      Fighters,      Hawkins      also      pursued      various      side      projects      including      the  \\n \\n  Birds      of      Satan,      NHC,      and      Chevy      Metal.\\nHis      motivation      for      such      ventures      was   \\n \\na      constant      drive      to      create  \\n \\n  and      his      love      for      music[7].\\nHawkins      was      also      known      for      his      unabashed      fanboy      nature,      often      vocalizing  \\n \\n  his      admiration      for      fellow      musicians      and      his      heroes[7].\\n#\\n \\n   Legacy      and      Impact Taylor      Hawkins      was      known      for      his      raw      and      authentic      drumming      style,      described      as      \"courageous,      damaged  \\n \\n  and      unflinchingly      authentic”[20].\\nHis      work      with      the      Foo      Fighters,      as      well      as      his      various      collaborations n’      roll[10].\\n \\n \\n  ‘ and      side      projects,      made      him   \\n \\na      celebrated      figure      in      rock  \\n \\n  Hawkins’      death      in      2022      was      met      with      heartfelt      tributes      from      colleagues      and      fans      around      the      world.\\n \\n \\n  Notable      tributes      came      from      rock      legends      like      Roger      Taylor      of      Queen,      who      considered      Hawkins      as   \\n \\na      kind,  \\n \\n  brilliant      man      and      an      inspirational      mentor,      likening      his      death      to      \"losing   \\n \\na      younger      favourite      brother”[21].\\n \\n \\n  Similarly,      Led      Zeppelin’s      Jimmy      Page      admired      his      technique,      energy      and      spirited      enthusiasm[21].\\n \\n \\n  An      LA      tribute      concert      held      in      his      honor      included      guest      drummers      like      Lars      Ulrich      of      Metallica,      Travis  \\n \\n  Barker      of      blink-182,      and      Brad      Wilk      of      Rage      Against      the      Machine.\\nSingers      like      Miley      Cyrus      and      Alanis  \\n \\n  Morissette      also      performed      at      the      concert[22].\\n \\n \\n  Apart      from      his      music,      Taylor      Hawkins      also      contributed      to      charities      Music      Support      and      MusiCares,      both      of  \\n \\n  which      were      chosen      by      the      Hawkins      family[23].\\nHe      had      received      numerous      accolades      throughout      his      career,  \\n \\n  including      27      Grammy      nominations,      of      which      he      won      14[2].\\nIn      2021,      the      Foo      Fighters      were      inducted      into  \\n \\n  the      Rock      and      Roll      Hall      of      Fame[9].\\n#\\n \\n   Discography  \\n \\n  Taylor      Hawkins      also      led   \\n \\na      notable      music      career      through      his      own      side      projects      and      collaborations[10].\\n \\n \\n  Aside      from      his      work      with      the      Foo      Fighters,      Hawkins      formed      and      fronted      the      band      Taylor      Hawkins   \\n \\n&      The  \\n \\n  Coattail      Riders,   \\n \\na      project      which      originated      from      jamming      sessions      with      his      friend      Drew      Hester[10].\\n###      Taylor      Hawkins   \\n \\n&      The      Coattail      Riders  \\n \\n  Taylor      Hawkins   \\n \\n&      The      Coattail      Riders,   \\n \\na      band      formed      in      2004,      have      released      three      albums      and      their  \\n \\n  music      spans      genres      including      Hard      Rock,      Art      Rock,      and      Alternative      Rock[24][25][26].\\nThe      band      grew      from  \\n \\n  an      initial      casual      jamming      session,      gradually      evolving      into   \\n \\na      more      formal      arrangement      that      led      to      the  \\n \\n  production      of      record      albums.\\nNotably,      these      albums      featured      guest      appearances      by      renowned      musicians  \\n \\n  such      as      Dave      Grohl,      Queen’s      Brian      May      and      Roger      Taylor,      The      Cars’      Elliot      Easton,      Perry      Farrell,      and  \\n \\n  Jon      Davison,      who      is   \\n \\na      school      friend      of      Hawkins’[10].\\n###      Red      Light      Fever  \\n \\n  Red      Light      Fever,      released      on      April      19,      2010,      was      the      band’s      first      album[29][30].\\nPrior      to      its      release,  \\n \\n  Hawkins      revealed      in      an      interview      that      the      album      had      completed      the      recording      and      production      stages,      but  \\n \\n  its      title      and      release      date      were      yet      to      be      determined[29].\\nRed      Light      Fever      was      recorded      at      the      Foo  \\n \\n  Fighters’      Studio      606      in      California      and      featured      guest      musicians      such      as      Brian      May      and      Roger      Taylor      of  \\n \\n  Queen,      Dave      Grohl      of      Foo      Fighters,      and      Elliot      Easton      of      The      Cars[29][30].\\n##      Get      the      Money  \\n \\n  Get      the      Money,      the      third      album      from      Taylor      Hawkins   \\n \\n&      The      Coattail      Riders,      was      released      on      November      8,  \\n \\n  2019[29].\\nThe      album’s      first      single,      \"Crossed      the      Line”,      released      on      October      15,      2019,      featured      Dave  \\n \\n  Grohl      and      Jon      Davison,      the      frontman      of      Yes[29].\\nThe      music      video      for      the      single      \"I      Really      Blew      It”      also  \\n \\n  featured      appearances      from      Grohl      and      Perry      Farrel1[29].\\n#\\n \\n   Collaborations      and      Guest      Appearances  \\n \\n  Throughout      his      career,      Taylor      Hawkins      collaborated      with      various      prominent      artists      and      bands.\\nThe  \\n \\n  Coattail      Riders’      albums      notably      featured      appearances      from      luminaries      such      as      Brian      May      and      Roger      Taylor  \\n \\n  of      Queen,      Chrissie      Hynde,      Nancy      Wilson      of      Heart,      Sex      Pistol      Steve      Jones      and      James      Gang’s      Joe      Walsh[28].\\n \\n \\n  Hawkins      also      fronted      another      group,      The      Birds      of      Satan,      which      evolved      from      his      heavy      rock      covers      band,  \\n \\n  Chevy      Metal[28].\\nDespite      his      diverse      musical      engagements,      Hawkins      always      maintained   \\n \\na      close      allegiance      with      the      Foo  \\n \\n  Fighters,      which      remained      the      center      of      his      music      life[7][28].\\n#\\n \\n   Tragic      Passing  \\n \\n  Taylor      Hawkins,      the      esteemed      drummer      of      the      alt-rock      band      Foo      Fighters,      passed      away      suddenly      on      March  \\n \\n  25,      2022,      while      on      tour      with      his      band      in      Bogota,      Colombia[34].\\nThe      official      cause      of      death      was      cardiac  \\n \\n  arrest,      though      inquiries      were      raised      concerning      the      presence      of      drugs      in      his      system      and      their      potential  \\n \\n  contribution      to      his      death[33][34].\\nOn      the      night      of      his      passing,      paramedics      were      called      to      the      Four  \\n \\n  Seasons      hotel      in      Bogota      due      to      reports      of      chest      pain      from      an      unnamed      guest,      later      revealed      to      be  \\n \\n  Hawkins[34].\\nUnfortunately,      resuscitation      efforts      were      unsuccessful,      and      Hawkins      was      declared      dead      at  \\n \\n  the      scene[34].\\n \\n \\n  The      news      of      Hawkins’      sudden      demise      was      announced      on      the      morning      of      March      25th,      2022,      which      left      the      music  \\n \\n  world      in      shock[32].\\nThe      band      confirmed      the      news      with   \\n \\na      short      statement,      expressing      their      devastation  \\n \\n  at      the      loss      of      Hawkins,      whose      \"musical      spirit      and      infectious      laughter”      would      live      on      forever[32].\\n \\n \\n  As   \\n \\na      result      of      Hawkins’      untimely      passing,      the      band      canceled      their      ongoing      South      American      tour[33].\\nThe  \\n \\n  festival      stage      at      the      Estéreo      Picnic      Festival,      where      the      Foo      Fighters      were      scheduled      to      perform      that  \\n \\n  night,      was      transformed      into   \\n \\na      candlelight      vigil      in      memory      of      Hawkins[33].\\n##      Tributes      and      Remembrances  \\n \\n  In      the      wake      of      Hawkins’      death,      tributes      from      fans      and      colleagues      alike      poured      in      from      around      the  \\n \\n  world[21][31].\\nAmong      the      many      paying      their      respects      were      legendary      rock      and      roll      musicians      like      Roger  \\n \\n  Taylor,      the      drummer      of      Queen,      who      Hawkins      credited      with      inspiring      his      own      career      behind      the      drum      set[21].\\n \\n \\n  In      heartfelt      social      media      posts,      Taylor      described      Hawkins      as      an      \"inspirational      mentor”      and   \\n \\na      \"kind  \\n \\n  brilliant      man\"[21],      while      Led      Zeppelin’s      Jimmy      Page      reminisced      about      sharing      the      stage      with      Hawkins  \\n \\n  and      praised      his      \"technique,      energy      and      spirited      enthusiasm\"[21].\\n \\n \\n  There      were      also      numerous      onstage      tributes      to      Hawkins.\\nNotably,      Miley      Cyrus      expressed      her      grief      and      sent  \\n \\n  peaceful      wishes      to      the      Foo      Fighters      and      the      Hawkins      family      during   \\n \\na      performance      at      Lollapalooza[31].\\n \\n \\n  Similarly,      Liam      Gallagher      of      Oasis      dedicated      one      of      the      band’s      biggest      hits      to      Hawkins      during   \\n \\na      concert  \\n \\n  at      the      Royal      Albert      Hall      in      London[31].\\nFans      gathered      outside      the      hotel      where      Hawkins      died,      lighting      candles,      leaving      flowers,      and      singing      the  \\n \\n  band’s      songs      in      his      honor[31].\\n \\n \\n  Hawkins’      life      and      career      were      celebrated      in   \\n \\na      star-studded      tribute      concert      in      Los      Angeles,      which      saw  \\n \\n  performances      from      over      5@      musicians,      including      his      former      bands      and      colleagues      from      Def      Leppard,      Queen,  \\n \\n  and      Foo      Fighters[22].\\nTable      12:      STORM’s      generated      article      for      “Taylor      Hawkins”.\\n“#’,      “##”      indicate      the      section      title      and      subsection      title  \\n \\n  respectively.\\nNumbers      in      brackets      indicate      the      cited      references.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 39, 'section_title': 'Verifiability'}, 'page_content': 'Verifiability\\n \\n \\n  1:      No      supporting      evidence;      claims      are      unsubstantiated.\\n \\n \\n  2:      Rarely      supported      with      evidence;      many      claims      are      unsubstantiated.\\n \\n \\n  3:      Inconsistently      verified;      some      claims      are      supported;      evidence      is      occasionally      provided.\\n \\n \\n  4:      Generally      verified;      claims      are      usually      supported      with      evidence;      however,      there      might      be   \\n \\na      few      instances      where      verification      is      lacking  \\n \\n  5:      Well-supported;      claims      are      very      well      supported      with      credible      evidence,      and      instances      of      unsupported      claims      are      rare.\\n \\n \\n  6:      Very      well-supported;      almost      every      claim      is      substantiated      with      credible      evidence,      showing   \\n \\na      high      level      of      thorough      verification.\\n \\n \\n  7:      Exemplary      verification;      each      claim      is      supported      by      robust,      credible      evidence      from      authoritative      sources,      reflecting      strict      adherence      to      the      no  \\n \\n  original      research      policy.\\nTable      10:      Scoring      rubrics      on   \\n \\na      1-7      scale      for      human      evaluation.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 40, 'section_title': 'Issue       Mentioned      Time       Example      Comments'}, 'page_content': 'Issue       Mentioned      Time       Example      Comments\\n \\n \\n  The      word      “significant”      is      used      17      times      in      this      article.\\nVague      and      unsupported      claims      are  \\n \\n  made      about      broader      political      importance      and      “pivotal      role[s]”,      and      is      unencyclopedic.\\nUse      of      emotional      words,  \\n \\n  (comment      on      article      Lahaina,      Hawaii) [] but they still have not fixed the issue of neutral point of view.\\nIt is also evident in this  \\n \\n  article      that      the      writer’s      standpoint      is      biased      towards      Taylor      Swift.\\nOther      than      that,      it      did  \\n \\n  a      good      job      at      summarizing      key      points      and      putting      depth      into      this.\\n \\n \\n  unneutral      12      (comment      on      article      Speak      Now      (Taylor’s      Version))  \\n \\n  “The      film      was      also      featured      in      an      art      and      film      festival      hosted      by      The      California      Endowment,  \\n \\n  highlighting      the      power      of      stories      in      reshaping      narratives      about      communities.”\\nYes,      technically  \\n \\n  the      source      says      that,      but      it’s   \\n \\na      stretch      to      say      in      Wikipedia      voice      and      just      sounds      like  \\n \\n  non-neutral,      promotional      prose.\\n(comment      on      article      Gehraiyaan)  \\n \\n  Polling      from      America      shouldn’t      be      included      and      links      to      climate      change      shouldn’t      be  \\n \\n  made      unless      explicitly      connected      by      the      source.\\n(comment      on      article      Typhoon      Hinnamnor)  \\n \\n  Red      herring      fallacy,   \\n \\nu      Sourcing      seems      mostly      fine,      though      some      aren’t      directly      related      (Ex.      39,40).\\n \\n \\n  associating      unrelated      sources      (comment      on      article      Gehraiyaan)  \\n \\n  Here      is   \\n \\na      lengthy      digression      about      KISS,      not      necessary      because      the      article      on      the      band  \\n \\n  should      be      linked      to.\\n(comment      on      article      2022      AFL      Grand      Final)  \\n \\n  “One      study,      conducted      by      Sinéad      Griffin,   \\n \\na      physicist      at      the      Lawrence      Berkeley      National  \\n \\n  Laboratory,      provided      some      analysis      of      LK-99’s      abilities      using      supercomputer      simulations[20].”\\n \\n \\n  This      is      not      enough      information      about      the      analysis,      which      would      have      been      very      useful      in      the  \\n \\n  rr  \\n \\n.    \\n.\\n     article.\\n(comment      on      article      LK-99)  \\n \\n  Missing      important      information   \\n \\n6       Although      the      earthquake’s      immediate      aftermath      and      response      are      adequately      covered,      there  \\n \\n  could      be      more      about      the      long-term      socioeconomic      impact      and      recovery      processes.\\n \\n \\n  (comment      on      article      2022      West      Java      earthquake)  \\n \\n  Words      like      “now”      should      be      avoided      in      Wikipedia      articles      to      prevent      them      from      becoming  \\n \\n  dated      and      phrases      such      as,      “as      of      December      2023”      should      be      used      instead.\\n \\n \\n  Improper      handling      of   \\n \\n5      (comment      on      article      Cyclone      Batsirai)  \\n \\n  time-sensitive      information      “as      of      December      13”      doesn’t      specify   \\n \\na      year,      and      is      old      information  \\n \\n  (comment      on      article      2022      West      Java      earthquake) too      many      subsections      in      the      “Recovery      and      Rehabilitation”      section  \\n \\n  (comment      on      article      2022      West      Java      earthquake)\\n      (comment      on      article      2022      Crimean      Bridge      explosion)\\n \\n \\n  Section      organization      problem   \\n \\n5   \\n \\nI      do      not      like      how      the      article      is      organized,      with      too      many      headers      cluttering      the      article,  \\n \\n  making      it      not      as      readable.\\nOther      than      that,      the      AI      did      great      work      on      the      piece.\\n |       Select      a      key       8      v       Selected      Key:      8       Title:      Taylor      Hawkins      -      marshall.com       Snippets:       In      1995,      off      the      back      of      her      hit      album       ‘Jagged      Little      Pill’,      Canadian      American       superstar      Alanis      Morissette      recruited      him       to      be      her      touring      drummer      for      her      18       month      album      tour,      along      with      him       featuring      in      the      music      videos      for      “You       Oughta      Know’,      “All|      Really      Want”      and       “You      Learn’.      It      was      during      this      tour      that      he       met      his      musical      soul      mate,      Dave      Grohl.       The      tour      with      Alanis      ended      and      he\\\\\\\\\\\\\\'d      heard       that      Dave      and      Foo      Fighters      were      looking       for      anew      drummer,      so      Taylor      enquired.       Dave      initially      thought      that      Taylor      wouldn’t       be      interested      in      joining      as      Alanis       Morissette      was      much      bigger      than      the      Foo       Fighters      at      that      time,      but      Taylor      jumped      at       the      chance      due      to      his      overwhelming       desire      to      be      in      a      rock      band.      It      was      from       that      point      that      the      bromance      between       them      began      and      on      March      18th      1997       Taylor      was      announced      as      their      new       drummer.       Url:      https://marshall.com/live-for-       music/drum-icons/taylor-hawkins | Logout  \\n \\n  Select      an      option:  \\n \\n  (11,      ‘Taylor      Hawkins      [Quite      Richard}’)   \\n \\n¥       You      selected:      Taylor      Hawkins      [Quite      Richard]  \\n \\n  Taylor      Hawkins  \\n \\n  Oliver      Taylor      Hawkins      (1972-2022)      was      an      American      musician,      best      known      as      the      drummer      for      the      rock      band      Foo      Fighters[1].\\nBorn      in      Fort      Walton,      Texas,      Hawkins’      love      for      music      was      ignited      at   \\n \\na      young  \\n \\n  age,      particularly      after      watching   \\n \\na      Queen      concert      in      1982[2][3][5].\\nHe      kick-started      his      professional      career      as      the      drummer      for      Alanis      Morissette’s      band      during      her      world      tour      for      the      hit      album      ‘Jagged      Little  \\n \\n  Pill\\\\\\\\\\\\\\'[8][9].\\nHis      talents      were      recognized      by      Dave      Grohl,      who      invited      him      to      join      the      Foo      Fighters      in      1997,      marking      the      beginning      of      his      impactful      tenure      with      the      band{7][8].\\n \\n \\n  Hawkins      was      celebrated      for      his      versatile      drumming      style,      drawing      influence      from      renowned      drummers      like      Roger      Taylor,      Neil      Peart,      Phil      Collins,      Alex      Van      Halen,      and      Stewart      Copeland[14).\\nHis  \\n \\n  performances,      marked      by   \\n \\na      unique      energy      and      aggressive      style      of      play,      earned      him      recognition      as      one      of      the      top      rock      drummers      of      his      era[15].\\nApart      from      his      role      in      the      Foo      Fighters,      Hawkins\\\\\\\\\\\\\\'      passion  \\n \\n  for      music      saw      him      involved      in      numerous      side      projects      and      collaborations,      cementing      his      place      in      the      world      of      rock      music(10].\\n \\n \\n  Outside      of      his      professional      life,      Hawkins      was      known      for      his      resilience      and      dedication      to      his      family.\\nDespite      personal      struggles,      including   \\n \\na      near-fatal      drug      overdose      in      2001,      Hawkins      remained      committed  \\n \\n  to      his      musical      career{4][9].\\nHis      legacy      continues      to      inspire      musicians      and      fans      alike,      as      his      contributions      to      rock      music,      coupled      with      his   \\n \\ni       industry(13}.\\n \\n \\n  spirit,      made      him      an      icon      in      the      music  \\n \\n  Hawkins’      sudden      death      in      2022      while      on      tour      in      Bogotd,      Colombia,      sent      shockwaves      through      the      music      world[34].\\nTributes      poured      in      from      around      the      globe,      reflecting      the      respect      and      admiration  \\n \\n  Hawkins      had      garnered      during      his      lifetime[21][31].\\nHis      life      and      career      were      honored      at   \\n \\na      star-studded      tribute      concert      in      Los      Angeles,      attesting      to      the      enduring      impact      of      his      music[22].\\n | Early      Life      and      Background\\n |       Oliver      Taylor      Hawkins,      known      as      Taylor      Hawkins,      was      born      and      raised      in      Fort      Walton,      Texas[3].      His      family      moved      to      Laguna      Beach,      California      when      he      was      four      years      old[3].      He      has      two      younger      siblings,       a      brother      named      Jason,      and      a      sister      named      Heather[3].      Asa      child,      Hawkins      wa:      influenced      by      his      paternal      Josie      Hawkins,      who      had      grown      up      during      the      Great      Depression      and       lived      in      Jackson,      Mississippi(2].\\n |       During      his      high      school      days      at      Laguna      Beach      High      School,      from      where      he      graduated      in      1990,      he      became      friends      with      Jon      Davison,      who      later      became      the      lead      vocalist      of      the      band      Yes(2][3].      His      interest      in       music      was      nurtured      from      an      early      age,      particularly      after      watching      a      Queen      concert      in      1982      which      inspired      him      to      learn      to      play      the      drums(2][5].      He      noted      that      music      was      a      constant      presence      in      his      family       Taylor      Hawkins\\n\\n \\n \\n  Oliver      Taylor      Hawkins      (1972-2022)      was      an      American      musician,      best      known      as      the      drummer      for      the      rock  \\n \\n  band      Foo      Fighters[1].\\nBorn      in      Fort      Walton,      Texas,      Hawkins’      love      for      music      was      ignited      at   \\n \\na      young      age,  \\n \\n  particularly      after      watching   \\n \\na      Queen      concert      in      1982[2][3][5].\\nHe      kick-started      his      professional      career  \\n \\n  as      the      drummer      for      Alanis      Morissette’s      band      during      her      world      tour      for      the      hit      album      ’Jagged      Little  \\n \\n  Pill’[8]L[9].\\nHis      talents      were      recognized      by      Dave      Grohl,      who      invited      him      to      join      the      Foo      Fighters      in  \\n \\n  1997,      marking      the      beginning      of      his      impactful      tenure      with      the      band[7][8].\\n \\n \\n  Hawkins      was      celebrated      for      his      versatile      drumming      style,      drawing      influence      from      renowned      drummers      like  \\n \\n  Roger      Taylor,      Neil      Peart,      Phil      Collins,      Alex      Van      Halen,      and      Stewart      Copeland[14].\\nHis      performances,  \\n \\n  marked      by   \\n \\na      unique      energy      and      aggressive      style      of      play,      earned      him      recognition      as      one      of      the      top      rock  \\n \\n  drummers      of      his      era[15].\\nApart      from      his      role      in      the      Foo      Fighters,      Hawkins’      passion      for      music      saw  \\n \\n  him      involved      in      numerous      side      projects      and      collaborations,      cementing      his      place      in      the      world      of      rock  \\n \\n  music[10].\\n \\n \\n  Outside      of      his      professional      life,      Hawkins      was      known      for      his      resilience      and      dedication      to      his      family.\\n \\n \\n  Despite      personal      struggles,      including   \\n \\na      near-fatal      drug      overdose      in      2001,      Hawkins      remained      committed      to  \\n \\n  his      musical      career[4][9].\\nHis      legacy      continues      to      inspire      musicians      and      fans      alike,      as      his      contributions  \\n \\n  to      rock      music,      coupled      with      his      indomitable      spirit,      made      him      an      unforgettable      icon      in      the      music  \\n \\n  industry[13].\\n \\n \\n  Hawkins’      sudden      death      in      2022      while      on      tour      in      Bogota,      Colombia,      sent      shockwaves      through      the      music  \\n \\n  world[34].\\nTributes      poured      in      from      around      the      globe,      reflecting      the      respect      and      admiration      Hawkins  \\n \\n  had      garnered      during      his      lifetime[21][31].\\nHis      life      and      career      were      honored      at   \\n \\na      star-studded      tribute  \\n \\n  concert      in      Los      Angeles,      attesting      to      the      enduring      impact      of      his      music[22].\\n#\\n \\n   Early      Life      and      Background  \\n \\n  Oliver      Taylor      Hawkins,      known      as      Taylor      Hawkins,      was      born      and      raised      in      Fort      Walton,      Texas[3].\\nHis  \\n \\n  family      moved      to      Laguna      Beach,      California      when      he      was      four      years      old[3].\\nHe      has      two      younger      siblings,  \\n \\n  a      brother      named      Jason,      and   \\n \\na      sister      named      Heather[3].\\nAs   \\n \\na      child,      Hawkins      was      particularly      influenced  \\n \\n  by      his      paternal      grandmother,      Josie      Hawkins,      who      had      grown      up      during      the      Great      Depression      and      lived      in  \\n \\n  Jackson,      Mississippi[1].\\n \\n \\n  During      his      high      school      days      at      Laguna      Beach      High      School,      from      where      he      graduated      in      1990,      he      became  \\n \\n  friends      with      Jon      Davison,      who      later      became      the      lead      vocalist      of      the      band      Yes[2][3].\\nHis      interest      in  \\n \\n  music      was      nurtured      from      an      early      age,      particularly      after      watching   \\n \\na      Queen      concert      in      1982      which      inspired  \\n \\n  him      to      learn      to      play      the      drums[2][5].\\nHe      noted      that      music      was   \\n \\na      constant      presence      in      his      family      home[5].\\n \\n \\n  Despite      facing      certain      hardships      during      his      upbringing,      including      his      mother’s      struggles      with      \"demons\",  \\n \\n  Hawkins      pursued      his      musical      ambitions[4].\\nHe      credits      his      older      sister      Heather      for      taking      care      of      the  \\n \\n  family      during      difficult      times[4].\\n \\n \\n  His      first      major      musical      experience      came      from      playing      drums      for      Alanis      Morissette’s      album,      Jagged      Little  \\n \\n  Pill,      and      accompanying      her      on      the      subsequent      tour[3].\\nThis      marked      the      beginning      of      his      professional  \\n \\n  career      in      the      music      industry.\\n#\\n \\n   Career  \\n \\n  Taylor      Hawkins      began      his      professional      music      career      playing      in      Alanis      Morissette’s      band      during      her  \\n \\n  18-month      world      tour      in      support      of      the      hit      album      ’Jagged      Little      Pill’      from      1995      to      1997[8][9].\\nHis  \\n \\n  performances      not      only      in      the      tour      but      also      in      the      music      videos      for      “You      Oughta      Know”,      “All   \\n \\nI      Really      Want”  \\n \\n  and      “You      Learn”      introduced      him      to      the      world      of      rock      music      and      ultimately      led      to      his      meeting      with      Dave  \\n \\n  Grohl[8].\\nThroughout      this      time,      Hawkins      contributed      significantly      to      the      band’s      sound      and      performance,  \\n \\n  transforming      the      songs      from      their      original      drum      loop      format      to   \\n \\na      rock-band      vibe      that      resonated      with  \\n \\n  audiences[1][7].\\n \\n \\n  In      1997,      Hawkins      was      asked      by      Grohl      to      join      the      Foo      Fighters,      an      invitation      that      he      readily      accepted[7][8].\\n \\n \\n  At      the      time,      Grohl      thought      it      was   \\n \\na      long      shot      to      recruit      Hawkins      given      that      Morissette      was      at      the      height  \\n \\n  of      her      career,      but      Hawkins’      desire      to      be   \\n \\na      part      of   \\n \\na      rock      band      compelled      him      to      make      the      move[7].\\nThis  \\n \\n  marked      the      beginning      of      Hawkins’      tenure      as      the      drummer      of      the      Foo      Fighters,   \\n \\na      role      that      he      would      play  \\n \\n  until      his      passing[6][9].\\n \\n \\n  Apart      from      his      work      with      Morissette      and      the      Foo      Fighters,      Hawkins      had      an      array      of      other      musical  \\n \\n  experiences[10].\\nHe      drummed      for      Sass      Jordan      before      joining      Morissette’s      touring      band[10].\\nHe      was      part  \\n \\n  of      an      ad      hoc      drum      supergroup      called      SOS      Allstars      and      filled      the      void      for      Coheed      and      Cambria’s      2007  \\n \\n  album      after      their      drummer      Josh      Eppard      left      the      group[10].\\nIn      addition,      Hawkins      formed      his      own      side  \\n \\n  project,      the      Coattail      Riders,      in      2005,      through      which      he      recorded      his      own      music      and      took      the      project      on  \\n \\n  the      road,      performing      in      small      clubs      despite      the      Foo      Fighters’      arena-status[7].\\nHis      son,      Shane      Hawkins,  \\n \\n  has      since      taken      on      his      father’s      legacy,      joining      the      Foo      Fighters      for   \\n \\na      performance      during      the      Boston  \\n \\n  Calling      Music      Festival      in      2023[6].\\n#\\n \\n   Musical      Style      and      Influences  \\n \\n  Taylor      Hawkins      was   \\n \\na      profound      drummer,      with      his      musical      style      and      influences      spreading      across   \\n \\na      wide  \\n \\n  array      of      rock      genres[11].\\nKnown      for      his      passionate      fandom      of      groups      that      came      before      him,      Hawkins  \\n \\n  regularly      expressed      his      admiration      for      bands      like      Rush,      Genesis,      and      the      Police,      all      of      which      featured  \\n \\n  some      of      the      greatest      drummers      in      rock      history      like      Neil      Peart,      Phil      Collins,      and      Stewart      Copeland[11].\\nHe      was      heavily      influenced      by      his      love      for      classic      rock,      as      evidenced      by      his      performances,      where      he  \\n \\n  covered      songs      from      bands      like      Van      Halen[11].\\n \\n \\n  Hawkins      drew      influences      from   \\n \\na      variety      of      drumming      styles,      developing   \\n \\na      signature      style      inspired      by  \\n \\n  greats      like      Roger      Taylor,      Neil      Peart,      Phil      Collins,      Alex      Van      Halen,      and      Stewart      Copeland[14].\\nThis  \\n \\n  distinctive      style      and      influence      extended      to      his      drum      kit,      which      incorporated      elements      like      rototoms  \\n \\n  and      concert      toms[14].\\n \\n \\n  Beyond      his      influences,      Hawkins      had   \\n \\na      unique      energy      that      made      him      stand      out      as   \\n \\na      drummer.\\nHis      performances  \\n \\n  were      recognized      for      their      power,      and      he      was      known      for      his      enthusiastic      and      aggressive      style      of      play[15].\\n \\n \\n  This      earned      him      recognition      as      one      of      the      top      rock      drummers      of      his      time,      with      his      passion      for      music  \\n \\n  living      on      through      his      performances[14].\\n \\n \\n  Through      his      career,      Hawkins      left      an      indelible      mark      on      rock      music,      through      his      distinct      style,      passion,  \\n \\n  and      contributions      to      the      music      industry[13].\\nHis      love      for      music      and      dedication      to      his      craft      made      him       an      unforgettable      icon      in      the      world      of      rock      music[13].\\n#\\n \\n   Personal      Life  \\n \\n  Taylor      Hawkins      married      Alison      Hawkins,      an      American      celebrity      and      entrepreneur,      in      2005[18].\\nThe      couple  \\n \\n  had      three      children,      Oliver,      Annabelle,      and      Everleigh[19].\\nHawkins’      commitment      to      his      family      was      evident;\\n \\n \\n  in      fact,      he      even      wrote   \\n \\na      song      for      his      middle      child,      Annabelle[9].\\n \\n \\n  In      his      personal      life,      Hawkins      had      also      struggled      with      drug      use,      which      nearly      claimed      his      life      in   \\n \\na      2001  \\n \\n  overdose[9][7][4].\\nHowever,      he      managed      to      overcome      this      challenge,      and      later      expressed      gratitude      for  \\n \\n  the      experience      as   \\n \\na      lesson      that      allowed      him      to      realize      the      destructive      path      he      was      on[7].\\n \\n \\n  Outside      of      his      main      role      in      the      Foo      Fighters,      Hawkins      also      pursued      various      side      projects      including      the  \\n \\n  Birds      of      Satan,      NHC,      and      Chevy      Metal.\\nHis      motivation      for      such      ventures      was   \\n \\na      constant      drive      to      create  \\n \\n  and      his      love      for      music[7].\\nHawkins      was      also      known      for      his      unabashed      fanboy      nature,      often      vocalizing  \\n \\n  his      admiration      for      fellow      musicians      and      his      heroes[7].\\n#\\n \\n   Legacy      and      Impact Taylor      Hawkins      was      known      for      his      raw      and      authentic      drumming      style,      described      as      \"courageous,      damaged  \\n \\n  and      unflinchingly      authentic”[20].\\nHis      work      with      the      Foo      Fighters,      as      well      as      his      various      collaborations n’      roll[10].\\n \\n \\n  ‘ and      side      projects,      made      him   \\n \\na      celebrated      figure      in      rock  \\n \\n  Hawkins’      death      in      2022      was      met      with      heartfelt      tributes      from      colleagues      and      fans      around      the      world.\\n \\n \\n  Notable      tributes      came      from      rock      legends      like      Roger      Taylor      of      Queen,      who      considered      Hawkins      as   \\n \\na      kind,  \\n \\n  brilliant      man      and      an      inspirational      mentor,      likening      his      death      to      \"losing   \\n \\na      younger      favourite      brother”[21].\\n \\n \\n  Similarly,      Led      Zeppelin’s      Jimmy      Page      admired      his      technique,      energy      and      spirited      enthusiasm[21].\\n \\n \\n  An      LA      tribute      concert      held      in      his      honor      included      guest      drummers      like      Lars      Ulrich      of      Metallica,      Travis  \\n \\n  Barker      of      blink-182,      and      Brad      Wilk      of      Rage      Against      the      Machine.\\nSingers      like      Miley      Cyrus      and      Alanis  \\n \\n  Morissette      also      performed      at      the      concert[22].\\n \\n \\n  Apart      from      his      music,      Taylor      Hawkins      also      contributed      to      charities      Music      Support      and      MusiCares,      both      of  \\n \\n  which      were      chosen      by      the      Hawkins      family[23].\\nHe      had      received      numerous      accolades      throughout      his      career,  \\n \\n  including      27      Grammy      nominations,      of      which      he      won      14[2].\\nIn      2021,      the      Foo      Fighters      were      inducted      into  \\n \\n  the      Rock      and      Roll      Hall      of      Fame[9].\\n#\\n \\n   Discography  \\n \\n  Taylor      Hawkins      also      led   \\n \\na      notable      music      career      through      his      own      side      projects      and      collaborations[10].\\n \\n \\n  Aside      from      his      work      with      the      Foo      Fighters,      Hawkins      formed      and      fronted      the      band      Taylor      Hawkins   \\n \\n&      The  \\n \\n  Coattail      Riders,   \\n \\na      project      which      originated      from      jamming      sessions      with      his      friend      Drew      Hester[10].\\n###      Taylor      Hawkins   \\n \\n&      The      Coattail      Riders  \\n \\n  Taylor      Hawkins   \\n \\n&      The      Coattail      Riders,   \\n \\na      band      formed      in      2004,      have      released      three      albums      and      their  \\n \\n  music      spans      genres      including      Hard      Rock,      Art      Rock,      and      Alternative      Rock[24][25][26].\\nThe      band      grew      from  \\n \\n  an      initial      casual      jamming      session,      gradually      evolving      into   \\n \\na      more      formal      arrangement      that      led      to      the  \\n \\n  production      of      record      albums.\\nNotably,      these      albums      featured      guest      appearances      by      renowned      musicians  \\n \\n  such      as      Dave      Grohl,      Queen’s      Brian      May      and      Roger      Taylor,      The      Cars’      Elliot      Easton,      Perry      Farrell,      and  \\n \\n  Jon      Davison,      who      is   \\n \\na      school      friend      of      Hawkins’[10].\\n###      Red      Light      Fever  \\n \\n  Red      Light      Fever,      released      on      April      19,      2010,      was      the      band’s      first      album[29][30].\\nPrior      to      its      release,  \\n \\n  Hawkins      revealed      in      an      interview      that      the      album      had      completed      the      recording      and      production      stages,      but  \\n \\n  its      title      and      release      date      were      yet      to      be      determined[29].\\nRed      Light      Fever      was      recorded      at      the      Foo  \\n \\n  Fighters’      Studio      606      in      California      and      featured      guest      musicians      such      as      Brian      May      and      Roger      Taylor      of  \\n \\n  Queen,      Dave      Grohl      of      Foo      Fighters,      and      Elliot      Easton      of      The      Cars[29][30].\\n##      Get      the      Money  \\n \\n  Get      the      Money,      the      third      album      from      Taylor      Hawkins   \\n \\n&      The      Coattail      Riders,      was      released      on      November      8,  \\n \\n  2019[29].\\nThe      album’s      first      single,      \"Crossed      the      Line”,      released      on      October      15,      2019,      featured      Dave  \\n \\n  Grohl      and      Jon      Davison,      the      frontman      of      Yes[29].\\nThe      music      video      for      the      single      \"I      Really      Blew      It”      also  \\n \\n  featured      appearances      from      Grohl      and      Perry      Farrel1[29].\\n#\\n \\n   Collaborations      and      Guest      Appearances  \\n \\n  Throughout      his      career,      Taylor      Hawkins      collaborated      with      various      prominent      artists      and      bands.\\nThe  \\n \\n  Coattail      Riders’      albums      notably      featured      appearances      from      luminaries      such      as      Brian      May      and      Roger      Taylor  \\n \\n  of      Queen,      Chrissie      Hynde,      Nancy      Wilson      of      Heart,      Sex      Pistol      Steve      Jones      and      James      Gang’s      Joe      Walsh[28].\\n \\n \\n  Hawkins      also      fronted      another      group,      The      Birds      of      Satan,      which      evolved      from      his      heavy      rock      covers      band,  \\n \\n  Chevy      Metal[28].\\nDespite      his      diverse      musical      engagements,      Hawkins      always      maintained   \\n \\na      close      allegiance      with      the      Foo  \\n \\n  Fighters,      which      remained      the      center      of      his      music      life[7][28].\\n#\\n \\n   Tragic      Passing  \\n \\n  Taylor      Hawkins,      the      esteemed      drummer      of      the      alt-rock      band      Foo      Fighters,      passed      away      suddenly      on      March  \\n \\n  25,      2022,      while      on      tour      with      his      band      in      Bogota,      Colombia[34].\\nThe      official      cause      of      death      was      cardiac  \\n \\n  arrest,      though      inquiries      were      raised      concerning      the      presence      of      drugs      in      his      system      and      their      potential  \\n \\n  contribution      to      his      death[33][34].\\nOn      the      night      of      his      passing,      paramedics      were      called      to      the      Four  \\n \\n  Seasons      hotel      in      Bogota      due      to      reports      of      chest      pain      from      an      unnamed      guest,      later      revealed      to      be  \\n \\n  Hawkins[34].\\nUnfortunately,      resuscitation      efforts      were      unsuccessful,      and      Hawkins      was      declared      dead      at  \\n \\n  the      scene[34].\\n \\n \\n  The      news      of      Hawkins’      sudden      demise      was      announced      on      the      morning      of      March      25th,      2022,      which      left      the      music  \\n \\n  world      in      shock[32].\\nThe      band      confirmed      the      news      with   \\n \\na      short      statement,      expressing      their      devastation  \\n \\n  at      the      loss      of      Hawkins,      whose      \"musical      spirit      and      infectious      laughter”      would      live      on      forever[32].\\n \\n \\n  As   \\n \\na      result      of      Hawkins’      untimely      passing,      the      band      canceled      their      ongoing      South      American      tour[33].\\nThe  \\n \\n  festival      stage      at      the      Estéreo      Picnic      Festival,      where      the      Foo      Fighters      were      scheduled      to      perform      that  \\n \\n  night,      was      transformed      into   \\n \\na      candlelight      vigil      in      memory      of      Hawkins[33].\\n##      Tributes      and      Remembrances  \\n \\n  In      the      wake      of      Hawkins’      death,      tributes      from      fans      and      colleagues      alike      poured      in      from      around      the  \\n \\n  world[21][31].\\nAmong      the      many      paying      their      respects      were      legendary      rock      and      roll      musicians      like      Roger  \\n \\n  Taylor,      the      drummer      of      Queen,      who      Hawkins      credited      with      inspiring      his      own      career      behind      the      drum      set[21].\\n \\n \\n  In      heartfelt      social      media      posts,      Taylor      described      Hawkins      as      an      \"inspirational      mentor”      and   \\n \\na      \"kind  \\n \\n  brilliant      man\"[21],      while      Led      Zeppelin’s      Jimmy      Page      reminisced      about      sharing      the      stage      with      Hawkins  \\n \\n  and      praised      his      \"technique,      energy      and      spirited      enthusiasm\"[21].\\n \\n \\n  There      were      also      numerous      onstage      tributes      to      Hawkins.\\nNotably,      Miley      Cyrus      expressed      her      grief      and      sent  \\n \\n  peaceful      wishes      to      the      Foo      Fighters      and      the      Hawkins      family      during   \\n \\na      performance      at      Lollapalooza[31].\\n \\n \\n  Similarly,      Liam      Gallagher      of      Oasis      dedicated      one      of      the      band’s      biggest      hits      to      Hawkins      during   \\n \\na      concert  \\n \\n  at      the      Royal      Albert      Hall      in      London[31].\\nFans      gathered      outside      the      hotel      where      Hawkins      died,      lighting      candles,      leaving      flowers,      and      singing      the  \\n \\n  band’s      songs      in      his      honor[31].\\n \\n \\n  Hawkins’      life      and      career      were      celebrated      in   \\n \\na      star-studded      tribute      concert      in      Los      Angeles,      which      saw  \\n \\n  performances      from      over      5@      musicians,      including      his      former      bands      and      colleagues      from      Def      Leppard,      Queen,  \\n \\n  and      Foo      Fighters[22].\\nTable      12:      STORM’s      generated      article      for      “Taylor      Hawkins”.\\n“#’,      “##”      indicate      the      section      title      and      subsection      title  \\n \\n  respectively.\\nNumbers      in      brackets      indicate      the      cited      references.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 41, 'section_title': '      (comment      on      article      2022      Crimean      Bridge      explosion)'}, 'page_content': '      (comment      on      article      2022      Crimean      Bridge      explosion)\\n \\n \\n  Section      organization      problem   \\n \\n5   \\n \\nI      do      not      like      how      the      article      is      organized,      with      too      many      headers      cluttering      the      article,  \\n \\n  making      it      not      as      readable.\\nOther      than      that,      the      AI      did      great      work      on      the      piece.\\n |       Select      a      key       8      v       Selected      Key:      8       Title:      Taylor      Hawkins      -      marshall.com       Snippets:       In      1995,      off      the      back      of      her      hit      album       ‘Jagged      Little      Pill’,      Canadian      American       superstar      Alanis      Morissette      recruited      him       to      be      her      touring      drummer      for      her      18       month      album      tour,      along      with      him       featuring      in      the      music      videos      for      “You       Oughta      Know’,      “All|      Really      Want”      and       “You      Learn’.      It      was      during      this      tour      that      he       met      his      musical      soul      mate,      Dave      Grohl.       The      tour      with      Alanis      ended      and      he\\\\\\\\\\\\\\'d      heard       that      Dave      and      Foo      Fighters      were      looking       for      anew      drummer,      so      Taylor      enquired.       Dave      initially      thought      that      Taylor      wouldn’t       be      interested      in      joining      as      Alanis       Morissette      was      much      bigger      than      the      Foo       Fighters      at      that      time,      but      Taylor      jumped      at       the      chance      due      to      his      overwhelming       desire      to      be      in      a      rock      band.      It      was      from       that      point      that      the      bromance      between       them      began      and      on      March      18th      1997       Taylor      was      announced      as      their      new       drummer.       Url:      https://marshall.com/live-for-       music/drum-icons/taylor-hawkins | Logout  \\n \\n  Select      an      option:  \\n \\n  (11,      ‘Taylor      Hawkins      [Quite      Richard}’)   \\n \\n¥       You      selected:      Taylor      Hawkins      [Quite      Richard]  \\n \\n  Taylor      Hawkins  \\n \\n  Oliver      Taylor      Hawkins      (1972-2022)      was      an      American      musician,      best      known      as      the      drummer      for      the      rock      band      Foo      Fighters[1].\\nBorn      in      Fort      Walton,      Texas,      Hawkins’      love      for      music      was      ignited      at   \\n \\na      young  \\n \\n  age,      particularly      after      watching   \\n \\na      Queen      concert      in      1982[2][3][5].\\nHe      kick-started      his      professional      career      as      the      drummer      for      Alanis      Morissette’s      band      during      her      world      tour      for      the      hit      album      ‘Jagged      Little  \\n \\n  Pill\\\\\\\\\\\\\\'[8][9].\\nHis      talents      were      recognized      by      Dave      Grohl,      who      invited      him      to      join      the      Foo      Fighters      in      1997,      marking      the      beginning      of      his      impactful      tenure      with      the      band{7][8].\\n \\n \\n  Hawkins      was      celebrated      for      his      versatile      drumming      style,      drawing      influence      from      renowned      drummers      like      Roger      Taylor,      Neil      Peart,      Phil      Collins,      Alex      Van      Halen,      and      Stewart      Copeland[14).\\nHis  \\n \\n  performances,      marked      by   \\n \\na      unique      energy      and      aggressive      style      of      play,      earned      him      recognition      as      one      of      the      top      rock      drummers      of      his      era[15].\\nApart      from      his      role      in      the      Foo      Fighters,      Hawkins\\\\\\\\\\\\\\'      passion  \\n \\n  for      music      saw      him      involved      in      numerous      side      projects      and      collaborations,      cementing      his      place      in      the      world      of      rock      music(10].\\n \\n \\n  Outside      of      his      professional      life,      Hawkins      was      known      for      his      resilience      and      dedication      to      his      family.\\nDespite      personal      struggles,      including   \\n \\na      near-fatal      drug      overdose      in      2001,      Hawkins      remained      committed  \\n \\n  to      his      musical      career{4][9].\\nHis      legacy      continues      to      inspire      musicians      and      fans      alike,      as      his      contributions      to      rock      music,      coupled      with      his   \\n \\ni       industry(13}.\\n \\n \\n  spirit,      made      him      an      icon      in      the      music  \\n \\n  Hawkins’      sudden      death      in      2022      while      on      tour      in      Bogotd,      Colombia,      sent      shockwaves      through      the      music      world[34].\\nTributes      poured      in      from      around      the      globe,      reflecting      the      respect      and      admiration  \\n \\n  Hawkins      had      garnered      during      his      lifetime[21][31].\\nHis      life      and      career      were      honored      at   \\n \\na      star-studded      tribute      concert      in      Los      Angeles,      attesting      to      the      enduring      impact      of      his      music[22].\\n | Early      Life      and      Background\\n |       Oliver      Taylor      Hawkins,      known      as      Taylor      Hawkins,      was      born      and      raised      in      Fort      Walton,      Texas[3].      His      family      moved      to      Laguna      Beach,      California      when      he      was      four      years      old[3].      He      has      two      younger      siblings,       a      brother      named      Jason,      and      a      sister      named      Heather[3].      Asa      child,      Hawkins      wa:      influenced      by      his      paternal      Josie      Hawkins,      who      had      grown      up      during      the      Great      Depression      and       lived      in      Jackson,      Mississippi(2].\\n |       During      his      high      school      days      at      Laguna      Beach      High      School,      from      where      he      graduated      in      1990,      he      became      friends      with      Jon      Davison,      who      later      became      the      lead      vocalist      of      the      band      Yes(2][3].      His      interest      in       music      was      nurtured      from      an      early      age,      particularly      after      watching      a      Queen      concert      in      1982      which      inspired      him      to      learn      to      play      the      drums(2][5].      He      noted      that      music      was      a      constant      presence      in      his      family       Taylor      Hawkins\\n\\n \\n \\n  Oliver      Taylor      Hawkins      (1972-2022)      was      an      American      musician,      best      known      as      the      drummer      for      the      rock  \\n \\n  band      Foo      Fighters[1].\\nBorn      in      Fort      Walton,      Texas,      Hawkins’      love      for      music      was      ignited      at   \\n \\na      young      age,  \\n \\n  particularly      after      watching   \\n \\na      Queen      concert      in      1982[2][3][5].\\nHe      kick-started      his      professional      career  \\n \\n  as      the      drummer      for      Alanis      Morissette’s      band      during      her      world      tour      for      the      hit      album      ’Jagged      Little  \\n \\n  Pill’[8]L[9].\\nHis      talents      were      recognized      by      Dave      Grohl,      who      invited      him      to      join      the      Foo      Fighters      in  \\n \\n  1997,      marking      the      beginning      of      his      impactful      tenure      with      the      band[7][8].\\n \\n \\n  Hawkins      was      celebrated      for      his      versatile      drumming      style,      drawing      influence      from      renowned      drummers      like  \\n \\n  Roger      Taylor,      Neil      Peart,      Phil      Collins,      Alex      Van      Halen,      and      Stewart      Copeland[14].\\nHis      performances,  \\n \\n  marked      by   \\n \\na      unique      energy      and      aggressive      style      of      play,      earned      him      recognition      as      one      of      the      top      rock  \\n \\n  drummers      of      his      era[15].\\nApart      from      his      role      in      the      Foo      Fighters,      Hawkins’      passion      for      music      saw  \\n \\n  him      involved      in      numerous      side      projects      and      collaborations,      cementing      his      place      in      the      world      of      rock  \\n \\n  music[10].\\n \\n \\n  Outside      of      his      professional      life,      Hawkins      was      known      for      his      resilience      and      dedication      to      his      family.\\n \\n \\n  Despite      personal      struggles,      including   \\n \\na      near-fatal      drug      overdose      in      2001,      Hawkins      remained      committed      to  \\n \\n  his      musical      career[4][9].\\nHis      legacy      continues      to      inspire      musicians      and      fans      alike,      as      his      contributions  \\n \\n  to      rock      music,      coupled      with      his      indomitable      spirit,      made      him      an      unforgettable      icon      in      the      music  \\n \\n  industry[13].\\n \\n \\n  Hawkins’      sudden      death      in      2022      while      on      tour      in      Bogota,      Colombia,      sent      shockwaves      through      the      music  \\n \\n  world[34].\\nTributes      poured      in      from      around      the      globe,      reflecting      the      respect      and      admiration      Hawkins  \\n \\n  had      garnered      during      his      lifetime[21][31].\\nHis      life      and      career      were      honored      at   \\n \\na      star-studded      tribute  \\n \\n  concert      in      Los      Angeles,      attesting      to      the      enduring      impact      of      his      music[22].\\n#\\n \\n   Early      Life      and      Background  \\n \\n  Oliver      Taylor      Hawkins,      known      as      Taylor      Hawkins,      was      born      and      raised      in      Fort      Walton,      Texas[3].\\nHis  \\n \\n  family      moved      to      Laguna      Beach,      California      when      he      was      four      years      old[3].\\nHe      has      two      younger      siblings,  \\n \\n  a      brother      named      Jason,      and   \\n \\na      sister      named      Heather[3].\\nAs   \\n \\na      child,      Hawkins      was      particularly      influenced  \\n \\n  by      his      paternal      grandmother,      Josie      Hawkins,      who      had      grown      up      during      the      Great      Depression      and      lived      in  \\n \\n  Jackson,      Mississippi[1].\\n \\n \\n  During      his      high      school      days      at      Laguna      Beach      High      School,      from      where      he      graduated      in      1990,      he      became  \\n \\n  friends      with      Jon      Davison,      who      later      became      the      lead      vocalist      of      the      band      Yes[2][3].\\nHis      interest      in  \\n \\n  music      was      nurtured      from      an      early      age,      particularly      after      watching   \\n \\na      Queen      concert      in      1982      which      inspired  \\n \\n  him      to      learn      to      play      the      drums[2][5].\\nHe      noted      that      music      was   \\n \\na      constant      presence      in      his      family      home[5].\\n \\n \\n  Despite      facing      certain      hardships      during      his      upbringing,      including      his      mother’s      struggles      with      \"demons\",  \\n \\n  Hawkins      pursued      his      musical      ambitions[4].\\nHe      credits      his      older      sister      Heather      for      taking      care      of      the  \\n \\n  family      during      difficult      times[4].\\n \\n \\n  His      first      major      musical      experience      came      from      playing      drums      for      Alanis      Morissette’s      album,      Jagged      Little  \\n \\n  Pill,      and      accompanying      her      on      the      subsequent      tour[3].\\nThis      marked      the      beginning      of      his      professional  \\n \\n  career      in      the      music      industry.\\n#\\n \\n   Career  \\n \\n  Taylor      Hawkins      began      his      professional      music      career      playing      in      Alanis      Morissette’s      band      during      her  \\n \\n  18-month      world      tour      in      support      of      the      hit      album      ’Jagged      Little      Pill’      from      1995      to      1997[8][9].\\nHis  \\n \\n  performances      not      only      in      the      tour      but      also      in      the      music      videos      for      “You      Oughta      Know”,      “All   \\n \\nI      Really      Want”  \\n \\n  and      “You      Learn”      introduced      him      to      the      world      of      rock      music      and      ultimately      led      to      his      meeting      with      Dave  \\n \\n  Grohl[8].\\nThroughout      this      time,      Hawkins      contributed      significantly      to      the      band’s      sound      and      performance,  \\n \\n  transforming      the      songs      from      their      original      drum      loop      format      to   \\n \\na      rock-band      vibe      that      resonated      with  \\n \\n  audiences[1][7].\\n \\n \\n  In      1997,      Hawkins      was      asked      by      Grohl      to      join      the      Foo      Fighters,      an      invitation      that      he      readily      accepted[7][8].\\n \\n \\n  At      the      time,      Grohl      thought      it      was   \\n \\na      long      shot      to      recruit      Hawkins      given      that      Morissette      was      at      the      height  \\n \\n  of      her      career,      but      Hawkins’      desire      to      be   \\n \\na      part      of   \\n \\na      rock      band      compelled      him      to      make      the      move[7].\\nThis  \\n \\n  marked      the      beginning      of      Hawkins’      tenure      as      the      drummer      of      the      Foo      Fighters,   \\n \\na      role      that      he      would      play  \\n \\n  until      his      passing[6][9].\\n \\n \\n  Apart      from      his      work      with      Morissette      and      the      Foo      Fighters,      Hawkins      had      an      array      of      other      musical  \\n \\n  experiences[10].\\nHe      drummed      for      Sass      Jordan      before      joining      Morissette’s      touring      band[10].\\nHe      was      part  \\n \\n  of      an      ad      hoc      drum      supergroup      called      SOS      Allstars      and      filled      the      void      for      Coheed      and      Cambria’s      2007  \\n \\n  album      after      their      drummer      Josh      Eppard      left      the      group[10].\\nIn      addition,      Hawkins      formed      his      own      side  \\n \\n  project,      the      Coattail      Riders,      in      2005,      through      which      he      recorded      his      own      music      and      took      the      project      on  \\n \\n  the      road,      performing      in      small      clubs      despite      the      Foo      Fighters’      arena-status[7].\\nHis      son,      Shane      Hawkins,  \\n \\n  has      since      taken      on      his      father’s      legacy,      joining      the      Foo      Fighters      for   \\n \\na      performance      during      the      Boston  \\n \\n  Calling      Music      Festival      in      2023[6].\\n#\\n \\n   Musical      Style      and      Influences  \\n \\n  Taylor      Hawkins      was   \\n \\na      profound      drummer,      with      his      musical      style      and      influences      spreading      across   \\n \\na      wide  \\n \\n  array      of      rock      genres[11].\\nKnown      for      his      passionate      fandom      of      groups      that      came      before      him,      Hawkins  \\n \\n  regularly      expressed      his      admiration      for      bands      like      Rush,      Genesis,      and      the      Police,      all      of      which      featured  \\n \\n  some      of      the      greatest      drummers      in      rock      history      like      Neil      Peart,      Phil      Collins,      and      Stewart      Copeland[11].\\nHe      was      heavily      influenced      by      his      love      for      classic      rock,      as      evidenced      by      his      performances,      where      he  \\n \\n  covered      songs      from      bands      like      Van      Halen[11].\\n \\n \\n  Hawkins      drew      influences      from   \\n \\na      variety      of      drumming      styles,      developing   \\n \\na      signature      style      inspired      by  \\n \\n  greats      like      Roger      Taylor,      Neil      Peart,      Phil      Collins,      Alex      Van      Halen,      and      Stewart      Copeland[14].\\nThis  \\n \\n  distinctive      style      and      influence      extended      to      his      drum      kit,      which      incorporated      elements      like      rototoms  \\n \\n  and      concert      toms[14].\\n \\n \\n  Beyond      his      influences,      Hawkins      had   \\n \\na      unique      energy      that      made      him      stand      out      as   \\n \\na      drummer.\\nHis      performances  \\n \\n  were      recognized      for      their      power,      and      he      was      known      for      his      enthusiastic      and      aggressive      style      of      play[15].\\n \\n \\n  This      earned      him      recognition      as      one      of      the      top      rock      drummers      of      his      time,      with      his      passion      for      music  \\n \\n  living      on      through      his      performances[14].\\n \\n \\n  Through      his      career,      Hawkins      left      an      indelible      mark      on      rock      music,      through      his      distinct      style,      passion,  \\n \\n  and      contributions      to      the      music      industry[13].\\nHis      love      for      music      and      dedication      to      his      craft      made      him       an      unforgettable      icon      in      the      world      of      rock      music[13].\\n#\\n \\n   Personal      Life  \\n \\n  Taylor      Hawkins      married      Alison      Hawkins,      an      American      celebrity      and      entrepreneur,      in      2005[18].\\nThe      couple  \\n \\n  had      three      children,      Oliver,      Annabelle,      and      Everleigh[19].\\nHawkins’      commitment      to      his      family      was      evident;\\n \\n \\n  in      fact,      he      even      wrote   \\n \\na      song      for      his      middle      child,      Annabelle[9].\\n \\n \\n  In      his      personal      life,      Hawkins      had      also      struggled      with      drug      use,      which      nearly      claimed      his      life      in   \\n \\na      2001  \\n \\n  overdose[9][7][4].\\nHowever,      he      managed      to      overcome      this      challenge,      and      later      expressed      gratitude      for  \\n \\n  the      experience      as   \\n \\na      lesson      that      allowed      him      to      realize      the      destructive      path      he      was      on[7].\\n \\n \\n  Outside      of      his      main      role      in      the      Foo      Fighters,      Hawkins      also      pursued      various      side      projects      including      the  \\n \\n  Birds      of      Satan,      NHC,      and      Chevy      Metal.\\nHis      motivation      for      such      ventures      was   \\n \\na      constant      drive      to      create  \\n \\n  and      his      love      for      music[7].\\nHawkins      was      also      known      for      his      unabashed      fanboy      nature,      often      vocalizing  \\n \\n  his      admiration      for      fellow      musicians      and      his      heroes[7].\\n#\\n \\n   Legacy      and      Impact Taylor      Hawkins      was      known      for      his      raw      and      authentic      drumming      style,      described      as      \"courageous,      damaged  \\n \\n  and      unflinchingly      authentic”[20].\\nHis      work      with      the      Foo      Fighters,      as      well      as      his      various      collaborations n’      roll[10].\\n \\n \\n  ‘ and      side      projects,      made      him   \\n \\na      celebrated      figure      in      rock  \\n \\n  Hawkins’      death      in      2022      was      met      with      heartfelt      tributes      from      colleagues      and      fans      around      the      world.\\n \\n \\n  Notable      tributes      came      from      rock      legends      like      Roger      Taylor      of      Queen,      who      considered      Hawkins      as   \\n \\na      kind,  \\n \\n  brilliant      man      and      an      inspirational      mentor,      likening      his      death      to      \"losing   \\n \\na      younger      favourite      brother”[21].\\n \\n \\n  Similarly,      Led      Zeppelin’s      Jimmy      Page      admired      his      technique,      energy      and      spirited      enthusiasm[21].\\n \\n \\n  An      LA      tribute      concert      held      in      his      honor      included      guest      drummers      like      Lars      Ulrich      of      Metallica,      Travis  \\n \\n  Barker      of      blink-182,      and      Brad      Wilk      of      Rage      Against      the      Machine.\\nSingers      like      Miley      Cyrus      and      Alanis  \\n \\n  Morissette      also      performed      at      the      concert[22].\\n \\n \\n  Apart      from      his      music,      Taylor      Hawkins      also      contributed      to      charities      Music      Support      and      MusiCares,      both      of  \\n \\n  which      were      chosen      by      the      Hawkins      family[23].\\nHe      had      received      numerous      accolades      throughout      his      career,  \\n \\n  including      27      Grammy      nominations,      of      which      he      won      14[2].\\nIn      2021,      the      Foo      Fighters      were      inducted      into  \\n \\n  the      Rock      and      Roll      Hall      of      Fame[9].\\n#\\n \\n   Discography  \\n \\n  Taylor      Hawkins      also      led   \\n \\na      notable      music      career      through      his      own      side      projects      and      collaborations[10].\\n \\n \\n  Aside      from      his      work      with      the      Foo      Fighters,      Hawkins      formed      and      fronted      the      band      Taylor      Hawkins   \\n \\n&      The  \\n \\n  Coattail      Riders,   \\n \\na      project      which      originated      from      jamming      sessions      with      his      friend      Drew      Hester[10].\\n###      Taylor      Hawkins   \\n \\n&      The      Coattail      Riders  \\n \\n  Taylor      Hawkins   \\n \\n&      The      Coattail      Riders,   \\n \\na      band      formed      in      2004,      have      released      three      albums      and      their  \\n \\n  music      spans      genres      including      Hard      Rock,      Art      Rock,      and      Alternative      Rock[24][25][26].\\nThe      band      grew      from  \\n \\n  an      initial      casual      jamming      session,      gradually      evolving      into   \\n \\na      more      formal      arrangement      that      led      to      the  \\n \\n  production      of      record      albums.\\nNotably,      these      albums      featured      guest      appearances      by      renowned      musicians  \\n \\n  such      as      Dave      Grohl,      Queen’s      Brian      May      and      Roger      Taylor,      The      Cars’      Elliot      Easton,      Perry      Farrell,      and  \\n \\n  Jon      Davison,      who      is   \\n \\na      school      friend      of      Hawkins’[10].\\n###      Red      Light      Fever  \\n \\n  Red      Light      Fever,      released      on      April      19,      2010,      was      the      band’s      first      album[29][30].\\nPrior      to      its      release,  \\n \\n  Hawkins      revealed      in      an      interview      that      the      album      had      completed      the      recording      and      production      stages,      but  \\n \\n  its      title      and      release      date      were      yet      to      be      determined[29].\\nRed      Light      Fever      was      recorded      at      the      Foo  \\n \\n  Fighters’      Studio      606      in      California      and      featured      guest      musicians      such      as      Brian      May      and      Roger      Taylor      of  \\n \\n  Queen,      Dave      Grohl      of      Foo      Fighters,      and      Elliot      Easton      of      The      Cars[29][30].\\n##      Get      the      Money  \\n \\n  Get      the      Money,      the      third      album      from      Taylor      Hawkins   \\n \\n&      The      Coattail      Riders,      was      released      on      November      8,  \\n \\n  2019[29].\\nThe      album’s      first      single,      \"Crossed      the      Line”,      released      on      October      15,      2019,      featured      Dave  \\n \\n  Grohl      and      Jon      Davison,      the      frontman      of      Yes[29].\\nThe      music      video      for      the      single      \"I      Really      Blew      It”      also  \\n \\n  featured      appearances      from      Grohl      and      Perry      Farrel1[29].\\n#\\n \\n   Collaborations      and      Guest      Appearances  \\n \\n  Throughout      his      career,      Taylor      Hawkins      collaborated      with      various      prominent      artists      and      bands.\\nThe  \\n \\n  Coattail      Riders’      albums      notably      featured      appearances      from      luminaries      such      as      Brian      May      and      Roger      Taylor  \\n \\n  of      Queen,      Chrissie      Hynde,      Nancy      Wilson      of      Heart,      Sex      Pistol      Steve      Jones      and      James      Gang’s      Joe      Walsh[28].\\n \\n \\n  Hawkins      also      fronted      another      group,      The      Birds      of      Satan,      which      evolved      from      his      heavy      rock      covers      band,  \\n \\n  Chevy      Metal[28].\\nDespite      his      diverse      musical      engagements,      Hawkins      always      maintained   \\n \\na      close      allegiance      with      the      Foo  \\n \\n  Fighters,      which      remained      the      center      of      his      music      life[7][28].\\n#\\n \\n   Tragic      Passing  \\n \\n  Taylor      Hawkins,      the      esteemed      drummer      of      the      alt-rock      band      Foo      Fighters,      passed      away      suddenly      on      March  \\n \\n  25,      2022,      while      on      tour      with      his      band      in      Bogota,      Colombia[34].\\nThe      official      cause      of      death      was      cardiac  \\n \\n  arrest,      though      inquiries      were      raised      concerning      the      presence      of      drugs      in      his      system      and      their      potential  \\n \\n  contribution      to      his      death[33][34].\\nOn      the      night      of      his      passing,      paramedics      were      called      to      the      Four  \\n \\n  Seasons      hotel      in      Bogota      due      to      reports      of      chest      pain      from      an      unnamed      guest,      later      revealed      to      be  \\n \\n  Hawkins[34].\\nUnfortunately,      resuscitation      efforts      were      unsuccessful,      and      Hawkins      was      declared      dead      at  \\n \\n  the      scene[34].\\n \\n \\n  The      news      of      Hawkins’      sudden      demise      was      announced      on      the      morning      of      March      25th,      2022,      which      left      the      music  \\n \\n  world      in      shock[32].\\nThe      band      confirmed      the      news      with   \\n \\na      short      statement,      expressing      their      devastation  \\n \\n  at      the      loss      of      Hawkins,      whose      \"musical      spirit      and      infectious      laughter”      would      live      on      forever[32].\\n \\n \\n  As   \\n \\na      result      of      Hawkins’      untimely      passing,      the      band      canceled      their      ongoing      South      American      tour[33].\\nThe  \\n \\n  festival      stage      at      the      Estéreo      Picnic      Festival,      where      the      Foo      Fighters      were      scheduled      to      perform      that  \\n \\n  night,      was      transformed      into   \\n \\na      candlelight      vigil      in      memory      of      Hawkins[33].\\n##      Tributes      and      Remembrances  \\n \\n  In      the      wake      of      Hawkins’      death,      tributes      from      fans      and      colleagues      alike      poured      in      from      around      the  \\n \\n  world[21][31].\\nAmong      the      many      paying      their      respects      were      legendary      rock      and      roll      musicians      like      Roger  \\n \\n  Taylor,      the      drummer      of      Queen,      who      Hawkins      credited      with      inspiring      his      own      career      behind      the      drum      set[21].\\n \\n \\n  In      heartfelt      social      media      posts,      Taylor      described      Hawkins      as      an      \"inspirational      mentor”      and   \\n \\na      \"kind  \\n \\n  brilliant      man\"[21],      while      Led      Zeppelin’s      Jimmy      Page      reminisced      about      sharing      the      stage      with      Hawkins  \\n \\n  and      praised      his      \"technique,      energy      and      spirited      enthusiasm\"[21].\\n \\n \\n  There      were      also      numerous      onstage      tributes      to      Hawkins.\\nNotably,      Miley      Cyrus      expressed      her      grief      and      sent  \\n \\n  peaceful      wishes      to      the      Foo      Fighters      and      the      Hawkins      family      during   \\n \\na      performance      at      Lollapalooza[31].\\n \\n \\n  Similarly,      Liam      Gallagher      of      Oasis      dedicated      one      of      the      band’s      biggest      hits      to      Hawkins      during   \\n \\na      concert  \\n \\n  at      the      Royal      Albert      Hall      in      London[31].\\nFans      gathered      outside      the      hotel      where      Hawkins      died,      lighting      candles,      leaving      flowers,      and      singing      the  \\n \\n  band’s      songs      in      his      honor[31].\\n \\n \\n  Hawkins’      life      and      career      were      celebrated      in   \\n \\na      star-studded      tribute      concert      in      Los      Angeles,      which      saw  \\n \\n  performances      from      over      5@      musicians,      including      his      former      bands      and      colleagues      from      Def      Leppard,      Queen,  \\n \\n  and      Foo      Fighters[22].\\nTable      12:      STORM’s      generated      article      for      “Taylor      Hawkins”.\\n“#’,      “##”      indicate      the      section      title      and      subsection      title  \\n \\n  respectively.\\nNumbers      in      brackets      indicate      the      cited      references.'}]\n"
     ]
    }
   ],
   "source": [
    "class Document:\n",
    "    def __init__(self, metadata, page_content):\n",
    "        self.metadata = metadata\n",
    "        self.page_content = page_content\n",
    "\n",
    "# List of Document objects\n",
    "section_documents =section_documents_2\n",
    "\n",
    "# Convert to list of dictionaries\n",
    "section_documents_dicts_2 = []\n",
    "for doc in section_documents:\n",
    "    doc_dict = {\n",
    "        'metadata': doc.metadata,\n",
    "        'page_content': doc.page_content\n",
    "    }\n",
    "    section_documents_dicts_2.append(doc_dict)\n",
    "\n",
    "# Print the result\n",
    "print(section_documents_dicts_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_links=[\n",
    "    'https://arxiv.org/pdf/2407.14562',\n",
    "    'https://arxiv.org/pdf/2407.14743',\n",
    "    'https://arxiv.org/pdf/2407.14662',\n",
    "    'https://arxiv.org/pdf/2407.15259',\n",
    "    'https://arxiv.org/pdf/2407.15527',\n",
    "    'https://arxiv.org/pdf/2407.12873',\n",
    "    'https://arxiv.org/pdf/2407.14525',\n",
    "    'https://arxiv.org/pdf/2407.14565',\n",
    "    'https://arxiv.org/pdf/2407.14568',\n",
    "    'https://arxiv.org/pdf/2407.14622',\n",
    "    'https://arxiv.org/pdf/2407.14631',\n",
    "    'https://arxiv.org/pdf/2407.14651',\n",
    "    'https://arxiv.org/pdf/2407.14658',\n",
    "    'https://arxiv.org/pdf/2407.14681',\n",
    "    'https://arxiv.org/pdf/2407.14717',\n",
    "    'https://arxiv.org/pdf/2407.14725',\n",
    "    'https://arxiv.org/pdf/2407.14735',\n",
    "    'https://arxiv.org/pdf/2407.14738',\n",
    "    'https://arxiv.org/pdf/2407.14741',\n",
    "    'https://arxiv.org/pdf/2407.14743',\n",
    "    'https://arxiv.org/pdf/2407.14765'\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'return_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# pdf_url = \"https://arxiv.org/pdf/2212.14024.pdf\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m pdf_reader \u001b[38;5;241m=\u001b[39m LayoutPDFReader(llmsherpa_api_url)\n\u001b[0;32m----> 7\u001b[0m doc \u001b[38;5;241m=\u001b[39m \u001b[43mpdf_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_pdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_url\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/T_RAG/.venv/lib/python3.12/site-packages/llmsherpa/readers/file_reader.py:73\u001b[0m, in \u001b[0;36mLayoutPDFReader.read_pdf\u001b[0;34m(self, path_or_url, contents)\u001b[0m\n\u001b[1;32m     71\u001b[0m parser_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_pdf(pdf_file)\n\u001b[1;32m     72\u001b[0m response_json \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(parser_response\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m---> 73\u001b[0m blocks \u001b[38;5;241m=\u001b[39m \u001b[43mresponse_json\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreturn_dict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblocks\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Document(blocks)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'return_dict'"
     ]
    }
   ],
   "source": [
    "from llmsherpa.readers import LayoutPDFReader\n",
    "\n",
    "llmsherpa_api_url = \"http://localhost:5010/api/parseDocument?renderFormat=all\"\n",
    "pdf_url = \"/home/ubuntu/T_RAG/testing/1910.13461v1.pdf\" \n",
    "pdf_reader = LayoutPDFReader(llmsherpa_api_url)\n",
    "doc = pdf_reader.read_pdf(pdf_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embedding = embedding_model.encode(\"hwo to dance\").tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
