{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  strategy=\"html\"\n",
    "from langchain_community.document_loaders.llmsherpa import LLMSherpaFileLoader\n",
    "loader = LLMSherpaFileLoader(\n",
    "    file_path=\"/home/ubuntu/T_RAG/testing/2407.13734v1.pdf\",\n",
    "    new_indent_parser=True,\n",
    "    apply_ocr=True,\n",
    "    strategy=\"html\",\n",
    "    llmsherpa_api_url=\"http://localhost:5010/api/parseDocument?renderFormat=all\",\n",
    ")\n",
    "html_docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '/home/ubuntu/T_RAG/testing/2407.13734v1.pdf'}, page_content='<html><h1>Understanding      Reinforcement      Learning-Based</h1><h1>Fine-Tuning      of      Diffusion      Models:      A      Tutorial      and      Review</h1><p>Masatoshi      Uehara*!,      Yulai      Zhao\\\\\\\\\\\\\\'*,      Tommaso      Biancalani!,      and      Sergey      Levine?</p><p>‘Genentech</p><li>*Princeton      University</li><p>>University      of      California,      Berkeley</p><h2>July      19,      2024</h2><h2>Abstract</h2><p> \\n \\n  This      tutorial      provides   \\n \\na      comprehensive      survey      of      methods      for      fine-tuning      diffusion      models  \\n \\n  to      optimize      downstream      reward      functions.\\nWhile      diffusion      models      are      widely      known      to      provide  \\n \\n  excellent      generative      modeling      capability,      practical      applications      in      domains      such      as      biology  \\n \\n  require      generating      samples      that      maximize      some      desired      metric      (e.g.,      translation      efficiency      in  \\n \\n  RNA,      docking      score      in      molecules,      stability      in      protein).\\nIn      these      cases,      the      diffusion      model      can  \\n \\n  be      optimized      not      only      to      generate      realistic      samples      but      also      to      maximize      the      measure      of      interest  \\n \\n  explicitly.\\nSuch      methods      are      based      on      concepts      from      reinforcement      learning      (RL).\\nWe      explain  \\n \\n  the      application      of      various      RL      algorithms,      including      PPO,      differentiable      optimization,      reward-  \\n \\n  weighted      MLE,      value-weighted      sampling,      and      path      consistency      learning,      tailored      specifically      for  \\n \\n  fine-tuning      diffusion      models.\\nWe      aim      to      explore      fundamental      aspects      such      as      the      strengths      and       limitations      of      different      RL-based      fine-tuning      algorithms      across      various      scenarios,      the      benefits  \\n \\n  of      RL-based      fine-tuning      compared      to      non-RL-based      approaches,      and      the      formal      objectives      of  \\n \\n  RL-based      fine-tuning      (target      distributions).\\nAdditionally,      we      aim      to      examine      their      connections  \\n \\n  with      related      topics      such      as      classifier      guidance,      Gflownets,      flow-based      diffusion      models,      path  \\n \\n  integral      control      theory,      and      sampling      from      unnormalized      distributions      such      as      MCMC.\\nThe  \\n \\n  code      of      this      tutorial      is      available      at      https://github.com/masa-ue/RLfinetuning_Diffusion_Bioseq.</p><p>arXiv:2407.13734v1      [cs.LG]      18      Jul      2024</p><h2>July      19,      2024</h2><h2>Abstract</h2><p> \\n \\n  This      tutorial      provides   \\n \\na      comprehensive      survey      of      methods      for      fine-tuning      diffusion      models  \\n \\n  to      optimize      downstream      reward      functions.\\nWhile      diffusion      models      are      widely      known      to      provide  \\n \\n  excellent      generative      modeling      capability,      practical      applications      in      domains      such      as      biology  \\n \\n  require      generating      samples      that      maximize      some      desired      metric      (e.g.,      translation      efficiency      in  \\n \\n  RNA,      docking      score      in      molecules,      stability      in      protein).\\nIn      these      cases,      the      diffusion      model      can  \\n \\n  be      optimized      not      only      to      generate      realistic      samples      but      also      to      maximize      the      measure      of      interest  \\n \\n  explicitly.\\nSuch      methods      are      based      on      concepts      from      reinforcement      learning      (RL).\\nWe      explain  \\n \\n  the      application      of      various      RL      algorithms,      including      PPO,      differentiable      optimization,      reward-  \\n \\n  weighted      MLE,      value-weighted      sampling,      and      path      consistency      learning,      tailored      specifically      for  \\n \\n  fine-tuning      diffusion      models.\\nWe      aim      to      explore      fundamental      aspects      such      as      the      strengths      and       limitations      of      different      RL-based      fine-tuning      algorithms      across      various      scenarios,      the      benefits  \\n \\n  of      RL-based      fine-tuning      compared      to      non-RL-based      approaches,      and      the      formal      objectives      of  \\n \\n  RL-based      fine-tuning      (target      distributions).\\nAdditionally,      we      aim      to      examine      their      connections  \\n \\n  with      related      topics      such      as      classifier      guidance,      Gflownets,      flow-based      diffusion      models,      path  \\n \\n  integral      control      theory,      and      sampling      from      unnormalized      distributions      such      as      MCMC.\\nThe  \\n \\n  code      of      this      tutorial      is      available      at      https://github.com/masa-ue/RLfinetuning_Diffusion_Bioseq.</p><p>arXiv:2407.13734v1      [cs.LG]      18      Jul      2024</p><h1>Introduction</h1><p> \\n \\n  Diffusion      models      (Sohl-Dickstein      et      al.,      2015;      Ho      et      al.,      2020;      Song      et      al.,      2020)      are      widely      rec-  \\n \\n  ognized      as      powerful      tools      for      generative      modeling.\\nThey      are      able      to      accurately      model      complex  \\n \\n  distributions      by      closely      emulating      the      characteristics      of      the      training      data.\\nThere      are      many      applica-  \\n \\n  tions      of      diffusion      models      in      various      fields,      including      computer      vision      (Podell      et      al.,      2023),      natural  \\n \\n  language      processing      (Austin      et      al.,      2021),      biology      (Avdeyev      et      al.,      2023;      Stark      et      al.,      2024;      Li      et      al., *“uehara.masatoshi@gene.com  \\n \\n  tyulaiz@princeton.\\nedu.\\nEqual      contribution.</p><p> \\n \\n  Reward      models  \\n \\n  Images   \\n \\n—      Aesthetic      score   \\n \\n|      Images   \\n \\n—      Aesthetic      score      Images   \\n \\n—      Aesthetic      score  \\n \\n  Molecules   \\n \\n~      QED      Molecules   \\n \\n—      QED      Molecules   \\n \\n—      QED  \\n \\n  DNAs   \\n \\n=      Cell-specificity   \\n \\n[      DNAs   \\n \\n=      Cell-specificity      DNAs   \\n \\n—      Cell-specificity freely   \\n \\nf      rel   \\n \\nf      rel      ainsoms.</p><p>es   \\n \\n(   \\n \\n_   \\n \\n|   \\n \\n) Images      with      high  \\n \\n  aesthetic      score</p><h2>Images</h2><p> \\n \\n  g   \\n \\n|   \\n \\na      ge   \\n \\nz       evi      do}   \\n \\nf      g,   \\n \\n2      3°      DNAs      with      high  \\n \\n  aor   \\n \\na   \\n \\n-      e      e       DNAs      io      si   \\n \\ni      og   \\n \\n+   \\n \\na      i:      cell-specificity</p><p>Figure      1:      Illustrative      examples      of      RL-based      fine-tuning,      aimed      at      optimizing      pre-trained      diffusion  \\n \\n  models      to      maximize      downstream      reward      functions.</p><p> \\n \\n  2023),      chemistry      (Jo      et      al.,      2022;      Xu      et      al.,      2022;      Hoogeboom      et      al.,      2022),      and      biology      (Avdeyev  \\n \\n  et      al.,      2023;      Stark      et      al.,      2024;      Campbell      et      al.,      2024).</p><p> \\n \\n  While      diffusion      models      exhibit      significant      power      in      capturing      the      training      data      distribution,  \\n \\n  there’s      often   \\n \\na      need      to      customize      these      models      for      particular      downstream      reward      functions.\\nFor  \\n \\n  instance,      in      computer      vision,      Stable      Diffusion      (Rombach      et      al.,      2022)      serves      as   \\n \\na      strong      backbone  \\n \\n  pre-trained      model.\\nHowever,      we      may      want      to      fine-tune      it      further      by      optimizing      downstream      reward  \\n \\n  functions      such      as      aesthetic      scores      or      human-alignment      scores      (Black      et      al.,      2023;      Fan      et      al.,      2023).\\n \\n \\n  Similarly,      in      fields      such      as      biology      and      chemistry,      various      sophisticated      diffusion      models      have  \\n \\n  been      developed      for      DNA,      RNA,      protein      sequences,      and      molecules,      effectively      modeling      biological  \\n \\n  and      chemical      spaces.\\nNonetheless,      biologists      and      chemists      typically      aim      to      optimize      specific  \\n \\n  downstream      objectives      such      as      cell-specific      expression      in      DNA      sequences      (Gosai      et      al.,      2023;      Lal  \\n \\n  et      al.,      2024;      Sarkar      et      al.,      2024),      translational      efficiency/stability      of      RNA      sequences      (Castillo-Hair  \\n \\n  and      Seelig,      2021;      Agarwal      and      Kelley,      2022),      stability/bioactivity      of      protein      sequence      (Frey      et      al.,  \\n \\n  2023;      Widatalla      et      al.,      2024)      or      QED/SA      scores      of      molecules      (Zhou      et      al.,      2019).</p><p> \\n \\n  To      achieve      this      goal,      numerous      algorithms      have      been      proposed      for      fine-tuning      diffusion      models  \\n \\n  via      reinforcement      learning      (RL)      (e.g.,      Black      et      al.      (2023);      Fan      et      al.\\n(2023);      Clark      et      al.\\n(2023);  \\n \\n  Prabhudesai      et      al.\\n(2023);      Uehara      et      al.\\n(2024)),      aiming      to      optimize      downstream      reward      functions.\\n \\n \\n  RL      is   \\n \\na      machine      learning      paradigm      where      agents      learn      to      make      sequential      decisions      to      maximize  \\n \\n  reward      signals      (Sutton      and      Barto,      2018;      Agarwal      et      al.,      2019).\\nIn      our      context,      RL      naturally      emerges  \\n \\n  as   \\n \\na      Suitable      approach      due      to      the      sequential      structure      inherent      in      diffusion      models,      where      each      time  \\n \\n  step      involves   \\n \\na      “decision”      corresponding      to      how      the      sample      is      denoised      at      that      step.\\nThis      tutorial  \\n \\n  aims      to      review      recent      works      for      readers      interested      in      understanding      the      fundamentals      of      RL-based  \\n \\n  fine-tuning      from   \\n \\na      holistic      perspective,      including      the      advantages      of      RL-based      fine-tuning      over  \\n \\n  non-RL      approaches,      the      pros      and      cons      of      different      RL-based      fine-tuning      algorithms,      the      formalized  \\n \\n  goal      of      RL-based      fine-tuning,      and      its      connections      with      related      topics      such      as      classifier      guidance.</p><p> \\n \\n  The      content      of      this      tutorial      is      primarily      divided      into      three      parts.\\nIn      addition,      as      an      implementation  \\n \\n  example,      we      also      release      the      code      that      employs      RL-based      fine-tuning      for      guided      biological      sequences  \\n \\n  (DNA/RNA)      generation      at      https://github.com/masa-ue/RLfinetuning_Diffusion_Bioseq.</p><li>1.      We      aim      to      provide   \\n \\na      comprehensive      overview      of      current      algorithms.\\nNotably,      given      the  \\n \\n  sequential      nature      of      diffusion      models,      we      can      naturally      frame      fine-tuning      as   \\n \\na      reinforcement  \\n \\n  learning      (RL)      problem      within      Markov      Decision      Processes      (MDPs),      as      detailed      in      Section   \\n \\n3       and      4.\\nTherefore,      we      can      employ      any      off-the-shelf      RL      algorithms      such      as      PPO      (Schulman  \\n \\n  et      al.,      2017),      differentiable      optimization      (direct      reward      backpropagation),      weighted      MLE  \\n \\n  (Peters      et      al.,      2010;      Peng      et      al.,      2019),      value-weighted      sampling      (close      to      classifier      guidance  \\n \\n  in      Dhariwal      and      Nichol      (2021)),      and      path      consistency      learning      (Nachum      et      al.,      2017).\\nWe  \\n \\n  discuss      these      algorithms      in      detail      in      Section      4.2      and      6.\\nInstead      of      merely      outlining      each  \\n \\n  algorithm,      we      aim      to      present      both      their      advantages      and      disadvantages      so      readers      can      select  \\n \\n  the      most      suitable      algorithms      for      their      specific      purposes.</li><li>2.      We      categorize      various      fine-tuning      scenarios      based      on      how      reward      feedback      is      acquired      in  \\n \\n  Section      7.\\nThis      distinction      is      pivotal      for      practical      algorithm      design.\\nFor      example,      if      we      can  \\n \\n  access      accurate      reward      functions,      computational      efficiency      would      become      our      primary      focus.\\n \\n \\n  However,      in      cases      where      reward      functions      are      unknown,      it      is      essential      to      learn      them      from  \\n \\n  data      with      reward      feedback,      leading      us      to      take      feedback      efficiency      and      distributional      shift      into  \\n \\n  consideration      as      well.\\nSpecifically,      when      reward      functions      need      to      be      learned      from      static  \\n \\n  offline      data      without      any      online      interactions,      we      must      address      the      issue      of      overoptimization,  \\n \\n  where      fine-tuned      models      are      misled      by      out-of-distribution      samples,      and      generate      samples  \\n \\n  with      low      genuine      rewards.\\nThis      is      crucial      because,      in      an      offline      scenario,      the      coverage      of  \\n \\n  offline      data      distribution      with      feedback      is      limited;      hence,      the      out-of-distribution      region      could  \\n \\n  be      extensive      (Uehara      et      al.,      2024).</li><li>3.      We      provide   \\n \\na      detailed      discussion      on      the      relationship      between      RL-based      fine-tuning      methods  \\n \\n  and      closely      related      methods      in      the      literature,      such      as      classifier      guidance      (Dhariwal      and      Nichol,  \\n \\n  2021)      in      Section      8,      flow-based      diffusion      models      (Liu      et      al.,      2022;      Lipman      et      al.,      2023;      Tong  \\n \\n  et      al.,      2023)      in      Section      9,      sampling      from      unnormalized      distributions      (Zhang      and      Chen,      2021)  \\n \\n  in      Section      10,      Gflownets      (Bengio      et      al.,      2023)      in      Section      6.3,      and      path      integral      control      theory  \\n \\n  (Theodorou      et      al.,      2010;      Williams      et      al.,      2017;      Kazim      et      al.,      2024)      in      Section      6.2.3.      We  \\n \\n  summarize      the      key      messages      as      follows.</li><p>¢\\n \\n   Section      6.3:      The      losses      used      in      Gflownets      are      fundamentally      equivalent      to      those      derived  \\n \\n  from   \\n \\na      specific      RL      algorithm      called      path      consistency      learning.</p><p> \\n \\n \\n¢      Section      8:      Classifier      guidance      employed      in      conditional      generation      is      regarded      as   \\n \\na       specific      RL-based      fine-tuning      method,      which      we      call      value-weighted      sampling.\\nAs  \\n \\n  formalized      in      Zhao      et      al.\\n(2024),      this      observation      indicates      that      any      off-the-shelf      RL-  \\n \\n  based      fine-tuning      algorithms      (e.g.,      PPO      and      differentiable      optimization)      can      be      applied  \\n \\n  to      conditional      generation.</p><p> \\n \\n \\n¢      Section      10:      Sampling      from      unnormalized      distributions,      often      referred      to      as      Gibbs  \\n \\n  distributions,      is      an      important      and      challenging      problem      in      diverse      domains.\\nWhile  \\n \\n  MCMC      methods      are      traditionally      used      for      this      task,      recognizing      its      similarity      to      the  \\n \\n  objectives      of      RL-based      fine-tuning      suggests      that      off-the-shelf      RL      algorithms      can      also  \\n \\n  effectively      address      the      challenge      of      sampling      from      unnormalized      distributions.</p><h2>Preliminaries</h2><li>1.1 DiffusionModels 2 2.20.\\n00 2 ee ee</li><table><th><td colSpan=1>1.1.1 Score-Based Diffusion Models (Optional)</td><td colSpan=1></td><td colSpan=1></td></th><tr><td colSpan=1>1.2 Fine-Tuning Diffusion Models withRL</td><td colSpan=1>0.00.0.</td><td colSpan=1></td></tr><tr><td colSpan=1>1.2.1. Brief Overview: Fine-tuning withRL 1.2.2 Motivation for Using RL over Non-RL Alternatives.</td><td colSpan=1></td><td colSpan=1>2.</td></tr><tr><td>Brief      Overview      of      Entropy-Regularized      MDPs</td></tr><tr><td>Fine-Tuning      Diffusion      Models      with      RL      in      Entropy      Regularized      MDPs</td></tr><tr><td>Theory      of      RL-Based      Fine-Tuning</td></tr></table><h2>RL-Based      Fine-Tuning      Algorithms      1:      Non-Distribution-Constrained      Approaches</h2><h2>RL-Based      Fine-Tuning      Algorithms      2:      Distribution-Constrained      Approaches</h2><h2>Fine-Tuning      Settings      Taxonomy</h2><h2>Connection      with      Classifier      Guidance</h2><p> \\n \\n  Connection      with      Flow-Based      Diffusion      Models  \\n \\n  10  \\n \\n  10  \\n \\n  11 12  \\n \\n  14  \\n \\n  14  \\n \\n  15  \\n \\n  16  \\n \\n  16  \\n \\n  17  \\n \\n  18  \\n \\n  19  \\n \\n  21  \\n \\n  21  \\n \\n  23  \\n \\n  24  \\n \\n  25  \\n \\n  25  \\n \\n  28  \\n \\n  28  \\n \\n  28</p><p>10      Connection      with      Sampling      from      Unnormalized      Distributions      35 10.1 Markov Chain Monte Carlo(MCMC) 2.00000 eee ene 35 10.2 RL-Based Approaches 0 2.0.0 pee ee 36</p><p>11      Closely      Related      Directions      36</p><p>12      Summary      37</p><p>1\\n \\n   Preliminaries</p><p>In      this      section,      we      outline      the      fundamentals      of      diffusion      models      and      elucidate      the      objective      of  \\n \\n  fine-tuning      them.</p><h3>1.1.      Diffusion      Models</h3><p>dels  \\n \\n  We      present      an      overview      of      denoising      diffusion      probabilistic      models      (DDPM)      (Ho      et      al.,      2020).\\n \\n \\n  For      more      details,      refer      to      Yang      et      al.\\n(2023);      Cao      et      al.\\n(2024);      Chen      et      al.\\n(2024);      Tang      and      Zhao  \\n \\n  (2024).</p><p> \\n \\n  In      diffusion      models,      the      objective      is      to      develop   \\n \\na      deep      generative      model      that      accurately      captures  \\n \\n  the      true      data      distribution.\\nSpecifically,      denoting      the      data      distribution      by      pr.\\n \\n \\n€      A(4’)      where   \\n \\n¥      is      an  \\n \\n  input      space,   \\n \\na      DDPM      aims      to      approximate      p,,.\\nusing   \\n \\na      parametric      model      structured      as 1 p(xo3      8)   \\n \\n=   \\n \\n[      rlco      O)dx1.r,      where      p(%o:7;      9)   \\n \\n=      pr+i(@7;      9)      [[      peter:      0).</p><p>t=T When   \\n \\n4      is      an      Euclidean      space      (in      R®),      the      forward      process      is      modeled      as      the      following      dynamics:</p><p>pra(er)      =N(0,1),      pe(ve-r)2;      9)   \\n \\n=      N(p(a1,      t;      8),      07      (4)   \\n \\nx      D),  \\n \\n  where      N(-,-)      denotes   \\n \\na      normal      distribution,   \\n \\nJ      is      an      identity      matrix      and   \\n \\np   \\n \\n:      R¢   \\n \\nx      [0,7]   \\n \\n>      R¢.\\nIn  \\n \\n  DDPMs,      we      aim      to      obtain   \\n \\na      set      of      policies      (i.e.,      denoising      process)      {p;}{_741,      pr:   \\n \\n&   \\n \\n—      A(A&)      such  \\n \\n  that      p(x;      0)   \\n \\n©      Ppre(#o).\\nIndeed,      by      optimizing      the      variational      bound      on      the      negative      log-likelihood,  \\n \\n  we      can      derive      such   \\n \\na      set      of      policies.\\nFor      more      details,      refer      to      Section      1.1.1.  \\n \\n  Hereafter,      we      consider   \\n \\na      situation      where      we      have   \\n \\na      pre-trained      diffusion      model      that      is      already  \\n \\n  trained      on   \\n \\na      large      dataset,      such      that      the      model      can      accurately      capture      the      underlying      data      distribution.</p><p> \\n \\n  pre We      refer      to      the      pre-trained      policies      as      {p?\\n\"*(-|-)}{_7,,,      and      to      the      marginal      distribution      at      t   \\n \\n=   \\n \\n0       induced      by      the      pre-trained      diffusion      model      as      pp.\\nIn      other      words, 1\\n \\n    pr      (x0)   \\n \\n=      [iler)      [[°@eledderr.</p><p>t=T  \\n \\n  Remark   \\n \\n1      (Non-Euclidean      space).\\nFor      simplicity,      we      typically      assume      that      the      domain      space      is  \\n \\n  Euclidean.\\nHowever,      we      can      easily      extend      most      of      the      discussion      to   \\n \\na      more      general      space,      such      as  \\n \\n  a      Riemannian      manifold      (De      Bortoli      et      al.,      2022)      or      discrete      space      (Austin      et      al.,      2021;      Campbell  \\n \\n  et      al.,      2022;      Benton      et      al.,      2024;      Lou      et      al.,      2023).</p><p> \\n \\n  Remark   \\n \\n2      (Conditional      generative      models).\\nPre-trained      models      can      be      conditional      diffusion      models,  \\n \\n  such      as      text-to-image      diffusion      models      (Ramesh      et      al.,      2022).\\nThe      extension      is      straightforward:  \\n \\n  augmenting      the      input      spaces      of      policies      with      an      additional      space      on      which      we      want      to      condition.\\n \\n \\n  More      specifically,      by      denoting      that      space      by      c   \\n \\n€      C,      each      policy      becomes      p;(x¢|%1,      650)   \\n \\n:   \\n \\n&   \\n \\nx      C   \\n \\n>       A(X).</p><p> \\n \\n  Remark   \\n \\n3      (Extension      to      Continuous-time      diffusion      models).\\nJn      this      tutorial,      our      discussion      on  \\n \\n  fine-tuning      diffusion      models      will      be      primarily      formulated      on      the      discrete-time      formulation,      as      we      did  \\n \\n  above      Nonetheless,      much      of      our      discussion      is      also      applicable      to      continuous-time      diffusion      models,  \\n \\n  as      formalized      in      Uehara      et      al.\\n(2024)</p><h4>1.1.1      Score-Based      Diffusion      Models      (Optional)</h4><p>nal)  \\n \\n  We      briefly      discuss      how      to      train      diffusion      models      in   \\n \\na      continuous-time      framework      (Song      et      al.,  \\n \\n  2021).\\nIn      our      tutorial,      this      section      is      mainly      used      to      discuss      several      algorithms      (e.g.,      value-weighted  \\n \\n  sampling      in      Section      6.2)      and      their      relationship      with      flow-based      diffusion      models      (Section      9)      later.</p><p> \\n \\n  Therefore,      readers      may      skip      it      based      on      their      individual      needs.</p><p> \\n \\n  The      training      process      of      diffusion      models      can      be      summarized      as      follows.\\nOur      objective      is      to      train  \\n \\n  a      sequential      mapping      from   \\n \\na      known      noise      distribution      to   \\n \\na      data      distribution,      formalized      through  \\n \\n  a      stochastic      differential      equation      (SDE).\\nFirstly,      we      define   \\n \\na      forward      (fixed)      SDE      that      maps      from  \\n \\n  the      data      distribution      to      the      noise      distribution.\\nThen,   \\n \\na      time-reversal      SDE      is      expressed      as      an      SDE,  \\n \\n  which      includes      the      (unknown)      score      function.\\nNow,      by      learning      this      unknown      score      function      from  \\n \\n  the      training      data,      the      time-reversal      SDE      can      be      utilized      as   \\n \\na      generative      model.\\nHere      are      the      details.</p><p>Forward      and      time-reversal      SDE.\\nFirstly,      we      introduce   \\n \\na      forward      (a.k.a.,      reference)      Stochastic  \\n \\n  differential      equations      (SDE)      from   \\n \\n0      to      7’.\\n \\n \\nA      common      choice      is   \\n \\na      variance-preserving      (VP)      process:</p><p>t   \\n \\n€      [0,7];      dx,   \\n \\n=      —0.52,dt   \\n \\n+      du;,,      x9   \\n \\n~      p(x),      (1)</p><p>where      dw;      represents      standard      Brownian      motion.\\nHere      are      two      crucial      observations:</p><p>¢\\n \\n   As      T      approaches      oo,      the      limiting      distribution      is      \\\\\\\\\\\\\\\\V(0,      I).</p><p>¢\\n \\n   The      time-reversal      SDE      (Anderson,      1982),      which      preserves      the      marginal      distribution,      is  \\n \\n  expressed      as      follows:</p><p>t   \\n \\n€      [0,7];      dz,   \\n \\n=      [0.5z,   \\n \\n+   \\n \\nV      log      qr_s(%)|dt   \\n \\n+      du,.\\n(2) Here,      g,   \\n \\n€      A(R“)      denotes      the      marginal      distribution      at      time      t      induced      by      the      reference      SDE (1).\\nNotably,      the      marginal      distribution      of      zr_;      is      the      same      as      the      one      of      x;      induced      by      the  \\n \\n  reference      SDE.</p><p> \\n \\n  These      observations      suggest      that      with      sufficiently      large      T,      starting      from   \\n \\nV      (0,      I)      and      following      the  \\n \\n  time-reversal      SDE      (2)   \\n \\n,      we      are      able      to      sample      from      the      data      distribution      (i.e.,      p?\\n\"°)      at      the      terminal  \\n \\n  time      point      7’.</p><p>Training      score      functions      from      the      training      data.\\nNow,      the      remaining      question      is      how      to      estimate  \\n \\n  the      marginal      score      function   \\n \\nV      log      q(-)      in      (2).\\nThis      can      be      computed      using      the      following      principles:</p><p>°\\n \\n   We      have   \\n \\nV      log      (a+)   \\n \\n=      Exgxppre[Va,      log      qyo(ae   \\n \\n|      2o)],      where      q@jo      represents      the      conditional  \\n \\n  distribution      of      x;      given      Xo      induced      by      the      reference      SDE.</p><p>¢\\n \\n   The      conditional      score      function      V,,,      log      dejo      (Xe   \\n \\n|      xo)      can      be      analytically      derived      as  \\n \\n  °</p><h5>      Myo      —      Ut</h5><p> \\n \\n  {ory      ue   \\n \\n=      exp(—0.5t)xo,      {o?\\n}?\\n \\n \\n=   \\n \\n1   \\n \\n—      exp(—0.5t).\\n(3) t Subsequently,      by      defining   \\n \\na      parameterized      model      sg   \\n \\n:      R¢   \\n \\nx      [0,7]   \\n \\n>      R¢      and      utilizing      the      following weighted      regression,      the      marginal      score      is      estimated:</p><p>A\\n \\n \\n  Dore   \\n \\n=      argmin      Eywuni((0,7))      ce~aejo(w+|ar0),c0~pPr®      [A(t)      ||      Ver,      log      jo      (Le   \\n \\n|      ro)   \\n \\n_      So(£t,      t)      17)      (4)</p><p> \\n \\n  where   \\n \\n2   \\n \\n:      [0,7]   \\n \\n+   \\n \\nR      is   \\n \\na      weighted      function.\\n \\n \\n  Note      we      typically      use      another      parametrization      well.\\nFrom      (3),      we      can      easily      see      that      this      score      is  \\n \\n  estimated      by      introducing      networks      €9   \\n \\n:      R@   \\n \\nx      [0,7]   \\n \\n—      R®,      which      aims      to      estimate      the      noise      Bere      eO</p><p>Ore   \\n \\n=      argmin      Ey.\\nuni({0,7]),cr~p2xo+o?\\ne~N      (0,1)      ,co~pPte      [A(t)/{op}?\\nlle   \\n \\n—      €9      (1,      t)      7)   \\n \\n:      (5)</p><p>Then,      by      denoting      the      training      data      as      {a}      and      setting      A(t)   \\n \\n=      {a7      }”,      the      actual      loss      function      from the      training      data      is argmin   \\n \\n)      Ey      ai(o.r))a   \\n \\nO      peal      +e02,e~N      (0,1)      [Ile   \\n \\n—      €o(ae?,      t)      [17].\\n(6) i=1 Inference      time.\\nOnce      this      Bore      (or      Ore)      is      learned,      we      insert      it      into      the      time-reversal      SDE      (2)      and use      it      as   \\n \\na      generative      model.\\nUltimately,      with      standard      discretization,      we      obtain:</p><p>p(      xe,      t;      a)   \\n \\n=      24+      (0.54,   \\n \\n+      Sire      (,,T      —t)|(ot),      {o?(t)}?\\n \\n \\n=      (6t).</p><p>Equivalently,      this      is (21,      t;      Ore)   \\n \\n=   \\n \\n2   \\n \\n+      [0.52   \\n \\n—      1/o?\\n \\n \\nx      e%,,,(a1,T   \\n \\n—      t)]|      (6t),      (7)</p><p>where      (dt)      denotes      the      discretization      step.</p><p> \\n \\n  Equivalence      to      DDPM.\\nThe      objective      function      derived      here      is      equivalent      to      the      one      formulated  \\n \\n  based      on      variational      inference      in      the      discretized      formulation,      which      is      commonly      referred      to      as  \\n \\n  DDPMs      (Ho      et      al.,      2020).\\nIn      DDPMs      (Ho      et      al.,      2020),      we      often      see      the      following      form:</p><p> \\n \\n \\n1      1-      (ey;</p><p> \\n \\n  it;      0)   \\n \\n=      ~~   \\n \\n’      ),  \\n \\n  p(xy   \\n \\n)      Xt      Jina      ae      [om</p><h6>Vor</h6><p>When      a;   \\n \\n=   \\n \\n1   \\n \\n—      (dt),      the      above      is      equivalent      to      (7)      when      dt      goes      to   \\n \\n0      by      noting      1/,/a;   \\n \\n©      1+0.5(d¢).</p><h3>1.2      Fine-Tuning      Diffusion      Models      with      RL</h3><p>RL  \\n \\n  Importantly,      our      focus      on      RL-based      fine-tuning      distinguishes      itself      from      the      standard      fine-tuning  \\n \\n  methods.\\nStandard      fine-tuning      typically      involves      scenarios      where      we      have      pre-trained      models  \\n \\n  (e.g.,      diffusion      models)      and      new      training      data      {2,      y}.\\nIn      such      cases,      the      common      approach  \\n \\n  for      fine-tuning      is      to      retrain      diffusion      models      with      the      new      training      data      using      the      same      loss  \\n \\n  function      employed      during      pre-training.\\nIn      sharp      contrast,      RL-based      fine-tuning      directly      employs  \\n \\n  the      downstream      reward      functions      as      the      primary      optimization      objectives,      making      the      loss      functions  \\n \\n  different      from      those      used      in      pre-training.</p><p> \\n \\n  Hereafter,      we      start      with   \\n \\na      concise      overview      of      RL-based      fine-tuning.\\nThen,      before      delving      into  \\n \\n  specifics,      we      discuss      simpler      non-RL      alternatives      to      provide      motivation      for      adopting      RL-based  \\n \\n  fine-tuning.</p><li>1.2.1      Brief      Overview:      Fine-tuning      with      RL</li><p>In      this      article,      we      explore      the      fine-tuning      of      pre-trained      diffusion      models      to      optimize      downstream  \\n \\n  reward      functions   \\n \\nr   \\n \\n:      R¢   \\n \\n+      R.      In      domains      such      as      images,      these      backbone      diffusion      models      to      be  \\n \\n  fine-tuned      include      Stable      Diffusion      (Rombach      et      al.,      2022),      while      the      reward      functions      are      aesthetic  \\n \\n  scores      and      alignment      scores      (Clark      et      al.,      2023;      Black      et      al.,      2023;      Fan      et      al.,      2023).\\nMore      examples  \\n \\n  are      detailed      in      the      introduction.\\nThese      rewards      are      often      unknown,      necessitating      learning      from  \\n \\n  data      with      feedback:      {2      r(x™)}.\\nWe      will      explore      this      aspect      further      in      Section      7.\\nUntil      then,      we  \\n \\n  assume   \\n \\nr      is      known.\\n \\n \\n  Now,      readers      may      wonder      about      the      objectives      we      aim      to      achieve      during      the      fine-tuning      process.\\n \\n \\n  A      natural      approach      is      to      define      the      optimization      problem:</p><p>argmax      E,.\\n[r()|      (8)  \\n \\n  qeA(¥)  \\n \\n  where   \\n \\nq      is      initialized      with   \\n \\na      pre-trained      diffusion      model      p?\\n*®   \\n \\n€      A(2).\\nIn      this      tutorial,      we      will      detail  \\n \\n  the      procedure      of      solving      (8)      with      RL      in      the      upcoming      sections.\\nIn      essence,      we      leverage      the      fact  \\n \\n  that      diffusion      models      are      formulated      as   \\n \\na      sequential      decision-making      problem,      where      each      decision  \\n \\n  corresponds      to      how      samples      are      denoised.</p><p> \\n \\n  Although      the      above      objective      function      (8)      is      reasonable,      the      resulting      distribution      might      deviate  \\n \\n  too      much      from      the      pre-trained      diffusion      model.\\nTo      circumvent      this      issue,   \\n \\na      natural      way      is      to      add       penalization      against      pre-trained      diffusion      models.\\nThen,      the      target      distribution      is      defined      as:</p><p>argmax      E,.4[r(x)]   \\n \\n—      aKL(q||p’*).\\n(9)  \\n \\n  qeA(¥) Notably,      (9)      reduces      to      the      following      distribution:</p><li>(10)</li><p>5\\n \\n   exp(r()/a)p\"\"()  \\n \\n  Pr)   \\n \\n=      Fra)      apa)</p><p> \\n \\n  Here,      the      first      term      in      (9)      corresponds      to      the      mean      reward,      which      we      want      to      optimize      in      the  \\n \\n  fine-tuning      process.\\nThe      second      term      in      (10)      serves      as   \\n \\na      penalty      term,      indicating      the      deviation      of   \\n \\nq       from      the      pre-trained      model.\\nThe      parameter   \\n \\na      controls      the      strength      of      this      regularization      term.\\nThe  \\n \\n  proper      choice      of   \\n \\na      depends      on      the      task      we      are      interested      in.</p><h6>1.2.2      Motivation      for      Using      RL      over      Non-RL      Alternatives</h6><p>ives  \\n \\n  To      achieve      our      goal      of      maximizing      downstream      reward      functions      with      diffusion      models,      readers  \\n \\n  may      question      whether      alternative      approaches      can      be      employed      apart      from      RL.\\nHere,      we      investigate  \\n \\n  these      potential      alternatives      and      explain      why      RL      approaches      may      offer      advantages      over      them.</p><p> \\n \\n  Rejection      sampling.\\nOne      approach      involves      generating      multiple      samples      from      pre-trained      diffu-  \\n \\n  sion      models      and      selecting      only      those      with      high      rewards.\\nThis      method,      called      rejection      sampling,  \\n \\n  operates      without      needing      fine-tuning.\\nHowever,      rejection      sampling      is      effective      primarily      when      the  \\n \\n  pre-trained      model      already      has   \\n \\na      high      probability      of      producing      high-reward      samples.\\nIt      resembles  \\n \\n  sampling      from   \\n \\na      prior      distribution      to      obtain      posterior      samples      (in      this      case,      high-reward      points).\\n \\n \\n  This      approach      works      efficiently      when      the      posterior      closely      matches      the      prior      but      can      become      highly  \\n \\n  inefficient      otherwise.\\nIn      contrast,      by      explicitly      updating      weight      in      diffusion      models,      RL-based      fine-  \\n \\n  tuning      allows      us      to      obtain      these      high-reward      samples,      which      are      seldom      generated      by      pre-trained  \\n \\n  models.</p><p> \\n \\n  Conditional      diffusion      models      (classifier-free      guidance).\\nIn      conditional      generative      models,      the  \\n \\n  general      goal      is      to      sample      from      p(|c),      where   \\n \\nx      is      the      output      and      c      denotes      the      conditioning      variable.\\n \\n \\n  For      example,      in      text-to-language      diffusion      models,      c      is   \\n \\na      text,      and   \\n \\nx      is      the      generated      image.\\nSimilarly,  \\n \\n  in      the      context      of      protein      engineering      for      addressing      inverse      folding      problems,      models      often      define      c       as      the      protein      backbone      structure      and   \\n \\nx      as      the      corresponding      amino      acid      sequence.\\nHere,      using      the  \\n \\n  training      data      {c,      ¢},      the      model      is      trained      by      using      the      loss      function:</p><p>argmin   \\n \\n»      E,uni(l0,7)2)      ~upa      +o?\\nsewN\\\\\\\\\\\\\\'(O)      [lle   \\n \\n—      co(2”,      }),      t)      I\"), t where      the      denoising      function      €g      additionally      receives      the      conditioning      information      c      as      input.\\nIn  \\n \\n  practice,   \\n \\na      variety      of      improvements      such      as      classifier-free      guidance      (Ho      and      Salimans,      2022)      can  \\n \\n  further      improve      the      model’s      ability      to      learn      the      conditional      distribution      p(|c).</p><p> \\n \\n  These      conditional      generative      models      can      be      used      to      optimize      down-stream      rewards      by      condi-  \\n \\n  tioning      on      the      reward      values,      then      sampling   \\n \\nx      conditioned      on      high      reward      values      (Krishnamoorthy  \\n \\n  et      al.,      2023;      Yuan      et      al.,      2023).\\nWhile      this      method      is,      in      principle,      capable      of      generating      plausible x\\n \\n   values      across   \\n \\na      range      of      reward      levels      within      the      training      data      distribution,      it      is      not      the      most  \\n \\n  effective      optimization      strategy.\\nThis      is      primarily      because      high-reward      inputs      frequently      reside  \\n \\n  in      the      tails      of      the      training      distribution      or      even      beyond      it.\\nConsequently,      this      method      may      not  \\n \\n  effectively      generate      high-reward      samples      that      lie      outside      the      training      data      distribution.\\nIn      contrast,  \\n \\n  RL-based      fine-tuning      has      the      capability      to      generate      samples      with      higher      rewards      beyond      the      training  \\n \\n  data.\\nThis      is      achieved      by      explicitly      maximizing      reward      models      learned      from      the      training      data  \\n \\n  and      leveraging      their      extrapolative      capabilities      of      reward      models,      as      theoretically      formalized      and       empirically      observed      in      Uehara      et      al.\\n(2024).</p><p>Reward-weighted      training.\\nAnother      alternative      approach      is      to      use   \\n \\na      reward-weighted      version  \\n \\n  of      the      standard      training      loss      for      diffusion      models.\\nSuppose      that      we      have      data      {x      r(a)}.\\nThen, after      learning   \\n \\na      reward   \\n \\n7   \\n \\n:   \\n \\n¥   \\n \\n—   \\n \\nR      with      regression      from      the      data,      to      achieve      our      goal,      it      looks  \\n \\n  natural      to      use   \\n \\na      reward-weighted      version      of      the      training      loss      for      diffusion      models      (5),      1.e.,.\\na7      (4)      i)   \\n \\n2 areaun   \\n \\n»      E,uni((0,2))      peel?\\n+eo¢,enncoal      (to      ile   \\n \\n—      co(ay?,t)      |\") i\\n \\n    There      are      two      potential      drawbacks      to      this      approach.\\nFirst,      in      practice,      it      may      struggle      to      generate  \\n \\n  samples      with      higher      rewards      beyond      the      training      data.\\nAs      we      will      explain      later,      many      RL      algorithms  \\n \\n  are      more      directly      focused      on      optimizing      reward      functions,      which      are      expected      to      excel      in      obtaining  \\n \\n  samples      with      high      rewards      not      observed      in      the      original      data,      as      empirically      observed      in      Black      et      al.\\n \\n \\n  (2023).\\nSecond,      when      fine-tuning   \\n \\na      conditional      diffusion      model      p(x|c),      the      alternative      approach  \\n \\n  here      requires   \\n \\na      pair      of      {c,      «}      during      fine-tuning      to      ensure      the      validity      of      the      loss      function.\\nWhen  \\n \\n  we      only      have      data      {2      r(x      )}      but      not      {c,      2,      r(a)},      this      implies      that      we      might      need      to      solve an      inverse      problem      from   \\n \\nz      to      c,      which      can      often      be      challenging.\\nIn      contrast,      in      these      scenarios,      RL  \\n \\n  algorithms,      which      we      will      introduce      later,      can      operate      without      needing      such      pairs      {c,      x},      as  \\n \\n  long      as      we      have      learned      reward      functions      7.  \\n \\n  Finally,      it      should      be      noted      that      reward-weighted      training      technically      falls      under      the      broader  \\n \\n  category      of      RL      methods.\\nIt      shares   \\n \\na      close      connection      with      “reward-weighted      MLE”      introduced      in  \\n \\n  Section      6.1,      as      discussed      later.\\nEmploying      this      reward-weighted      MLE      helps      address      the      second  \\n \\n  concern      of      “reward-weighted      training”      mentioned      earlier.</p><p>2\\n \\n   Brief      Overview      of      Entropy-Regularized      MDPs</p><p> \\n \\n  In      this      tutorial,      we      explain      how      fine-tuning      diffusion      models      can      be      naturally      formulated      as      an      RL  \\n \\n  problem      in      entropy-regularized      MDPs.\\nThis      perspective      is      natural      because      RL      involves      sequential  \\n \\n  decision-making,      and   \\n \\na      diffusion      model      is      formulated      as   \\n \\na      sequential      problem      where      each      denoising  \\n \\n  step      is   \\n \\na      decision-making      process.\\nTo      connect      diffusion      models      with      RL,      we      begin      with   \\n \\na      concise  \\n \\n  overview      of      RL      in      standard      entropy-regularized      MDPs      (Haarnoja      et      al.,      2017;      Neu      et      al.,      2017;</p><p> \\n \\n  Geist      et      al.,      2019;      Schulman      et      al.,      2017).</p><li>2.1      MDPs</li><p> \\n \\n  An      MDP      is      defined      as      follows:      {S,A,      {P\"*}/_5,      {r:}-9,      po}      where      S      is      the      state      space,   \\n \\nA      is      the  \\n \\n  action      space,      P;\"*      is   \\n \\na      transition      dynamic      mapping:      S   \\n \\nx   \\n \\nA   \\n \\n—      A(S),      7,   \\n \\n:      S   \\n \\nx   \\n \\nA   \\n \\n>   \\n \\nR      denotes  \\n \\n  reward      received      at      t      and      po      is      an      initial      distribution      over      S.   \\n \\nA      policy      7,   \\n \\n:      S   \\n \\n—      A(A)      is   \\n \\na      map      from  \\n \\n  any      state      s   \\n \\n€      S      to      the      distribution      over      actions.\\nThe      standard      goal      in      RL      is      to      solve</p><h2>Images</h2><p> \\n \\n  g   \\n \\n|   \\n \\na      ge   \\n \\nz       evi      do}   \\n \\nf      g,   \\n \\n2      3°      DNAs      with      high  \\n \\n  aor   \\n \\na   \\n \\n-      e      e       DNAs      io      si   \\n \\ni      og   \\n \\n+   \\n \\na      i:      cell-specificity</p><p>Figure      1:      Illustrative      examples      of      RL-based      fine-tuning,      aimed      at      optimizing      pre-trained      diffusion  \\n \\n  models      to      maximize      downstream      reward      functions.</p><p> \\n \\n  2023),      chemistry      (Jo      et      al.,      2022;      Xu      et      al.,      2022;      Hoogeboom      et      al.,      2022),      and      biology      (Avdeyev  \\n \\n  et      al.,      2023;      Stark      et      al.,      2024;      Campbell      et      al.,      2024).</p><p> \\n \\n  While      diffusion      models      exhibit      significant      power      in      capturing      the      training      data      distribution,  \\n \\n  there’s      often   \\n \\na      need      to      customize      these      models      for      particular      downstream      reward      functions.\\nFor  \\n \\n  instance,      in      computer      vision,      Stable      Diffusion      (Rombach      et      al.,      2022)      serves      as   \\n \\na      strong      backbone  \\n \\n  pre-trained      model.\\nHowever,      we      may      want      to      fine-tune      it      further      by      optimizing      downstream      reward  \\n \\n  functions      such      as      aesthetic      scores      or      human-alignment      scores      (Black      et      al.,      2023;      Fan      et      al.,      2023).\\n \\n \\n  Similarly,      in      fields      such      as      biology      and      chemistry,      various      sophisticated      diffusion      models      have  \\n \\n  been      developed      for      DNA,      RNA,      protein      sequences,      and      molecules,      effectively      modeling      biological  \\n \\n  and      chemical      spaces.\\nNonetheless,      biologists      and      chemists      typically      aim      to      optimize      specific  \\n \\n  downstream      objectives      such      as      cell-specific      expression      in      DNA      sequences      (Gosai      et      al.,      2023;      Lal  \\n \\n  et      al.,      2024;      Sarkar      et      al.,      2024),      translational      efficiency/stability      of      RNA      sequences      (Castillo-Hair  \\n \\n  and      Seelig,      2021;      Agarwal      and      Kelley,      2022),      stability/bioactivity      of      protein      sequence      (Frey      et      al.,  \\n \\n  2023;      Widatalla      et      al.,      2024)      or      QED/SA      scores      of      molecules      (Zhou      et      al.,      2019).</p><p> \\n \\n  To      achieve      this      goal,      numerous      algorithms      have      been      proposed      for      fine-tuning      diffusion      models  \\n \\n  via      reinforcement      learning      (RL)      (e.g.,      Black      et      al.      (2023);      Fan      et      al.\\n(2023);      Clark      et      al.\\n(2023);  \\n \\n  Prabhudesai      et      al.\\n(2023);      Uehara      et      al.\\n(2024)),      aiming      to      optimize      downstream      reward      functions.\\n \\n \\n  RL      is   \\n \\na      machine      learning      paradigm      where      agents      learn      to      make      sequential      decisions      to      maximize  \\n \\n  reward      signals      (Sutton      and      Barto,      2018;      Agarwal      et      al.,      2019).\\nIn      our      context,      RL      naturally      emerges  \\n \\n  as   \\n \\na      Suitable      approach      due      to      the      sequential      structure      inherent      in      diffusion      models,      where      each      time  \\n \\n  step      involves   \\n \\na      “decision”      corresponding      to      how      the      sample      is      denoised      at      that      step.\\nThis      tutorial  \\n \\n  aims      to      review      recent      works      for      readers      interested      in      understanding      the      fundamentals      of      RL-based  \\n \\n  fine-tuning      from   \\n \\na      holistic      perspective,      including      the      advantages      of      RL-based      fine-tuning      over  \\n \\n  non-RL      approaches,      the      pros      and      cons      of      different      RL-based      fine-tuning      algorithms,      the      formalized  \\n \\n  goal      of      RL-based      fine-tuning,      and      its      connections      with      related      topics      such      as      classifier      guidance.</p><p> \\n \\n  The      content      of      this      tutorial      is      primarily      divided      into      three      parts.\\nIn      addition,      as      an      implementation  \\n \\n  example,      we      also      release      the      code      that      employs      RL-based      fine-tuning      for      guided      biological      sequences  \\n \\n  (DNA/RNA)      generation      at      https://github.com/masa-ue/RLfinetuning_Diffusion_Bioseq.</p><li>1.      We      aim      to      provide   \\n \\na      comprehensive      overview      of      current      algorithms.\\nNotably,      given      the  \\n \\n  sequential      nature      of      diffusion      models,      we      can      naturally      frame      fine-tuning      as   \\n \\na      reinforcement  \\n \\n  learning      (RL)      problem      within      Markov      Decision      Processes      (MDPs),      as      detailed      in      Section   \\n \\n3       and      4.\\nTherefore,      we      can      employ      any      off-the-shelf      RL      algorithms      such      as      PPO      (Schulman  \\n \\n  et      al.,      2017),      differentiable      optimization      (direct      reward      backpropagation),      weighted      MLE  \\n \\n  (Peters      et      al.,      2010;      Peng      et      al.,      2019),      value-weighted      sampling      (close      to      classifier      guidance  \\n \\n  in      Dhariwal      and      Nichol      (2021)),      and      path      consistency      learning      (Nachum      et      al.,      2017).\\nWe  \\n \\n  discuss      these      algorithms      in      detail      in      Section      4.2      and      6.\\nInstead      of      merely      outlining      each  \\n \\n  algorithm,      we      aim      to      present      both      their      advantages      and      disadvantages      so      readers      can      select  \\n \\n  the      most      suitable      algorithms      for      their      specific      purposes.</li><li>2.      We      categorize      various      fine-tuning      scenarios      based      on      how      reward      feedback      is      acquired      in  \\n \\n  Section      7.\\nThis      distinction      is      pivotal      for      practical      algorithm      design.\\nFor      example,      if      we      can  \\n \\n  access      accurate      reward      functions,      computational      efficiency      would      become      our      primary      focus.\\n \\n \\n  However,      in      cases      where      reward      functions      are      unknown,      it      is      essential      to      learn      them      from  \\n \\n  data      with      reward      feedback,      leading      us      to      take      feedback      efficiency      and      distributional      shift      into  \\n \\n  consideration      as      well.\\nSpecifically,      when      reward      functions      need      to      be      learned      from      static  \\n \\n  offline      data      without      any      online      interactions,      we      must      address      the      issue      of      overoptimization,  \\n \\n  where      fine-tuned      models      are      misled      by      out-of-distribution      samples,      and      generate      samples  \\n \\n  with      low      genuine      rewards.\\nThis      is      crucial      because,      in      an      offline      scenario,      the      coverage      of  \\n \\n  offline      data      distribution      with      feedback      is      limited;      hence,      the      out-of-distribution      region      could  \\n \\n  be      extensive      (Uehara      et      al.,      2024).</li><li>3.      We      provide   \\n \\na      detailed      discussion      on      the      relationship      between      RL-based      fine-tuning      methods  \\n \\n  and      closely      related      methods      in      the      literature,      such      as      classifier      guidance      (Dhariwal      and      Nichol,  \\n \\n  2021)      in      Section      8,      flow-based      diffusion      models      (Liu      et      al.,      2022;      Lipman      et      al.,      2023;      Tong  \\n \\n  et      al.,      2023)      in      Section      9,      sampling      from      unnormalized      distributions      (Zhang      and      Chen,      2021)  \\n \\n  in      Section      10,      Gflownets      (Bengio      et      al.,      2023)      in      Section      6.3,      and      path      integral      control      theory  \\n \\n  (Theodorou      et      al.,      2010;      Williams      et      al.,      2017;      Kazim      et      al.,      2024)      in      Section      6.2.3.      We  \\n \\n  summarize      the      key      messages      as      follows.</li><p>¢\\n \\n   Section      6.3:      The      losses      used      in      Gflownets      are      fundamentally      equivalent      to      those      derived  \\n \\n  from   \\n \\na      specific      RL      algorithm      called      path      consistency      learning.</p><p> \\n \\n \\n¢      Section      8:      Classifier      guidance      employed      in      conditional      generation      is      regarded      as   \\n \\na       specific      RL-based      fine-tuning      method,      which      we      call      value-weighted      sampling.\\nAs  \\n \\n  formalized      in      Zhao      et      al.\\n(2024),      this      observation      indicates      that      any      off-the-shelf      RL-  \\n \\n  based      fine-tuning      algorithms      (e.g.,      PPO      and      differentiable      optimization)      can      be      applied  \\n \\n  to      conditional      generation.</p><p> \\n \\n \\n¢      Section      10:      Sampling      from      unnormalized      distributions,      often      referred      to      as      Gibbs  \\n \\n  distributions,      is      an      important      and      challenging      problem      in      diverse      domains.\\nWhile  \\n \\n  MCMC      methods      are      traditionally      used      for      this      task,      recognizing      its      similarity      to      the  \\n \\n  objectives      of      RL-based      fine-tuning      suggests      that      off-the-shelf      RL      algorithms      can      also  \\n \\n  effectively      address      the      challenge      of      sampling      from      unnormalized      distributions.</p><h2>Preliminaries</h2><li>1.1 DiffusionModels 2 2.20.\\n00 2 ee ee</li><table><th><td colSpan=1>1.1.1 Score-Based Diffusion Models (Optional)</td><td colSpan=1></td><td colSpan=1></td></th><tr><td colSpan=1>1.2 Fine-Tuning Diffusion Models withRL</td><td colSpan=1>0.00.0.</td><td colSpan=1></td></tr><tr><td colSpan=1>1.2.1. Brief Overview: Fine-tuning withRL 1.2.2 Motivation for Using RL over Non-RL Alternatives.</td><td colSpan=1></td><td colSpan=1>2.</td></tr><tr><td>Brief      Overview      of      Entropy-Regularized      MDPs</td></tr><tr><td>Fine-Tuning      Diffusion      Models      with      RL      in      Entropy      Regularized      MDPs</td></tr><tr><td>Theory      of      RL-Based      Fine-Tuning</td></tr></table><h2>RL-Based      Fine-Tuning      Algorithms      1:      Non-Distribution-Constrained      Approaches</h2><h2>RL-Based      Fine-Tuning      Algorithms      2:      Distribution-Constrained      Approaches</h2><h2>Fine-Tuning      Settings      Taxonomy</h2><h2>Connection      with      Classifier      Guidance</h2><p> \\n \\n  Connection      with      Flow-Based      Diffusion      Models  \\n \\n  10  \\n \\n  10  \\n \\n  11 12  \\n \\n  14  \\n \\n  14  \\n \\n  15  \\n \\n  16  \\n \\n  16  \\n \\n  17  \\n \\n  18  \\n \\n  19  \\n \\n  21  \\n \\n  21  \\n \\n  23  \\n \\n  24  \\n \\n  25  \\n \\n  25  \\n \\n  28  \\n \\n  28  \\n \\n  28</p><p>10      Connection      with      Sampling      from      Unnormalized      Distributions      35 10.1 Markov Chain Monte Carlo(MCMC) 2.00000 eee ene 35 10.2 RL-Based Approaches 0 2.0.0 pee ee 36</p><p>11      Closely      Related      Directions      36</p><p>12      Summary      37</p><p>1\\n \\n   Preliminaries</p><p>In      this      section,      we      outline      the      fundamentals      of      diffusion      models      and      elucidate      the      objective      of  \\n \\n  fine-tuning      them.</p><h3>1.1.      Diffusion      Models</h3><p>dels  \\n \\n  We      present      an      overview      of      denoising      diffusion      probabilistic      models      (DDPM)      (Ho      et      al.,      2020).\\n \\n \\n  For      more      details,      refer      to      Yang      et      al.\\n(2023);      Cao      et      al.\\n(2024);      Chen      et      al.\\n(2024);      Tang      and      Zhao  \\n \\n  (2024).</p><p> \\n \\n  In      diffusion      models,      the      objective      is      to      develop   \\n \\na      deep      generative      model      that      accurately      captures  \\n \\n  the      true      data      distribution.\\nSpecifically,      denoting      the      data      distribution      by      pr.\\n \\n \\n€      A(4’)      where   \\n \\n¥      is      an  \\n \\n  input      space,   \\n \\na      DDPM      aims      to      approximate      p,,.\\nusing   \\n \\na      parametric      model      structured      as 1 p(xo3      8)   \\n \\n=   \\n \\n[      rlco      O)dx1.r,      where      p(%o:7;      9)   \\n \\n=      pr+i(@7;      9)      [[      peter:      0).</p><p>t=T When   \\n \\n4      is      an      Euclidean      space      (in      R®),      the      forward      process      is      modeled      as      the      following      dynamics:</p><p>pra(er)      =N(0,1),      pe(ve-r)2;      9)   \\n \\n=      N(p(a1,      t;      8),      07      (4)   \\n \\nx      D),  \\n \\n  where      N(-,-)      denotes   \\n \\na      normal      distribution,   \\n \\nJ      is      an      identity      matrix      and   \\n \\np   \\n \\n:      R¢   \\n \\nx      [0,7]   \\n \\n>      R¢.\\nIn  \\n \\n  DDPMs,      we      aim      to      obtain   \\n \\na      set      of      policies      (i.e.,      denoising      process)      {p;}{_741,      pr:   \\n \\n&   \\n \\n—      A(A&)      such  \\n \\n  that      p(x;      0)   \\n \\n©      Ppre(#o).\\nIndeed,      by      optimizing      the      variational      bound      on      the      negative      log-likelihood,  \\n \\n  we      can      derive      such   \\n \\na      set      of      policies.\\nFor      more      details,      refer      to      Section      1.1.1.  \\n \\n  Hereafter,      we      consider   \\n \\na      situation      where      we      have   \\n \\na      pre-trained      diffusion      model      that      is      already  \\n \\n  trained      on   \\n \\na      large      dataset,      such      that      the      model      can      accurately      capture      the      underlying      data      distribution.</p><p> \\n \\n  pre We      refer      to      the      pre-trained      policies      as      {p?\\n\"*(-|-)}{_7,,,      and      to      the      marginal      distribution      at      t   \\n \\n=   \\n \\n0       induced      by      the      pre-trained      diffusion      model      as      pp.\\nIn      other      words, 1\\n \\n    pr      (x0)   \\n \\n=      [iler)      [[°@eledderr.</p><p>t=T  \\n \\n  Remark   \\n \\n1      (Non-Euclidean      space).\\nFor      simplicity,      we      typically      assume      that      the      domain      space      is  \\n \\n  Euclidean.\\nHowever,      we      can      easily      extend      most      of      the      discussion      to   \\n \\na      more      general      space,      such      as  \\n \\n  a      Riemannian      manifold      (De      Bortoli      et      al.,      2022)      or      discrete      space      (Austin      et      al.,      2021;      Campbell  \\n \\n  et      al.,      2022;      Benton      et      al.,      2024;      Lou      et      al.,      2023).</p><p> \\n \\n  Remark   \\n \\n2      (Conditional      generative      models).\\nPre-trained      models      can      be      conditional      diffusion      models,  \\n \\n  such      as      text-to-image      diffusion      models      (Ramesh      et      al.,      2022).\\nThe      extension      is      straightforward:  \\n \\n  augmenting      the      input      spaces      of      policies      with      an      additional      space      on      which      we      want      to      condition.\\n \\n \\n  More      specifically,      by      denoting      that      space      by      c   \\n \\n€      C,      each      policy      becomes      p;(x¢|%1,      650)   \\n \\n:   \\n \\n&   \\n \\nx      C   \\n \\n>       A(X).</p><p> \\n \\n  Remark   \\n \\n3      (Extension      to      Continuous-time      diffusion      models).\\nJn      this      tutorial,      our      discussion      on  \\n \\n  fine-tuning      diffusion      models      will      be      primarily      formulated      on      the      discrete-time      formulation,      as      we      did  \\n \\n  above      Nonetheless,      much      of      our      discussion      is      also      applicable      to      continuous-time      diffusion      models,  \\n \\n  as      formalized      in      Uehara      et      al.\\n(2024)</p><h4>1.1.1      Score-Based      Diffusion      Models      (Optional)</h4><p>nal)  \\n \\n  We      briefly      discuss      how      to      train      diffusion      models      in   \\n \\na      continuous-time      framework      (Song      et      al.,  \\n \\n  2021).\\nIn      our      tutorial,      this      section      is      mainly      used      to      discuss      several      algorithms      (e.g.,      value-weighted  \\n \\n  sampling      in      Section      6.2)      and      their      relationship      with      flow-based      diffusion      models      (Section      9)      later.</p><p> \\n \\n  Therefore,      readers      may      skip      it      based      on      their      individual      needs.</p><p> \\n \\n  The      training      process      of      diffusion      models      can      be      summarized      as      follows.\\nOur      objective      is      to      train  \\n \\n  a      sequential      mapping      from   \\n \\na      known      noise      distribution      to   \\n \\na      data      distribution,      formalized      through  \\n \\n  a      stochastic      differential      equation      (SDE).\\nFirstly,      we      define   \\n \\na      forward      (fixed)      SDE      that      maps      from  \\n \\n  the      data      distribution      to      the      noise      distribution.\\nThen,   \\n \\na      time-reversal      SDE      is      expressed      as      an      SDE,  \\n \\n  which      includes      the      (unknown)      score      function.\\nNow,      by      learning      this      unknown      score      function      from  \\n \\n  the      training      data,      the      time-reversal      SDE      can      be      utilized      as   \\n \\na      generative      model.\\nHere      are      the      details.</p><p>Forward      and      time-reversal      SDE.\\nFirstly,      we      introduce   \\n \\na      forward      (a.k.a.,      reference)      Stochastic  \\n \\n  differential      equations      (SDE)      from   \\n \\n0      to      7’.\\n \\n \\nA      common      choice      is   \\n \\na      variance-preserving      (VP)      process:</p><p>t   \\n \\n€      [0,7];      dx,   \\n \\n=      —0.52,dt   \\n \\n+      du;,,      x9   \\n \\n~      p(x),      (1)</p><p>where      dw;      represents      standard      Brownian      motion.\\nHere      are      two      crucial      observations:</p><p>¢\\n \\n   As      T      approaches      oo,      the      limiting      distribution      is      \\\\\\\\\\\\\\\\V(0,      I).</p><p>¢\\n \\n   The      time-reversal      SDE      (Anderson,      1982),      which      preserves      the      marginal      distribution,      is  \\n \\n  expressed      as      follows:</p><p>t   \\n \\n€      [0,7];      dz,   \\n \\n=      [0.5z,   \\n \\n+   \\n \\nV      log      qr_s(%)|dt   \\n \\n+      du,.\\n(2) Here,      g,   \\n \\n€      A(R“)      denotes      the      marginal      distribution      at      time      t      induced      by      the      reference      SDE (1).\\nNotably,      the      marginal      distribution      of      zr_;      is      the      same      as      the      one      of      x;      induced      by      the  \\n \\n  reference      SDE.</p><p> \\n \\n  These      observations      suggest      that      with      sufficiently      large      T,      starting      from   \\n \\nV      (0,      I)      and      following      the  \\n \\n  time-reversal      SDE      (2)   \\n \\n,      we      are      able      to      sample      from      the      data      distribution      (i.e.,      p?\\n\"°)      at      the      terminal  \\n \\n  time      point      7’.</p><p>Training      score      functions      from      the      training      data.\\nNow,      the      remaining      question      is      how      to      estimate  \\n \\n  the      marginal      score      function   \\n \\nV      log      q(-)      in      (2).\\nThis      can      be      computed      using      the      following      principles:</p><p>°\\n \\n   We      have   \\n \\nV      log      (a+)   \\n \\n=      Exgxppre[Va,      log      qyo(ae   \\n \\n|      2o)],      where      q@jo      represents      the      conditional  \\n \\n  distribution      of      x;      given      Xo      induced      by      the      reference      SDE.</p><p>¢\\n \\n   The      conditional      score      function      V,,,      log      dejo      (Xe   \\n \\n|      xo)      can      be      analytically      derived      as  \\n \\n  °</p><h5>      Myo      —      Ut</h5><p> \\n \\n  {ory      ue   \\n \\n=      exp(—0.5t)xo,      {o?\\n}?\\n \\n \\n=   \\n \\n1   \\n \\n—      exp(—0.5t).\\n(3) t Subsequently,      by      defining   \\n \\na      parameterized      model      sg   \\n \\n:      R¢   \\n \\nx      [0,7]   \\n \\n>      R¢      and      utilizing      the      following weighted      regression,      the      marginal      score      is      estimated:</p><p>A\\n \\n \\n  Dore   \\n \\n=      argmin      Eywuni((0,7))      ce~aejo(w+|ar0),c0~pPr®      [A(t)      ||      Ver,      log      jo      (Le   \\n \\n|      ro)   \\n \\n_      So(£t,      t)      17)      (4)</p><p> \\n \\n  where   \\n \\n2   \\n \\n:      [0,7]   \\n \\n+   \\n \\nR      is   \\n \\na      weighted      function.\\n \\n \\n  Note      we      typically      use      another      parametrization      well.\\nFrom      (3),      we      can      easily      see      that      this      score      is  \\n \\n  estimated      by      introducing      networks      €9   \\n \\n:      R@   \\n \\nx      [0,7]   \\n \\n—      R®,      which      aims      to      estimate      the      noise      Bere      eO</p><p>Ore   \\n \\n=      argmin      Ey.\\nuni({0,7]),cr~p2xo+o?\\ne~N      (0,1)      ,co~pPte      [A(t)/{op}?\\nlle   \\n \\n—      €9      (1,      t)      7)   \\n \\n:      (5)</p><p>Then,      by      denoting      the      training      data      as      {a}      and      setting      A(t)   \\n \\n=      {a7      }”,      the      actual      loss      function      from the      training      data      is argmin   \\n \\n)      Ey      ai(o.r))a   \\n \\nO      peal      +e02,e~N      (0,1)      [Ile   \\n \\n—      €o(ae?,      t)      [17].\\n(6) i=1 Inference      time.\\nOnce      this      Bore      (or      Ore)      is      learned,      we      insert      it      into      the      time-reversal      SDE      (2)      and use      it      as   \\n \\na      generative      model.\\nUltimately,      with      standard      discretization,      we      obtain:</p><p>p(      xe,      t;      a)   \\n \\n=      24+      (0.54,   \\n \\n+      Sire      (,,T      —t)|(ot),      {o?(t)}?\\n \\n \\n=      (6t).</p><p>Equivalently,      this      is (21,      t;      Ore)   \\n \\n=   \\n \\n2   \\n \\n+      [0.52   \\n \\n—      1/o?\\n \\n \\nx      e%,,,(a1,T   \\n \\n—      t)]|      (6t),      (7)</p><p>where      (dt)      denotes      the      discretization      step.</p><p> \\n \\n  Equivalence      to      DDPM.\\nThe      objective      function      derived      here      is      equivalent      to      the      one      formulated  \\n \\n  based      on      variational      inference      in      the      discretized      formulation,      which      is      commonly      referred      to      as  \\n \\n  DDPMs      (Ho      et      al.,      2020).\\nIn      DDPMs      (Ho      et      al.,      2020),      we      often      see      the      following      form:</p><p> \\n \\n \\n1      1-      (ey;</p><p> \\n \\n  it;      0)   \\n \\n=      ~~   \\n \\n’      ),  \\n \\n  p(xy   \\n \\n)      Xt      Jina      ae      [om</p><h6>Vor</h6><p>When      a;   \\n \\n=   \\n \\n1   \\n \\n—      (dt),      the      above      is      equivalent      to      (7)      when      dt      goes      to   \\n \\n0      by      noting      1/,/a;   \\n \\n©      1+0.5(d¢).</p><h3>1.2      Fine-Tuning      Diffusion      Models      with      RL</h3><p>RL  \\n \\n  Importantly,      our      focus      on      RL-based      fine-tuning      distinguishes      itself      from      the      standard      fine-tuning  \\n \\n  methods.\\nStandard      fine-tuning      typically      involves      scenarios      where      we      have      pre-trained      models  \\n \\n  (e.g.,      diffusion      models)      and      new      training      data      {2,      y}.\\nIn      such      cases,      the      common      approach  \\n \\n  for      fine-tuning      is      to      retrain      diffusion      models      with      the      new      training      data      using      the      same      loss  \\n \\n  function      employed      during      pre-training.\\nIn      sharp      contrast,      RL-based      fine-tuning      directly      employs  \\n \\n  the      downstream      reward      functions      as      the      primary      optimization      objectives,      making      the      loss      functions  \\n \\n  different      from      those      used      in      pre-training.</p><p> \\n \\n  Hereafter,      we      start      with   \\n \\na      concise      overview      of      RL-based      fine-tuning.\\nThen,      before      delving      into  \\n \\n  specifics,      we      discuss      simpler      non-RL      alternatives      to      provide      motivation      for      adopting      RL-based  \\n \\n  fine-tuning.</p><li>1.2.1      Brief      Overview:      Fine-tuning      with      RL</li><p>In      this      article,      we      explore      the      fine-tuning      of      pre-trained      diffusion      models      to      optimize      downstream  \\n \\n  reward      functions   \\n \\nr   \\n \\n:      R¢   \\n \\n+      R.      In      domains      such      as      images,      these      backbone      diffusion      models      to      be  \\n \\n  fine-tuned      include      Stable      Diffusion      (Rombach      et      al.,      2022),      while      the      reward      functions      are      aesthetic  \\n \\n  scores      and      alignment      scores      (Clark      et      al.,      2023;      Black      et      al.,      2023;      Fan      et      al.,      2023).\\nMore      examples  \\n \\n  are      detailed      in      the      introduction.\\nThese      rewards      are      often      unknown,      necessitating      learning      from  \\n \\n  data      with      feedback:      {2      r(x™)}.\\nWe      will      explore      this      aspect      further      in      Section      7.\\nUntil      then,      we  \\n \\n  assume   \\n \\nr      is      known.\\n \\n \\n  Now,      readers      may      wonder      about      the      objectives      we      aim      to      achieve      during      the      fine-tuning      process.\\n \\n \\n  A      natural      approach      is      to      define      the      optimization      problem:</p><p>argmax      E,.\\n[r()|      (8)  \\n \\n  qeA(¥)  \\n \\n  where   \\n \\nq      is      initialized      with   \\n \\na      pre-trained      diffusion      model      p?\\n*®   \\n \\n€      A(2).\\nIn      this      tutorial,      we      will      detail  \\n \\n  the      procedure      of      solving      (8)      with      RL      in      the      upcoming      sections.\\nIn      essence,      we      leverage      the      fact  \\n \\n  that      diffusion      models      are      formulated      as   \\n \\na      sequential      decision-making      problem,      where      each      decision  \\n \\n  corresponds      to      how      samples      are      denoised.</p><p> \\n \\n  Although      the      above      objective      function      (8)      is      reasonable,      the      resulting      distribution      might      deviate  \\n \\n  too      much      from      the      pre-trained      diffusion      model.\\nTo      circumvent      this      issue,   \\n \\na      natural      way      is      to      add       penalization      against      pre-trained      diffusion      models.\\nThen,      the      target      distribution      is      defined      as:</p><p>argmax      E,.4[r(x)]   \\n \\n—      aKL(q||p’*).\\n(9)  \\n \\n  qeA(¥) Notably,      (9)      reduces      to      the      following      distribution:</p><li>(10)</li><p>5\\n \\n   exp(r()/a)p\"\"()  \\n \\n  Pr)   \\n \\n=      Fra)      apa)</p><p> \\n \\n  Here,      the      first      term      in      (9)      corresponds      to      the      mean      reward,      which      we      want      to      optimize      in      the  \\n \\n  fine-tuning      process.\\nThe      second      term      in      (10)      serves      as   \\n \\na      penalty      term,      indicating      the      deviation      of   \\n \\nq       from      the      pre-trained      model.\\nThe      parameter   \\n \\na      controls      the      strength      of      this      regularization      term.\\nThe  \\n \\n  proper      choice      of   \\n \\na      depends      on      the      task      we      are      interested      in.</p><h6>1.2.2      Motivation      for      Using      RL      over      Non-RL      Alternatives</h6><p>ives  \\n \\n  To      achieve      our      goal      of      maximizing      downstream      reward      functions      with      diffusion      models,      readers  \\n \\n  may      question      whether      alternative      approaches      can      be      employed      apart      from      RL.\\nHere,      we      investigate  \\n \\n  these      potential      alternatives      and      explain      why      RL      approaches      may      offer      advantages      over      them.</p><p> \\n \\n  Rejection      sampling.\\nOne      approach      involves      generating      multiple      samples      from      pre-trained      diffu-  \\n \\n  sion      models      and      selecting      only      those      with      high      rewards.\\nThis      method,      called      rejection      sampling,  \\n \\n  operates      without      needing      fine-tuning.\\nHowever,      rejection      sampling      is      effective      primarily      when      the  \\n \\n  pre-trained      model      already      has   \\n \\na      high      probability      of      producing      high-reward      samples.\\nIt      resembles  \\n \\n  sampling      from   \\n \\na      prior      distribution      to      obtain      posterior      samples      (in      this      case,      high-reward      points).\\n \\n \\n  This      approach      works      efficiently      when      the      posterior      closely      matches      the      prior      but      can      become      highly  \\n \\n  inefficient      otherwise.\\nIn      contrast,      by      explicitly      updating      weight      in      diffusion      models,      RL-based      fine-  \\n \\n  tuning      allows      us      to      obtain      these      high-reward      samples,      which      are      seldom      generated      by      pre-trained  \\n \\n  models.</p><p> \\n \\n  Conditional      diffusion      models      (classifier-free      guidance).\\nIn      conditional      generative      models,      the  \\n \\n  general      goal      is      to      sample      from      p(|c),      where   \\n \\nx      is      the      output      and      c      denotes      the      conditioning      variable.\\n \\n \\n  For      example,      in      text-to-language      diffusion      models,      c      is   \\n \\na      text,      and   \\n \\nx      is      the      generated      image.\\nSimilarly,  \\n \\n  in      the      context      of      protein      engineering      for      addressing      inverse      folding      problems,      models      often      define      c       as      the      protein      backbone      structure      and   \\n \\nx      as      the      corresponding      amino      acid      sequence.\\nHere,      using      the  \\n \\n  training      data      {c,      ¢},      the      model      is      trained      by      using      the      loss      function:</p><p>argmin   \\n \\n»      E,uni(l0,7)2)      ~upa      +o?\\nsewN\\\\\\\\\\\\\\'(O)      [lle   \\n \\n—      co(2”,      }),      t)      I\"), t where      the      denoising      function      €g      additionally      receives      the      conditioning      information      c      as      input.\\nIn  \\n \\n  practice,   \\n \\na      variety      of      improvements      such      as      classifier-free      guidance      (Ho      and      Salimans,      2022)      can  \\n \\n  further      improve      the      model’s      ability      to      learn      the      conditional      distribution      p(|c).</p><p> \\n \\n  These      conditional      generative      models      can      be      used      to      optimize      down-stream      rewards      by      condi-  \\n \\n  tioning      on      the      reward      values,      then      sampling   \\n \\nx      conditioned      on      high      reward      values      (Krishnamoorthy  \\n \\n  et      al.,      2023;      Yuan      et      al.,      2023).\\nWhile      this      method      is,      in      principle,      capable      of      generating      plausible x\\n \\n   values      across   \\n \\na      range      of      reward      levels      within      the      training      data      distribution,      it      is      not      the      most  \\n \\n  effective      optimization      strategy.\\nThis      is      primarily      because      high-reward      inputs      frequently      reside  \\n \\n  in      the      tails      of      the      training      distribution      or      even      beyond      it.\\nConsequently,      this      method      may      not  \\n \\n  effectively      generate      high-reward      samples      that      lie      outside      the      training      data      distribution.\\nIn      contrast,  \\n \\n  RL-based      fine-tuning      has      the      capability      to      generate      samples      with      higher      rewards      beyond      the      training  \\n \\n  data.\\nThis      is      achieved      by      explicitly      maximizing      reward      models      learned      from      the      training      data  \\n \\n  and      leveraging      their      extrapolative      capabilities      of      reward      models,      as      theoretically      formalized      and       empirically      observed      in      Uehara      et      al.\\n(2024).</p><p>Reward-weighted      training.\\nAnother      alternative      approach      is      to      use   \\n \\na      reward-weighted      version  \\n \\n  of      the      standard      training      loss      for      diffusion      models.\\nSuppose      that      we      have      data      {x      r(a)}.\\nThen, after      learning   \\n \\na      reward   \\n \\n7   \\n \\n:   \\n \\n¥   \\n \\n—   \\n \\nR      with      regression      from      the      data,      to      achieve      our      goal,      it      looks  \\n \\n  natural      to      use   \\n \\na      reward-weighted      version      of      the      training      loss      for      diffusion      models      (5),      1.e.,.\\na7      (4)      i)   \\n \\n2 areaun   \\n \\n»      E,uni((0,2))      peel?\\n+eo¢,enncoal      (to      ile   \\n \\n—      co(ay?,t)      |\") i\\n \\n    There      are      two      potential      drawbacks      to      this      approach.\\nFirst,      in      practice,      it      may      struggle      to      generate  \\n \\n  samples      with      higher      rewards      beyond      the      training      data.\\nAs      we      will      explain      later,      many      RL      algorithms  \\n \\n  are      more      directly      focused      on      optimizing      reward      functions,      which      are      expected      to      excel      in      obtaining  \\n \\n  samples      with      high      rewards      not      observed      in      the      original      data,      as      empirically      observed      in      Black      et      al.\\n \\n \\n  (2023).\\nSecond,      when      fine-tuning   \\n \\na      conditional      diffusion      model      p(x|c),      the      alternative      approach  \\n \\n  here      requires   \\n \\na      pair      of      {c,      «}      during      fine-tuning      to      ensure      the      validity      of      the      loss      function.\\nWhen  \\n \\n  we      only      have      data      {2      r(x      )}      but      not      {c,      2,      r(a)},      this      implies      that      we      might      need      to      solve an      inverse      problem      from   \\n \\nz      to      c,      which      can      often      be      challenging.\\nIn      contrast,      in      these      scenarios,      RL  \\n \\n  algorithms,      which      we      will      introduce      later,      can      operate      without      needing      such      pairs      {c,      x},      as  \\n \\n  long      as      we      have      learned      reward      functions      7.  \\n \\n  Finally,      it      should      be      noted      that      reward-weighted      training      technically      falls      under      the      broader  \\n \\n  category      of      RL      methods.\\nIt      shares   \\n \\na      close      connection      with      “reward-weighted      MLE”      introduced      in  \\n \\n  Section      6.1,      as      discussed      later.\\nEmploying      this      reward-weighted      MLE      helps      address      the      second  \\n \\n  concern      of      “reward-weighted      training”      mentioned      earlier.</p><p>2\\n \\n   Brief      Overview      of      Entropy-Regularized      MDPs</p><p> \\n \\n  In      this      tutorial,      we      explain      how      fine-tuning      diffusion      models      can      be      naturally      formulated      as      an      RL  \\n \\n  problem      in      entropy-regularized      MDPs.\\nThis      perspective      is      natural      because      RL      involves      sequential  \\n \\n  decision-making,      and   \\n \\na      diffusion      model      is      formulated      as   \\n \\na      sequential      problem      where      each      denoising  \\n \\n  step      is   \\n \\na      decision-making      process.\\nTo      connect      diffusion      models      with      RL,      we      begin      with   \\n \\na      concise  \\n \\n  overview      of      RL      in      standard      entropy-regularized      MDPs      (Haarnoja      et      al.,      2017;      Neu      et      al.,      2017;</p><p> \\n \\n  Geist      et      al.,      2019;      Schulman      et      al.,      2017).</p><li>2.1      MDPs</li><p> \\n \\n  An      MDP      is      defined      as      follows:      {S,A,      {P\"*}/_5,      {r:}-9,      po}      where      S      is      the      state      space,   \\n \\nA      is      the  \\n \\n  action      space,      P;\"*      is   \\n \\na      transition      dynamic      mapping:      S   \\n \\nx   \\n \\nA   \\n \\n—      A(S),      7,   \\n \\n:      S   \\n \\nx   \\n \\nA   \\n \\n>   \\n \\nR      denotes  \\n \\n  reward      received      at      t      and      po      is      an      initial      distribution      over      S.   \\n \\nA      policy      7,   \\n \\n:      S   \\n \\n—      A(A)      is   \\n \\na      map      from  \\n \\n  any      state      s   \\n \\n€      S      to      the      distribution      over      actions.\\nThe      standard      goal      in      RL      is      to      solve</p><h3>1.1.      Diffusion      Models</h3><p>dels  \\n \\n  We      present      an      overview      of      denoising      diffusion      probabilistic      models      (DDPM)      (Ho      et      al.,      2020).\\n \\n \\n  For      more      details,      refer      to      Yang      et      al.\\n(2023);      Cao      et      al.\\n(2024);      Chen      et      al.\\n(2024);      Tang      and      Zhao  \\n \\n  (2024).</p><p> \\n \\n  In      diffusion      models,      the      objective      is      to      develop   \\n \\na      deep      generative      model      that      accurately      captures  \\n \\n  the      true      data      distribution.\\nSpecifically,      denoting      the      data      distribution      by      pr.\\n \\n \\n€      A(4’)      where   \\n \\n¥      is      an  \\n \\n  input      space,   \\n \\na      DDPM      aims      to      approximate      p,,.\\nusing   \\n \\na      parametric      model      structured      as 1 p(xo3      8)   \\n \\n=   \\n \\n[      rlco      O)dx1.r,      where      p(%o:7;      9)   \\n \\n=      pr+i(@7;      9)      [[      peter:      0).</p><p>t=T When   \\n \\n4      is      an      Euclidean      space      (in      R®),      the      forward      process      is      modeled      as      the      following      dynamics:</p><p>pra(er)      =N(0,1),      pe(ve-r)2;      9)   \\n \\n=      N(p(a1,      t;      8),      07      (4)   \\n \\nx      D),  \\n \\n  where      N(-,-)      denotes   \\n \\na      normal      distribution,   \\n \\nJ      is      an      identity      matrix      and   \\n \\np   \\n \\n:      R¢   \\n \\nx      [0,7]   \\n \\n>      R¢.\\nIn  \\n \\n  DDPMs,      we      aim      to      obtain   \\n \\na      set      of      policies      (i.e.,      denoising      process)      {p;}{_741,      pr:   \\n \\n&   \\n \\n—      A(A&)      such  \\n \\n  that      p(x;      0)   \\n \\n©      Ppre(#o).\\nIndeed,      by      optimizing      the      variational      bound      on      the      negative      log-likelihood,  \\n \\n  we      can      derive      such   \\n \\na      set      of      policies.\\nFor      more      details,      refer      to      Section      1.1.1.  \\n \\n  Hereafter,      we      consider   \\n \\na      situation      where      we      have   \\n \\na      pre-trained      diffusion      model      that      is      already  \\n \\n  trained      on   \\n \\na      large      dataset,      such      that      the      model      can      accurately      capture      the      underlying      data      distribution.</p><p> \\n \\n  pre We      refer      to      the      pre-trained      policies      as      {p?\\n\"*(-|-)}{_7,,,      and      to      the      marginal      distribution      at      t   \\n \\n=   \\n \\n0       induced      by      the      pre-trained      diffusion      model      as      pp.\\nIn      other      words, 1\\n \\n    pr      (x0)   \\n \\n=      [iler)      [[°@eledderr.</p><p>t=T  \\n \\n  Remark   \\n \\n1      (Non-Euclidean      space).\\nFor      simplicity,      we      typically      assume      that      the      domain      space      is  \\n \\n  Euclidean.\\nHowever,      we      can      easily      extend      most      of      the      discussion      to   \\n \\na      more      general      space,      such      as  \\n \\n  a      Riemannian      manifold      (De      Bortoli      et      al.,      2022)      or      discrete      space      (Austin      et      al.,      2021;      Campbell  \\n \\n  et      al.,      2022;      Benton      et      al.,      2024;      Lou      et      al.,      2023).</p><p> \\n \\n  Remark   \\n \\n2      (Conditional      generative      models).\\nPre-trained      models      can      be      conditional      diffusion      models,  \\n \\n  such      as      text-to-image      diffusion      models      (Ramesh      et      al.,      2022).\\nThe      extension      is      straightforward:  \\n \\n  augmenting      the      input      spaces      of      policies      with      an      additional      space      on      which      we      want      to      condition.\\n \\n \\n  More      specifically,      by      denoting      that      space      by      c   \\n \\n€      C,      each      policy      becomes      p;(x¢|%1,      650)   \\n \\n:   \\n \\n&   \\n \\nx      C   \\n \\n>       A(X).</p><p> \\n \\n  Remark   \\n \\n3      (Extension      to      Continuous-time      diffusion      models).\\nJn      this      tutorial,      our      discussion      on  \\n \\n  fine-tuning      diffusion      models      will      be      primarily      formulated      on      the      discrete-time      formulation,      as      we      did  \\n \\n  above      Nonetheless,      much      of      our      discussion      is      also      applicable      to      continuous-time      diffusion      models,  \\n \\n  as      formalized      in      Uehara      et      al.\\n(2024)</p><h4>1.1.1      Score-Based      Diffusion      Models      (Optional)</h4><p>nal)  \\n \\n  We      briefly      discuss      how      to      train      diffusion      models      in   \\n \\na      continuous-time      framework      (Song      et      al.,  \\n \\n  2021).\\nIn      our      tutorial,      this      section      is      mainly      used      to      discuss      several      algorithms      (e.g.,      value-weighted  \\n \\n  sampling      in      Section      6.2)      and      their      relationship      with      flow-based      diffusion      models      (Section      9)      later.</p><p> \\n \\n  Therefore,      readers      may      skip      it      based      on      their      individual      needs.</p><p> \\n \\n  The      training      process      of      diffusion      models      can      be      summarized      as      follows.\\nOur      objective      is      to      train  \\n \\n  a      sequential      mapping      from   \\n \\na      known      noise      distribution      to   \\n \\na      data      distribution,      formalized      through  \\n \\n  a      stochastic      differential      equation      (SDE).\\nFirstly,      we      define   \\n \\na      forward      (fixed)      SDE      that      maps      from  \\n \\n  the      data      distribution      to      the      noise      distribution.\\nThen,   \\n \\na      time-reversal      SDE      is      expressed      as      an      SDE,  \\n \\n  which      includes      the      (unknown)      score      function.\\nNow,      by      learning      this      unknown      score      function      from  \\n \\n  the      training      data,      the      time-reversal      SDE      can      be      utilized      as   \\n \\na      generative      model.\\nHere      are      the      details.</p><p>Forward      and      time-reversal      SDE.\\nFirstly,      we      introduce   \\n \\na      forward      (a.k.a.,      reference)      Stochastic  \\n \\n  differential      equations      (SDE)      from   \\n \\n0      to      7’.\\n \\n \\nA      common      choice      is   \\n \\na      variance-preserving      (VP)      process:</p><p>t   \\n \\n€      [0,7];      dx,   \\n \\n=      —0.52,dt   \\n \\n+      du;,,      x9   \\n \\n~      p(x),      (1)</p><p>where      dw;      represents      standard      Brownian      motion.\\nHere      are      two      crucial      observations:</p><p>¢\\n \\n   As      T      approaches      oo,      the      limiting      distribution      is      \\\\\\\\\\\\\\\\V(0,      I).</p><p>¢\\n \\n   The      time-reversal      SDE      (Anderson,      1982),      which      preserves      the      marginal      distribution,      is  \\n \\n  expressed      as      follows:</p><p>t   \\n \\n€      [0,7];      dz,   \\n \\n=      [0.5z,   \\n \\n+   \\n \\nV      log      qr_s(%)|dt   \\n \\n+      du,.\\n(2) Here,      g,   \\n \\n€      A(R“)      denotes      the      marginal      distribution      at      time      t      induced      by      the      reference      SDE (1).\\nNotably,      the      marginal      distribution      of      zr_;      is      the      same      as      the      one      of      x;      induced      by      the  \\n \\n  reference      SDE.</p><p> \\n \\n  These      observations      suggest      that      with      sufficiently      large      T,      starting      from   \\n \\nV      (0,      I)      and      following      the  \\n \\n  time-reversal      SDE      (2)   \\n \\n,      we      are      able      to      sample      from      the      data      distribution      (i.e.,      p?\\n\"°)      at      the      terminal  \\n \\n  time      point      7’.</p><p>Training      score      functions      from      the      training      data.\\nNow,      the      remaining      question      is      how      to      estimate  \\n \\n  the      marginal      score      function   \\n \\nV      log      q(-)      in      (2).\\nThis      can      be      computed      using      the      following      principles:</p><p>°\\n \\n   We      have   \\n \\nV      log      (a+)   \\n \\n=      Exgxppre[Va,      log      qyo(ae   \\n \\n|      2o)],      where      q@jo      represents      the      conditional  \\n \\n  distribution      of      x;      given      Xo      induced      by      the      reference      SDE.</p><p>¢\\n \\n   The      conditional      score      function      V,,,      log      dejo      (Xe   \\n \\n|      xo)      can      be      analytically      derived      as  \\n \\n  °</p><h5>      Myo      —      Ut</h5><p> \\n \\n  {ory      ue   \\n \\n=      exp(—0.5t)xo,      {o?\\n}?\\n \\n \\n=   \\n \\n1   \\n \\n—      exp(—0.5t).\\n(3) t Subsequently,      by      defining   \\n \\na      parameterized      model      sg   \\n \\n:      R¢   \\n \\nx      [0,7]   \\n \\n>      R¢      and      utilizing      the      following weighted      regression,      the      marginal      score      is      estimated:</p><p>A\\n \\n \\n  Dore   \\n \\n=      argmin      Eywuni((0,7))      ce~aejo(w+|ar0),c0~pPr®      [A(t)      ||      Ver,      log      jo      (Le   \\n \\n|      ro)   \\n \\n_      So(£t,      t)      17)      (4)</p><p> \\n \\n  where   \\n \\n2   \\n \\n:      [0,7]   \\n \\n+   \\n \\nR      is   \\n \\na      weighted      function.\\n \\n \\n  Note      we      typically      use      another      parametrization      well.\\nFrom      (3),      we      can      easily      see      that      this      score      is  \\n \\n  estimated      by      introducing      networks      €9   \\n \\n:      R@   \\n \\nx      [0,7]   \\n \\n—      R®,      which      aims      to      estimate      the      noise      Bere      eO</p><p>Ore   \\n \\n=      argmin      Ey.\\nuni({0,7]),cr~p2xo+o?\\ne~N      (0,1)      ,co~pPte      [A(t)/{op}?\\nlle   \\n \\n—      €9      (1,      t)      7)   \\n \\n:      (5)</p><p>Then,      by      denoting      the      training      data      as      {a}      and      setting      A(t)   \\n \\n=      {a7      }”,      the      actual      loss      function      from the      training      data      is argmin   \\n \\n)      Ey      ai(o.r))a   \\n \\nO      peal      +e02,e~N      (0,1)      [Ile   \\n \\n—      €o(ae?,      t)      [17].\\n(6) i=1 Inference      time.\\nOnce      this      Bore      (or      Ore)      is      learned,      we      insert      it      into      the      time-reversal      SDE      (2)      and use      it      as   \\n \\na      generative      model.\\nUltimately,      with      standard      discretization,      we      obtain:</p><p>p(      xe,      t;      a)   \\n \\n=      24+      (0.54,   \\n \\n+      Sire      (,,T      —t)|(ot),      {o?(t)}?\\n \\n \\n=      (6t).</p><p>Equivalently,      this      is (21,      t;      Ore)   \\n \\n=   \\n \\n2   \\n \\n+      [0.52   \\n \\n—      1/o?\\n \\n \\nx      e%,,,(a1,T   \\n \\n—      t)]|      (6t),      (7)</p><p>where      (dt)      denotes      the      discretization      step.</p><p> \\n \\n  Equivalence      to      DDPM.\\nThe      objective      function      derived      here      is      equivalent      to      the      one      formulated  \\n \\n  based      on      variational      inference      in      the      discretized      formulation,      which      is      commonly      referred      to      as  \\n \\n  DDPMs      (Ho      et      al.,      2020).\\nIn      DDPMs      (Ho      et      al.,      2020),      we      often      see      the      following      form:</p><p> \\n \\n \\n1      1-      (ey;</p><p> \\n \\n  it;      0)   \\n \\n=      ~~   \\n \\n’      ),  \\n \\n  p(xy   \\n \\n)      Xt      Jina      ae      [om</p><h6>Vor</h6><p>When      a;   \\n \\n=   \\n \\n1   \\n \\n—      (dt),      the      above      is      equivalent      to      (7)      when      dt      goes      to   \\n \\n0      by      noting      1/,/a;   \\n \\n©      1+0.5(d¢).</p><h4>1.1.1      Score-Based      Diffusion      Models      (Optional)</h4><p>nal)  \\n \\n  We      briefly      discuss      how      to      train      diffusion      models      in   \\n \\na      continuous-time      framework      (Song      et      al.,  \\n \\n  2021).\\nIn      our      tutorial,      this      section      is      mainly      used      to      discuss      several      algorithms      (e.g.,      value-weighted  \\n \\n  sampling      in      Section      6.2)      and      their      relationship      with      flow-based      diffusion      models      (Section      9)      later.</p><p> \\n \\n  Therefore,      readers      may      skip      it      based      on      their      individual      needs.</p><p> \\n \\n  The      training      process      of      diffusion      models      can      be      summarized      as      follows.\\nOur      objective      is      to      train  \\n \\n  a      sequential      mapping      from   \\n \\na      known      noise      distribution      to   \\n \\na      data      distribution,      formalized      through  \\n \\n  a      stochastic      differential      equation      (SDE).\\nFirstly,      we      define   \\n \\na      forward      (fixed)      SDE      that      maps      from  \\n \\n  the      data      distribution      to      the      noise      distribution.\\nThen,   \\n \\na      time-reversal      SDE      is      expressed      as      an      SDE,  \\n \\n  which      includes      the      (unknown)      score      function.\\nNow,      by      learning      this      unknown      score      function      from  \\n \\n  the      training      data,      the      time-reversal      SDE      can      be      utilized      as   \\n \\na      generative      model.\\nHere      are      the      details.</p><p>Forward      and      time-reversal      SDE.\\nFirstly,      we      introduce   \\n \\na      forward      (a.k.a.,      reference)      Stochastic  \\n \\n  differential      equations      (SDE)      from   \\n \\n0      to      7’.\\n \\n \\nA      common      choice      is   \\n \\na      variance-preserving      (VP)      process:</p><p>t   \\n \\n€      [0,7];      dx,   \\n \\n=      —0.52,dt   \\n \\n+      du;,,      x9   \\n \\n~      p(x),      (1)</p><p>where      dw;      represents      standard      Brownian      motion.\\nHere      are      two      crucial      observations:</p><p>¢\\n \\n   As      T      approaches      oo,      the      limiting      distribution      is      \\\\\\\\\\\\\\\\V(0,      I).</p><p>¢\\n \\n   The      time-reversal      SDE      (Anderson,      1982),      which      preserves      the      marginal      distribution,      is  \\n \\n  expressed      as      follows:</p><p>t   \\n \\n€      [0,7];      dz,   \\n \\n=      [0.5z,   \\n \\n+   \\n \\nV      log      qr_s(%)|dt   \\n \\n+      du,.\\n(2) Here,      g,   \\n \\n€      A(R“)      denotes      the      marginal      distribution      at      time      t      induced      by      the      reference      SDE (1).\\nNotably,      the      marginal      distribution      of      zr_;      is      the      same      as      the      one      of      x;      induced      by      the  \\n \\n  reference      SDE.</p><p> \\n \\n  These      observations      suggest      that      with      sufficiently      large      T,      starting      from   \\n \\nV      (0,      I)      and      following      the  \\n \\n  time-reversal      SDE      (2)   \\n \\n,      we      are      able      to      sample      from      the      data      distribution      (i.e.,      p?\\n\"°)      at      the      terminal  \\n \\n  time      point      7’.</p><p>Training      score      functions      from      the      training      data.\\nNow,      the      remaining      question      is      how      to      estimate  \\n \\n  the      marginal      score      function   \\n \\nV      log      q(-)      in      (2).\\nThis      can      be      computed      using      the      following      principles:</p><p>°\\n \\n   We      have   \\n \\nV      log      (a+)   \\n \\n=      Exgxppre[Va,      log      qyo(ae   \\n \\n|      2o)],      where      q@jo      represents      the      conditional  \\n \\n  distribution      of      x;      given      Xo      induced      by      the      reference      SDE.</p><p>¢\\n \\n   The      conditional      score      function      V,,,      log      dejo      (Xe   \\n \\n|      xo)      can      be      analytically      derived      as  \\n \\n  °</p><h5>      Myo      —      Ut</h5><p> \\n \\n  {ory      ue   \\n \\n=      exp(—0.5t)xo,      {o?\\n}?\\n \\n \\n=   \\n \\n1   \\n \\n—      exp(—0.5t).\\n(3) t Subsequently,      by      defining   \\n \\na      parameterized      model      sg   \\n \\n:      R¢   \\n \\nx      [0,7]   \\n \\n>      R¢      and      utilizing      the      following weighted      regression,      the      marginal      score      is      estimated:</p><p>A\\n \\n \\n  Dore   \\n \\n=      argmin      Eywuni((0,7))      ce~aejo(w+|ar0),c0~pPr®      [A(t)      ||      Ver,      log      jo      (Le   \\n \\n|      ro)   \\n \\n_      So(£t,      t)      17)      (4)</p><p> \\n \\n  where   \\n \\n2   \\n \\n:      [0,7]   \\n \\n+   \\n \\nR      is   \\n \\na      weighted      function.\\n \\n \\n  Note      we      typically      use      another      parametrization      well.\\nFrom      (3),      we      can      easily      see      that      this      score      is  \\n \\n  estimated      by      introducing      networks      €9   \\n \\n:      R@   \\n \\nx      [0,7]   \\n \\n—      R®,      which      aims      to      estimate      the      noise      Bere      eO</p><p>Ore   \\n \\n=      argmin      Ey.\\nuni({0,7]),cr~p2xo+o?\\ne~N      (0,1)      ,co~pPte      [A(t)/{op}?\\nlle   \\n \\n—      €9      (1,      t)      7)   \\n \\n:      (5)</p><p>Then,      by      denoting      the      training      data      as      {a}      and      setting      A(t)   \\n \\n=      {a7      }”,      the      actual      loss      function      from the      training      data      is argmin   \\n \\n)      Ey      ai(o.r))a   \\n \\nO      peal      +e02,e~N      (0,1)      [Ile   \\n \\n—      €o(ae?,      t)      [17].\\n(6) i=1 Inference      time.\\nOnce      this      Bore      (or      Ore)      is      learned,      we      insert      it      into      the      time-reversal      SDE      (2)      and use      it      as   \\n \\na      generative      model.\\nUltimately,      with      standard      discretization,      we      obtain:</p><p>p(      xe,      t;      a)   \\n \\n=      24+      (0.54,   \\n \\n+      Sire      (,,T      —t)|(ot),      {o?(t)}?\\n \\n \\n=      (6t).</p><p>Equivalently,      this      is (21,      t;      Ore)   \\n \\n=   \\n \\n2   \\n \\n+      [0.52   \\n \\n—      1/o?\\n \\n \\nx      e%,,,(a1,T   \\n \\n—      t)]|      (6t),      (7)</p><p>where      (dt)      denotes      the      discretization      step.</p><p> \\n \\n  Equivalence      to      DDPM.\\nThe      objective      function      derived      here      is      equivalent      to      the      one      formulated  \\n \\n  based      on      variational      inference      in      the      discretized      formulation,      which      is      commonly      referred      to      as  \\n \\n  DDPMs      (Ho      et      al.,      2020).\\nIn      DDPMs      (Ho      et      al.,      2020),      we      often      see      the      following      form:</p><p> \\n \\n \\n1      1-      (ey;</p><p> \\n \\n  it;      0)   \\n \\n=      ~~   \\n \\n’      ),  \\n \\n  p(xy   \\n \\n)      Xt      Jina      ae      [om</p><h6>Vor</h6><p>When      a;   \\n \\n=   \\n \\n1   \\n \\n—      (dt),      the      above      is      equivalent      to      (7)      when      dt      goes      to   \\n \\n0      by      noting      1/,/a;   \\n \\n©      1+0.5(d¢).</p><h5>      Myo      —      Ut</h5><p> \\n \\n  {ory      ue   \\n \\n=      exp(—0.5t)xo,      {o?\\n}?\\n \\n \\n=   \\n \\n1   \\n \\n—      exp(—0.5t).\\n(3) t Subsequently,      by      defining   \\n \\na      parameterized      model      sg   \\n \\n:      R¢   \\n \\nx      [0,7]   \\n \\n>      R¢      and      utilizing      the      following weighted      regression,      the      marginal      score      is      estimated:</p><p>A\\n \\n \\n  Dore   \\n \\n=      argmin      Eywuni((0,7))      ce~aejo(w+|ar0),c0~pPr®      [A(t)      ||      Ver,      log      jo      (Le   \\n \\n|      ro)   \\n \\n_      So(£t,      t)      17)      (4)</p><p> \\n \\n  where   \\n \\n2   \\n \\n:      [0,7]   \\n \\n+   \\n \\nR      is   \\n \\na      weighted      function.\\n \\n \\n  Note      we      typically      use      another      parametrization      well.\\nFrom      (3),      we      can      easily      see      that      this      score      is  \\n \\n  estimated      by      introducing      networks      €9   \\n \\n:      R@   \\n \\nx      [0,7]   \\n \\n—      R®,      which      aims      to      estimate      the      noise      Bere      eO</p><p>Ore   \\n \\n=      argmin      Ey.\\nuni({0,7]),cr~p2xo+o?\\ne~N      (0,1)      ,co~pPte      [A(t)/{op}?\\nlle   \\n \\n—      €9      (1,      t)      7)   \\n \\n:      (5)</p><p>Then,      by      denoting      the      training      data      as      {a}      and      setting      A(t)   \\n \\n=      {a7      }”,      the      actual      loss      function      from the      training      data      is argmin   \\n \\n)      Ey      ai(o.r))a   \\n \\nO      peal      +e02,e~N      (0,1)      [Ile   \\n \\n—      €o(ae?,      t)      [17].\\n(6) i=1 Inference      time.\\nOnce      this      Bore      (or      Ore)      is      learned,      we      insert      it      into      the      time-reversal      SDE      (2)      and use      it      as   \\n \\na      generative      model.\\nUltimately,      with      standard      discretization,      we      obtain:</p><p>p(      xe,      t;      a)   \\n \\n=      24+      (0.54,   \\n \\n+      Sire      (,,T      —t)|(ot),      {o?(t)}?\\n \\n \\n=      (6t).</p><p>Equivalently,      this      is (21,      t;      Ore)   \\n \\n=   \\n \\n2   \\n \\n+      [0.52   \\n \\n—      1/o?\\n \\n \\nx      e%,,,(a1,T   \\n \\n—      t)]|      (6t),      (7)</p><p>where      (dt)      denotes      the      discretization      step.</p><p> \\n \\n  Equivalence      to      DDPM.\\nThe      objective      function      derived      here      is      equivalent      to      the      one      formulated  \\n \\n  based      on      variational      inference      in      the      discretized      formulation,      which      is      commonly      referred      to      as  \\n \\n  DDPMs      (Ho      et      al.,      2020).\\nIn      DDPMs      (Ho      et      al.,      2020),      we      often      see      the      following      form:</p><p> \\n \\n \\n1      1-      (ey;</p><p> \\n \\n  it;      0)   \\n \\n=      ~~   \\n \\n’      ),  \\n \\n  p(xy   \\n \\n)      Xt      Jina      ae      [om</p><h6>Vor</h6><p>When      a;   \\n \\n=   \\n \\n1   \\n \\n—      (dt),      the      above      is      equivalent      to      (7)      when      dt      goes      to   \\n \\n0      by      noting      1/,/a;   \\n \\n©      1+0.5(d¢).</p><h6>Vor</h6><p>When      a;   \\n \\n=   \\n \\n1   \\n \\n—      (dt),      the      above      is      equivalent      to      (7)      when      dt      goes      to   \\n \\n0      by      noting      1/,/a;   \\n \\n©      1+0.5(d¢).</p><h3>1.2      Fine-Tuning      Diffusion      Models      with      RL</h3><p>RL  \\n \\n  Importantly,      our      focus      on      RL-based      fine-tuning      distinguishes      itself      from      the      standard      fine-tuning  \\n \\n  methods.\\nStandard      fine-tuning      typically      involves      scenarios      where      we      have      pre-trained      models  \\n \\n  (e.g.,      diffusion      models)      and      new      training      data      {2,      y}.\\nIn      such      cases,      the      common      approach  \\n \\n  for      fine-tuning      is      to      retrain      diffusion      models      with      the      new      training      data      using      the      same      loss  \\n \\n  function      employed      during      pre-training.\\nIn      sharp      contrast,      RL-based      fine-tuning      directly      employs  \\n \\n  the      downstream      reward      functions      as      the      primary      optimization      objectives,      making      the      loss      functions  \\n \\n  different      from      those      used      in      pre-training.</p><p> \\n \\n  Hereafter,      we      start      with   \\n \\na      concise      overview      of      RL-based      fine-tuning.\\nThen,      before      delving      into  \\n \\n  specifics,      we      discuss      simpler      non-RL      alternatives      to      provide      motivation      for      adopting      RL-based  \\n \\n  fine-tuning.</p><li>1.2.1      Brief      Overview:      Fine-tuning      with      RL</li><p>In      this      article,      we      explore      the      fine-tuning      of      pre-trained      diffusion      models      to      optimize      downstream  \\n \\n  reward      functions   \\n \\nr   \\n \\n:      R¢   \\n \\n+      R.      In      domains      such      as      images,      these      backbone      diffusion      models      to      be  \\n \\n  fine-tuned      include      Stable      Diffusion      (Rombach      et      al.,      2022),      while      the      reward      functions      are      aesthetic  \\n \\n  scores      and      alignment      scores      (Clark      et      al.,      2023;      Black      et      al.,      2023;      Fan      et      al.,      2023).\\nMore      examples  \\n \\n  are      detailed      in      the      introduction.\\nThese      rewards      are      often      unknown,      necessitating      learning      from  \\n \\n  data      with      feedback:      {2      r(x™)}.\\nWe      will      explore      this      aspect      further      in      Section      7.\\nUntil      then,      we  \\n \\n  assume   \\n \\nr      is      known.\\n \\n \\n  Now,      readers      may      wonder      about      the      objectives      we      aim      to      achieve      during      the      fine-tuning      process.\\n \\n \\n  A      natural      approach      is      to      define      the      optimization      problem:</p><p>argmax      E,.\\n[r()|      (8)  \\n \\n  qeA(¥)  \\n \\n  where   \\n \\nq      is      initialized      with   \\n \\na      pre-trained      diffusion      model      p?\\n*®   \\n \\n€      A(2).\\nIn      this      tutorial,      we      will      detail  \\n \\n  the      procedure      of      solving      (8)      with      RL      in      the      upcoming      sections.\\nIn      essence,      we      leverage      the      fact  \\n \\n  that      diffusion      models      are      formulated      as   \\n \\na      sequential      decision-making      problem,      where      each      decision  \\n \\n  corresponds      to      how      samples      are      denoised.</p><p> \\n \\n  Although      the      above      objective      function      (8)      is      reasonable,      the      resulting      distribution      might      deviate  \\n \\n  too      much      from      the      pre-trained      diffusion      model.\\nTo      circumvent      this      issue,   \\n \\na      natural      way      is      to      add       penalization      against      pre-trained      diffusion      models.\\nThen,      the      target      distribution      is      defined      as:</p><p>argmax      E,.4[r(x)]   \\n \\n—      aKL(q||p’*).\\n(9)  \\n \\n  qeA(¥) Notably,      (9)      reduces      to      the      following      distribution:</p><li>(10)</li><p>5\\n \\n   exp(r()/a)p\"\"()  \\n \\n  Pr)   \\n \\n=      Fra)      apa)</p><p> \\n \\n  Here,      the      first      term      in      (9)      corresponds      to      the      mean      reward,      which      we      want      to      optimize      in      the  \\n \\n  fine-tuning      process.\\nThe      second      term      in      (10)      serves      as   \\n \\na      penalty      term,      indicating      the      deviation      of   \\n \\nq       from      the      pre-trained      model.\\nThe      parameter   \\n \\na      controls      the      strength      of      this      regularization      term.\\nThe  \\n \\n  proper      choice      of   \\n \\na      depends      on      the      task      we      are      interested      in.</p><h6>1.2.2      Motivation      for      Using      RL      over      Non-RL      Alternatives</h6><p>ives  \\n \\n  To      achieve      our      goal      of      maximizing      downstream      reward      functions      with      diffusion      models,      readers  \\n \\n  may      question      whether      alternative      approaches      can      be      employed      apart      from      RL.\\nHere,      we      investigate  \\n \\n  these      potential      alternatives      and      explain      why      RL      approaches      may      offer      advantages      over      them.</p><p> \\n \\n  Rejection      sampling.\\nOne      approach      involves      generating      multiple      samples      from      pre-trained      diffu-  \\n \\n  sion      models      and      selecting      only      those      with      high      rewards.\\nThis      method,      called      rejection      sampling,  \\n \\n  operates      without      needing      fine-tuning.\\nHowever,      rejection      sampling      is      effective      primarily      when      the  \\n \\n  pre-trained      model      already      has   \\n \\na      high      probability      of      producing      high-reward      samples.\\nIt      resembles  \\n \\n  sampling      from   \\n \\na      prior      distribution      to      obtain      posterior      samples      (in      this      case,      high-reward      points).\\n \\n \\n  This      approach      works      efficiently      when      the      posterior      closely      matches      the      prior      but      can      become      highly  \\n \\n  inefficient      otherwise.\\nIn      contrast,      by      explicitly      updating      weight      in      diffusion      models,      RL-based      fine-  \\n \\n  tuning      allows      us      to      obtain      these      high-reward      samples,      which      are      seldom      generated      by      pre-trained  \\n \\n  models.</p><p> \\n \\n  Conditional      diffusion      models      (classifier-free      guidance).\\nIn      conditional      generative      models,      the  \\n \\n  general      goal      is      to      sample      from      p(|c),      where   \\n \\nx      is      the      output      and      c      denotes      the      conditioning      variable.\\n \\n \\n  For      example,      in      text-to-language      diffusion      models,      c      is   \\n \\na      text,      and   \\n \\nx      is      the      generated      image.\\nSimilarly,  \\n \\n  in      the      context      of      protein      engineering      for      addressing      inverse      folding      problems,      models      often      define      c       as      the      protein      backbone      structure      and   \\n \\nx      as      the      corresponding      amino      acid      sequence.\\nHere,      using      the  \\n \\n  training      data      {c,      ¢},      the      model      is      trained      by      using      the      loss      function:</p><p>argmin   \\n \\n»      E,uni(l0,7)2)      ~upa      +o?\\nsewN\\\\\\\\\\\\\\'(O)      [lle   \\n \\n—      co(2”,      }),      t)      I\"), t where      the      denoising      function      €g      additionally      receives      the      conditioning      information      c      as      input.\\nIn  \\n \\n  practice,   \\n \\na      variety      of      improvements      such      as      classifier-free      guidance      (Ho      and      Salimans,      2022)      can  \\n \\n  further      improve      the      model’s      ability      to      learn      the      conditional      distribution      p(|c).</p><p> \\n \\n  These      conditional      generative      models      can      be      used      to      optimize      down-stream      rewards      by      condi-  \\n \\n  tioning      on      the      reward      values,      then      sampling   \\n \\nx      conditioned      on      high      reward      values      (Krishnamoorthy  \\n \\n  et      al.,      2023;      Yuan      et      al.,      2023).\\nWhile      this      method      is,      in      principle,      capable      of      generating      plausible x\\n \\n   values      across   \\n \\na      range      of      reward      levels      within      the      training      data      distribution,      it      is      not      the      most  \\n \\n  effective      optimization      strategy.\\nThis      is      primarily      because      high-reward      inputs      frequently      reside  \\n \\n  in      the      tails      of      the      training      distribution      or      even      beyond      it.\\nConsequently,      this      method      may      not  \\n \\n  effectively      generate      high-reward      samples      that      lie      outside      the      training      data      distribution.\\nIn      contrast,  \\n \\n  RL-based      fine-tuning      has      the      capability      to      generate      samples      with      higher      rewards      beyond      the      training  \\n \\n  data.\\nThis      is      achieved      by      explicitly      maximizing      reward      models      learned      from      the      training      data  \\n \\n  and      leveraging      their      extrapolative      capabilities      of      reward      models,      as      theoretically      formalized      and       empirically      observed      in      Uehara      et      al.\\n(2024).</p><p>Reward-weighted      training.\\nAnother      alternative      approach      is      to      use   \\n \\na      reward-weighted      version  \\n \\n  of      the      standard      training      loss      for      diffusion      models.\\nSuppose      that      we      have      data      {x      r(a)}.\\nThen, after      learning   \\n \\na      reward   \\n \\n7   \\n \\n:   \\n \\n¥   \\n \\n—   \\n \\nR      with      regression      from      the      data,      to      achieve      our      goal,      it      looks  \\n \\n  natural      to      use   \\n \\na      reward-weighted      version      of      the      training      loss      for      diffusion      models      (5),      1.e.,.\\na7      (4)      i)   \\n \\n2 areaun   \\n \\n»      E,uni((0,2))      peel?\\n+eo¢,enncoal      (to      ile   \\n \\n—      co(ay?,t)      |\") i\\n \\n    There      are      two      potential      drawbacks      to      this      approach.\\nFirst,      in      practice,      it      may      struggle      to      generate  \\n \\n  samples      with      higher      rewards      beyond      the      training      data.\\nAs      we      will      explain      later,      many      RL      algorithms  \\n \\n  are      more      directly      focused      on      optimizing      reward      functions,      which      are      expected      to      excel      in      obtaining  \\n \\n  samples      with      high      rewards      not      observed      in      the      original      data,      as      empirically      observed      in      Black      et      al.\\n \\n \\n  (2023).\\nSecond,      when      fine-tuning   \\n \\na      conditional      diffusion      model      p(x|c),      the      alternative      approach  \\n \\n  here      requires   \\n \\na      pair      of      {c,      «}      during      fine-tuning      to      ensure      the      validity      of      the      loss      function.\\nWhen  \\n \\n  we      only      have      data      {2      r(x      )}      but      not      {c,      2,      r(a)},      this      implies      that      we      might      need      to      solve an      inverse      problem      from   \\n \\nz      to      c,      which      can      often      be      challenging.\\nIn      contrast,      in      these      scenarios,      RL  \\n \\n  algorithms,      which      we      will      introduce      later,      can      operate      without      needing      such      pairs      {c,      x},      as  \\n \\n  long      as      we      have      learned      reward      functions      7.  \\n \\n  Finally,      it      should      be      noted      that      reward-weighted      training      technically      falls      under      the      broader  \\n \\n  category      of      RL      methods.\\nIt      shares   \\n \\na      close      connection      with      “reward-weighted      MLE”      introduced      in  \\n \\n  Section      6.1,      as      discussed      later.\\nEmploying      this      reward-weighted      MLE      helps      address      the      second  \\n \\n  concern      of      “reward-weighted      training”      mentioned      earlier.</p><p>2\\n \\n   Brief      Overview      of      Entropy-Regularized      MDPs</p><p> \\n \\n  In      this      tutorial,      we      explain      how      fine-tuning      diffusion      models      can      be      naturally      formulated      as      an      RL  \\n \\n  problem      in      entropy-regularized      MDPs.\\nThis      perspective      is      natural      because      RL      involves      sequential  \\n \\n  decision-making,      and   \\n \\na      diffusion      model      is      formulated      as   \\n \\na      sequential      problem      where      each      denoising  \\n \\n  step      is   \\n \\na      decision-making      process.\\nTo      connect      diffusion      models      with      RL,      we      begin      with   \\n \\na      concise  \\n \\n  overview      of      RL      in      standard      entropy-regularized      MDPs      (Haarnoja      et      al.,      2017;      Neu      et      al.,      2017;</p><p> \\n \\n  Geist      et      al.,      2019;      Schulman      et      al.,      2017).</p><li>2.1      MDPs</li><p> \\n \\n  An      MDP      is      defined      as      follows:      {S,A,      {P\"*}/_5,      {r:}-9,      po}      where      S      is      the      state      space,   \\n \\nA      is      the  \\n \\n  action      space,      P;\"*      is   \\n \\na      transition      dynamic      mapping:      S   \\n \\nx   \\n \\nA   \\n \\n—      A(S),      7,   \\n \\n:      S   \\n \\nx   \\n \\nA   \\n \\n>   \\n \\nR      denotes  \\n \\n  reward      received      at      t      and      po      is      an      initial      distribution      over      S.   \\n \\nA      policy      7,   \\n \\n:      S   \\n \\n—      A(A)      is   \\n \\na      map      from  \\n \\n  any      state      s   \\n \\n€      S      to      the      distribution      over      actions.\\nThe      standard      goal      in      RL      is      to      solve</p><h6>1.2.2      Motivation      for      Using      RL      over      Non-RL      Alternatives</h6><p>ives  \\n \\n  To      achieve      our      goal      of      maximizing      downstream      reward      functions      with      diffusion      models,      readers  \\n \\n  may      question      whether      alternative      approaches      can      be      employed      apart      from      RL.\\nHere,      we      investigate  \\n \\n  these      potential      alternatives      and      explain      why      RL      approaches      may      offer      advantages      over      them.</p><p> \\n \\n  Rejection      sampling.\\nOne      approach      involves      generating      multiple      samples      from      pre-trained      diffu-  \\n \\n  sion      models      and      selecting      only      those      with      high      rewards.\\nThis      method,      called      rejection      sampling,  \\n \\n  operates      without      needing      fine-tuning.\\nHowever,      rejection      sampling      is      effective      primarily      when      the  \\n \\n  pre-trained      model      already      has   \\n \\na      high      probability      of      producing      high-reward      samples.\\nIt      resembles  \\n \\n  sampling      from   \\n \\na      prior      distribution      to      obtain      posterior      samples      (in      this      case,      high-reward      points).\\n \\n \\n  This      approach      works      efficiently      when      the      posterior      closely      matches      the      prior      but      can      become      highly  \\n \\n  inefficient      otherwise.\\nIn      contrast,      by      explicitly      updating      weight      in      diffusion      models,      RL-based      fine-  \\n \\n  tuning      allows      us      to      obtain      these      high-reward      samples,      which      are      seldom      generated      by      pre-trained  \\n \\n  models.</p><p> \\n \\n  Conditional      diffusion      models      (classifier-free      guidance).\\nIn      conditional      generative      models,      the  \\n \\n  general      goal      is      to      sample      from      p(|c),      where   \\n \\nx      is      the      output      and      c      denotes      the      conditioning      variable.\\n \\n \\n  For      example,      in      text-to-language      diffusion      models,      c      is   \\n \\na      text,      and   \\n \\nx      is      the      generated      image.\\nSimilarly,  \\n \\n  in      the      context      of      protein      engineering      for      addressing      inverse      folding      problems,      models      often      define      c       as      the      protein      backbone      structure      and   \\n \\nx      as      the      corresponding      amino      acid      sequence.\\nHere,      using      the  \\n \\n  training      data      {c,      ¢},      the      model      is      trained      by      using      the      loss      function:</p><p>argmin   \\n \\n»      E,uni(l0,7)2)      ~upa      +o?\\nsewN\\\\\\\\\\\\\\'(O)      [lle   \\n \\n—      co(2”,      }),      t)      I\"), t where      the      denoising      function      €g      additionally      receives      the      conditioning      information      c      as      input.\\nIn  \\n \\n  practice,   \\n \\na      variety      of      improvements      such      as      classifier-free      guidance      (Ho      and      Salimans,      2022)      can  \\n \\n  further      improve      the      model’s      ability      to      learn      the      conditional      distribution      p(|c).</p><p> \\n \\n  These      conditional      generative      models      can      be      used      to      optimize      down-stream      rewards      by      condi-  \\n \\n  tioning      on      the      reward      values,      then      sampling   \\n \\nx      conditioned      on      high      reward      values      (Krishnamoorthy  \\n \\n  et      al.,      2023;      Yuan      et      al.,      2023).\\nWhile      this      method      is,      in      principle,      capable      of      generating      plausible x\\n \\n   values      across   \\n \\na      range      of      reward      levels      within      the      training      data      distribution,      it      is      not      the      most  \\n \\n  effective      optimization      strategy.\\nThis      is      primarily      because      high-reward      inputs      frequently      reside  \\n \\n  in      the      tails      of      the      training      distribution      or      even      beyond      it.\\nConsequently,      this      method      may      not  \\n \\n  effectively      generate      high-reward      samples      that      lie      outside      the      training      data      distribution.\\nIn      contrast,  \\n \\n  RL-based      fine-tuning      has      the      capability      to      generate      samples      with      higher      rewards      beyond      the      training  \\n \\n  data.\\nThis      is      achieved      by      explicitly      maximizing      reward      models      learned      from      the      training      data  \\n \\n  and      leveraging      their      extrapolative      capabilities      of      reward      models,      as      theoretically      formalized      and       empirically      observed      in      Uehara      et      al.\\n(2024).</p><p>Reward-weighted      training.\\nAnother      alternative      approach      is      to      use   \\n \\na      reward-weighted      version  \\n \\n  of      the      standard      training      loss      for      diffusion      models.\\nSuppose      that      we      have      data      {x      r(a)}.\\nThen, after      learning   \\n \\na      reward   \\n \\n7   \\n \\n:   \\n \\n¥   \\n \\n—   \\n \\nR      with      regression      from      the      data,      to      achieve      our      goal,      it      looks  \\n \\n  natural      to      use   \\n \\na      reward-weighted      version      of      the      training      loss      for      diffusion      models      (5),      1.e.,.\\na7      (4)      i)   \\n \\n2 areaun   \\n \\n»      E,uni((0,2))      peel?\\n+eo¢,enncoal      (to      ile   \\n \\n—      co(ay?,t)      |\") i\\n \\n    There      are      two      potential      drawbacks      to      this      approach.\\nFirst,      in      practice,      it      may      struggle      to      generate  \\n \\n  samples      with      higher      rewards      beyond      the      training      data.\\nAs      we      will      explain      later,      many      RL      algorithms  \\n \\n  are      more      directly      focused      on      optimizing      reward      functions,      which      are      expected      to      excel      in      obtaining  \\n \\n  samples      with      high      rewards      not      observed      in      the      original      data,      as      empirically      observed      in      Black      et      al.\\n \\n \\n  (2023).\\nSecond,      when      fine-tuning   \\n \\na      conditional      diffusion      model      p(x|c),      the      alternative      approach  \\n \\n  here      requires   \\n \\na      pair      of      {c,      «}      during      fine-tuning      to      ensure      the      validity      of      the      loss      function.\\nWhen  \\n \\n  we      only      have      data      {2      r(x      )}      but      not      {c,      2,      r(a)},      this      implies      that      we      might      need      to      solve an      inverse      problem      from   \\n \\nz      to      c,      which      can      often      be      challenging.\\nIn      contrast,      in      these      scenarios,      RL  \\n \\n  algorithms,      which      we      will      introduce      later,      can      operate      without      needing      such      pairs      {c,      x},      as  \\n \\n  long      as      we      have      learned      reward      functions      7.  \\n \\n  Finally,      it      should      be      noted      that      reward-weighted      training      technically      falls      under      the      broader  \\n \\n  category      of      RL      methods.\\nIt      shares   \\n \\na      close      connection      with      “reward-weighted      MLE”      introduced      in  \\n \\n  Section      6.1,      as      discussed      later.\\nEmploying      this      reward-weighted      MLE      helps      address      the      second  \\n \\n  concern      of      “reward-weighted      training”      mentioned      earlier.</p><p>2\\n \\n   Brief      Overview      of      Entropy-Regularized      MDPs</p><p> \\n \\n  In      this      tutorial,      we      explain      how      fine-tuning      diffusion      models      can      be      naturally      formulated      as      an      RL  \\n \\n  problem      in      entropy-regularized      MDPs.\\nThis      perspective      is      natural      because      RL      involves      sequential  \\n \\n  decision-making,      and   \\n \\na      diffusion      model      is      formulated      as   \\n \\na      sequential      problem      where      each      denoising  \\n \\n  step      is   \\n \\na      decision-making      process.\\nTo      connect      diffusion      models      with      RL,      we      begin      with   \\n \\na      concise  \\n \\n  overview      of      RL      in      standard      entropy-regularized      MDPs      (Haarnoja      et      al.,      2017;      Neu      et      al.,      2017;</p><p> \\n \\n  Geist      et      al.,      2019;      Schulman      et      al.,      2017).</p><li>2.1      MDPs</li><p> \\n \\n  An      MDP      is      defined      as      follows:      {S,A,      {P\"*}/_5,      {r:}-9,      po}      where      S      is      the      state      space,   \\n \\nA      is      the  \\n \\n  action      space,      P;\"*      is   \\n \\na      transition      dynamic      mapping:      S   \\n \\nx   \\n \\nA   \\n \\n—      A(S),      7,   \\n \\n:      S   \\n \\nx   \\n \\nA   \\n \\n>   \\n \\nR      denotes  \\n \\n  reward      received      at      t      and      po      is      an      initial      distribution      over      S.   \\n \\nA      policy      7,   \\n \\n:      S   \\n \\n—      A(A)      is   \\n \\na      map      from  \\n \\n  any      state      s   \\n \\n€      S      to      the      distribution      over      actions.\\nThe      standard      goal      in      RL      is      to      solve</p><h1>T</h1><p>So      ri(si,      2)      (11)  \\n \\n  argmax      Ey      ,,}  \\n \\n  {re}  \\n \\n  In      entropy-regularized      MDPs,      we      consider      the      following      regularized      objective      instead:</p><p>t=0  \\n \\n  where      E;,,}[-]      is      the      expectation      induced      both      policy   \\n \\n7      and      the      transition      dynamics      as      follows:  \\n \\n  So   \\n \\n~      Po,      do   \\n \\n~      To(-|80),      51   \\n \\n~      P§\"(-|S0,@0),°+:.\\nAs      we      will      soon      detail      in      the      next      section  \\n \\n  (Section      3),      diffusion      models      can      naturally      be      framed      as      MDPs      as      each      policy      corresponds      to   \\n \\na       denoising      process      in      diffusion      models.</p><p>T       {iy}   \\n \\n=      argmax      Ein)      Son      (51,      a4)   \\n \\n—      AKL(m:(-|s¢),      7:      (-]54))      (12) mt}      t—0  \\n \\n  where      7’   \\n \\n:      S      —>      A(A)      is   \\n \\na      certain      reference      policy.\\nThe      arg      max      solution      is      often      called   \\n \\na      set      of      soft  \\n \\n  optimal      policies.\\nCompared      to   \\n \\na      standard      objective      (11),      here      we      add      KL      terms      against      reference  \\n \\n  policies.\\nThis      addition      aims      to      ensure      that      soft      optimal      policies      closely      align      with      the      reference  \\n \\n  policies.\\nIn      the      context      of      fine-tuning      diffusion      models,      these      reference      policies      correspond      to  \\n \\n  the      pre-trained      diffusion      models,      as      we      aim      to      maintain      similarity      between      the      fine-tuned      and       pre-trained      models.</p><p> \\n \\n  This      entropy-regularized      objective      in      (12)      has      been      widely      employed      in      RL      literature      due      to  \\n \\n  several      benefits      (Levine,      2018).\\nFor      instance,      in      online      RL,      it      is      known      that      these      policies      have  \\n \\n  good      exploration      properties      by      setting      reference      policies      as      uniform      policies      (Fox      et      al.,      2015;</p><p> \\n \\n  Haarnoja      et      al.,      2017).\\nIn      offline      RL,      Wu      et      al.\\n(2019)      suggests      using      these      policies      as      conservative  \\n \\n  policies      by      setting      reference      policies      close      to      behavior      policies      (policies      used      to      collect      offline      data).\\n \\n \\n  Additionally,      in      inverse      RL,      this      soft      optimal      policy      is      used      as      an      expert      policy      in      scenarios      where  \\n \\n  rewards      are      unobservable,      only      trajectories      from      expert      policies      are      available      (typically      referred      to  \\n \\n  as      maximum      entropy      RL      as      Ziebart      et      al.      (2008);      Wulfmeier      et      al.\\n(2015);      Finn      et      al.\\n(2016)).</p><h6>2.2      Key      Concepts:      Soft      Q-functions,      Soft      Bellman      Equations</h6><p>ions.</p><p> \\n \\n  The      crucial      question      in      RL      is      how      to      devise      algorithms      that      effectively      solve      the      optimization  \\n \\n  problem      (12).\\nThese      algorithms      are      later      used      as      fine-tuning      algorithms      of      diffusion      models.\\nTo      see  \\n \\n  these      algorithms,      we      rely      on      several      critical      concepts      in      entropy-regularized      MDPs.\\nSpecifically,  \\n \\n  soft-optimal      policies      (i.e.,      solutions      to      (12))      can      be      expressed      analytically      as   \\n \\na      blend      of      soft      Q-  \\n \\n  functions      and      reference      policies.\\nFurthermore,      these      soft      Q-functions      are      defined      as      solutions      to  \\n \\n  equations      known      as      soft      Bellman      equations.\\nWe      elaborate      on      these      foundational      concepts      below.</p><p>Soft      Q-functions      and      soft      optimal      policies.\\nSoft      optimal      policies      are      expressed      as   \\n \\na      blend      of      soft  \\n \\n  Q-functions      and      reference      policies.\\nTo      see      it,      we      define      the      soft      Q-function      as      follows:</p><p>T       q(se,      a)   \\n \\n=      Egnsy      So      ral      ($4,      4%)   \\n \\n—      OBL      (m1      (-|8e41)      ley      1(-/Se41))      [S45      Ge   \\n \\n|   \\n \\n-      (13) k=t Then,      by      comparing      (13)      and      (12),      we      clearly      have m\\n \\n   =\\n \\n   argmax      Eq,~n(s,)[Ge(se,      a)   \\n \\n—      AKL(a(-|5¢)|]77(-   \\n \\n|      S2)|s¢].\\n(14)  \\n \\n  TE[XA(X)] Hence,      by      calculating      the      above      explicitly,   \\n \\na      soft      optimal      policy      in      (12)      is      described      as      follows:</p><p> \\n \\n  we)      og      _£xBlan(s.)      fant)  \\n \\n  mi      CIS)   \\n \\n&      Fe      qi(s,      a)      (ayn      (als)da      as)  \\n \\n  Soft      Bellman      equations.\\nWe      have      already      defined      soft      Q-functions      in      (13).\\nHowever,      this      form  \\n \\n  includes      the      soft      optimal      policies.\\nActually,      without      using      soft      optimal      policies,      the      soft      Q-function  \\n \\n  satisfies      the      following      recursive      equation      (a.k.a.      soft      Bellman      equation):</p><p>4(      St,      At)   \\n \\n=      Epps}      [rosuay)   \\n \\n+      alog   \\n \\n{   \\n \\n[      explara(siara)/a}n{(alsis)aa}   \\n \\n|      sia  \\n \\n.      (16)</p><p>This      is      proven      by      noting      we      recursively      have a\\n \\n   (Se,      ar)   \\n \\n=      Bers}      [re(      Se,      at)   \\n \\n+      G41      St41;      di41)   \\n \\n_      akKL      (74,4      (-|Se41),      Tai      (-|Se41))[Se,      ar|</p><p>By      substituting      (15)      into      the      above,      we      obtain      the      soft      Bellman      equation      (16).</p><p> \\n \\n  Soft      value      functions.\\nSo      far,      we      have      defined      the      soft      Q-functions,      which      depend      on      both      states  \\n \\n  and      actions.\\nWe      can      now      introduce   \\n \\na      related      concept      that      depends      solely      on      states,      termed      the      soft  \\n \\n  value      function.\\nThe      soft      value      function      is      defined      as      follows:</p><li>v4      (Sz)   \\n \\n=      tary T Tk(Sk;      Qk)   \\n \\n—      onto)</li><p>k=t Then,      the      soft      optimal      policy      in      (14)      is      also      written      as r*(-|s)   \\n \\nx      exp(q(s,      -)/o)m      Cs)  \\n \\n  ‘      exp(u;(s)/a) (17)</p><p>because      we      have exp      (ee)   \\n \\n=   \\n \\n/      exp      (a   \\n \\n)      m(a   \\n \\n|      s)da.</p><p>Then,      substituting      the      above      in      the      soft      Bellman      equation      (16),      it      is      written      as a\\n \\n   (Se,      at)   \\n \\n=      Egrs}      [7      (Se;      at)   \\n \\n+      p41      (Se41)|S¢,      ar).</p><p> \\n \\n  Algorithms      in      entropy-regularized      MDPs.\\nAs      outlined      in      Levine      (2018),      to      solve      (12),      various  \\n \\n  well-known      algorithms      exist      in      the      literature      on      RL.\\nThe      abovementioned      concepts      are      useful      in  \\n \\n  constructing      these      algorithms.\\nThese      include      policy      gradients,      which      gradually      optimize   \\n \\na      policy  \\n \\n  using   \\n \\na      policy      neural      network;      soft      Q-learning      algorithms,      which      utilize      the      soft-Bellman      equation  \\n \\n  and      approximate      the      soft-value      function      with   \\n \\na      value      neural      network;      and      soft      actor-critic      algorithms  \\n \\n  that      leverage      both      policy      and      value      neural      networks.\\nWe      will      explore      how      these      algorithms      can      be  \\n \\n  applied      in      the      context      of      diffusion      models      shortly      in      Section      4.2      and      6.</p><p>3\\n \\n   Fine-Tuning      Diffusion      Models      with      RL      in      Entropy      Regular- ized      MDPs</p><p> \\n \\n  In      this      section,      as      done      in      Fan      et      al.\\n(2023);      Black      et      al.\\n(2023);      Uehara      et      al.\\n(2024),      we      illustrate  \\n \\n  how      fine-tuning      can      be      formulated      as      an      RL      problem      in      soft-entropy      regularized      MDPs,      where      each  \\n \\n  PT   \\n \\nx      PT-1      rp_9      PT-2      P2      r1      Pl      Lo  \\n \\n  LT      T-1</p><p>Figure      2:      Formulating      fine-tuning      in      diffusion      models      using      MDPs.</p><p> \\n \\n  denoising      step      of      diffusion      models      corresponds      to   \\n \\na      policy      in      RL.\\nFinally,      we      outline   \\n \\na      specific      RL  \\n \\n  problem      of      interest      in      our      context.</p><p> \\n \\n  To      cast      fine-tuning      diffusion      models      as      an      RL      problem,      we      start      with      defining      the      following  \\n \\n  MDP:</p><p>The      state      space      S      and      action      space   \\n \\nA      correspond      to      the      input      space      1.</p><p>The      transition      dynamics      at      time      t      (i.e.,      P;)      is      an      identity      map      6(5;4;   \\n \\n=      a;).</p><p>The      reward      at      time      t   \\n \\n€      |0,---   \\n \\n,      7]      (i.e.,      r¢)      is      provided      only      at      T      as   \\n \\nr      (down-stream      reward  \\n \\n  function);      but   \\n \\n0      at      other      time      steps.</p><p>The      policy      at      time      t      (i.e,      7)      corresponds      to      pr41;_,:   \\n \\n¥   \\n \\n>      A(X).</p><p>The      initial      distribution      at      time   \\n \\n0      corresponds      to      pr,   \\n \\n€      A(4).\\nWith      slight      abuse      of      notation,  \\n \\n  we      often      denote      it      by      pr41(-|-),      while      this      is      just      pr+4(-).</p><p> \\n \\n  The      reference      policy      at      t      (i.e.,      7;)      corresponds      to   \\n \\na      denoising      process      in      the      pre-trained      model  \\n \\n  pre  \\n \\n  Pr+i-t We      list      several      things      to      note.</p><p> \\n \\n \\n¢      We      reverse      the      time-evolving      process      to      adhere      to      the      standard      notation      in      diffusion      models,  \\n \\n  i.e.,      from      t   \\n \\n=      T\\\\\\\\\\\\\\'      tot   \\n \\n=      0.\\nHence,      s;      in      standard      MDPs      corresponds      to      x74      _;      in      diffusion  \\n \\n  models.</p><p>¢\\n \\n   In      our      context,      unlike      standard      RL      scenarios,      the      transition      dynamics      are      known.</p><p>Key      RL      Problem.\\nNow,      by      reformulating      the      original      objective      of      standard      RL      into      our      contexts,  \\n \\n  the      objective      function      in      (12)      reduces      to      the      following:</p><p> \\n \\n  {pi}:   \\n \\n=      argmax      Eg,   \\n \\n3      (r(x)   \\n \\na      Deep      Epp      [KL      (pe(-|24)      lve      (-|ee))]      8)  \\n \\n  {pie      [RES      A(RY)]      Hay  \\n \\n  Reward      KL      penalty where      the      expectation      E,,,,;[-]      is      taken      with      respect      to      Tierys      Di(@r-1|@1),      Le.\\n@r   \\n \\n~      prai(-),      7-1   \\n \\n~       pr-i(-   \\n \\n|      &r-1),r_-2   \\n \\n~      pr—a(-   \\n \\n|      er_2),--+.\\nIn      this      article,      we      set      this      as      an      objective      function      in  \\n \\n  fine-tuning      diffusion      models.\\nThis      objective      is      natural      as      it      seeks      to      optimize      sequential      denoising  \\n \\n  processes      to      maximize      downstream      rewards      while      maintaining      proximity      to      pre-trained      models.</p><p> \\n \\n  Subsequently,      we      investigate      several      algorithms      to      solve      (18).\\nBefore      discussing      these      algorithms,  \\n \\n  we      summarize      several      key      theoretical      properties      that      will      aid      their      derivation.</p><p>4\\n \\n   Theory      of      RL-Based      Fine-Tuning</p><p> \\n \\n  So      far,      we      have      introduced   \\n \\na      certain      RL      problem      (i.e.,      (18))      as   \\n \\na      fine-tuning      diffusion      model.\\nIn  \\n \\n  this      section,      we      explain      that      solving      this      RL      problem      allows      us      to      achieve      the      target      distribution  \\n \\n  discussed      in      Section      1.2.1.\\nAdditionally,      we      present      several      important      theoretical      properties,      such  \\n \\n  as      the      analytical      form      of      marginal      distributions      and      posterior      distributions      induced      by      fine-tuned  \\n \\n  models.\\nThis      formulation      is      also      instrumental      in      introducing      several      algorithms      (reward-weighted  \\n \\n  MLE,      value-weighted      sampling,      and      path      consistency      learning      in      Section      6),      and      establishing  \\n \\n  connections      with      related      areas      (classifier      guidance      in      Section      8,      and      flow-based      diffusion      models      in  \\n \\n  Section      9).\\nWe      start      with      several      key      concepts.</p><li>4.1      Key      Concepts:      Soft      Value      functions      and      Soft      Bellman      Equations.</li><p> \\n \\n  Now,      reflecting      on      how      soft      optimal      policies      are      expressed      using      soft      value      functions      in      Section   \\n \\n2      in  \\n \\n  the      context      of      standard      RL      problems,      we      derive      several      important      concepts      applicable      to      fine-tuning  \\n \\n  diffusion      models.\\nThese      concepts      are      later      useful      in      constructing      algorithms      to      solve      our      RL      problem  \\n \\n  (18).</p><p>      Firstly,      as      we      see      in      (15),      soft-optimal      policies      are      characterized      as: Pe      (-|e1) expt      a(/aypl(      a9       f      exp(vr-a(ve-1)/@)      pe      (ar-1      |      v4)dx1-1 where      soft-value      functions      are      defined      as ve(es)      =      Egppy[r(@o)      —      oD      KL(pe(-|ere)      lle      Cle)      |e), 1       (a1,      t1-1)      =      Egsy(r(@o)      —      a      $5      KL(pe(-|e)      Pee      (len)      [es      era]      =      era      (1-1). k=t+1 Secondly,      as      we      see      in      (16),      the      soft-value      functions      are      also      recursively      defined      by      the      soft       Bellman      equations:       7      (22)      _      fexp      (uae)      De      (X41      |      v,)dx,-1      (t      =T+1,---      ,1), vo(%o)      =      (x0). (20)</p><p>Now      substituting      the      above      in      (19),      we      obtain sg.y   \\n \\n=      ex      (trea      )/a)eP\"C   \\n \\n|      2)  \\n \\n  PEC|e)      exp(v;      (x)      /a)      :</p><p> \\n \\n  As      mentioned      earlier,      these      soft      value      functions      and      their      recursive      form      will      later      serve      as      the  \\n \\n  basis      for      constructing      several      concrete      fine-tuning      algorithms      (such      as      reward-weighted      MLE      and       value-weighted      sampling      in      Section      6).</p><h6>2.2      Key      Concepts:      Soft      Q-functions,      Soft      Bellman      Equations</h6><p>ions.</p><p> \\n \\n  The      crucial      question      in      RL      is      how      to      devise      algorithms      that      effectively      solve      the      optimization  \\n \\n  problem      (12).\\nThese      algorithms      are      later      used      as      fine-tuning      algorithms      of      diffusion      models.\\nTo      see  \\n \\n  these      algorithms,      we      rely      on      several      critical      concepts      in      entropy-regularized      MDPs.\\nSpecifically,  \\n \\n  soft-optimal      policies      (i.e.,      solutions      to      (12))      can      be      expressed      analytically      as   \\n \\na      blend      of      soft      Q-  \\n \\n  functions      and      reference      policies.\\nFurthermore,      these      soft      Q-functions      are      defined      as      solutions      to  \\n \\n  equations      known      as      soft      Bellman      equations.\\nWe      elaborate      on      these      foundational      concepts      below.</p><p>Soft      Q-functions      and      soft      optimal      policies.\\nSoft      optimal      policies      are      expressed      as   \\n \\na      blend      of      soft  \\n \\n  Q-functions      and      reference      policies.\\nTo      see      it,      we      define      the      soft      Q-function      as      follows:</p><p>T       q(se,      a)   \\n \\n=      Egnsy      So      ral      ($4,      4%)   \\n \\n—      OBL      (m1      (-|8e41)      ley      1(-/Se41))      [S45      Ge   \\n \\n|   \\n \\n-      (13) k=t Then,      by      comparing      (13)      and      (12),      we      clearly      have m\\n \\n   =\\n \\n   argmax      Eq,~n(s,)[Ge(se,      a)   \\n \\n—      AKL(a(-|5¢)|]77(-   \\n \\n|      S2)|s¢].\\n(14)  \\n \\n  TE[XA(X)] Hence,      by      calculating      the      above      explicitly,   \\n \\na      soft      optimal      policy      in      (12)      is      described      as      follows:</p><p> \\n \\n  we)      og      _£xBlan(s.)      fant)  \\n \\n  mi      CIS)   \\n \\n&      Fe      qi(s,      a)      (ayn      (als)da      as)  \\n \\n  Soft      Bellman      equations.\\nWe      have      already      defined      soft      Q-functions      in      (13).\\nHowever,      this      form  \\n \\n  includes      the      soft      optimal      policies.\\nActually,      without      using      soft      optimal      policies,      the      soft      Q-function  \\n \\n  satisfies      the      following      recursive      equation      (a.k.a.      soft      Bellman      equation):</p><p>4(      St,      At)   \\n \\n=      Epps}      [rosuay)   \\n \\n+      alog   \\n \\n{   \\n \\n[      explara(siara)/a}n{(alsis)aa}   \\n \\n|      sia  \\n \\n.      (16)</p><p>This      is      proven      by      noting      we      recursively      have a\\n \\n   (Se,      ar)   \\n \\n=      Bers}      [re(      Se,      at)   \\n \\n+      G41      St41;      di41)   \\n \\n_      akKL      (74,4      (-|Se41),      Tai      (-|Se41))[Se,      ar|</p><p>By      substituting      (15)      into      the      above,      we      obtain      the      soft      Bellman      equation      (16).</p><p> \\n \\n  Soft      value      functions.\\nSo      far,      we      have      defined      the      soft      Q-functions,      which      depend      on      both      states  \\n \\n  and      actions.\\nWe      can      now      introduce   \\n \\na      related      concept      that      depends      solely      on      states,      termed      the      soft  \\n \\n  value      function.\\nThe      soft      value      function      is      defined      as      follows:</p><li>v4      (Sz)   \\n \\n=      tary T Tk(Sk;      Qk)   \\n \\n—      onto)</li><p>k=t Then,      the      soft      optimal      policy      in      (14)      is      also      written      as r*(-|s)   \\n \\nx      exp(q(s,      -)/o)m      Cs)  \\n \\n  ‘      exp(u;(s)/a) (17)</p><p>because      we      have exp      (ee)   \\n \\n=   \\n \\n/      exp      (a   \\n \\n)      m(a   \\n \\n|      s)da.</p><p>Then,      substituting      the      above      in      the      soft      Bellman      equation      (16),      it      is      written      as a\\n \\n   (Se,      at)   \\n \\n=      Egrs}      [7      (Se;      at)   \\n \\n+      p41      (Se41)|S¢,      ar).</p><p> \\n \\n  Algorithms      in      entropy-regularized      MDPs.\\nAs      outlined      in      Levine      (2018),      to      solve      (12),      various  \\n \\n  well-known      algorithms      exist      in      the      literature      on      RL.\\nThe      abovementioned      concepts      are      useful      in  \\n \\n  constructing      these      algorithms.\\nThese      include      policy      gradients,      which      gradually      optimize   \\n \\na      policy  \\n \\n  using   \\n \\na      policy      neural      network;      soft      Q-learning      algorithms,      which      utilize      the      soft-Bellman      equation  \\n \\n  and      approximate      the      soft-value      function      with   \\n \\na      value      neural      network;      and      soft      actor-critic      algorithms  \\n \\n  that      leverage      both      policy      and      value      neural      networks.\\nWe      will      explore      how      these      algorithms      can      be  \\n \\n  applied      in      the      context      of      diffusion      models      shortly      in      Section      4.2      and      6.</p><p>3\\n \\n   Fine-Tuning      Diffusion      Models      with      RL      in      Entropy      Regular- ized      MDPs</p><p> \\n \\n  In      this      section,      as      done      in      Fan      et      al.\\n(2023);      Black      et      al.\\n(2023);      Uehara      et      al.\\n(2024),      we      illustrate  \\n \\n  how      fine-tuning      can      be      formulated      as      an      RL      problem      in      soft-entropy      regularized      MDPs,      where      each  \\n \\n  PT   \\n \\nx      PT-1      rp_9      PT-2      P2      r1      Pl      Lo  \\n \\n  LT      T-1</p><p>Figure      2:      Formulating      fine-tuning      in      diffusion      models      using      MDPs.</p><p> \\n \\n  denoising      step      of      diffusion      models      corresponds      to   \\n \\na      policy      in      RL.\\nFinally,      we      outline   \\n \\na      specific      RL  \\n \\n  problem      of      interest      in      our      context.</p><p> \\n \\n  To      cast      fine-tuning      diffusion      models      as      an      RL      problem,      we      start      with      defining      the      following  \\n \\n  MDP:</p><p>The      state      space      S      and      action      space   \\n \\nA      correspond      to      the      input      space      1.</p><p>The      transition      dynamics      at      time      t      (i.e.,      P;)      is      an      identity      map      6(5;4;   \\n \\n=      a;).</p><p>The      reward      at      time      t   \\n \\n€      |0,---   \\n \\n,      7]      (i.e.,      r¢)      is      provided      only      at      T      as   \\n \\nr      (down-stream      reward  \\n \\n  function);      but   \\n \\n0      at      other      time      steps.</p><p>The      policy      at      time      t      (i.e,      7)      corresponds      to      pr41;_,:   \\n \\n¥   \\n \\n>      A(X).</p><p>The      initial      distribution      at      time   \\n \\n0      corresponds      to      pr,   \\n \\n€      A(4).\\nWith      slight      abuse      of      notation,  \\n \\n  we      often      denote      it      by      pr41(-|-),      while      this      is      just      pr+4(-).</p><p> \\n \\n  The      reference      policy      at      t      (i.e.,      7;)      corresponds      to   \\n \\na      denoising      process      in      the      pre-trained      model  \\n \\n  pre  \\n \\n  Pr+i-t We      list      several      things      to      note.</p><p> \\n \\n \\n¢      We      reverse      the      time-evolving      process      to      adhere      to      the      standard      notation      in      diffusion      models,  \\n \\n  i.e.,      from      t   \\n \\n=      T\\\\\\\\\\\\\\'      tot   \\n \\n=      0.\\nHence,      s;      in      standard      MDPs      corresponds      to      x74      _;      in      diffusion  \\n \\n  models.</p><p>¢\\n \\n   In      our      context,      unlike      standard      RL      scenarios,      the      transition      dynamics      are      known.</p><p>Key      RL      Problem.\\nNow,      by      reformulating      the      original      objective      of      standard      RL      into      our      contexts,  \\n \\n  the      objective      function      in      (12)      reduces      to      the      following:</p><p> \\n \\n  {pi}:   \\n \\n=      argmax      Eg,   \\n \\n3      (r(x)   \\n \\na      Deep      Epp      [KL      (pe(-|24)      lve      (-|ee))]      8)  \\n \\n  {pie      [RES      A(RY)]      Hay  \\n \\n  Reward      KL      penalty where      the      expectation      E,,,,;[-]      is      taken      with      respect      to      Tierys      Di(@r-1|@1),      Le.\\n@r   \\n \\n~      prai(-),      7-1   \\n \\n~       pr-i(-   \\n \\n|      &r-1),r_-2   \\n \\n~      pr—a(-   \\n \\n|      er_2),--+.\\nIn      this      article,      we      set      this      as      an      objective      function      in  \\n \\n  fine-tuning      diffusion      models.\\nThis      objective      is      natural      as      it      seeks      to      optimize      sequential      denoising  \\n \\n  processes      to      maximize      downstream      rewards      while      maintaining      proximity      to      pre-trained      models.</p><p> \\n \\n  Subsequently,      we      investigate      several      algorithms      to      solve      (18).\\nBefore      discussing      these      algorithms,  \\n \\n  we      summarize      several      key      theoretical      properties      that      will      aid      their      derivation.</p><p>4\\n \\n   Theory      of      RL-Based      Fine-Tuning</p><p> \\n \\n  So      far,      we      have      introduced   \\n \\na      certain      RL      problem      (i.e.,      (18))      as   \\n \\na      fine-tuning      diffusion      model.\\nIn  \\n \\n  this      section,      we      explain      that      solving      this      RL      problem      allows      us      to      achieve      the      target      distribution  \\n \\n  discussed      in      Section      1.2.1.\\nAdditionally,      we      present      several      important      theoretical      properties,      such  \\n \\n  as      the      analytical      form      of      marginal      distributions      and      posterior      distributions      induced      by      fine-tuned  \\n \\n  models.\\nThis      formulation      is      also      instrumental      in      introducing      several      algorithms      (reward-weighted  \\n \\n  MLE,      value-weighted      sampling,      and      path      consistency      learning      in      Section      6),      and      establishing  \\n \\n  connections      with      related      areas      (classifier      guidance      in      Section      8,      and      flow-based      diffusion      models      in  \\n \\n  Section      9).\\nWe      start      with      several      key      concepts.</p><li>4.1      Key      Concepts:      Soft      Value      functions      and      Soft      Bellman      Equations.</li><p> \\n \\n  Now,      reflecting      on      how      soft      optimal      policies      are      expressed      using      soft      value      functions      in      Section   \\n \\n2      in  \\n \\n  the      context      of      standard      RL      problems,      we      derive      several      important      concepts      applicable      to      fine-tuning  \\n \\n  diffusion      models.\\nThese      concepts      are      later      useful      in      constructing      algorithms      to      solve      our      RL      problem  \\n \\n  (18).</p><p>      Firstly,      as      we      see      in      (15),      soft-optimal      policies      are      characterized      as: Pe      (-|e1) expt      a(/aypl(      a9       f      exp(vr-a(ve-1)/@)      pe      (ar-1      |      v4)dx1-1 where      soft-value      functions      are      defined      as ve(es)      =      Egppy[r(@o)      —      oD      KL(pe(-|ere)      lle      Cle)      |e), 1       (a1,      t1-1)      =      Egsy(r(@o)      —      a      $5      KL(pe(-|e)      Pee      (len)      [es      era]      =      era      (1-1). k=t+1 Secondly,      as      we      see      in      (16),      the      soft-value      functions      are      also      recursively      defined      by      the      soft       Bellman      equations:       7      (22)      _      fexp      (uae)      De      (X41      |      v,)dx,-1      (t      =T+1,---      ,1), vo(%o)      =      (x0). (20)</p><p>Now      substituting      the      above      in      (19),      we      obtain sg.y   \\n \\n=      ex      (trea      )/a)eP\"C   \\n \\n|      2)  \\n \\n  PEC|e)      exp(v;      (x)      /a)      :</p><p> \\n \\n  As      mentioned      earlier,      these      soft      value      functions      and      their      recursive      form      will      later      serve      as      the  \\n \\n  basis      for      constructing      several      concrete      fine-tuning      algorithms      (such      as      reward-weighted      MLE      and       value-weighted      sampling      in      Section      6).</p><h3>4.2      Induced      Distributions      after      Fine-Tuning</h3><p>ning Now,      with      the      above      preparation,      we      can      show      that      the      induced      distribution      derived      by      this      soft  \\n \\n  optimal      policy      is      actually      equal      to      our      target      distribution.</p><p>Theorem   \\n \\n1      (Theorem   \\n \\n|      in      Uehara      et      al.      (2024)).\\nLet      p*(-)      be      an      induced      distribution      at      time   \\n \\n0 from optimal policies {pf }j_7 41) 1-e P*(%o) = Tera pt (a@1-1|@1) }dx1.r.\\nThe distribution p* is      equal      to      the      target      distribution      (10),      i.e., p*(a)   \\n \\n=      p,(2).</p><p> \\n \\n  This      theorem      states      that      after      solving      (18)      and      obtaining      soft      optimal      policies,      we      can      sample  \\n \\n  from      the      target      distribution      by      sequentially      running      policies      from      p7.,,      to      pj.\\nThus,      (18)      serves      as   \\n \\na       natural      objective      for      fine-tuning.\\nThis      fact      is      also      useful      in      deriving   \\n \\na      connection      with      classifier  \\n \\n  guidance      in      Section      8.</p><p>Marginal      distributions.\\nWe      can      derive      the      marginal      distribution      as      follows.</p><p>Theorem   \\n \\n2      (Theorem   \\n \\n2      in      Uehara      et      al.      (2024)      ).\\nLet      ps(x,)      be      the      marginal      distributions      at      t       induced      by      soft-optimal      policies      {p¥}}_741,      i.e.\\nDi(@0)   \\n \\n=      f(T]      Pe(@e—1zn)      }dvipir.\\nThen, x\\n \\n   _\\n \\n   exp(vi(a4)/a)pP      (zt)  \\n \\n  Pr      (xz)      —_      C      ’</p><p>where      v;(-)      is      the      soft-value      function.</p><p>Interestingly,      the      normalizing      constant      is      independent      of   \\n \\n¢      in      the      above      theorem.</p><p>Posterior      distributions.\\nWe      can      derive      the      posterior      distribution      as      follows.</p><p> \\n \\n  Theorem   \\n \\n3      (Theorem   \\n \\n3      in      Uehara      et      al.      (2024)).\\nDenote      the      posterior      distribution      of      x4      given  \\n \\n  24_1      for      the      distribution      induced      by      soft-optimal      policies      {py      }i_7,      by      {p*}°(-   \\n \\n|      -).      We      define      the  \\n \\n  analogous      objective      for   \\n \\na      pre-trained      policy      and      denote      it      by      p?\\\\\\\\\\\\\\'*(-   \\n \\n|      -).      Then,      we      get {p\"}}(@+|@1-1)   \\n \\n=      pe      (@4|X4-1)-</p><p> \\n \\n  This      theorem      indicates      that      after      solving      (18),      the      posterior      distribution      induced      by      pre-trained  \\n \\n  models      remains      preserved.\\nThis      property      plays      an      important      role      in      constructing      PCL      (path  \\n \\n  consistency      learning)      in      Section      6.3.  \\n \\n  Remark   \\n \\n4      (Continuous-time      formulation).\\nFor      simplicity,      our      explanation      is      generally      based      on  \\n \\n  the      discrete-time      formulation.\\nHowever,      as      training      of      diffusion      models      could      be      formulated      in  \\n \\n  the      continuous-time      formulation      (Song      et      al.,      2021),      we      can      still      extend      most      of      our      discussion      of  \\n \\n  fine-tuning      in      our      tutorial      in      the      continuous-time      formulation.\\nFor      example,      the      above      Theorems      are  \\n \\n  extended      to      the      continuous      time      formulation      in      Uehara      et      al.\\n(2024).</p><p> \\n \\n  Table      1:      Description      of      each      RL      algorithm      for      fine-tuning      diffusion      models      (note      that      value-  \\n \\n  weighted      sampling      is      technically      not   \\n \\na      fine-tuning      algorithm.)      Note      (1)      “Without      learning      value  \\n \\n  functions”      refers      to      the      capability      of      algorithms      to      directly      utilize      non-differentiable      black-box  \\n \\n  reward      feedback,      bypassing      the      necessity      to      train      differentiable      reward      functions,      (2)      “Distribution-  \\n \\n  constrained”      indicates      that      the      algorithms      are      designed      to      maintain      proximity      to      pre-trained      models.</p><p> \\n \\n  Based      on      it,      the      practical      recommendation      of      algorithms      is      summarized      in      Figure      3.</p><p>Memory      Computational      Without      learning      Distribution-  \\n \\n  efficiency   \\n \\n—      efficiency      value      functions      constrained</p><p>Soft      PPO   \\n \\nv      Vv</p><p>Reward      backpropagation   \\n \\nv      v</p><p>Reward-weighted      MLE   \\n \\nv   \\n \\nv      v</p><p>Value-weighted      sampling   \\n \\nv      v</p><p>Path      consistency      learning   \\n \\nv      v</p><p>5\\n \\n   RL-Based      Fine-Tuning      Algorithms      1:      Non-Distribution-Constrained</p><h2>Approaches</h2><p> \\n \\n  So      far,      we      have      explained      how      to      frame      fine-tuning      diffusion      models      as      the      RL      problem      in  \\n \\n  entropy-regularized      MDPs.\\nMoving      forward,      we      summarize      actual      algorithms      that      can      solve      the      RL  \\n \\n  problem      of      interest      described      by      Equation      (18).\\nIn      this      section,      we      introduce      two      algorithms:      PPO       and      direct      reward      backpropagation.\\n \\n \\n  These      algorithms      are      originally      designed      to      optimize      reward      functions      directly,      meaning      they  \\n \\n  operate      effectively      even      without      entropy      regularization      (i.e.,   \\n \\na   \\n \\n=      0).\\nConsequently,      they      are      well-  \\n \\n  suited      for      generating      samples      with      high      rewards      that      may      not      be      in      the      original      training      dataset.\\nMore  \\n \\n  distribution-constrained      algorithms      that      align      closely      with      pre-trained      models      will      be      discussed      in  \\n \\n  the      subsequent      section      (Section      4.2).\\nTherefore,      we      classify      algorithms      in      this      section      (i.e.,      PPO      and       direct      reward      backpropagation)      as      non-distribution-constrained      approaches.\\nThe      whole      summary      is  \\n \\n  described      in      Table      1.</p><h3>5.1      Soft      Proximal      Policy      Optimization      (PPO)</h3><p>PPO)  \\n \\n  In      order      to      solve      Equation      (18),      Fan      et      al.\\n(2023);      Clark      et      al.\\n(2023)      propose      using      PPO      (Schulman  \\n \\n  et      al.,      2017).\\nPPO      has      been      widely      used      in      RL,      as      well      as,      in      the      literature      in      fine-tuning      LLMs,  \\n \\n  due      to      its      stability      and      simplicity.\\nIn      the      standard      context      of      RL,      this      is      especially      preferred      over  \\n \\n  Q-learning      when      the      action      space      is      high-dimensional.</p><p> \\n \\n  The      PPO      algorithm      is      described      in      Algorithm      1.\\nThis      is      an      iterative      procedure      of      updating  \\n \\n  a      parameter      7.\\nEach      iteration      comprises      two      steps:      firstly,      samples      are      generated      by      executing  \\n \\n  Algorithm   \\n \\n1      Soft      PPO</p><p> \\n \\n  1:      Require:      Pre-trained      model      {N(p(1,      t;      Opre),      77      (t))      }i_-74.1,      batch      size      m,      parameter   \\n \\na   \\n \\n€      R*,  \\n \\n  learning      rate  \\n \\n  2:      Initialize:      0;   \\n \\n=      Opre  \\n \\n  3:      for      s   \\n \\n€      [1,---      ,S]      do  \\n \\n  4:      Collect   \\n \\nm      samples      fa      (0)      }9_      from   \\n \\na      current      diffusion      model      (i.e.,      generating      by      sequen- tially      running      polices      {N(p(2z,      t;      9),      07(t))      }t_p4,      from      t   \\n \\n=      T   \\n \\n+   \\n \\n1      tot   \\n \\n=      1)  \\n \\n  5:      Update      as      follows      (several      times      if      needed):</p><h3>5.1      Soft      Proximal      Policy      Optimization      (PPO)</h3><p>PPO)  \\n \\n  In      order      to      solve      Equation      (18),      Fan      et      al.\\n(2023);      Clark      et      al.\\n(2023)      propose      using      PPO      (Schulman  \\n \\n  et      al.,      2017).\\nPPO      has      been      widely      used      in      RL,      as      well      as,      in      the      literature      in      fine-tuning      LLMs,  \\n \\n  due      to      its      stability      and      simplicity.\\nIn      the      standard      context      of      RL,      this      is      especially      preferred      over  \\n \\n  Q-learning      when      the      action      space      is      high-dimensional.</p><p> \\n \\n  The      PPO      algorithm      is      described      in      Algorithm      1.\\nThis      is      an      iterative      procedure      of      updating  \\n \\n  a      parameter      7.\\nEach      iteration      comprises      two      steps:      firstly,      samples      are      generated      by      executing  \\n \\n  Algorithm   \\n \\n1      Soft      PPO</p><p> \\n \\n  1:      Require:      Pre-trained      model      {N(p(1,      t;      Opre),      77      (t))      }i_-74.1,      batch      size      m,      parameter   \\n \\na   \\n \\n€      R*,  \\n \\n  learning      rate  \\n \\n  2:      Initialize:      0;   \\n \\n=      Opre  \\n \\n  3:      for      s   \\n \\n€      [1,---      ,S]      do  \\n \\n  4:      Collect   \\n \\nm      samples      fa      (0)      }9_      from   \\n \\na      current      diffusion      model      (i.e.,      generating      by      sequen- tially      running      polices      {N(p(2z,      t;      9),      07(t))      }t_p4,      from      t   \\n \\n=      T   \\n \\n+   \\n \\n1      tot   \\n \\n=      1)  \\n \\n  5:      Update      as      follows      (several      times      if      needed):</p><h2>      A541      <—      O5      —      7      Ve      )      )      i      {le      ))</h2><p>Co,      (i)      1).\\ng       F(a,      xt”))   \\n \\n-      Clip      Pere      so)      l—e,l+e</p><p>where  \\n \\n  1   \\n \\nm    \\n \\nY    \\n.\\n     Je,      (i)   \\n \\n4 t=T+1      i=1 p(x,      ja;      9) pai,      |al;      As) (21)</p><h3>P(x,      |e      ;      5)</h3><p>t;      0)   \\n \\n—      t:      9Pre)      ||2  \\n \\n  F4(Xo,      Lt)   \\n \\n=      —r(ap)   \\n \\n+      ole   \\n \\n?      ae   \\n \\n’      II     .</p><p>lo=0.></p><p>KL      term 6:      end      for  \\n \\n  7;      Output:      Policy      {p.(-   \\n \\n|      0s)      Jars</p><p> \\n \\n  current      policies      to      construct      the      loss      function      (inspired      by      policy      gradient      formulation);      secondly,  \\n \\n  the      parameter   \\n \\n6      is      updated      by      computing      the      gradient      of      the      loss      function.</p><p> \\n \\n  PPO      offers      several      advantages.\\nThe      approach      is      known      for      its      stability      and      relatively      straight-  \\n \\n  forward      implementation.\\nStability      comes      from      the      conservative      parameter      updates.\\nIndeed,      PPO       builds      upon      TRPO      (Schulman      et      al.,      2015),      where      parameters      are      conservatively      updated      with   \\n \\na      KL  \\n \\n  penalty      term      (between      @,,,      and      @,)      to      prevent      significant      deviation      from      the      current      parameter.\\nThis  \\n \\n  gives      us      stability      in      the      optimization      landscape.\\nFurthermore,      in      Algorithm      1,      we      do      not      necessarily  \\n \\n  need      to      rely      on      value      functions,      although      they      could      be      useful      for      variance      reduction.\\nAs      discussed  \\n \\n  in      the      subsequent      subsection,      this      can      be      advantageous      compared      to      other      methods,      especially      since  \\n \\n  learning      value      functions      can      be      challenging      in      high-dimensional      spaces,      particularly      within      the  \\n \\n  context      of      diffusion      models.</p><h3>5.2      Direct      Reward      Backpropagation</h3><p>tion  \\n \\n  Another      standard      approach      is   \\n \\na      differentiable      optimization      (Clark      et      al.,      2023;      Prabhudesai      et      al.,  \\n \\n  2023;      Uehara      et      al.,      2024),      where      gradients      are      directly      propagated      from      reward      functions      to      update  \\n \\n  policies.</p><p> \\n \\n  The      entire      algorithm      is      detailed      in      Algorithm      2.\\nThis      reward      backpropagation      entails      an      iterative  \\n \\n  process      of      updating   \\n \\na      parameter      0.\\nEach      iteration      comprises      two      steps;      firstly,      samples      are      generated  \\n \\n  by      executing      current      policies      to      approximate      the      expectation      in      the      loss      function,      which      is      directly  \\n \\n  derived      from      (18);      second,      the      current      parameter   \\n \\n@      is      updated      by      computing      the      gradient      of      the      loss  \\n \\n  Algorithm   \\n \\n2      Reward      backpropagation</p><p>1:      Require:      Pre-trained      model      {N(p(21,      t;      pre),      77      (t))      }i_-74.1,      batch      size      m,      parameter   \\n \\na   \\n \\n€      R*,  \\n \\n  learning      rate   \\n \\n7       2:      Train   \\n \\na      differentiable      reward      function      (if      reward      feedback      is      not      differentiable)  \\n \\n  3:      Initialize:      0;   \\n \\n=      Opre  \\n \\n  4:      for      s   \\n \\n€      {1,---   \\n \\n,      S|]      do  \\n \\n  5:      Collect   \\n \\nm      samples      {a      (0)      }9_»      from   \\n \\na      current      diffusion      model      (i.e.,      generating      by      sequen- tially      running      polices      {N(p(2,      t;      9),      07(t))      }i_74,      from      t   \\n \\n=      T   \\n \\n+      1tot   \\n \\n=      1)  \\n \\n  6:      Update      6,      to      0,11:</p><p> \\n \\n  Baar      Ba   \\n \\n|      ESF      (al(B)      —c      Sp      WALee      Bt)   \\n \\n—      vas\"):      Bore?\\nYY  \\n \\n  s+l      Ss   \\n \\nv   \\n \\nm   \\n \\n4   \\n \\nr      Lo   \\n \\na      20?\\n(t)      0=0.°  \\n \\n  i=1      t=T+1 (22)</p><p>7:      end      for  \\n \\n  8:      Output:      Policy      {~(-   \\n \\n|      -;4s)      inna</p><p>function.</p><p> \\n \\n  Advantages      over      PPO.      This      approach      offers      further      simplicity      in      implementation      in   \\n \\na      case      where  \\n \\n  we      already      have   \\n \\na      pre-trained      differentiable      reward      model.\\nFurthermore,      the      training      speed      is      much  \\n \\n  faster      since      we      are      directly      back-propagating      from      rewards.</p><p> \\n \\n  Potential      disadvantages      over      PPO.      Reward      backpropagation      may      face      memory      inefficiency  \\n \\n  issues.\\nHowever,      there      are      strategies      to      mitigate      this      challenge.\\nFirstly,      implementing      gradient  \\n \\n  accumulation      can      help      to      keep   \\n \\na      large      batch      size.\\nSecondly,      as      proposed      in      DRaFT      (Clark      et      al.,  \\n \\n  2023),      propagating      rewards      backward      from      time   \\n \\n0      to      k      (k      is      an      intermediate      step      smaller      than      7’)  \\n \\n  and      updating      policies      from      k      to   \\n \\n0      can      still      yield      high      performance.\\nThirdly,      drawing      from      insights      in  \\n \\n  the      literature      on      neural      SDE/ODE      (Chen      et      al.,      2018),      more      memory-efficient      advanced      techniques  \\n \\n  such      as      adjoint      methods      could      be      helpful.</p><p> \\n \\n  Another      potential      drawback      is      the      requirement      for      “differentiable”      reward      functions.\\nOften,  \\n \\n  reward      functions      are      obtained      in   \\n \\na      non-differentiable      black-box      way      (e.g.,      computational      feedback  \\n \\n  derived      from      physical      simulations).\\nIn      such      scenarios,      using      direct      backpropagation      necessitates  \\n \\n  the      learning      of      differentiable      reward      functions      even      if      accurate      reward      feedback      is      available.\\nThis  \\n \\n  learning      step      can      pose      challenges      as      it      involves      data      collection      and      constructing      suitable      reward  \\n \\n  models.</p><p>6\\n \\n   RL-Based      Fine-Tuning      Algorithms      2:      Distribution-Constrained</p><h3>P(x,      |e      ;      5)</h3><p>t;      0)   \\n \\n—      t:      9Pre)      ||2  \\n \\n  F4(Xo,      Lt)   \\n \\n=      —r(ap)   \\n \\n+      ole   \\n \\n?      ae   \\n \\n’      II     .</p><p>lo=0.></p><p>KL      term 6:      end      for  \\n \\n  7;      Output:      Policy      {p.(-   \\n \\n|      0s)      Jars</p><p> \\n \\n  current      policies      to      construct      the      loss      function      (inspired      by      policy      gradient      formulation);      secondly,  \\n \\n  the      parameter   \\n \\n6      is      updated      by      computing      the      gradient      of      the      loss      function.</p><p> \\n \\n  PPO      offers      several      advantages.\\nThe      approach      is      known      for      its      stability      and      relatively      straight-  \\n \\n  forward      implementation.\\nStability      comes      from      the      conservative      parameter      updates.\\nIndeed,      PPO       builds      upon      TRPO      (Schulman      et      al.,      2015),      where      parameters      are      conservatively      updated      with   \\n \\na      KL  \\n \\n  penalty      term      (between      @,,,      and      @,)      to      prevent      significant      deviation      from      the      current      parameter.\\nThis  \\n \\n  gives      us      stability      in      the      optimization      landscape.\\nFurthermore,      in      Algorithm      1,      we      do      not      necessarily  \\n \\n  need      to      rely      on      value      functions,      although      they      could      be      useful      for      variance      reduction.\\nAs      discussed  \\n \\n  in      the      subsequent      subsection,      this      can      be      advantageous      compared      to      other      methods,      especially      since  \\n \\n  learning      value      functions      can      be      challenging      in      high-dimensional      spaces,      particularly      within      the  \\n \\n  context      of      diffusion      models.</p><h3>5.2      Direct      Reward      Backpropagation</h3><p>tion  \\n \\n  Another      standard      approach      is   \\n \\na      differentiable      optimization      (Clark      et      al.,      2023;      Prabhudesai      et      al.,  \\n \\n  2023;      Uehara      et      al.,      2024),      where      gradients      are      directly      propagated      from      reward      functions      to      update  \\n \\n  policies.</p><p> \\n \\n  The      entire      algorithm      is      detailed      in      Algorithm      2.\\nThis      reward      backpropagation      entails      an      iterative  \\n \\n  process      of      updating   \\n \\na      parameter      0.\\nEach      iteration      comprises      two      steps;      firstly,      samples      are      generated  \\n \\n  by      executing      current      policies      to      approximate      the      expectation      in      the      loss      function,      which      is      directly  \\n \\n  derived      from      (18);      second,      the      current      parameter   \\n \\n@      is      updated      by      computing      the      gradient      of      the      loss  \\n \\n  Algorithm   \\n \\n2      Reward      backpropagation</p><p>1:      Require:      Pre-trained      model      {N(p(21,      t;      pre),      77      (t))      }i_-74.1,      batch      size      m,      parameter   \\n \\na   \\n \\n€      R*,  \\n \\n  learning      rate   \\n \\n7       2:      Train   \\n \\na      differentiable      reward      function      (if      reward      feedback      is      not      differentiable)  \\n \\n  3:      Initialize:      0;   \\n \\n=      Opre  \\n \\n  4:      for      s   \\n \\n€      {1,---   \\n \\n,      S|]      do  \\n \\n  5:      Collect   \\n \\nm      samples      {a      (0)      }9_»      from   \\n \\na      current      diffusion      model      (i.e.,      generating      by      sequen- tially      running      polices      {N(p(2,      t;      9),      07(t))      }i_74,      from      t   \\n \\n=      T   \\n \\n+      1tot   \\n \\n=      1)  \\n \\n  6:      Update      6,      to      0,11:</p><p> \\n \\n  Baar      Ba   \\n \\n|      ESF      (al(B)      —c      Sp      WALee      Bt)   \\n \\n—      vas\"):      Bore?\\nYY  \\n \\n  s+l      Ss   \\n \\nv   \\n \\nm   \\n \\n4   \\n \\nr      Lo   \\n \\na      20?\\n(t)      0=0.°  \\n \\n  i=1      t=T+1 (22)</p><p>7:      end      for  \\n \\n  8:      Output:      Policy      {~(-   \\n \\n|      -;4s)      inna</p><p>function.</p><p> \\n \\n  Advantages      over      PPO.      This      approach      offers      further      simplicity      in      implementation      in   \\n \\na      case      where  \\n \\n  we      already      have   \\n \\na      pre-trained      differentiable      reward      model.\\nFurthermore,      the      training      speed      is      much  \\n \\n  faster      since      we      are      directly      back-propagating      from      rewards.</p><p> \\n \\n  Potential      disadvantages      over      PPO.      Reward      backpropagation      may      face      memory      inefficiency  \\n \\n  issues.\\nHowever,      there      are      strategies      to      mitigate      this      challenge.\\nFirstly,      implementing      gradient  \\n \\n  accumulation      can      help      to      keep   \\n \\na      large      batch      size.\\nSecondly,      as      proposed      in      DRaFT      (Clark      et      al.,  \\n \\n  2023),      propagating      rewards      backward      from      time   \\n \\n0      to      k      (k      is      an      intermediate      step      smaller      than      7’)  \\n \\n  and      updating      policies      from      k      to   \\n \\n0      can      still      yield      high      performance.\\nThirdly,      drawing      from      insights      in  \\n \\n  the      literature      on      neural      SDE/ODE      (Chen      et      al.,      2018),      more      memory-efficient      advanced      techniques  \\n \\n  such      as      adjoint      methods      could      be      helpful.</p><p> \\n \\n  Another      potential      drawback      is      the      requirement      for      “differentiable”      reward      functions.\\nOften,  \\n \\n  reward      functions      are      obtained      in   \\n \\na      non-differentiable      black-box      way      (e.g.,      computational      feedback  \\n \\n  derived      from      physical      simulations).\\nIn      such      scenarios,      using      direct      backpropagation      necessitates  \\n \\n  the      learning      of      differentiable      reward      functions      even      if      accurate      reward      feedback      is      available.\\nThis  \\n \\n  learning      step      can      pose      challenges      as      it      involves      data      collection      and      constructing      suitable      reward  \\n \\n  models.</p><p>6\\n \\n   RL-Based      Fine-Tuning      Algorithms      2:      Distribution-Constrained</p><h2>Approaches</h2><p> \\n \\n  In      this      section,      following      Section      4.2,      we      present      three      additional      algorithms      (reward-weighted  \\n \\n  MLE,      value-weighted      sampling,      and      path      consistency      learning)      aimed      at      solving      the      RL      problem  \\n \\n  of      interest      defined      by      Equation      (18).\\nIn      the      context      of      standard      RL,      these      algorithms      are      tailored  \\n \\n  to      align      closely      with      reference      policies,      specifically      pre-trained      diffusion      models      in      our      context.\\n \\n \\n  Formally,      indeed,      all      algorithms      in      this      section      are      not      well-defined      when   \\n \\na   \\n \\n=   \\n \\n0      (1.e.,      without  \\n \\n  entropy      regularization).\\nHence,      we      categorize      these      three      algorithms      as      distribution-constrained  \\n \\n  approaches.</p><p> \\n \\n  The      algorithms      in      this      section      excel      in      preserving      the      characteristics      of      pre-trained      diffusion  \\n \\n  models.\\nPractically,      this      property      becomes      especially      crucial      when      reward      functions      are      learned      from  \\n \\n  training      data,      and      we      want      to      avoid      being      fooled      by      distribution      samples      (a.k.a.      overoptimization      as  \\n \\n  detailed      in      Section      7.3).\\nHowever,      as   \\n \\na      caveat,      this      property      might      also      pose      challenges      in      effectively  \\n \\n  generating      high-reward      samples      beyond      the      training      data.\\nThis      implies      that      these      approaches      may  \\n \\n  not      be      suitable      when      accurate      reward      feedback      is      readily      available      without      learning.\\nHence,      we  \\n \\n  generally      recommend      using      them      when      reward      functions      are      unknown.</p><h3>6.1      Reward-Weighted      MLE</h3><p>MLE  \\n \\n  Here,      we      elucidate      an      approach      based      on      reward-weighted      MLE      (Peters      et      al.,      2010),   \\n \\na      technique  \\n \\n  commonly      employed      in      offline      RL      (Peng      et      al.,      2019).\\nWhile      Fan      et      al.\\n(2023,      Algorithm      2)      and       Zhang      and      Xu      (2023)      propose      variations      of      reward-weighted      MLE      for      diffusion      models,      the      specific  \\n \\n  formulation      of      reward-weighted      MLE      discussed      here      does      not      seem      to      have      been      explicitly      detailed  \\n \\n  previously.\\nTherefore,      unlike      the      previous      section,      we      start      by      outlining      the      detailed      rationale      for  \\n \\n  this      approach.\\nSubsequently,      we      provide   \\n \\na      comprehensive      explanation      of      the      algorithm.\\nFinally,      we  \\n \\n  delve      into      its      connection      with      the      original      training      loss      of      diffusion      models.</p><p>Motivation.\\nFirst,      from      Theorem      2,      recall      the      form      of      the      optimal      policy      p¥(x_1|x,):</p><p>exp(v¢—-1      (+1)      /a:)      pe      (a1      |) exp(u;(x;)/@)  \\n \\n  Py      (Lt-1|2t)   \\n \\n= Now,      we      have:</p><p>p=      argmin      Ey,xu,[KL(p;      (-|2+)||Pe(-|2+))|  \\n \\n  pt:X      A(X) where      u,   \\n \\n€      A(%)      is   \\n \\na      roll-in      distribution      encompassing      the      entire      space   \\n \\nV      This      can      be      reformulated  \\n \\n  as      value-weighted      MLE      as      follows.</p><p>Lemma   \\n \\n1      (Value-weighted      MLE).\\nWhen      II,   \\n \\n=      [¥   \\n \\n—      A(4)],      the      policy      pt      is      equal      to</p><p>Ut—-1\\\\\\\\\\\\\\\\      Tt-1  \\n \\n  PC)   \\n \\n=      Ar      gMAX      Bey,      sph      (ne)      seeme      lex»      (eaGey      ’)      log      r(e-aley)     .</p><p>t      t       This      lemma      illustrates      that      if      v,-;      is      known,      py      can      be      estimated      using      weighted      maximum  \\n \\n  likelihood      estimation      (MLE).\\nWhile      this      formulation      is      commonly      used      in      standard      RL      (Peng      et      al.,  \\n \\n  2019),      in      our      context      of      fine-tuning      diffusion      models,      learning   \\n \\na      value      function      is      often      challenging.\\n \\n \\n  Interestingly,      this      reward-weighted      MLE      can      be      performed      without      directly      estimating      the      soft      value  \\n \\n  function      after      proper      reformulation.\\nTo      demonstrate      this,      let’s      utilize      the      following      lemma:</p><h3>6.1      Reward-Weighted      MLE</h3><p>MLE  \\n \\n  Here,      we      elucidate      an      approach      based      on      reward-weighted      MLE      (Peters      et      al.,      2010),   \\n \\na      technique  \\n \\n  commonly      employed      in      offline      RL      (Peng      et      al.,      2019).\\nWhile      Fan      et      al.\\n(2023,      Algorithm      2)      and       Zhang      and      Xu      (2023)      propose      variations      of      reward-weighted      MLE      for      diffusion      models,      the      specific  \\n \\n  formulation      of      reward-weighted      MLE      discussed      here      does      not      seem      to      have      been      explicitly      detailed  \\n \\n  previously.\\nTherefore,      unlike      the      previous      section,      we      start      by      outlining      the      detailed      rationale      for  \\n \\n  this      approach.\\nSubsequently,      we      provide   \\n \\na      comprehensive      explanation      of      the      algorithm.\\nFinally,      we  \\n \\n  delve      into      its      connection      with      the      original      training      loss      of      diffusion      models.</p><p>Motivation.\\nFirst,      from      Theorem      2,      recall      the      form      of      the      optimal      policy      p¥(x_1|x,):</p><p>exp(v¢—-1      (+1)      /a:)      pe      (a1      |) exp(u;(x;)/@)  \\n \\n  Py      (Lt-1|2t)   \\n \\n= Now,      we      have:</p><p>p=      argmin      Ey,xu,[KL(p;      (-|2+)||Pe(-|2+))|  \\n \\n  pt:X      A(X) where      u,   \\n \\n€      A(%)      is   \\n \\na      roll-in      distribution      encompassing      the      entire      space   \\n \\nV      This      can      be      reformulated  \\n \\n  as      value-weighted      MLE      as      follows.</p><p>Lemma   \\n \\n1      (Value-weighted      MLE).\\nWhen      II,   \\n \\n=      [¥   \\n \\n—      A(4)],      the      policy      pt      is      equal      to</p><p>Ut—-1\\\\\\\\\\\\\\\\      Tt-1  \\n \\n  PC)   \\n \\n=      Ar      gMAX      Bey,      sph      (ne)      seeme      lex»      (eaGey      ’)      log      r(e-aley)     .</p><p>t      t       This      lemma      illustrates      that      if      v,-;      is      known,      py      can      be      estimated      using      weighted      maximum  \\n \\n  likelihood      estimation      (MLE).\\nWhile      this      formulation      is      commonly      used      in      standard      RL      (Peng      et      al.,  \\n \\n  2019),      in      our      context      of      fine-tuning      diffusion      models,      learning   \\n \\na      value      function      is      often      challenging.\\n \\n \\n  Interestingly,      this      reward-weighted      MLE      can      be      performed      without      directly      estimating      the      soft      value  \\n \\n  function      after      proper      reformulation.\\nTo      demonstrate      this,      let’s      utilize      the      following      lemma:</p><h2>Algorithm      3      Reward-weighed      MLE</h2><p>:\\n \\n   Require:      Pre-trained      model      {N(p(+,      t;      Opre),      07      (t))      }i-741,      batch      size      m,      parameter   \\n \\na   \\n \\n€      Rt, learning      rate   \\n \\n7 — 2:      Initialize:      6;   \\n \\n=      Opre  \\n \\n  3:      for      s   \\n \\n€      [1,---      ,S]      do 4.      fork   \\n \\n€      [2      +1,---      ,1]      do  \\n \\n  5:      Collect   \\n \\nm      samples      {aye      from</p><p> \\n \\n \\na      policy      pr+i(:   \\n \\n|      -3Os)i-°+      Peer|5      9s),      PP      Cl).\\nPr      Cl).\\n \\n \\n  6:      end      for  \\n \\n  7:      Update      @,      to      6,1;      as      follows:</p><p>1\\n \\n   m\\n \\n   (i,t)      (i,t)      (i,t)      4,   \\n \\n2       r(x      XL,   \\n \\n1   \\n \\n—      play’,      t3   \\n \\n0 t=T+1      i=1   \\n \\no      tor}</p><p>8:      end      for  \\n \\n  9:      Output:      Policy      {p:(-   \\n \\n|      39s)      }ir41</p><p>Lemma   \\n \\n2      (Characterization      of      soft      optimal      value      functions).</p><p>v,(@e)      ro0) exp      (ee)   \\n \\n~      Bey      mpP      (a1),      tr_1      pe,      (et)      lex»   \\n \\n(   \\n \\na   \\n \\n-     .</p><p>Recall      Eg,py|-|24]      means      E       zo~pP\"®      (1)      ae-1p?™S      (e)      [|e]:</p><p>Proof.\\nThis      is      obtained      by      recursively      using      the      soft-Bellman      equation      (20):</p><p>en      (22D)   \\n \\n&      few      (2%)      spears      ates   \\n \\n=      >=      Bie      fxn      (2) (23)</p><h1>O</h1><p>Algorithm.\\nNow,      we      are      ready      to      present      the      algorithm.\\nBy      combining      Lemma   \\n \\n|      and      Lemma      2,  \\n \\n  we      obtain      the      following.</p><p>Lemma   \\n \\n3      (Reward-weighted      MLE).\\nWhen      Il,   \\n \\n=      [¥   \\n \\n>      A(X)], *\\n \\n   r(x)</p><p>Pye   \\n \\n=      ATBMAX      Ep   \\n \\ny      mph      (1)y--      ee      a~pP      (este      exw   \\n \\n(   \\n \\na   \\n \\n)      tor      r(eale)   \\n \\n,      (25) Pt      t       Proof.\\nUsing      Lemma      2,      we      have Up—-1(Lt-1)  \\n \\n  By,      ywpPP      (-[ae),cevur      lexp   \\n \\n(   \\n \\na   \\n \\n)      log      r(esale)</p><p>r(x)  \\n \\n  The      rest      of      the      proof      is      obvious      by      using      Lemma      1.   \\n \\nO 0  \\n \\n  =      Ba      spledirnn      [Bp      [exp      (TE)      joa]      town      (esal)</p><p>r(x) =\\n \\n   Legg?\"\\n(01)      yeep\",      (ae)      eewur      exw      (a)      log      r(es-al)      :</p><p> \\n \\n  Then,      after      approximating      the      expectation      in      (25),      by      using   \\n \\na      Gaussian      policy      class      with      the  \\n \\n  mean      parameterized      by      neural      networks      as   \\n \\na      policy      class      II;,      we      can      estimate      p;.\\nFinally,      the      entire  \\n \\n  algorithm      is      described      in      Algorithm      3.  \\n \\n  Here,      we      give      two      remarks.\\nThis      is      an      off-policy      algorithm.\\nHence,      we      can      use      any      roll-in  \\n \\n  policies      as      wu;      in      Lemma      3.\\nIn      Algorithm      3,      we      use      the      current      policy      as   \\n \\na      roll-in      policy.\\nAdditionally,  \\n \\n  in      Algorithm      3,      the      loss      function      (24)      is      derived      by      recalling      that,      up      to      constant,   \\n \\n—      log      p;(2_1|2:)  \\n \\n  is      equal      to   \\n \\nA      ;</p><p> \\n \\n  Advantages      and      potential      disadvantages.\\nLike      PPO      in      Section      5.1,      this      approach      is      expected      to  \\n \\n  be      memory      efficient,      and      does      not      require      learning      differentiable      reward      functions,      which      can      often  \\n \\n  be      challenging.\\nHowever,      compared      to      direct      reward      backpropagation      Section      5.2,      it      might      not      be  \\n \\n  as      computationally      efficient.\\nFurthermore,      unlike      PPO,      this      algorithm      is      not      effective      when   \\n \\na   \\n \\n=      0,  \\n \\n  potentially      limiting      its      ability      to      generate      samples      with      extremely      high      rewards.</p><h2>6.1.1      Relation      with      Loss      Functions      for      the      Original      Training      Objective</h2><p>tive In      this      subsection,      we      explore      the      connection      with      the      original      loss      function      of      pre-trained      diffusion  \\n \\n  models.\\nTo      see      that,      as      we      see      in      Section      1.1.1,      recall ol)   \\n \\n=      al   \\n \\n4      (0.50,   \\n \\n—      1/o%eg,,,      (a1,      T   \\n \\n—      t)](5t)      +e?\\n02,      €)   \\n \\n~      N(0,1).</p><p> \\n \\n \\n/       \\\\\\\\\\\\\\\\-  \\n \\n  (al?\\nt;Opre) Then,      the      loss      function      (24)      in      reward-weighted      MLE      reduces      to</p><table><th><td colSpan=1>      =      r(x”)       A,      —      Vo      S>      Ss\"      exp      |      ~~      —       t=T+1      i=1</td><td colSpan=1>      .      _      ;      2       (i,t)      (ay,      Tt;      Opre)      _      eal,      T      -t;      0)       ey      _       {or}?</td><td colSpan=1>|o=0</td></th><tr><td colSpan=1></td><td colSpan=1>      2</td><td colSpan=1>      (26)</td></tr></table><p>This      objective      function      closely      resembles      the      reward-weighted      version      of      the      loss      function      (5)      used  \\n \\n  for      training      pre-trained      diffusion      models.</p><h3>6.2      Value-Weighted      Sampling</h3><p>ling  \\n \\n  Thus      far,      we      have      discussed      methods      for      fine-tuning      pre-trained      diffusion      models.\\nNow,      let’s      delve  \\n \\n  into      an      alternative      approach      during      inference      that      aims      to      sample      from      the      target      distribution      p,.\\n \\n \\n  without      explicitly      fine-tuning      the      diffusion      models.\\nIn      essence,      this      approach      involves      incorporating  \\n \\n  gradients      of      value      functions      during      inference      alongside      the      denoising      process      in      pre-trained      diffusion  \\n \\n  models.\\nHence,      we      refer      to      this      approach      as      value-weighted      sampling.\\nWhile      it      seems      that      this  \\n \\n  method      has      not      been      explicitly      formalized      in      previous      literature,      the      value-weighted      sampling  \\n \\n  closely      connects      with      classifier      guidance,      as      discussed      in      Section      8.1.  \\n \\n  Before      delving      into      the      algorithmic      details,      we      outline      the      motivation.\\nSubsequently,      we      present  \\n \\n  the      concrete      algorithm      and      discuss      its      advantages—specifically,      its      capability      to      operate      without      the  \\n \\n  necessity      of      fine-tuning      diffusion      models—and      its      disadvantage,      which      involves      the      need      to      obtain  \\n \\n  differentiable      value      functions.</p><p>Algorithm   \\n \\n4      Value-weighted      sampling 1:      Require:      Pre-trained      model      {p?\\n\"*(2,_1|r1)      }:   \\n \\n=      {N(0(      22,      t;      Opre),      07(t))      be.\\n \\n \\n  2:      Estimate   \\n \\nv   \\n \\n:   \\n \\n¥   \\n \\nx      [0,7]   \\n \\n—   \\n \\nR      and      denote      it      by      i(.,      -) ¢\\n \\n   (1)      Monte-Carlo      approach      in      (28) e      (2)      Value      iteration      approach      (Soft      Q-learning)      in      (29)      in      Section      6.2.1 ¢\\n \\n   (3)      Approximation      using      Tweedie’s      formula      in      Section      6.2.2 3:      fort   \\n \\n€      [7      +1,---   \\n \\n,      1}      do  \\n \\n  4:      Set.\\na?\\nt      Ve0   \\n \\nx      xL=Lt  \\n \\n  P(t,      t)   \\n \\n=   \\n \\n(   \\n \\n)      \\\\\\\\\\\\\\\\   \\n \\nu   \\n \\n+      p(xz,t      Are)</p><p>5:      end      for  \\n \\n  6:      Output:      {N(p      O(      Le,      t),o7(t))      ery</p><p> \\n \\n  Motivation.\\nConsidering   \\n \\na      Gaussian      policy      7:1   \\n \\n~      N(/(2:,t),07(t)),      we      aim      to      determine  \\n \\n  P(x4,t;      0)      such      that      N(A(2x;,      t;      0),      0?(t))      closely      approximates      p*(-|x,).\\nHere,      typically,      we      have  \\n \\n  p(xz,t;0)   \\n \\n=      a,   \\n \\n+      (t)g(a;,t)      and      o?(t)   \\n \\n=      g(t)(ot)      for      certain      function      g   \\n \\n:   \\n \\nY   \\n \\nx      [0,7]   \\n \\n>      R¢  \\n \\n  and      g   \\n \\n:      [0,7]   \\n \\n—      R,      as      we      have      explained      in      Section      1.1.1.\\nThen,      using   \\n \\nx      as      equality      up      to      the  \\n \\n  normalizing      constant, Piles      |)      exp(eralar-s)/a)      exp      (BEE      OI)</p><p>2\\n \\n   exp(vy(am)   \\n \\n+      Ver(      ee)»      {a1   \\n \\n—      3)      exp      (—      “2      Clr      uF)</p><p>_\\n \\n   lve1   \\n \\n—      ae   \\n \\n—      (58)      9(t)      Vora)      /o   \\n \\n—      (6t)9(ae,      t))      |?\\n \\n \\n  esp      (-      0.59(#)      (6t)      )</p><h4>Hence,      we      can      approximate:</h4><p> \\n \\n  o°(t)      Vu;      (a4)  \\n \\n  a   \\n \\n+      p(      xz,      ti      Ore):      (27)  \\n \\n  P(x,      t)      od</p><p>Remark      5.\\nNote      while      the      above      derivation      is      heuristic,      it      is      formalized      in      Uehara      et      al.\\n(2024,  \\n \\n  Lemma      2)      in      the      continuous-time      formulation.</p><p> \\n \\n  Algorithm.\\nUtilizing      (27),      the      entire      algorithm      of      value-weighted      sampling      is      outlined      in      Algo-  \\n \\n  rithm      4.\\nThis      method      doesn’t      involve      updating      parameters      in   \\n \\na      diffusion      model;      hence,      it’s      not   \\n \\na       fine-tuning      method.</p><p> \\n \\n  Note      that      in      this      algorithm,      we      require   \\n \\na      form      of      v;(-)      to      compute      gradients.\\nThis      value      function  \\n \\n  can      be      estimated      through      regression      using      the      characterization      described      in      Lemma      2.\\nHere,      inspired  \\n \\n  by      Lemma      2,      we      use      the      following      loss      function:</p><p>1\\n \\n   om      (i,t)      (i,t)   \\n \\n°       o(2)   \\n \\n=      argmin      S-      S-      {es      (me)   \\n \\n—      exp   \\n \\n|   \\n \\n,      (28)</p><p>h:[R4,{0,T]]-R      t=T+1      i=1</p><p>where      the      data      is      collected      in      an      off-policy      way.\\nLater,      we      explain      two      alternative      approaches      in  \\n \\n  Section      6.2.1      and      Section      6.2.2.</p><p> \\n \\n  Advantages      and      potential      disadvantages.\\nThe      value-weighted      sampling      is      straightforward      and       less      memory-intensive      since      it      avoids      fine-tuning.\\nTherefore,      it      presents      an      appealing      simple      option  \\n \\n  if      it      performs      well.\\nIndeed,      in      various      inverse      problems,      such      as      inpainting,      super-resolution,      and       colorization,      setting      r()      as   \\n \\na      likelihood      of      the      measurement      model   \\n \\ny   \\n \\n=      g(a)      +e      (where   \\n \\ny      represents  \\n \\n  actual      measurements,   \\n \\n€      denotes      noise,      and      g      defines      the      measurement      function)      has      proven      highly  \\n \\n  successful      (Chung      et      al.,      2022;      Bansal      et      al.,      2023;      Chung      et      al.,      2022).</p><p> \\n \\n  The      potential      drawback      compared      to      fine-tuning      algorithms      so      far      (i.e.,      PPO      and      reward-  \\n \\n  weighted      MLE)      is      the      necessity      to      learn      differetiable      soft      value      functions      like      direct      reward      back-  \\n \\n  propagation.\\nThis      learning      process      is      often      not      straightforward      as      explained      in      Section      5.2.\\nThe  \\n \\n  previously      discussed      fine-tuning      algorithms      (i.e.,      PPO,      reward-weighted      MLE)      circumvent      this  \\n \\n  requirement      by      obtaining      rewards      to      go      via      Monte      Carlo      approaches,      thereby      avoiding      the      need      for  \\n \\n  soft      value      functions.</p><h2>6.2.1      Soft      Q-learning</h2><p>ning  \\n \\n  We      have      elucidated      that      leveraging      Lemma      2,      we      can      estimate      soft      value      functions      v;(-)      based  \\n \\n  on      Equation      (28)      in   \\n \\na      Monte      Carlo      way.\\nSubsequently,      these      soft      value      functions      are      used      in  \\n \\n  value-weighted      sampling      to      sample      from      the      target      distribution      p,.\\nAlternatively,      there      is      another  \\n \\n  method      that      involves      using      soft      Bellman      equations      to      estimate      soft      value      functions      v;(-).\\nThis  \\n \\n  technique      is      commonly      called      soft      Q-learning      in      the      context      of      standard      RL.</p><p> \\n \\n  First,      recalling      the      soft      Bellman      equations      in      (16),      we      have en      (22)   \\n \\n=      fey      (=)      pa      nd</p><p>Taking      the      logarithm,      we      obtain u(ay)   \\n \\n=      alog   \\n \\nf      exp      (eaen)      De      (@4-1   \\n \\n|      @4)dxy-1.</p><p>a</p><h3>Hence,</h3><p> \\n \\n \\n_   \\n \\n:      (xt)      Up-1(Lt-1)   \\n \\n°       vu   \\n \\n=      argmin      Ey,.u,   \\n \\n|   \\n \\n4      ——   \\n \\n—      log   \\n \\n|      exp   \\n \\n|      ————   \\n \\n]      Dpre(@e-1]      21)      da4_1  \\n \\n  a   \\n \\na h:X->R  \\n \\n  where      u,   \\n \\n€      A(%)      is      any      roll-in      distribution      that      covers      the      entire      space      V.      Using      this      relation      and       replacing      the      expectation      E,,,.,,,      with      empirical      approximation,      we      are      able      to      estimate      soft      value  \\n \\n  functions      v;      in   \\n \\na      recursive      manner:</p><p> \\n \\n \\n1   \\n \\nm      ~(j—1)   \\n \\n°       aj   \\n \\n;   \\n \\ni      Ur      Ut   \\n \\ny       {6      (x)},   \\n \\n—       argmin   \\n \\ny      S      {ht   \\n \\n)   \\n \\n—      log   \\n \\n/      exp      (eee)      Ppre(@1—1|}      yan] h:      [R¢,[0,7]]|-R      t=T+1      i=1</p><p>where      the      data      is      collected      in      an      off-policy      way.</p><p> \\n \\n  Remark      6.\\nAlthough      soft      Q-learning      is      widely      used      in      standard      RL      (Schulman      et      al.,      2017),      it  \\n \\n  cannot      be      directly      applied      to      our      fine-tuning      context      without      resorting      to      value-weighted      sampling  \\n \\n  or      value-weighted      MLE.\\nThis      is      because,      even      if      we      estimate      soft-value      functions      as      %;,      substituting  \\n \\n  Uz      with      0,      in      the      soft-optimal      policy      results      in      an      unnormalized      policy.<ul><li>6.2.2.\\nApproximation      using      Tweedie’s      formula</li></ul></p><p> \\n \\n  So      far,      we      have      explained      two      approaches:   \\n \\na      Monte      Carlo      approach      and   \\n \\na      value      iteration      approach      to  \\n \\n  estimate      soft      value      functions.\\nHowever,      learning      value      functions      in      (28)      can      still      be      often      challenging  \\n \\n  in      practice.\\nTherefore,      we      can      employ      approximation      strategies      inspired      by      recent      literature      on  \\n \\n  classifier      guidance      (e.g.,      reconstruction      guidance      (Ho      et      al.,      2022),      manifold      constrained      gradients  \\n \\n  (Chung      et      al.,      2022),      universal      guidance      (Bansal      et      al.,      2023),      and      diffusion      posterior      sampling  \\n \\n  (Chung      et      al.,      2022)).</p><p> \\n \\n  Specifically,      we      adopt      the      following      approximation:</p><p>ur(xz)   \\n \\n=      alog      Egpry      lex      (“)      oy   \\n \\n=      alog      (/      exp      (om)   \\n \\np      (colar      (30)</p><p>~\\n \\n   alog      (exw      (ceo)   \\n \\n)   \\n \\n»       £o(t1)   \\n \\n=      Egprey|xo   \\n \\n|      xe],      (31) a\\n \\n    =\\n \\n   1r(Xo(2t)).</p><p> \\n \\n  Here,      we      replace      the      integration      in      (30)      with   \\n \\na      Dirac      delta      distribution      with      the      posterior      mean.\\n \\n \\n  Importantly,      we      can      calculate      Zo      (x;)   \\n \\n=      E,»*-[xo   \\n \\n|      x4]      using      the      pre-trained      (score-based)      diffusion  \\n \\n  model      based      on      Tweedie’s      formula:</p><table><tr><td colSpan=1>      O12</td><td colSpan=1><p>]\\n \\n \\n  E,pre      [xo   \\n \\n|      x1   \\n \\n—      Ut   \\n \\n+      {or}   \\n \\nV      0g      Ge(&e)  \\n \\n  Le</p></td></tr></table><p>Recall      that      the      notation      p/?,      0?,      q,      are      defined      in      (3).\\nFinally,      by      recalling   \\n \\nV      log   \\n \\n@   \\n \\n=      Si      ore      (x;,t)      in  \\n \\n  score-based      diffusion      models,      we      can      approximate      Vv;(a)      with</p><h4>Le</h4><p> \\n \\n  and      plug      it      into      Algorithm   \\n \\n4      (i.e.,      value-weighted      sampling).\\n \\n \\n  As      mentioned      earlier,      this      approach      has      been      practically      widely      used      in      the      context      of      classifier  \\n \\n  guidance.\\nDespite      its      simplicity,      the      approximation      error      can      be      significant,      as      it      can      not      be  \\n \\n  diminished      even      with   \\n \\na      large      sample      size      because      the      discrepancy      from      (30)      to      (31)      is      not      merely   \\n \\na       statistical      error      that      diminishes      with      increasing      sample      size.</p><li>6.2.3.\\nZeroth-Order      Guidance      using      Path      Integral      Control</li><p> \\n \\n  In      the      previous      subsection      (Section      6.2.2),      we      discussed      an      approximation      technique      to      bypass      the  \\n \\n  necessity      of      learning      explicit      value      functions.\\nAnother      approach      to      bypass      this      requirement      is      to      use  \\n \\n  control-based      techniques      for      obtaining      soft-optimal      policies,      a.k.a.,      path      integral      control      (Theodorou  \\n \\n  et      al.,      2010;      Williams      et      al.,      2017;      Kazim      et      al.,      2024).</p><p> \\n \\n  Recall      that      we      have      p(x;,t;      6)   \\n \\n=      2;   \\n \\n+      (dt)g(x+,      t)      and      o°(t)   \\n \\n=      g(t)(dt),      the      motivation      behind this      approach      is      as      follows.\\nInitially,      we      have:</p><h5>Vo,      exp(v(#1)/@)</h5><p>=\\n \\n   Egppreyexp(vr_1      (22-1)      /@)   \\n \\n|      24]      (Soft      Bellman      equation) _\\n \\n   (1   \\n \\n—   \\n \\nx   \\n \\n—      (Ot)      G(a2,      t))?\\n=\\n \\n   Vz»,   \\n \\n{   \\n \\n[      explatena/ay)      exp      (-      0-5a(t)      (ot)   \\n \\n)      in,-}</p><p>=\\n \\n   Byes      [ese      a(era/a))      {ea      ~~      (61)g(nn,)}      EE      Cah</p><p>~\\n \\n   arp      Buty      lexr(era(ea/a))      Gera   \\n \\n—      21   \\n \\n~      (8t)ale      plod</p><p>_t   \\n \\n1       Ht)      ot)      Eg,prey      [E      ppp}      [exp(r(2o)/a)   \\n \\n|      T-1]      e|24|   \\n \\n=      Woon)      [exp(r(xo/a))e|x¢]      ;</p><h5>Now,      we      obtain:</h5><p>o7(t)Vnv(ae)   \\n \\n1      Eqppry      lexp(r(wo)/a)      eel  \\n \\n  Q   \\n \\n~   \\n \\na   \\n \\n*      Egyprey      lexp(r(2o)/a))      |x]      (32)</p><p> \\n \\n  Importantly,      this      approach      does      not      require      any      differentiation.\\nTherefore,      by      running      policies  \\n \\n  from      pre-trained      models      and      simply      using      Monte      Carlo      approximation      in      both      the      denominator  \\n \\n  and      numerator,      we      can      estimate      the      above      quantity      (the      right-hand      side      of      (32))      without      making  \\n \\n  any      models      (classifiers),      unlike      classifier      guidance.\\nNote      while      this      approach      is      widely      used      in      the  \\n \\n  control      community,      it      may      not      be      feasible      for      diffusion      models      due      to      the      high      dimensionality      of  \\n \\n  the      input      space.</p><h3>6.3      Path      Consistency      Learning      (Losses      Often      Used      in      Gflownets)</h3><p>ets)  \\n \\n  Now,      we      explain      how      to      apply      path      consistency      learning      (PCL)      (Nachum      et      al.,      2017)      to      fine-  \\n \\n  tune      diffusion      models.\\nIn      the      Gflownets      literature      (Bengio      et      al.,      2023),      it      seems      that      this      variant      is  \\n \\n  utilized      as      either   \\n \\na      detailed      balance      or   \\n \\na      trajectory      balance      loss,      as      discussed      in      Mohammadpour  \\n \\n  et      al.\\n(2023);      Tiapkin      et      al.\\n(2023);      Deleu      et      al.\\n(2024).\\nHowever,      to      the      best      of      our      knowledge,  \\n \\n  the      precise      formulation      of      path      consistency      learning      in      the      context      of      fine-tuning      diffusion      models  \\n \\n  has      not      been      established.\\nTherefore,      we      start      by      elucidating      the      rationale      of      PCL.\\nSubsequently,  \\n \\n  we      provide   \\n \\na      comprehensive      explanation      of      the      PCL.\\nFinally,      we      discuss      its      connection      with      the  \\n \\n  literature      on      Gflownets.</p><p>Motivation.\\nHere,      we      present      the      fundamental      principles      of      the      PCL.\\nTo      start      with,      we      prove      the  \\n \\n  following      lemma,      which      characterizes      soft-value      functions      and      soft-optimal      policies      recursively.</p><p>Algorithm   \\n \\n5      Path      Consistency      Learning      (Training      with      detailed      balance      loss)  \\n \\n  1:      Require:      Diffusion-model      {N(p(2,t;),07(t))}i-74,,  \\n \\n  -pre-trained   \\n \\n=      model  \\n \\n  {N      (p(t,      t;      Opre),      07      (t))      }i_744,      batch      size      m,      parameter   \\n \\na   \\n \\n€      Rt,      learning      rate   \\n \\n7       2:      Set   \\n \\na      model      {v;(-;@)}      to      learn      optimal      soft      value      function,      and   \\n \\na      model      {p;(-|-;@)}      to      learn  \\n \\n  optimal      polices.\\n \\n \\n  3:      Initialize:      0;   \\n \\n=      Opre  \\n \\n  4:      for      s   \\n \\n=      {1,---      ,S}do  \\n \\n  5:      Collect   \\n \\nm      samples      {a      (0)      }?_7,,      from   \\n \\na      current      diffusion      model      (i.e.,      generating      by      sequen- tially      running      polices      {N(p(2z,      t;      8),      07(t))      }i_-74,      from      t   \\n \\n=      T      to      t   \\n \\n=      0)  \\n \\n  6:      Setuo=r</p><li>(i)   \\n \\n?       a!\\n \\n \\n3   \\n \\na   \\n \\n4      Up—-1(@      13      ds)      re  \\n \\n  bs41   \\n \\n—      bs   \\n \\n—      Vo   \\n \\n3   \\n \\na      9)   \\n \\n+      log      p:(a\\\\\\\\\\\\\\\\,      |a\\\\\\\\\\\\\\\\;0,)   \\n \\n-      EP   \\n \\n—      Jog      rl      >}      Ibe:</li><p>a\\n \\n \\n  t=T+1      i=1  \\n \\n  (c}      ite)      ()      via)      43s)   \\n \\n;       Os41   \\n \\n<      Os   \\n \\n—      Vo   \\n \\n3      TE      velee      ids)      —S<   \\n \\n+      log      pe(ay      2s      |p      0)      —§      log      pee      (ar?\\nifort?\\nJe      lass</p><p>t=T+1      i=1 7:      end      for  \\n \\n  8:      Output:      {p,(x1-1      [215      Os)      fe</p><h3>Lemma      4      (1-step      Consistency      Equation).</h3><li>(2)   \\n \\n+      log      pi      (aa      |a1)   \\n \\n=      (eaeen)   \\n \\n+      log      pp      (x+-1|2+)      (33)</li><p> \\n \\n  Proof.\\nWe      consider      the      marginal      distribution      with      respect      to      x,      and      2;_,      induced      by      the      soft-  \\n \\n  optimal      policy,      and      denote      it      [(z;,      2:1)   \\n \\n€      A(X   \\n \\nx      X).\\nThen,      it      is      clearly      I(x,)l(a,_1   \\n \\n|      x)   \\n \\n=       I(x4-1)l      (xz   \\n \\n|      a1).\\nNow,      from      Theorem   \\n \\n2      that      characterizes      marginal      distributions,      and      Theorem   \\n \\n3       that      characterizes      posterior      distributions,      this      results      in      :</p><p>1\\n \\n   UE      L      re   \\n \\nx   \\n \\n1      Otte      me  \\n \\n  exp   \\n \\n(   \\n \\ni      2)      Pe      (xt)   \\n \\nx      pe      (®e-1|@e)   \\n \\n=   \\n \\nB      exp      (faeev)      Pra      (@1-1)   \\n \\nX      Pe      (1/01) a\\n \\n   —_-_——”’      C   \\n \\na      ~~  \\n \\n  C       we  \\n \\n  Optimal      policy  \\n \\n  “~   \\n \\n“      Posterior      distribution  \\n \\n  Marginal      distribution      at      t      Marginal      distribution      at      t-1 Rearranging      yields:</p><p>1\\n \\n   UL(&   \\n \\n1      Up—1      (Le      re  \\n \\n  G      exp   \\n \\n(      ))   \\n \\nx      Di      (@-1|%4)   \\n \\n=      G      exp      (eaeen)   \\n \\nx      pe      (@4-1|24)</p><p>Taking      the      logarithm,      the      statement      is      concluded.\\nO</p><p>Algorithm.\\nBeing      motivated      by      the      relation      in      (33),      after      initializing      v9   \\n \\n=      r,      we      obtain      the  \\n \\n  recursive      equation:</p><p>2 Olen      U_-1      (Lt  \\n \\n  (pp)   \\n \\n=      argmin   \\n \\n—      Egany   \\n \\n|      oO   \\n \\n4      tog      g      (arp      ng)   \\n \\n—      PEED)   \\n \\n—      dog      pP\"      (ya      er) g):%3R,g2):¥      A(X)   \\n \\na   \\n \\no U   \\n \\n_   \\n \\nx   \\n \\n_      re      re  \\n \\n  =      (MEY)      tog      peal      altiansa)      +++      Flog      pe      (esl),  \\n \\n  which      is      an      extension      of      (33).\\nThe      loss      function      based      on      the      above      k-step      consistency      equation  \\n \\n  could      make      training      faster      without      learning      value      functions      at      every      time      point,      as      noted      in      the  \\n \\n  literature      in      PCL.\\nIn      the      extreme      case      (i.e.,      when      we      recursively      apply      it      with      t   \\n \\n=      7’),      we      obtain      the  \\n \\n  following.</p><li>(34)  \\n \\n  where      u,   \\n \\n€      A(X)      is      any      exploratory      roll-in      distribution.\\nBased      on      this      algorithm,      we      outline      the  \\n \\n  entire      algorithm      in      Algorithm      5.  \\n \\n  We      make      several      important      remarks      regarding      Algorithm      5.\\nFirstly,      while      we      use      on-policy      data  \\n \\n  collection,      technically,      any      policy      can      be      used      in      this      off-policy      algorithm,      like      reward-weighted  \\n \\n  MLE.\\nSecondly,      in      practice,      it      might      be      preferable      to      utilize   \\n \\na      sub-trajectory      from      x;      to      x;_,      based  \\n \\n  on      the      following      expression:</li><p>UFZ  \\n \\n  log      pi_      pa      (Gee      |Vt—ng1)   \\n \\n+      +++   \\n \\n+      log      pi      (4-1      |a4)   \\n \\n+      (2)</p><h3>Corollary      1      (7-step      consistency).</h3><p>log      pf      (xo|21)   \\n \\n+      +++   \\n \\n+      log      pp(wr-a|er)   \\n \\n+      log      pp      (x7)      (35) r(x      re      re      re  \\n \\n  —      (=)   \\n \\n+      log      p?\\n*(xo|v1)      +--+   \\n \\n+      log      pr\"      (ar-i|er)   \\n \\n+      log      pe      (xr).</p><p>Proof.\\nWe      consider      the      marginal      distribution      with      respect      to      7,      +--+   \\n \\n,   \\n \\n%      induced      by      the      soft-optimal  \\n \\n  policy,      and      denote      it      I(vp,---      ,--+      ,%o)   \\n \\n€   \\n \\n¥      X--+   \\n \\nx      X.      We      have Uar)l(ar—1   \\n \\n|      Lr)      tee      I(xo|21)   \\n \\n=      U(x      )l(a4   \\n \\n|      Xo)      te      (ar   \\n \\n|      LT-1)      (36) From      Theorem   \\n \\n2      that      characterized      marginal      distributions,      and      Theorem   \\n \\n3      that      characterize      posterior  \\n \\n  distributions,      the      left      hand      side      of      (36)      is      equal      to *\\n \\n   *\\n \\n   *</p><p>Pr      (xr)   \\n \\nx      pp      (r|er-1)      X-++   \\n \\nx      pi      (Xo|@1) Marginal      distribution      atT      Optimal      policy      at      T   \\n \\n—   \\n \\n1      Optimal      policy      at   \\n \\n1 and      the      right-hand      side      in      (36)      is      equal      to exp(r      (xo)      /a)      pre      pre      pre  \\n \\n  aq      Po      (xo)   \\n \\nX      po      (a1   \\n \\n|      Zo)      X+++   \\n \\nX      prey      (@r   \\n \\n|      er-1)-.\\n \\n \\n|      ~~  \\n \\n  Marginal      distribution      at   \\n \\n0      Posterior      distribution      Posterior      distribution By      rearranging      the      term,      we      obtain      (35).\\nO</p><p> \\n \\n  Comparison      with      Gflownets.\\nIn      the      Gflownets      literature,      similar      losses      are      used.\\nFor      instance,  \\n \\n  the      loss      derived      from      (33)      or      (35)      is      commonly      known      as   \\n \\na      detailed      balance      loss      (Bengio      et      al.,  \\n \\n  2023)      or   \\n \\na      trajectory      loss      (Malkin      et      al.,      2022),      respectively.</p><p> \\n \\n  Note      in      general,      the      literature      in      Gflownets      primarily      focuses      on      sampling      from      unnormalized  \\n \\n  models      (distributions      proportional      to      exp(r(a))).\\nHence,      reference      policies      (i.e,      {p?\"\"}\\n \\n \\n)      or      latent  \\n \\n  states      (1.e.,      %7.;      before      x9)      are      introduced      without      relying      on      pre-trained      diffusion      models.\\nIn  \\n \\n  contrast,      in      our      context,      we      use      policies      derived      from      pre-trained      diffusion      models      as      reference  \\n \\n  policies,      leveraging      them      as      our      prior      knowledge.</p><p>7\\n \\n   Fine-Tuning      Settings      Taxonomy</p><p> \\n \\n  So      far,      we      implicitly      assume      we      have      access      to      reward      functions.\\nHowever,      these      functions      are      often  \\n \\n  unknown      and      need      to      be      learned      from      data.\\nWe      classify      several      settings      in      terms      of      whether      reward  \\n \\n  functions      are      available      or,      if      not,      how      they      could      be      learned.\\nThis      section      is      summarized      in      Figure      3.</p><h4>Direct      backpropagation      (Sec.      5.2)       _       Differentiable</h4><p>_\\n \\n   ——_      Non-differentiable PP      5.1</p><h4>Accurate      rewards      O      (Sec.      5.1)</h4><p> \\n \\n  feedback   \\n \\n;    \\n \\n—      Reward-weighted      MLE      (Sec.      6.1  \\n \\n  Xo      Unknown      reward      Memory      effi-      oe      eward-welgnte      (Sec.      6.1) unctions      —_\\\\\\\\\\\\\\\\_—      ciency</p><h4>No      fine-tuning      Computational</h4><p>efficiency —\\n \\n    NN      Direct      backpropagation      (Sec.      5.2)</p><h5>Value-weighted      sampling      (Sec.      6.2)</h5><p>Figure      3:      Practical      recommendation      of      RL-based      algorithms      to      optimize      downstream      reward  \\n \\n  functions.</p><h6>7.1      Fine-Tuning      with      Known,      Differentiable      Reward      Functions</h6><p>ions  \\n \\n  When      accurate      differentiable      reward      functions      are      available      (e.g.,      standard      in      computer      vision      tasks),  \\n \\n  our      primary      focus      is      typically      on      computational      and      memory      efficiency.\\nIn      general,      we      advocate      for  \\n \\n  employing      reward      backpropagation      (c.f.      Algorithm      2)      for      its      computational      efficiency.\\nAlternatively,  \\n \\n  if      memory      efficiency      is   \\n \\na      significant      concern,      we      suggest      using      PPO      due      to      its      stability.</p><h6>7.2      Fine-Tuning      with      Black-Box      Reward      Feedback</h6><p>back  \\n \\n  Here,      we      explore      scenarios      where      we      have      access      to      accurate      but      non-differentiable      (black-box)  \\n \\n  reward      feedback,      often      found      in      scientific      simulations.\\nThe      emphasis      still      remains      primarily      on  \\n \\n  computational      and      memory      efficiency.\\nIn      such      scenarios,      due      to      the      non-differentiability      of      reward  \\n \\n  feedback,      we      recommend      using      PPO      or      reward-weighted      MLE      for      the      following      reasons.</p><p> \\n \\n  In      the      preceding      section,      we      advocated      for      the      use      of      reward      backpropagation      due      to      its      com-  \\n \\n  putational      advantages.\\nHowever,      when      dealing      with      non-differentiable      feedback,      the      advantage  \\n \\n  of      reward      backpropagation      may      diminish.\\nThis      is      because      learning      from      such      feedback      requires  \\n \\n  learning      differentiable      reward      functions      to      make      the      algorithm      work      as      we      discuss      in      Section      5.2.  \\n \\n  However,      obtaining   \\n \\na      differentiable      reward      function      can      be      challenging      in      many      domains.\\nFor  \\n \\n  example,      in      molecular      property      prediction,      molecular      fingerprints      are      informative      features,      and       accurate      reward      functions      can      be      derived      by      training      simple      neural      networks      on      these      features  \\n \\n  (Pattanaik      and      Coley,      2020).\\nYet,      these      mappings,      grounded      in      prior      scientific      understanding,      are  \\n \\n  non-differentiable,      thereby      restricting      their      applicability.</p><p> \\n \\n  In      contrast,      PPO      and      reward-weighted      MLE      allow      for      policy      updates      without      the      explicit  \\n \\n  requirement      to      learn   \\n \\na      differentiable      reward      function,      offering   \\n \\na      significant      advantage      over      reward  \\n \\n  backpropagation.</p><h6>7.3      Fine-Tuning      with      Unknown      Rewards      Functions</h6><p>ions  \\n \\n  When      reward      functions      (or      computational      proxies      in      Section      7.2)      are      unavailable,      we      must      learn  \\n \\n  from      data      with      reward      feedback.\\nIn      such      cases,      compared      to      the      previous      scenarios,      two      important  \\n \\n  considerations      arise:</p><p>¢\\n \\n   Not      only      computational      or      memory      efficiency      but      also      feedback      efficiency      (i.e.,      sample  \\n \\n  efficiency      regarding      reward      feedback)      is      crucial.</p><p> \\n \\n \\n¢      Given      that      learned      reward      feedback      may      not      generalize      well      outside      the      training      data      distri-  \\n \\n  butions,      it      is      essential      to      constrain      fine-tuned      models      to      prevent      significant      divergence      from  \\n \\n  diffusion      models.\\nIn      these      situations,      not      only      soft      PPO      and      direct      backpropagation      but      also  \\n \\n  methods      discussed      in      Section      6,      such      as      reward-weighted      MLE,      can      be      particularly      effective.</p><p>Further,      the      scenario      involving      unknown      reward      functions      can      be      broadly      categorized      into      two      cases,  \\n \\n  which      we      will      discuss      in      more      detail.</p><p>Offline      scenario      (Figure      4).\\nIn      many      scenarios,      even  \\n \\n  in      cases      where      reward      functions      are      unknown,      we      of-  \\n \\n.     .</p><p>Offline      data      with ten      have      access      to      offline      data      with      reward      feedback      reward      feedback</p><li>(i)      (i)   \\n \\nO   \\n \\n;   \\n \\ni       a”      ra  \\n \\n.      ne      straightforward      approach      is      to</li><p> \\n \\n \\n{   \\n \\n,   \\n \\n(   \\n \\n)    \\n.\\n    .\\n \\n   g      PP  \\n \\n.      <=>      Learning      reward  \\n \\n  perform      regression      with      neural      networks      and      build   \\n \\na      models  \\n \\n  learned      regressor      *.\\nHowever,      this      learned      reward      func-      SS  \\n \\n  tion   \\n \\n*      might      not      generalize      well      beyond      the      distribution      x)  \\n \\n  of      the      offline      data.\\nIn      regions      outside      this      distribution,   \\n \\n7       could      assign   \\n \\na      high      value      even      when      the      actual      reward  \\n \\n  r      is      low      due      to      high      uncertainty.\\nConsequently,      we  \\n \\n  could      be      easily      misled      by      out-of-distribution      samples  \\n \\n  if      we      solely      optimize      7.\\n|Fine-tuning      diffusion      models   \\n \\n|       One      approach      to      alleviate      this      issue      is      to      adopt   \\n \\na       pessimistic      (i.e.,      conservative)      strategy,      as      proposed      in      Figure      4:      Offline      scenario  \\n \\n  Uehara      et      al.\\n(2024).\\nThis      technique      has      been      widely      Collecting      data]   \\n \\nY      Learning      reward  \\n \\n  applied      in      the      field      of      offline      RL      (Levine,      2018).\\nThe      aa  \\n \\n  main      idea      is      to      penalize   \\n \\n7      outside      the      distributions      us-  \\n \\n  ing      techniques      such      as      adding      an      explicit      penalty      term  \\n \\n  (Yu      et      al.,      2020;      Chang      et      al.,      2021),      bootstrapping,  \\n \\n  or      more      sophisticated      methods      (Kumar      et      al.,      2020;  \\n \\n  Xie      et      al.,      2021;      Rigter      et      al.,      2022;      Uehara      and      Sun,  \\n \\n  2021).\\nBy      doing      so,      we      can      avoid      being      fooled      by  \\n \\n  out-to-distribution      regions      while      still      benefiting      from  \\n \\n  the      extrapolation      capabilities      of      reward      models.</p><p>S—      diffusion      models      |</p><p>Figure      5:      Online      scenario</p><p> \\n \\n  Online      scenario      (Figure      5).\\nConsider      situations  \\n \\n  where      reward      functions      are      unknown,      but      we      can      gather      data      online.\\nThese      scenarios      are      often  \\n \\n  referred      to      as   \\n \\na      lab-in-the-loop      setting.</p><p> \\n \\n  In      such      scenarios,      Uehara      et      al.\\n(2024)      proposes      an      iterative      procedure      comprising      three      steps:</p><li>(1)      collecting      feedback      data      by      exploring      new      areas      (i.e.,      obtaining   \\n \\nx      from      current      diffusion      models  \\n \\n  and      corresponding      r(x)),      (2)      learning      the      reward      function      from      the      collected      feedback      data,      and</li><li>(3)      fine-tuning      current      diffusion      models      by      maximizing      the      learned      reward      function      enhanced      by  \\n \\n  an      optimistic      bonus      term,      using      algorithms      discussed      in      Section      4.2.\\nIn      contrast      to      standard      online  \\n \\n  PPO,      where      feedback      data      is      directly      used      in      the      fine-tuning      process,      actual      reward      feedback      is  \\n \\n  only      used      in      step      (2)      and      not      in      step      (3).\\nAn      additional      key      aspect      is      the      use      of      an      optimistic  \\n \\n  reward      function      that      encourages      exploration      beyond      the      distributions      of      the      current      diffusion      model.\\n \\n \\n  This      deliberate      separation      between      reward      learning      and      fine-tuning      steps,      coupled      with      the      use      of  \\n \\n  optimism,      significantly      enhances      feedback      efficiency.</li><p>8\\n \\n   Connection      with      Classifier      Guidance</p><p> \\n \\n  Classifier      guidance      (Dhariwal      and      Nichol,      2021)      is      commonly      used      for      conditional      generation      in  \\n \\n  diffusion      models.\\nInterestingly,      RL-based      methods      share   \\n \\na      close      connection      with      classifier      guidance.\\n \\n \\n  More      specifically,      in      this      section,      following      Zhao      et      al.\\n(2024),      we      elucidate      how      RL-based      methods  \\n \\n  can      be      applied      to      conditional      generation.\\nFurthermore,      from      this      unified      viewpoint,      we      highlight  \\n \\n  that      classifier      guidance      is      seen      as   \\n \\na      value-weighted      sampling      in      Section      4.2.</p><h6>8.1      Classfier      Guidance</h6><p>ance  \\n \\n  In      this      section,      we      first      introduce      classifier      guidance.\\nClassifier      guidance      utilizes      an      unconditional  \\n \\n  pre-trained      model      for      conditional      generation.\\nMore      specifically,      we      consider   \\n \\na      scenario      where      we  \\n \\n  have   \\n \\na      pre-trained      diffusion      model,      which      enables      us      to      sample      from      p?\\n\"*(x),      and      data      with      feedback  \\n \\n  {x,y},      where   \\n \\ny      represents      the      new      label      we      want      to      condition      on.\\nOur      objective      here      is      to      sample  \\n \\n  from      p(-|y)   \\n \\n€      [Y   \\n \\n>      ACR), P(-   \\n \\n|      y)   \\n \\nx      py(yl-)pP*(-).\\n(37)</p><p>where      p,   \\n \\n:   \\n \\nX      —>      A(Y)      denotes      the      conditional      distribution      of   \\n \\ny      given      x.</p><p>Motivation.\\nIn      classifier      guidance,      we      consider      the      following      policy:</p><p>pe      (-lare,      y)   \\n \\n=   \\n \\nN      (p(22,      t;      Opre)   \\n \\n+      a(t)      Ve      log      Vy      (Xt;      t),      a*(t))   \\n \\n’ dy(x,t)   \\n \\n=      Egy.\\nyap,      (-leo)      L(Y   \\n \\n=      y)|a,   \\n \\n=      2].</p><p>Under      the      continuous-time      formulation      of      diffusion      models,      applying      Doob’s      h-transform      (Rogers  \\n \\n  and      Williams,      2000),      it      is      shown      that      we      can      sample      from      p(x|y)      by      sequentially      running      {p7\"\\\\\\\\\\\\\\'(-|7+,      y)      }i-r41-</p><p>Algorithm.\\nSpecifically,      the      algorithm      comprises      two      steps.\\nIt’s      important      to      note      that      this      is      not  \\n \\n  a      fine-tuning      method      but      rather      an      inference-time      technique      as      we      freeze      the      pre-trained      model.</p><li>1. Learning      q,(x,t)      through      regression      by      gathering      data      containing      {z;,      y}.\\nHere,      for      each  \\n \\n  (Xo,      y),      using   \\n \\na      forward      policy      for      the      pre-training      (from   \\n \\n0      to      T),      we      obtain      2;      starting      from</li><p>Lo.</p><li>2. During      inference,      sequentially      execute      {p2\"(-|a,,      y)      }1_>   \\n \\n1      by      leverage      the      fact      that      compared</li><p>gui to      the      policy      in      pre-trained      diffusion      models,      the      mean      of      {p?\\n\"\\\\\\\\\\\\\\'(-|71,      y)      }i-74,      is      just      shifted  \\n \\n  with      an      additional      term      V,,      log      q,(2,      t).</p><p> \\n \\n  Notably,      the      method      described      above      can      be      reduced      to      value-weighted      sampling      (cf.      Sec-  \\n \\n  tion      6.2),      when      r(x,      y)   \\n \\n=      log      p(y|x).\\nInspired      by      such      an      observation,      we      apply      an      RL-based  \\n \\n  fine-tuning      method      for      conditional      generation      as      below.</p><h7>8.2.      RL-Based      Fine-Tuning      for      Conditional      Generation</h7><p>tion  \\n \\n  In      this      section,      following      Zhao      et      al.\\n(2024),      we      explain      an      RL-based      fine-tuning      approach      for  \\n \\n  conditional      generation.\\nThe      core      idea      is      that      setting      r(x,      y)   \\n \\n=      log      p(y|x)      allows      us      to      sample      from  \\n \\n  the      target      conditional      distribution      (37),      drawing      on      the      insights      discussed      in      Section      4.2.  \\n \\n  More      precisely,      introducing   \\n \\na      distribution      over   \\n \\ny      as      q(-),      we      formulate      the      following      RL      problem:</p><p>{Di      ft   \\n \\n=      argmax      Egy,      (-\\\\\\\\\\\\\\\\-y)}.yott      log      p(y|xo)   \\n \\n—      NeopyaKL(pe(-|e,      y)      ||P      (late).\\n \\n \\n  {prE[(V,.X)PA(A)]      Heyy (38)</p><p> \\n \\n  Compared      to      the      objective      function      (18)      in      Section      4.2,      the      main      differences      are:      (1)      policy      space  \\n \\n  is      expanded      to      accommodate      additional      control      (i.e.,      y):      pz(-|-,-)      C      [R¢   \\n \\nx   \\n \\nY   \\n \\n+      A(R®)],      (2)      the  \\n \\n  fine-tuning      objectives      are      set      as      log-likelihoods:      log      p(y|z).\\nThen,      following      Theorem      1,      we      can  \\n \\n  establish      the      following      result.</p><p>Corollary      2.\\nThe      distribution      induced      by      {p*}1_74      (ue.   \\n \\n[      {Teri      Ph      (ae-1   \\n \\n|      te,      y)}deur      is      the target      conditional      distribution      (37).</p><p> \\n \\n  The      above      corollary      suggests      that      for      conditional      generation,      we      can      utilize      various      off-the-shelf  \\n \\n  algorithms      discussed      in      Section      4.2      and      6,      such      as      PPO,      reward      backpropagation,      and      reward-  \\n \\n  weighted      MLE,      as      well      as      value-weighted      sampling.</p><p>9\\n \\n   Connection      with      Flow-Based      Diffusion      Models</p><p> \\n \\n  We      elucidate      the      close      relationship      between      bridge      matching      and      fine-tuning,      as      summarized      in  \\n \\n  Table      2.\\nBridge      (flow)      matching      (Liu      et      al.,      2022;      Tong      et      al.,      2023;      Lipman      et      al.,      2023;      Liu      et      al.,  \\n \\n  2022;      Shi      et      al.,      2024;      Albergo      and      Vanden-Eijnden,      2022),      has      recently      gained      popularity      as   \\n \\na       distinct      type      of      diffusion      model      from      score-matching-based      ones.\\nThese      works      suggest      that      training  \\n \\n  and      inference      speed      will      be      accelerated      due      to      the      ability      to      define   \\n \\na      more      direct      trajectory      from  \\n \\n  noise      to      data      distribution      than      score-matching-based      diffusion      models.</p><p> \\n \\n  The      connection      between      bridge      matching      and      fine-tuning      becomes      evident      in      the      continuous-  \\n \\n  time      formulation.\\nTo      begin,      we      first      provide   \\n \\na      brief      overview      of      the      continuous-time      formulation      of  \\n \\n  score-matching-based      diffusion      models.</p><p> \\n \\n  Table      2:      Comparison      between      training      score/flow-based      diffusion      models      and      fine-tuning.\\nWe  \\n \\n  optimize      the      parameter   \\n \\n6      in      P’,      which      represents      the      induced      distribution      over      trajectories      associated  \\n \\n  with      6.</p><h8>Loss      Note      of      Q</h8><p>Score-based      KL(Q\\\\\\\\\\\\\\'™¢|P?)      Qtime:      Time-reversal      of      reference      SDE      (reference training      SDE      is      defined      from   \\n \\n0      to      T’      where      the      initial      dis-  \\n \\n  tribute      at   \\n \\n0      is      data      distribution)</p><p> \\n \\n  Flow-based      KL(Q*°>      ||P?)      @Q*°°h:      Coupling      between      the      reference      SDE      from  \\n \\n  training      T      to   \\n \\n0      conditioning      at   \\n \\n0      (dist.      over      x.;|7o)      and    \\n \\na      data      distribution      at      time   \\n \\n0      (dis.      over      x).\\nNote  \\n \\n  that      reference      SDE      is      defined      from      7’      to      0.</p><p> \\n \\n  Fine-tuning      KL(P?||Q*°)      @*:      Coupling      between      the      pre-trained      SDE  \\n \\n  from      T      to   \\n \\n0      conditioned      on   \\n \\n0      (dist.      over      x7.1|X0)  \\n \\n  and   \\n \\na      data      distribution      at      time   \\n \\n0      (dis.      over      Xo) 9.1      Continuous      Time      Formulation      of      Score-Based      diffusion      models</p><p> \\n \\n  We      provide   \\n \\na      brief      overview      of      the      continuous-time      formulation      of      diffusion      models      (Song      et      al.,  \\n \\n  2021),      further      expanding      Section      1.1.1.\\nIn      this      formulation,      we      initially      define      the      forward      reference  \\n \\n  SDE,      spanning      from      time   \\n \\n0      to      7’      (e.g.,      variance      exploding      (VE)      process      or      variance      preserving      (VP)  \\n \\n  process),      by      setting      an      initial      distribution      at   \\n \\n0      as      the      data      distribution.\\nAdditionally,      the      reference  \\n \\n  SDE      is      tailored      to      converge      to      an      easy      distribution      at      time      7’,      such      as      the      normal      distribution.</p><p> \\n \\n  Subsequently,      we      consider      the      time-reversal      SDE      (Anderson,      1982),      progressing      from      T\\\\\\\\\\\\\\'      to      0.\\nIf  \\n \\n  we      could      learn      this      time-reversal      SDE,      it      would      enable      us      to      sample      from      the      data      distribution      by  \\n \\n  starting      with      the      easy      initial      distribution      (at      7’)      and      following      the      time-reversal      SDE      from      T      to   \\n \\n0      at  \\n \\n  inference      time.</p><p> \\n \\n  Training.\\nNow,      the      remaining      question      is      how      to      learn      this      time-reversal      SDE.\\nHere,      by      repre-  \\n \\n  senting      the      induced      distribution      over      trajectories      from      T      to   \\n \\n0      with      this      time-reversal      SDE      as      Q\\\\\\\\\\\\\\'™®,  \\n \\n  our      goal      is      to      train   \\n \\na      new      SDE,      parameterized      by   \\n \\n@      in      the      neural      network,      such      that      the      induced  \\n \\n  distribution      P®      over      trajectories      associated      with   \\n \\n6      aligns      with      Q\"™*.\\nSpecifically,      the      objective      is      to minimize      the      following      losses:</p><p> \\n \\n  argmin      KL(Q\"™||P*).\\n(39)  \\n \\n  0  \\n \\n  With      some      algebra,      leveraging      the      observation      that      the      time-reversal      SDE      comprises      the      original  \\n \\n  forward      SDE      and      the      scoring      term      of      the      marginal      distribution      (Anderson,      1982)      as      in      (2),      Song  \\n \\n  et      al.\\n(2021)      demonstrates      that      this      KL      loss      (39)      is      approximated      as Dore   \\n \\n=      argmin      Eeuni((0,7))01~qyo0(weleo)co~pP\"e      Fe      Fl      Vey      log      jo      (Xt   \\n \\n|      Xo)   \\n \\n_      so(Xt,      t)|I?\\n],      (40)  \\n \\n  which      is      equal      to      (4)      where      the      weight      A(t)      is      {o?\\n}?\\n(for      notation,      refer      to      Section      1.1.1).</p><h9>9.2      Flow-Based      (Bridge-Based)      Diffusion      Models</h9><p>dels We      adopt      the      explanation      outlined      in      Liu      et      al.\\n(2022).\\nIn      this      framework,      we      begin      by      considering  \\n \\n  a      reference      SDE      (from      T\\\\\\\\\\\\\\'      to      0),      x;   \\n \\n=      zr_;      where      z;      is      defined      as      Brownian      motion:</p><p>dz,   \\n \\n=      dwt,      te      (0,      7).\\n(41)</p><p> \\n \\n  Subsequently,      we      consider      the      coupling      of      the      reference      SDE      conditioned      on      state      at      time   \\n \\n0      (i.e.,  \\n \\n  distribution      of      xo.7      conditional      on      x)      and      the      data      distribution      at      time      0,      denoting      the      resulting  \\n \\n  distribution      as      Q*°?,      Informally,      this      Q*°°?\\nis      characterized      as      follows.</p><p>Informal      characterization      of      Q*”.\\nWhen      discretizing      the      time      step,      by      denoting      the      distribution  \\n \\n  induced      by      the      reference      SDE      as      p\"(x7,--+   \\n \\n,      29),      we      define QF      (a7,      tae   \\n \\n,      20)   \\n \\n=      p(x,      ay   \\n \\n|      r9)pP\"°      (x9).</p><p>The      distribution      Q*”      above      is   \\n \\na      limiting      distribution      obtained      by      making      the      discretization      step small.</p><p>Training.\\nIn      bridge-based      diffusion      models,      our      objective      is      to      train   \\n \\na      new      SDE,      parametrized      by  \\n \\n  6      (e.g.,   \\n \\na      neural      network),      to      align      the      induced      distribution      P®      with      Q*°?.\\nSpecifically,      bridge-based training      aims      to      minimize aremin      KL(Q*>||P’).\\n(42)  \\n \\n  0  \\n \\n  While      the      method      to      minimize      the      above      is      currently      unclear,      we      can      derive   \\n \\na      trainable      objective  \\n \\n  through      algebraic      manipulation.\\nInitially,      we      observe      that      the      reference      SDE      (41),      conditioned      on  \\n \\n  state      xo      at      time   \\n \\n0      (zr   \\n \\n=      D),      is      expressed      as:</p><p>b\\n \\n \\n  b—      2;</p><p>t   \\n \\n€      [0,T];dz?\\n \\n \\n=      g?(z?,t)dt      +du,,  \\n \\n  g(z?,t)   \\n \\n=   \\n \\na       (43)</p><p>using      the      seminal      “Doob’s      h-transform”      (Rogers      and      Williams,      2000).\\nThis      SDE      is      commonly  \\n \\n  known      as      the      Brownian      bridge.\\nNow,      we      consider   \\n \\na      new      SDE      parameterized      by      0:</p><p>t   \\n \\n€      [0,7];      dz   \\n \\n=      s(z,t;      @)dt   \\n \\n+      dw.\\n(44)</p><p>Then,      we      can      demonstrate      that      minimizing      the      KL      loss      in      (42)      is      equivalent      to argmin      E,W      unil0,7],1~4\"°      cop?\\n[Ilg”      (2,      t)   \\n \\n_      s(21,      t;      0)      M3]      (45)</p><p>where      q;°      denotes      the      induced      distribution      at      time      t      following      SDE      (43)      conditioned      on      xo,      and       s:      Xx      |0,T]   \\n \\n>   \\n \\n&      is      the      model      we      aim      to      optimize.</p><p> \\n \\n  Flow-based      diffusion      models.\\nThe      aforementioned      formulation      has      been      originally      proposed      as   \\n \\na       bridge-based      SDE      (Liu      et      al.,      2022)      because      the      conditional      SDE      is      often      referred      to      as   \\n \\na      Brownian  \\n \\n  bridge.\\nFlow-based      diffusion      models      proposed      in      Lipman      et      al.\\n(2023);      Tong      et      al.\\n(2023)      share   \\n \\na       close      relationship      with      bridge-based      diffusion      models.\\nThese      flow-based      models      use   \\n \\na      target      SDE (43)      and      an      SDE      with   \\n \\na      parameter      (44)      without      stochastic      terms      (1.e.,      no      dw,)      and      aim      to      minimize  \\n \\n  the      loss      function      (45).</p><p> \\n \\n  Comparison      with      score-based      diffusion      models.\\nBoth      score-based      diffusion      models      and      bridge-  \\n \\n  based      diffusion      models      aim      to      minimize      KL      divergences.\\nDespite      its      similarity,      in      contrast      to  \\n \\n  score-based      diffusion      models,      bridge-based      diffusion      models      target      Q*°>      rather      than      the      time-  \\n \\n  reversal      SDE      Q\"™®.\\nThe      distribution      Q*”      is      often      considered      preferable      because      it      can      impose      an  \\n \\n  SDE      that      navigates      efficiently      from   \\n \\n0      to      7’      (e.g.,   \\n \\na      Brownian      bridge      with      minimal      noise).\\nThis      can  \\n \\n  expedite      the      learning      process      since      we      lack      direct      control      over      the      time-reversal      SDE      in      score-based  \\n \\n  diffusion      models.</p><h10>9.3.      Connection      with      RL-Based      Fine-Tuning</h10><p>ning  \\n \\n  RL-based      fine-tuning      shares   \\n \\na      close      relationship      with      flow-based      training.\\nTo      illustrate      this,      consider  \\n \\n  the      reference      SDE      (from      7’      to      0)      following      the      pre-trained      diffusion      model.\\nThen,      akin      to      flow-  \\n \\n  based      training,      we      consider      the      coupling      of      the      reference      SDE      conditioned      on      state      at      time   \\n \\n0       (i.e.,      distribution      over      xo-r      conditioned      on      x9)      and      the      target      distribution      at      time   \\n \\n0      (i.e.,      p,(-)   \\n \\n«       exp(r(-)/a)p?\\n\"°(-)),      denoting      the      induced      distribution      as      Q*™.</p><p>Informal      characterization      of      Q™°*.\\nInformally,      Q*\"°      is      introduced      as      follows.\\nWhen      discretizing  \\n \\n  the      time      step,      by      denoting      the      distribution      induced      by      the      reference      SDE      as      p™\\\\\\\\\\\\\\'(xp,-+-   \\n \\n,      9),      we define Qi\"      (xr,   \\n \\na   \\n \\n,      0)   \\n \\n=      p(x,      nr      Ly   \\n \\n|      L0)Pr      (Xo).</p><p>The      distribution      Q*°      above      is   \\n \\na      limiting      distribution      obtained      by      making      the      discretization      step small.</p><p>Training.\\nNow,      we      introduce   \\n \\na      new      SDE      parameterized      by   \\n \\n@      such      as      an      neural      network      to      align  \\n \\n  the      induced      distribution      P’      with      Q®\"*.\\nActually,      in      the      continuous      formulation,      the      RL-problem      (18) in      Section      4.2      is      equal      to      solving argmin      KL(P?||Q\"™).\\n \\n \\n  0 This      formalization      has      been      presented      in      Uehara      et      al.\\n(2024).\\nIntuitively,      as      we      see      in      Theorem      3,  \\n \\n  this      is      expected      because      we      see      that      the      posterior      distribution      induced      by      pre-trained      models:</p><p>ref   \\n \\n(      PD      (ar,-++      21   \\n \\n|      to)      (=      vey      (er|er_-1)p      Pp      (ar-1   \\n \\n|      er_2)      +++      pp\"      (x1   \\n \\n|      20), remains      preserved      after      fine-tuning      based      on      the      RL-formulation      (18).</p><p> \\n \\n  Comparison      with      flow-based      training.\\nIn      contrast      to      flow-based      training,      RL-based      fine-tuning  \\n \\n  minimizes      the      inverse      KL      divergence      rather      than      the      KL      divergence.\\nIntuitively,      this      is      because,  \\n \\n  unlike      Qt™*      or      Q*>   \\n \\n,      we      cannot      sample      from      Q*”      (recall      that   \\n \\na      marginal      distribution      at      time   \\n \\n0      in  \\n \\n  Qi,      i.e.,      p,,      is      unnormalized);      on      the      other      hand,      the      marginal      distributions      at      time   \\n \\n0      in      both      Q\\\\\\\\\\\\\\'™*  \\n \\n  and      Q*°°>      are      data      distributions.</p><p> \\n \\n  Remark   \\n \\n7      (Extension      to      f-divergence).\\nEach      of      these      methods      can      be      technically      extended      by  \\n \\n  incorporating      more      general      divergences      beyond      KL      divergence.\\nFor      instance,      in      the      context      of  \\n \\n  fine-tuning,      please      see      Tang      (2024).</p><p>10      Connection      with      Sampling      from      Unnormalized      Distributions</p><p> \\n \\n  Numerous      studies      delve      into      sampling      from      an      unnormalized      distribution      proportional      to      exp(r()),  \\n \\n  commonly      known      as      the      Gibbs      distribution,      extensively      discussed      in      the      computational      statistics  \\n \\n  and      statistical      physics      community      (Robert,      2014).\\nThis      issue      is      pertinent      to      our      work,      as      during  \\n \\n  fine-tuning,      the      target      distribution      is      also      formulated      in      the      Gibbs      distribution      form      in      (10),      which      is  \\n \\n  proportional      to      exp(r(a))p?\\n\"*      (x).</p><p> \\n \\n  The      literature      employs      two      main      approaches      to      tackle      this      challenge:      Markov      Chain      Monte  \\n \\n  Carlo      (MCMC)      and      RL-based      methods.\\nIn      particular,      the      RL-based      diffusion      model      fine-tuning  \\n \\n  we      have      discussed      so      far      is      closely      related      to      the      latter      approach.\\nIn      this      section,      we      explain      and       compare      these      two      approaches.</p><h10>10.1      Markov      Chain      Monte      Carlo      (MCMC)</h10><p>CMC)  \\n \\n  In      MCMC,   \\n \\na      Markov      chain      is      constructed      to      approximate   \\n \\na      target      distribution      such      that      the      equilib-  \\n \\n  rium      distribution      of      the      Markov      chain      converges      to      the      target      distribution.\\nHowever,      dealing      with  \\n \\n  high-dimensional      domain      spaces      is      challenging      for      MCMC.\\n \\n \\nA      common      strategy      involves      leveraging  \\n \\n  gradient      information,      such      as      the      Metropolis-adjusted      Langevin      algorithm      (MALA)      (Besag,      1994)  \\n \\n  or      Hamiltonian      Monte      Carlo      (HMC)      (Neal      et      al.,      2011;      Girolami      and      Calderhead,      2011).\\nMALA  \\n \\n  shares      similarities      with      classifier      guidance      and      value-weighted      sampling,      as      both      methods      depend  \\n \\n  on      the      first-order      information      from      reward      (or      value)      functions.</p><p> \\n \\n  Can      we      use      MCMC      for      fine-tuning      diffusion      models?\\nIn      the      context      of      fine-tuning      dif-  \\n \\n  fusion      models,      while      it      may      seem      intuitive      to      sample      from      the      target      distribution      (10)      (i.e.,  \\n \\n  Dy   \\n \\n&      exp(r(x))pP\"*(x))      using      MCMC,      it’s      not      straightforward      for      the      following      reasons:</p><p>¢\\n \\n   We      lack      of      an      analytical      form      of      p?\\n\"*(-).<ul><li>*\\n \\n   Additionally,      even      if      we      can      estimate      p?\\n’°(-)      in      an      unbiased      manner,      as      in      Chen      et      al.\\n(2018),  \\n \\n  the      mixing      time      for      obtaining   \\n \\na      single      sample      in      MCMC      might      be      lengthy.</li></ul></p><p> \\n \\n  Therefore,      we      explore      creating   \\n \\na      generative      model      (i.e.,      diffusion      model)      for      this      task      using      RL-  \\n \\n  based      fine-tuning      so      that      we      can      easily      sample      from      p,(-)      during      inference      (i.e.,      simulating      the  \\n \\n  time-reversal      SDE      (2)      with      policies      from   \\n \\n¢   \\n \\n=      T\\\\\\\\\\\\\\'+   \\n \\n1      to      t   \\n \\n=      1),      without      relying      on      MCMC.\\nThis  \\n \\n  addresses      the      concern      of      MCMC      by      avoiding      the      need      to      estimate      p?\\n\"*(-)      and      minimizing      inference  \\n \\n  time,      albeit      at      the      expense      of      increased      training      time.</p><p>10.2}      RL-Based      Approaches  \\n \\n  While      MCMC      has      historically      been      widely      used      for      this      purpose,      recent      works      have      proposed      an  \\n \\n  RL-based      approach      or      its      variant      (e.g.,      Bernton      et      al.      (2019);      Heng      et      al.\\n(2020);      Huang      et      al.\\n(2023);</p><p> \\n \\n  Vargas      et      al.\\n(2023)).\\nFor      example,   \\n \\na      seminal      study      (Zhang      and      Chen,      2021)      introduced   \\n \\na      method  \\n \\n  very      similar      to      reward      backpropagation.\\nIn      their      work,      they      initially      define      Brownian      motion      as   \\n \\na       reference      SDE      (from      7’      to      0),      similar      to      the      pre-trained      diffusion      models      in      our      context.\\nThen,      by  \\n \\n  appropriately      setting      rewards,      Zhang      and      Chen      (2021)      aims      to      learn   \\n \\na      new      SDE      (from      T\\\\\\\\\\\\\\'      to      0)      such  \\n \\n  that      we      can      sample      from      the      target      distribution      at      the      endpoint      0.</p><p> \\n \\n  These      approaches      offer      advantages      over      MCMC      due      to      their      ability      to      minimize      inference      time  \\n \\n  (amortization)      by      constructing   \\n \\na      generative      model      comprising      policies      in      the      training      time      and      solely  \\n \\n  executing      learned      policies      during      inference.\\nUnlike      MCMC,      these      methods      significantly      reduce      the  \\n \\n  inference      cost      as      we      don’t      need      to      be      concerned      about      the      mixing      time.\\nAdditionally,      RL-based  \\n \\n  approaches      have      the      potential      to      effectively      handle      high-dimensional,      complex      (i.e.,      multi-modal)  \\n \\n  distributions      by      harnessing      the      considerable      expressive      power      arising      from      latent      states.</p><p> \\n \\n  Despite      these      advantages,      it      is      often      unclear      how      to      select   \\n \\na      reference      SDE      in      these      works.\\nIn  \\n \\n  contrast,      in      RL-based      fine-tuning      of      diffusion      models,      we      directly      leverage      pre-trained      diffusion  \\n \\n  models      as      the      reference      SDEs,      i.e.,      as      our      prior      knowledge.</p><p> \\n \\n  Key      message.\\nFinally,      it      is      worth      noting      that      we      can      technically      utilize      any      off-the-shelf      RL      algo-  \\n \\n  rithms      for      training.\\nFor      example,      Zhang      and      Chen      (2021)      employs      reward      backpropagation,      while  \\n \\n  works      in      Gflownets      typically      leverage      PCL,      as      we      mention      in      Section      6.3.\\nHowever,      additionally,      it  \\n \\n  is      possible      to      use      PPO      or      reward-weighted      MLE      mentioned      in      Section      4.2.</p><p>11      Closely      Related      Directions</p><p>Lastly,      we      highlight      several      closely      related      directions      we      have      not      yet      discussed.</p><p> \\n \\n  Aligning      text-to-image      models.\\nWe      recognize      that      current      efforts      in      fine-tuning      diffusion      models  \\n \\n  primarily      revolve      around      aligning      text-to-image      models      using      human      feedback      (Lee      et      al.,      2023;  \\n \\n  Wallace      et      al.,      2023;      Wu      et      al.,      2023;      Yang      et      al.,      2023).\\nIn      this      paper,      our      goal      is      to      consider   \\n \\na      more  \\n \\n  general      and      fundamental      formulation      that      is      not      tailored      to      any      particular      task.</p><p> \\n \\n  Diffusion      models      for      RL.\\nDiffusion      models      are      well-suited      for      specific      reinforcement      learning  \\n \\n  applications      due      to      their      ability      to      model      complex      and      multimodal      distributions      as      polices      (Janner  \\n \\n  et      al.,      2022;      Ajay      et      al.,      2023;      Wang      et      al.,      2022;      Hansen-Estruch      et      al.,      2023;      Du      et      al.,      2024;      Zhu  \\n \\n  et      al.,      2023).\\nFor      example,      Wang      et      al.\\n(2022)      use      conditional      diffusion      models      as      policies      and       demonstrate      their      effectiveness      in      typical      offline      RL      benchmarks.\\nWhile      RL-based      fine-tuning      also  \\n \\n  aims      to      maximize      certain      reward      functions,      the      primary      focus      of      this      tutorial      covers      methods      that  \\n \\n  leverage      pre-trained      diffusion      models      for      this      purpose.</p><p> \\n \\n  RL      from      human      feedback      (RLHF)      for      language      models.\\nRLHF      is      widely      discussed      in      the  \\n \\n  context      of      language      models,      where      many      algorithms      share      similarities      with      those      used      in      diffusion  \\n \\n  models      (Ouyang      et      al.,      2022;      Bai      et      al.,      2022;      Casper      et      al.,      2023).\\nFor      instance,      both      domains  \\n \\n  commonly      employ      PPO      as   \\n \\na      standard      approach,      and      reward-weighted      training      (or      reward-weighted  \\n \\n  MLE)      often      serves      as   \\n \\na      basic      baseline      approach.\\nDespite      these      similarities,      nuanced      differences  \\n \\n  exist      mainly      because      diffusion      models      typically      operate      in   \\n \\na      continuous      input      space      rather      than  \\n \\n  a      discrete      one.\\nMoreover,      techniques      like      value-weighted      sampling      emerge      uniquely      within      the  \\n \\n  context      of      diffusion      models.</p><p>12      Summary</p><p> \\n \\n  In      this      article,      we      comprehensively      explain      how      fine-tuning      diffusion      models      to      maximize      down-  \\n \\n  stream      reward      functions      can      be      formalized      as   \\n \\na      reinforcement      learning      (RL)      problem      in      entropy-  \\n \\n  regularized      Markov      decision      processes      (MDPs).\\nBased      on      this      viewpoint,      we      elaborate      on      the  \\n \\n  application      of      various      RL      algorithms,      such      as      PPO      and      reward-weighted      MLE,      specifically      tailored  \\n \\n  for      fine-tuning      diffusion      models.\\nAdditionally,      we      categorize      different      scenarios      based      on      how  \\n \\n  reward      feedback      is      obtained.\\nWhile      this      categorization      is      not      always      explicitly      mentioned      in      many  \\n \\n  existing      works,      it      is      crucial      when      selecting      appropriate      algorithms.\\nFinally,      we      discuss      the      rela-  \\n \\n  tionships      with      several      related      topics,      including      classifier      guidance,      Gflownets,      path      integral      control  \\n \\n  theory,      and      sampling      from      unnormalized      distributions.</p><h4>References</h4><p>Agarwal,      A.,      N.      Jiang,      S.      M.      Kakade,      and      W.      Sun      (2019).\\nReinforcement      learning:      Theory      and       algorithms.\\nCS      Dept.,      UW      Seattle,      Seattle,      WA,      USA,      Tech.\\nRep,      10-4.</p><p>Agarwal,      V.      and      D.      R.      Kelley      (2022).\\nThe      genetic      and      biochemical      determinants      of      mrna      degradation  \\n \\n  rates      in      mammals.\\nGenome      biology      23(1),      245.</p><p> \\n \\n  Ajay,      A.,      Y.      Du,      A.      Gupta,      J.      B.      Tenenbaum,      T.      S.      Jaakkola,      and      P.      Agrawal      (2023).\\nIs      conditional  \\n \\n  generative      modeling      all      you      need      for      decision      making?\\nIn      The      Eleventh      International      Conference  \\n \\n  on      Learning      Representations.</p><p>Albergo,      M.      S.      and      E.      Vanden-Eijnden      (2022).\\nBuilding      normalizing      flows      with      stochastic      inter-  \\n \\n  polants.\\narXiv      preprint      arXiv:2209.15571.</p><p>Anderson,      B.      D.      (1982).\\nReverse-time      diffusion      equation      models.\\nStochastic      Processes      and      their  \\n \\n  Applications      12(3),      313-326.</p><p> \\n \\n  Austin,      J.,      D.      D.      Johnson,      J.      Ho,      D.      Tarlow,      and      R.      Van      Den      Berg      (2021).\\nStructured      denoising  \\n \\n  diffusion      models      in      discrete      state-spaces.\\nAdvances      in      Neural      Information      Processing      Systems      34,  \\n \\n  17981-17993.</p><p>Avdeyey,      P.,      C.      Shi,      Y.      Tan,      K.      Dudnyk,      and      J.      Zhou      (2023).\\nDirichlet      diffusion      score      model      for  \\n \\n  biological      sequence      generation.\\narXiv      preprint      arXiv:2305.\\n10699.</p><p>Bai,      Y.,      S.      Kadavath,      S.      Kundu,      A.      Askell,      J.      Kernion,      A.      Jones,      A.      Chen,      A.      Goldie,      A.      Mirhoseini, C.      McKinnon,      et      al.\\n(2022).\\nConstitutional      ai:      Harmlessness      from      ai      feedback.\\narXiv      preprint  \\n \\n  arXiv:2212.08073.</p><p> \\n \\n  Bansal,      A.,      H.-M.\\nChu,      A.      Schwarzschild,      S.      Sengupta,      M.      Goldblum,      J.      Geiping,      and      T.      Goldstein  \\n \\n  (2023).\\nUniversal      guidance      for      diffusion      models.\\nIn      Proceedings      of      the      IEEE/CVF      Conference  \\n \\n  on      Computer      Vision      and      Pattern      Recognition,      pp.\\n843-852.</p><p>Bengio,      Y.,      S.      Lahlou,      T.      Deleu,      E.      J.      Hu,      M.      Tiwari,      and      E.      Bengio      (2023).\\nGflownet      foundations.\\n \\n \\n  Journal      of      Machine      Learning      Research      24(210),      1-55.</p><p> \\n \\n  Benton,      J.,      Y.      Shi,      V.      De      Bortoli,      G.      Deligiannidis,      and      A.      Doucet      (2024).\\nFrom      denoising      diffusions  \\n \\n  to      denoising      markov      models.\\nJournal      of      the      Royal      Statistical      Society      Series      B:      Statistical  \\n \\n  Methodology      86(2),      286-301.</p><p>Bernton,      E.,      J.      Heng,      A.      Doucet,      and      P.      E.      Jacob      (2019).\\nSchr\\\\\\\\\\\\\\\\””      odinger      bridge      samplers.\\narXiv  \\n \\n  preprint      arXiv:      1912.13170.</p><p>Besag,      J.\\n(1994).\\nComments      on      “representations      of      knowledge      in      complex      systems”      by      u.      grenander  \\n \\n  and      mi      miller.\\nJ.      Roy.\\nStatist.\\nSoc.      Ser.\\n \\n \\nB      56(591-592),      4.</p><p>Black,      K.,      M.      Janner,      Y.      Du,      I.      Kostrikov,      and      S.      Levine      (2023).\\nTraining      diffusion      models      with  \\n \\n  reinforcement      learning.\\narXiv      preprint      arXiv:2305.13301.</p><p> \\n \\n  Campbell,      A.,      J.      Benton,      V.      De      Bortoli,      T.      Rainforth,      G.      Deligiannidis,      and      A.      Doucet      (2022).\\n \\n \\nA       continuous      time      framework      for      discrete      denoising      models.\\nAdvances      in      Neural      Information  \\n \\n  Processing      Systems      35,      28266-28279.</p><p> \\n \\n  Campbell,      A.,      J.      Yim,      R.      Barzilay,      T.      Rainforth,      and      T.      Jaakkola      (2024).\\nGenerative      flows      on  \\n \\n  discrete      state-spaces:      Enabling      multimodal      flows      with      applications      to      protein      co-design.\\narXiv  \\n \\n  preprint      arXiv:2402.04997.</p><p>Cao,      H.,      C.      Tan,      Z.      Gao,      Y.      Xu,      G.      Chen,      P.-A.\\nHeng,      and      S.      Z.      Li      (2024).\\n \\n \\nA      survey      on      generative  \\n \\n  diffusion      models.\\n[EEE      Transactions      on      Knowledge      and      Data      Engineering.</p><p>Casper,      S.,      X.      Davies,      C.      Shi,      T.      K.      Gilbert,      J.      Scheurer,      J.      Rando,      R.      Freedman,      T.      Korbak, D.      Lindner,   \\n \\nP P.      Freire,      et      al.\\n(2023).\\nOpen      problems      and      fundamental      limitations      of      reinforcement  \\n \\n  learning      from      human      feedback.\\narXiv      preprint      arXiv:2307.15217.</p><p>Castillo-Hair,      S.      M.      and      G.      Seelig      (2021).\\nMachine      learning      for      designing      next-generation      mrna  \\n \\n  therapeutics.\\nAccounts      of      Chemical      Research      55(1),      24-34.</p><p>Chang,      J.      D.,      M.      Uehara,      D.      Sreenivas,      R.      Kidambi,      and      W.      Sun      (2021).\\nMitigating      covariate      shift  \\n \\n  in      imitation      learning      via      offline      data      without      great      coverage.\\narXiv      preprint      arXiv:2106.03207.</p><p>Chen,      M.,      S.      Mei,      J.      Fan,      and      M.      Wang      (2024).\\nAn      overview      of      diffusion      models:      Applications,  \\n \\n  guided      generation,      statistical      rates      and      optimization.\\narXiv      preprint      arXiv:2404.07771.</p><p>Chen,      R.      T.,      Y.      Rubanova,      J.      Bettencourt,      and      D.      K.      Duvenaud      (2018).\\nNeural      ordinary      differential  \\n \\n  equations.\\nAdvances      in      neural      information      processing      systems      31.</p><p>Chung,      H.,      J.      Kim,      M.      T.      Mccann,      M.      L.      Klasky,      and      J.      C.      Ye      (2022).\\nDiffusion      posterior      sampling  \\n \\n  for      general      noisy      inverse      problems.\\narXiv      preprint      arXiv:2209.\\n14687.</p><p> \\n \\n  Chung,      H.,      B.      Sim,      D.      Ryu,      and      J.      C.      Ye      (2022).\\nImproving      diffusion      models      for      inverse      problems  \\n \\n  using      manifold      constraints.\\nAdvances      in      Neural      Information      Processing      Systems      35,      25683-—  \\n \\n  25696.</p><p>Clark,      K.,      P.      Vicol,      K.      Swersky,      and      D.      J.      Fleet      (2023).\\nDirectly      fine-tuning      diffusion      models      on  \\n \\n  differentiable      rewards.\\narXiv      preprint      arXiv:2309.\\n17400.</p><p> \\n \\n  De      Bortoli,      V.,      E.      Mathieu,      M.      Hutchinson,      J.      Thornton,      Y.      W.      Teh,      and      A.      Doucet      (2022).\\nRieman-  \\n \\n  nian      score-based      generative      modelling.\\nAdvances      in      Neural      Information      Processing      Systems      35,  \\n \\n  2406-2422.</p><p>Deleu,      T.,      P.      Nouri,      N.      Malkin,      D.      Precup,      and      Y.      Bengio      (2024).\\nDiscrete      probabilistic      inference      as  \\n \\n  control      in      multi-path      environments.\\narXiv      preprint      arXiv:2402.10309.</p><p>Dhariwal,      P.      and      A.      Nichol      (2021).\\nDiffusion      models      beat      gans      on      image      synthesis.\\nAdvances      in  \\n \\n  neural      information      processing      systems      34,      8780-8794.</p><p> \\n \\n  Du,      Y.,      S.      Yang,      B.      Dai,      H.      Dai,      O.      Nachum,      J.      Tenenbaum,      D.      Schuurmans,      and      P.      Abbeel      (2024).\\n \\n \\n  Learning      universal      policies      via      text-guided      video      generation.\\nAdvances      in      Neural      Information  \\n \\n  Processing      Systems      36.</p><p>Fan,      Y.,      O.      Watkins,      Y.      Du,      H.      Liu,      M.      Ryu,      C.      Boutilier,      P.      Abbeel,      M.      Ghavamzadeh,      K.      Lee,      and K.      Lee      (2023) 023).\\nDPOK:      Reinforcement      learning      for      fine-tuning      text-to-image      diffusion      models.\\n \\n \\n  arXiv      preprint      arXiv:2305.16381.</p><p>Finn,      C.,      S.      Levine,      and      P.      Abbeel      (2016).\\nGuided      cost      learning:      Deep      inverse      optimal      control      via  \\n \\n  policy      optimization.\\nIn      International      conference      on      machine      learning,      pp.\\n49-58.\\nPMLR.</p><p>Fox,      R.,      A.      Pakman,      and      N.      Tishby      (2015).\\nTaming      the      noise      in      reinforcement      learning      via      soft  \\n \\n  updates.\\narXiv      preprint      arXiv:      1512.08562.</p><p>Frey,      N.      C.,      D.      Berenberg,      K.      Zadorozhny,      J.      Kleinhenz,      J.      Lafrance-Vanasse,      I.      Hotzel,      Y.      Wu, S.      Ra,   \\n \\nR R.      Bonneau,      K.      Cho,      et      al.\\n(2023).\\nProtein      discovery      with      discrete      walk-jump      sampling.\\n \\n \\n  arXiv      preprint      arXiv:2306.\\n12360.</p><p>Geist,      M.,      B.      Scherrer,      and      O.      Pietquin      (2019).\\n \\n \\nA      theory      of      regularized      markov      decision      processes.\\n \\n \\n  In      International      Conference      on      Machine      Learning,      pp.\\n2160-2169.\\nPMLR.</p><p> \\n \\n  Girolami,      M.      and      B.      Calderhead      (2011).\\nRiemann      manifold      langevin      and      hamiltonian      monte  \\n \\n  carlo      methods.\\nJournal      of      the      Royal      Statistical      Society      Series      B:      Statistical      Methodology      73(2),  \\n \\n  123-214.</p><p>Gosai,      S.      J.,      R.      I.      Castro,      N.      Fuentes,      J.      C.      Butts,      S.      Kales,      R.      R.      Noche,      K.      Mouri,      P.      C.      Sabeti, S.   \\n \\nK K.      Reilly,      and      R.      Tewhey      (2023).\\nMachine-guided      design      of      synthetic      cell      type-specific  \\n \\n  cis-regulatory      elements.\\nbioRxiv.</p><p>Haarnoja,      T.,      H.      Tang,      P.      Abbeel,      and      S.      Levine      (2017).\\nReinforcement      learning      with      deep      energy-  \\n \\n  based      policies.\\nIn      International      conference      on      machine      learning,      pp.\\n1352-1361.\\nPMLR.</p><p>Hansen-Estruch,      P.,      I.      Kostrikov,      M.      Janner,      J.      G.      Kuba,      and      S.      Levine      (2023).\\nIdql:      Implicit  \\n \\n  q-learning      as      an      actor-critic      method      with      diffusion      policies.\\narXiv      preprint      arXiv:2304.10573.</p><p>Heng,      J.,      A.      N.      Bishop,      G.      Deligiannidis,      and      A.      Doucet      (2020).\\nControlled      sequential      monte      carlo.</p><p>Ho,      J.,      A.      Jain,      and      P.      Abbeel      (2020).\\nDenoising      diffusion      probabilistic      models.\\nAdvances      in      neural  \\n \\n  information      processing      systems      33,      6840-6851.</p><p>Ho,      J.      and      T.      Salimans      (2022).\\nClassifier-free      diffusion      guidance.\\narXiv      preprint      arXiv:2207.12598.</p><p>Ho,      J.,      T.      Salimans,      A.      Gritsenko,      W.      Chan,      M.      Norouzi,      and      D.      J.      Fleet      (2022).\\nVideo      diffusion  \\n \\n  models.\\nAdvances      in      Neural      Information      Processing      Systems      35,      8633-8646.</p><p> \\n \\n  Hoogeboom,      E.,      V.      G.      Satorras,      C.      Vignac,      and      M.      Welling      (2022).\\nEquivariant      diffusion      for  \\n \\n  molecule      generation      in      3d.\\nIn      International      conference      on      machine      learning,      pp.\\n8867-8887.</p><h4>      PMLR.</h4><p>Huang,      X.,      H.      Dong,      H.      Yifan,      Y.      Ma,      and      T.      Zhang      (2023).\\nReverse      diffusion      monte      carlo.\\nIn      The  \\n \\n  Twelfth      International      Conference      on      Learning      Representations.</p><p>Janner,      M.,      Y.      Du,      J.      B.      Tenenbaum,      and      S.      Levine      (2022).\\nPlanning      with      diffusion      for      flexible  \\n \\n  behavior      synthesis.\\narXiv      preprint      arXiv:2205.09991.</p><p> \\n \\n  Jo,      J.,      S.      Lee,      and      S.      J.      Hwang      (2022).\\nScore-based      generative      modeling      of      graphs      via      the      system  \\n \\n  of      stochastic      differential      equations.\\nIn      International      Conference      on      Machine      Learning,      pp.\\n \\n \\n  10362-10383.\\nPMLR.</p><p> \\n \\n  Kazim,      M.,      J.      Hong,      M.-G.\\nKim,      and      K.-K.\\nK.      Kim      (2024).\\nRecent      advances      in      path      integral      control  \\n \\n  for      trajectory      optimization:      An      overview      in      theoretical      and      algorithmic      perspectives.\\nAnnual  \\n \\n  Reviews      in      Control      57,      100931.</p><p>Krishnamoorthy,      S.,      S.      M.      Mashkaria,      and      A.      Grover      (2023).\\nDiffusion      models      for      black-box  \\n \\n  optimization.\\narXiv      preprint      arXiv:2306.07180.</p><p>Kumar,      A.,      A.      Zhou,      G.      Tucker,      and      S.      Levine      (2020).\\nConservative      q-learning      for      offline      reinforce-  \\n \\n  ment      learning.\\nAdvances      in      Neural      Information      Processing      Systems      33,      1179-1191.</p><p>Lal,      A.,      D.      Garfield,      T.      Biancalani,      and      G.      Eraslan      (2024).\\nreglm:      Designing      realistic      regulatory  \\n \\n  dna      with      autoregressive      language      models.\\nbioRxiv,      2024-02.</p><p>Lee,      K.,      H.      Liu,      M.      Ryu,      O.      Watkins,      Y.      Du,      C.      Boutilier,      P.      Abbeel,      M.      Ghavamzadeh,      and      S.      S.      Gu  \\n \\n  (2023).\\nAligning      text-to-image      models      using      human      feedback.\\narXiv      preprint      arXiv:2302.12192.</p><p>Levine,      S.      (2018).\\nReinforcement      learning      and      control      as      probabilistic      inference:      Tutorial      and       review.\\narXiv      preprint      arXiv:      1805.00909.</p><p>Li,      Z.,      Y.      Ni,      T.      A.      B.      Huygelen,      A.      Das,      G.      Xia,      G.-B.\\nStan,      and      Y.      Zhao      (2023).\\nLatent      diffusion  \\n \\n  model      for      dna      sequence      generation.\\narXiv      preprint      arXiv:2310.06150.</p><p>Lipman,      Y.,      R.      T.      Chen,      H.      Ben-Hamu,      M.      Nickel,      and      M.      Le      (2023).\\nFlow      matching      for      generative  \\n \\n  modeling.\\nJCLR      2023.</p><p>Liu,      X.,      C.      Gong,      and      Q.      Liu      (2022).\\nFlow      straight      and      fast:      Learning      to      generate      and      transfer      data  \\n \\n  with      rectified      flow.\\narXiv      preprint      arXiv:2209.03003.</p><p>Liu,      X.,      L.      Wu,      M.      Ye,      and      Q.      Liu      (2022).\\nLet      us      build      bridges:      Understanding      and      extending  \\n \\n  diffusion      generative      models.\\narXiv      preprint      arXiv:2208.14699.</p><p>Lou,      A.,      C.      Meng,      and      S.      Ermon      (2023).\\nDiscrete      diffusion      language      modeling      by      estimating      the  \\n \\n  ratios      of      the      data      distribution.\\narXiv      preprint      arXiv:2310.16834.</p><p>Malkin,      N.,      M.      Jain,      E.      Bengio,      C.      Sun,      and      Y.      Bengio      (2022).\\nTrajectory      balance:      Improved      credit  \\n \\n  assignment      in      gflownets.\\nAdvances      in      Neural      Information      Processing      Systems      35,      5955-5967.</p><p>Mohammadpour,      S.,      E.      Bengio,      E.      Frejinger,      and      P.-L.\\nBacon      (2023).\\nMaximum      entropy      gflownets  \\n \\n  with      soft      q-learning.\\narXiv      preprint      arXiv:2312.14331.</p><p>Nachum,      O.,      M.      Norouzi,      K.      Xu,      and      D.      Schuurmans      (2017).\\nBridging      the      gap      between      value      and       policy      based      reinforcement      learning.\\nAdvances      in      neural      information      processing      systems      30.</p><p>Neal,      R.      M.      et      al.\\n(2011).\\nMcmce      using      hamiltonian      dynamics.\\nHandbook      of      markov      chain      monte  \\n \\n  carlo      2(11),      2.</p><p>Neu,      G.,      A.      Jonsson,      and      V.      Gémez      (2017).\\n \\n \\nA      unified      view      of      entropy-regularized      markov      decision  \\n \\n  processes.\\narXiv      preprint      arXiv:      1705.07798.</p><p>Ouyang,      L.,      J.      Wu,      X.      Jiang,      D.      Almeida,      C.      Wainwright,      P.      Mishkin,      C.      Zhang,      S.      Agarwal, K.      Slama,   \\n \\nA A.      Ray,      et      al.\\n(2022).\\nTraining      language      models      to      follow      instructions      with      human  \\n \\n  feedback.\\nAdvances      in      Neural      Information      Processing      Systems      35,      27730-27744.</p><p>Pattanaik,      L.      and      C.      W.      Coley      (2020).\\nMolecular      representation:      going      long      on      fingerprints.\\n \\n \\n  Chem      6(6),      1204-1207.</p><p>Peng,      X.      B.,      A.      Kumar,      G.      Zhang,      and      S.      Levine      (2019).\\nAdvantage-weighted      regression:      Simple  \\n \\n  and      scalable      off-policy      reinforcement      learning.\\narXiv      preprint      arXiv:1910.00177.</p><p>Peters,      J.,      K.      Mulling,      and      Y.      Altun      (2010).\\nRelative      entropy      policy      search.\\nIn      Proceedings      of      the  \\n \\n  AAAI      Conference      on      Artificial      Intelligence,      Volume      24,      pp.\\n1607-1612.</p><p> \\n \\n  Podell,      D.,      Z.      English,      K.      Lacey,      A.      Blattmann,      T.      Dockhorn,      J.      Miiller,      J.      Penna,      and      R.      Rombach  \\n \\n  (2023).\\nSdxl:      Improving      latent      diffusion      models      for      high-resolution      image      synthesis.\\narXiv  \\n \\n  preprint      arXiv:2307.01952.</p><p>Prabhudesai,      M.,      A.      Goyal,      D.      Pathak,      and      K.      Fragkiadaki      (2023).\\nAligning      text-to-image      diffusion  \\n \\n  models      with      reward      backpropagation.\\narXiv      preprint      arXiv:2310.03739.</p><p>Ramesh,      A.,      P.      Dhariwal,      A.      Nichol,      C.      Chu,      and      M.      Chen      (2022).\\nHierarchical      text-conditional  \\n \\n  image      generation      with      clip      latents.\\narXiv      preprint      arXiv:2204.06125      1(2),      3.</p><p>Rigter,      M.,      B.      Lacerda,      and      N.      Hawes      (2022).\\nRambo-rl:      Robust      adversarial      model-based      offline  \\n \\n  reinforcement      learning.\\nAdvances      in      neural      information      processing      systems      35,      16082-16097.</p><p>Robert,      C.      (2014).\\nStatistical      modeling      and      computation.</p><p>Rogers,      L.      C.      G.      and      D.      Williams      (2000).\\nDiffusions,      Markov      processes      and      martingales:      Volume      2,  \\n \\n  It6é      calculus,      Volume      2.\\nCambridge      university      press.</p><p> \\n \\n  Rombach,      R.,      A.      Blattmann,      D.      Lorenz,      P.      Esser,      and      B.      Ommer      (2022,      June).\\nHigh-resolution  \\n \\n  image      synthesis      with      latent      diffusion      models.\\nIn      Proceedings      of      the      IEEE/CVF      Conference      on  \\n \\n  Computer      Vision      and      Pattern      Recognition      (CVPR),      pp.\\n10684-10695.</p><p>Sarkar,      A.,      Z.      Tang,      C.      Zhao,      and      P.      Koo      (2024).\\nDesigning      dna      with      tunable      regulatory      activity  \\n \\n  using      discrete      diffusion.\\nbioRxiv,      2024-05.</p><p>Schulman,      J.,      X.      Chen,      and      P.      Abbeel      (2017).\\nEquivalence      between      policy      gradients      and      soft  \\n \\n  q-learning.\\narXiv      preprint      arXiv:      1704.06440.</p><p>Schulman,      J.,      S.      Levine,      P.      Abbeel,      M.      Jordan,      and      P.      Moritz      (2015).\\nTrust      region      policy      optimization.\\n \\n \\n  In      International      conference      on      machine      learning,      pp.\\n1889-1897.\\nPMLR.</p><p>Schulman,      J.,      F.      Wolski,      P.      Dhariwal,      A.      Radford,      and      O.      Klimov      (2017).\\nProximal      policy      optimiza-  \\n \\n  tion      algorithms.\\narXiv      preprint      arXiv:      1707.06347.</p><p>Shi,      Y.,      V.      De      Bortoli,      A.      Campbell,      and      A.      Doucet      (2024).\\nDiffusion      schrédinger      bridge      matching.\\n \\n \\n  Advances      in      Neural      Information      Processing      Systems      36.</p><p> \\n \\n  Sohl-Dickstein,      J.,      E.      Weiss,      N.      Maheswaranathan,      and      S.      Ganguli      (2015).\\nDeep      unsupervised  \\n \\n  learning      using      nonequilibrium      thermodynamics.\\nIn      /nternational      conference      on      machine      learning,  \\n \\n  pp.\\n2256-2265.\\nPMLR.</p><p>Song,      J.,      C.      Meng,      and      S.      Ermon      (2020).\\nDenoising      diffusion      implicit      models.\\narXiv      preprint  \\n \\n  arXiv:2010.02502.</p><p>Song,      Y.,      C.      Durkan,      I.      Murray,      and      S.      Ermon      (2021).\\nMaximum      likelihood      training      of      score-based  \\n \\n  diffusion      models.\\nAdvances      in      neural      information      processing      systems      34,      1415-1428.</p><p>Stark,      H.,      B.      Jing,      C.      Wang,      G.      Corso,      B.      Berger,      R.      Barzilay,      and      T.      Jaakkola      (2024).\\nDirichlet  \\n \\n  flow      matching      with      applications      to      dna      sequence      design.\\narXiv      preprint      arXiv:2402.05841.</p><p>Sutton,      R.      S.      and      A.      G.      Barto      (2018).\\nReinforcement      learning:      An      introduction.\\nMIT      press.</p><p>Tang,      W.      (2024).\\nFine-tuning      of      diffusion      models      via      stochastic      control:      entropy      regularization      and       beyond.\\narXiv      preprint      arXiv:2403.06279.</p><p>Tang,      W.      and      H.      Zhao      (2024).\\nScore-based      diffusion      models      via      stochastic      differential      equations—a  \\n \\n  technical      tutorial.\\narXiv      preprint      arXiv:2402.07487.</p><p>Theodorou,      E.,      J.      Buchli,      and      S.      Schaal      (2010).\\n \\n \\nA      generalized      path      integral      control      approach      to  \\n \\n  reinforcement      learning.\\nThe      Journal      of      Machine      Learning      Research      11,      3137-3181.</p><p>Tiapkin,      D.,      N.      Morozov,      A.      Naumov,      and      D.      Vetrov      (2023).\\nGenerative      flow      networks      as      entropy-  \\n \\n  regularized      rl.\\narXiv      preprint      arXiv:2310.12934.</p><p> \\n \\n  Tong,      A.,      N.      Malkin,      G.      Huguet,      Y.      Zhang,      J.      Rector-Brooks,      K.      Fatras,      G.      Wolf,      and      Y.      Bengio  \\n \\n  (2023).\\nConditional      flow      matching:      Simulation-free      dynamic      optimal      transport.\\narXiv      preprint  \\n \\n  arXiv:2302.00482.</p><p>Uehara,      M.      and      W.      Sun      (2021).\\nPessimistic      model-based      offline      reinforcement      learning      under  \\n \\n  partial      coverage.\\narXiv      preprint      arXiv:2107.06226.</p><p>Uehara,      M.,      Y.      Zhao,      K.      Black,      E.      Hajiramezanali,      G.      Scalia,      N.      L.      Diamant,      A.      M.      Tseng, T.      Biancalani,      and      S S.      Levine      (2024).\\nFine-tuning      of      continuous-time      diffusion      models      as      entropy-  \\n \\n  regularized      control.\\narXiv      preprint      arXiv:2402.15194.</p><p> \\n \\n  Uehara,      M.,      Y.      Zhao,      K.      Black,      E.      Hajiramezanali,      G.      Scalia,      N.      L.      Diamant,      A.      M.      Tseng,      S.      Levine,  \\n \\n  and      T.      Biancalani      (2024).\\nFeedback      efficient      online      fine-tuning      of      diffusion      models.\\narXiv      preprint  \\n \\n  arXiv:2402.\\n16359.</p><p> \\n \\n  Uehara,      M.,      Y.      Zhao,      E.      Hajiramezanali,      G.      Scalia,      G.      Eraslan,      A.      Lal,      S.      Levine,      and      T.      Biancalani  \\n \\n  (2024).\\nBridging      model-based      optimization      and      generative      modeling      via      conservative      fine-tuning  \\n \\n  of      diffusion      models.\\narXiv      preprint      arXiv:2405.19673.</p><p>Vargas,      F.,      W.      Grathwohl,      and      A.      Doucet      (2023).\\nDenoising      diffusion      samplers.\\narXiv      preprint  \\n \\n  arXiv:2302.\\n13834.</p><p> \\n \\n  Wallace,      B.,      M.      Dang,      R.      Rafailov,      L.      Zhou,      A.      Lou,      S.      Purushwalkam,      S.      Ermon,      C.      Xiong,      S.      Joty,  \\n \\n  and      N.      Naik      (2023).\\nDiffusion      model      alignment      using      direct      preference      optimization.\\narXiv  \\n \\n  preprint      arXiv:2311.12908.</p><p>Wang,      Z.,      J.      J.      Hunt,      and      M.      Zhou      (2022).\\nDiffusion      policies      as      an      expressive      policy      class      for      offline  \\n \\n  reinforcement      learning.\\narXiv      preprint      arXiv:2208.06193.</p><p>Widatalla,      T.,      R.      Rafailov,      and      B.      Hie      (2024).\\nAligning      protein      generative      models      with      experimental  \\n \\n  fitness      via      direct      preference      optimization.\\nbioRxiv,      2024-05.</p><p>Williams,      G.,      A.      Aldrich,      and      E.      A.      Theodorou      (2017).\\nModel      predictive      path      integral      control:      From  \\n \\n  theory      to      parallel      computation.\\nJournal      of      Guidance,      Control,      and      Dynamics      40(2),      344-357.</p><p>Wu,      X.,      K.      Sun,      F.      Zhu,      R.      Zhao,      and      H.      Li      (2023).\\nBetter      aligning      text-to-image      models      with  \\n \\n  human      preference.\\narXiv      preprint      arXiv:2303.14420.</p><p>Wu,      Y.,      G.      Tucker,      and      O.      Nachum      (2019).\\nBehavior      regularized      offline      reinforcement      learning.\\n \\n \\n  arXiv      preprint      arXiv:      1911.11361.</p><p>Wulfmeier,      M.,      P.      Ondruska,      and      I.      Posner      (2015).\\nMaximum      entropy      deep      inverse      reinforcement  \\n \\n  learning.\\narXiv      preprint      arXiv:      1507.04888.</p><p>Xie,      T.,      C.-A.\\nCheng,      N.      Jiang,      P.      Mineiro,      and      A.      Agarwal      (2021).\\nBellman-consistent      pessimism  \\n \\n  for      offline      reinforcement      learning.\\nAdvances      in      neural      information      processing      systems      34.</p><p>Xu,      M.,      L.      Yu,      Y.      Song,      C.      Shi,      S.      Ermon,      and      J.      Tang      (2022).\\nGeodiff:   \\n \\nA      geometric      diffusion  \\n \\n  model      for      molecular      conformation      generation.\\narXiv      preprint      arXiv:2203.02923.</p><p> \\n \\n  Yang,      L.,      Z.      Zhang,      Y.      Song,      S.      Hong,      R.      Xu,      Y.      Zhao,      W.      Zhang,      B.      Cui,      and      M.-H.\\nYang      (2023).\\n \\n \\n  Diffusion      models:   \\n \\nA      comprehensive      survey      of      methods      and      applications.\\nACM      Computing  \\n \\n  Surveys      56(4),      1-39.</p><p> \\n \\n  Yu,      T.,      G.      Thomas,      L.      Yu,      S.      Ermon,      J.      Y.      Zou,      S.      Levine,      C.      Finn,      and      T.      Ma      (2020).\\nMopo:  \\n \\n  Model-based      offline      policy      optimization.\\nAdvances      in      Neural      Information      Processing      Systems      33,  \\n \\n  14129-14142.</p><p> \\n \\n  Yuan,      H.,      K.      Huang,      C.      Ni,      M.      Chen,      and      M.      Wang      (2023).\\nReward-directed      conditional      diffusion:  \\n \\n  Provable      distribution      estimation      and      reward      improvement.\\nIn      Thirty-seventh      Conference      on  \\n \\n  Neural      Information      Processing      Systems.</p><p>Zhang,      H.      and      T.      Xu      (2023).\\nTowards      controllable      diffusion      models      via      reward-guided      exploration.\\n \\n \\n  arXiv      preprint      arXiv:2304.07132.</p><p>Zhang,      Q.      and      Y.      Chen      (2021).\\nPath      integral      sampler:   \\n \\na      stochastic      control      approach      for      sampling.\\n \\n \\n  arXiv      preprint      arXiv:2111,15141.</p><p> \\n \\n  Zhao,      Y.,      M.      Uehara,      G.      Scalia,      T.      Biancalani,      S.      Levine,      and      E.      Hajiramezanali      (2024).\\n \\n \\n  Adding      conditional      control      to      diffusion      models      with      reinforcement      learning.\\narXiv      preprint  \\n \\n  arXiv:2406.\\n12120.</p><p>Zhou,      Z.,      S.      Kearnes,      L.      Li,      R.      N.      Zare,      and      P.      Riley      (2019).\\nOptimization      of      molecules      via      deep  \\n \\n  reinforcement      learning.\\nScientific      reports      9(1),      10752.</p><p>Zhu,      Z.,      H.      Zhao,      H.      He,      Y.      Zhong,      S.      Zhang,      Y.      Yu,      and      W.      Zhang      (2023).\\nDiffusion      models      for  \\n \\n  reinforcement      learning:   \\n \\nA      survey.\\narXiv      preprint      arXiv:2311.01223.</p><p>Ziebart,      B.      D.,      A.      L.      Maas,      J.      A.      Bagnell,      A.      K.      Dey,      et      al.\\n(2008).\\nMaximum      entropy      inverse  \\n \\n  reinforcement      learning.\\nIn      Aaai,      Volume      8,      pp.\\n1433-1438.\\nChicago,      IL,      USA.</p><h2>6.1.1      Relation      with      Loss      Functions      for      the      Original      Training      Objective</h2><p>tive In      this      subsection,      we      explore      the      connection      with      the      original      loss      function      of      pre-trained      diffusion  \\n \\n  models.\\nTo      see      that,      as      we      see      in      Section      1.1.1,      recall ol)   \\n \\n=      al   \\n \\n4      (0.50,   \\n \\n—      1/o%eg,,,      (a1,      T   \\n \\n—      t)](5t)      +e?\\n02,      €)   \\n \\n~      N(0,1).</p><p> \\n \\n \\n/       \\\\\\\\\\\\\\\\-  \\n \\n  (al?\\nt;Opre) Then,      the      loss      function      (24)      in      reward-weighted      MLE      reduces      to</p><table><th><td colSpan=1>      =      r(x”)       A,      —      Vo      S>      Ss\"      exp      |      ~~      —       t=T+1      i=1</td><td colSpan=1>      .      _      ;      2       (i,t)      (ay,      Tt;      Opre)      _      eal,      T      -t;      0)       ey      _       {or}?</td><td colSpan=1>|o=0</td></th><tr><td colSpan=1></td><td colSpan=1>      2</td><td colSpan=1>      (26)</td></tr></table><p>This      objective      function      closely      resembles      the      reward-weighted      version      of      the      loss      function      (5)      used  \\n \\n  for      training      pre-trained      diffusion      models.</p><h3>6.2      Value-Weighted      Sampling</h3><p>ling  \\n \\n  Thus      far,      we      have      discussed      methods      for      fine-tuning      pre-trained      diffusion      models.\\nNow,      let’s      delve  \\n \\n  into      an      alternative      approach      during      inference      that      aims      to      sample      from      the      target      distribution      p,.\\n \\n \\n  without      explicitly      fine-tuning      the      diffusion      models.\\nIn      essence,      this      approach      involves      incorporating  \\n \\n  gradients      of      value      functions      during      inference      alongside      the      denoising      process      in      pre-trained      diffusion  \\n \\n  models.\\nHence,      we      refer      to      this      approach      as      value-weighted      sampling.\\nWhile      it      seems      that      this  \\n \\n  method      has      not      been      explicitly      formalized      in      previous      literature,      the      value-weighted      sampling  \\n \\n  closely      connects      with      classifier      guidance,      as      discussed      in      Section      8.1.  \\n \\n  Before      delving      into      the      algorithmic      details,      we      outline      the      motivation.\\nSubsequently,      we      present  \\n \\n  the      concrete      algorithm      and      discuss      its      advantages—specifically,      its      capability      to      operate      without      the  \\n \\n  necessity      of      fine-tuning      diffusion      models—and      its      disadvantage,      which      involves      the      need      to      obtain  \\n \\n  differentiable      value      functions.</p><p>Algorithm   \\n \\n4      Value-weighted      sampling 1:      Require:      Pre-trained      model      {p?\\n\"*(2,_1|r1)      }:   \\n \\n=      {N(0(      22,      t;      Opre),      07(t))      be.\\n \\n \\n  2:      Estimate   \\n \\nv   \\n \\n:   \\n \\n¥   \\n \\nx      [0,7]   \\n \\n—   \\n \\nR      and      denote      it      by      i(.,      -) ¢\\n \\n   (1)      Monte-Carlo      approach      in      (28) e      (2)      Value      iteration      approach      (Soft      Q-learning)      in      (29)      in      Section      6.2.1 ¢\\n \\n   (3)      Approximation      using      Tweedie’s      formula      in      Section      6.2.2 3:      fort   \\n \\n€      [7      +1,---   \\n \\n,      1}      do  \\n \\n  4:      Set.\\na?\\nt      Ve0   \\n \\nx      xL=Lt  \\n \\n  P(t,      t)   \\n \\n=   \\n \\n(   \\n \\n)      \\\\\\\\\\\\\\\\   \\n \\nu   \\n \\n+      p(xz,t      Are)</p><p>5:      end      for  \\n \\n  6:      Output:      {N(p      O(      Le,      t),o7(t))      ery</p><p> \\n \\n  Motivation.\\nConsidering   \\n \\na      Gaussian      policy      7:1   \\n \\n~      N(/(2:,t),07(t)),      we      aim      to      determine  \\n \\n  P(x4,t;      0)      such      that      N(A(2x;,      t;      0),      0?(t))      closely      approximates      p*(-|x,).\\nHere,      typically,      we      have  \\n \\n  p(xz,t;0)   \\n \\n=      a,   \\n \\n+      (t)g(a;,t)      and      o?(t)   \\n \\n=      g(t)(ot)      for      certain      function      g   \\n \\n:   \\n \\nY   \\n \\nx      [0,7]   \\n \\n>      R¢  \\n \\n  and      g   \\n \\n:      [0,7]   \\n \\n—      R,      as      we      have      explained      in      Section      1.1.1.\\nThen,      using   \\n \\nx      as      equality      up      to      the  \\n \\n  normalizing      constant, Piles      |)      exp(eralar-s)/a)      exp      (BEE      OI)</p><p>2\\n \\n   exp(vy(am)   \\n \\n+      Ver(      ee)»      {a1   \\n \\n—      3)      exp      (—      “2      Clr      uF)</p><p>_\\n \\n   lve1   \\n \\n—      ae   \\n \\n—      (58)      9(t)      Vora)      /o   \\n \\n—      (6t)9(ae,      t))      |?\\n \\n \\n  esp      (-      0.59(#)      (6t)      )</p><h4>Hence,      we      can      approximate:</h4><p> \\n \\n  o°(t)      Vu;      (a4)  \\n \\n  a   \\n \\n+      p(      xz,      ti      Ore):      (27)  \\n \\n  P(x,      t)      od</p><p>Remark      5.\\nNote      while      the      above      derivation      is      heuristic,      it      is      formalized      in      Uehara      et      al.\\n(2024,  \\n \\n  Lemma      2)      in      the      continuous-time      formulation.</p><p> \\n \\n  Algorithm.\\nUtilizing      (27),      the      entire      algorithm      of      value-weighted      sampling      is      outlined      in      Algo-  \\n \\n  rithm      4.\\nThis      method      doesn’t      involve      updating      parameters      in   \\n \\na      diffusion      model;      hence,      it’s      not   \\n \\na       fine-tuning      method.</p><p> \\n \\n  Note      that      in      this      algorithm,      we      require   \\n \\na      form      of      v;(-)      to      compute      gradients.\\nThis      value      function  \\n \\n  can      be      estimated      through      regression      using      the      characterization      described      in      Lemma      2.\\nHere,      inspired  \\n \\n  by      Lemma      2,      we      use      the      following      loss      function:</p><p>1\\n \\n   om      (i,t)      (i,t)   \\n \\n°       o(2)   \\n \\n=      argmin      S-      S-      {es      (me)   \\n \\n—      exp   \\n \\n|   \\n \\n,      (28)</p><p>h:[R4,{0,T]]-R      t=T+1      i=1</p><p>where      the      data      is      collected      in      an      off-policy      way.\\nLater,      we      explain      two      alternative      approaches      in  \\n \\n  Section      6.2.1      and      Section      6.2.2.</p><p> \\n \\n  Advantages      and      potential      disadvantages.\\nThe      value-weighted      sampling      is      straightforward      and       less      memory-intensive      since      it      avoids      fine-tuning.\\nTherefore,      it      presents      an      appealing      simple      option  \\n \\n  if      it      performs      well.\\nIndeed,      in      various      inverse      problems,      such      as      inpainting,      super-resolution,      and       colorization,      setting      r()      as   \\n \\na      likelihood      of      the      measurement      model   \\n \\ny   \\n \\n=      g(a)      +e      (where   \\n \\ny      represents  \\n \\n  actual      measurements,   \\n \\n€      denotes      noise,      and      g      defines      the      measurement      function)      has      proven      highly  \\n \\n  successful      (Chung      et      al.,      2022;      Bansal      et      al.,      2023;      Chung      et      al.,      2022).</p><p> \\n \\n  The      potential      drawback      compared      to      fine-tuning      algorithms      so      far      (i.e.,      PPO      and      reward-  \\n \\n  weighted      MLE)      is      the      necessity      to      learn      differetiable      soft      value      functions      like      direct      reward      back-  \\n \\n  propagation.\\nThis      learning      process      is      often      not      straightforward      as      explained      in      Section      5.2.\\nThe  \\n \\n  previously      discussed      fine-tuning      algorithms      (i.e.,      PPO,      reward-weighted      MLE)      circumvent      this  \\n \\n  requirement      by      obtaining      rewards      to      go      via      Monte      Carlo      approaches,      thereby      avoiding      the      need      for  \\n \\n  soft      value      functions.</p><h3>6.2      Value-Weighted      Sampling</h3><p>ling  \\n \\n  Thus      far,      we      have      discussed      methods      for      fine-tuning      pre-trained      diffusion      models.\\nNow,      let’s      delve  \\n \\n  into      an      alternative      approach      during      inference      that      aims      to      sample      from      the      target      distribution      p,.\\n \\n \\n  without      explicitly      fine-tuning      the      diffusion      models.\\nIn      essence,      this      approach      involves      incorporating  \\n \\n  gradients      of      value      functions      during      inference      alongside      the      denoising      process      in      pre-trained      diffusion  \\n \\n  models.\\nHence,      we      refer      to      this      approach      as      value-weighted      sampling.\\nWhile      it      seems      that      this  \\n \\n  method      has      not      been      explicitly      formalized      in      previous      literature,      the      value-weighted      sampling  \\n \\n  closely      connects      with      classifier      guidance,      as      discussed      in      Section      8.1.  \\n \\n  Before      delving      into      the      algorithmic      details,      we      outline      the      motivation.\\nSubsequently,      we      present  \\n \\n  the      concrete      algorithm      and      discuss      its      advantages—specifically,      its      capability      to      operate      without      the  \\n \\n  necessity      of      fine-tuning      diffusion      models—and      its      disadvantage,      which      involves      the      need      to      obtain  \\n \\n  differentiable      value      functions.</p><p>Algorithm   \\n \\n4      Value-weighted      sampling 1:      Require:      Pre-trained      model      {p?\\n\"*(2,_1|r1)      }:   \\n \\n=      {N(0(      22,      t;      Opre),      07(t))      be.\\n \\n \\n  2:      Estimate   \\n \\nv   \\n \\n:   \\n \\n¥   \\n \\nx      [0,7]   \\n \\n—   \\n \\nR      and      denote      it      by      i(.,      -) ¢\\n \\n   (1)      Monte-Carlo      approach      in      (28) e      (2)      Value      iteration      approach      (Soft      Q-learning)      in      (29)      in      Section      6.2.1 ¢\\n \\n   (3)      Approximation      using      Tweedie’s      formula      in      Section      6.2.2 3:      fort   \\n \\n€      [7      +1,---   \\n \\n,      1}      do  \\n \\n  4:      Set.\\na?\\nt      Ve0   \\n \\nx      xL=Lt  \\n \\n  P(t,      t)   \\n \\n=   \\n \\n(   \\n \\n)      \\\\\\\\\\\\\\\\   \\n \\nu   \\n \\n+      p(xz,t      Are)</p><p>5:      end      for  \\n \\n  6:      Output:      {N(p      O(      Le,      t),o7(t))      ery</p><p> \\n \\n  Motivation.\\nConsidering   \\n \\na      Gaussian      policy      7:1   \\n \\n~      N(/(2:,t),07(t)),      we      aim      to      determine  \\n \\n  P(x4,t;      0)      such      that      N(A(2x;,      t;      0),      0?(t))      closely      approximates      p*(-|x,).\\nHere,      typically,      we      have  \\n \\n  p(xz,t;0)   \\n \\n=      a,   \\n \\n+      (t)g(a;,t)      and      o?(t)   \\n \\n=      g(t)(ot)      for      certain      function      g   \\n \\n:   \\n \\nY   \\n \\nx      [0,7]   \\n \\n>      R¢  \\n \\n  and      g   \\n \\n:      [0,7]   \\n \\n—      R,      as      we      have      explained      in      Section      1.1.1.\\nThen,      using   \\n \\nx      as      equality      up      to      the  \\n \\n  normalizing      constant, Piles      |)      exp(eralar-s)/a)      exp      (BEE      OI)</p><p>2\\n \\n   exp(vy(am)   \\n \\n+      Ver(      ee)»      {a1   \\n \\n—      3)      exp      (—      “2      Clr      uF)</p><p>_\\n \\n   lve1   \\n \\n—      ae   \\n \\n—      (58)      9(t)      Vora)      /o   \\n \\n—      (6t)9(ae,      t))      |?\\n \\n \\n  esp      (-      0.59(#)      (6t)      )</p><h4>Hence,      we      can      approximate:</h4><p> \\n \\n  o°(t)      Vu;      (a4)  \\n \\n  a   \\n \\n+      p(      xz,      ti      Ore):      (27)  \\n \\n  P(x,      t)      od</p><p>Remark      5.\\nNote      while      the      above      derivation      is      heuristic,      it      is      formalized      in      Uehara      et      al.\\n(2024,  \\n \\n  Lemma      2)      in      the      continuous-time      formulation.</p><p> \\n \\n  Algorithm.\\nUtilizing      (27),      the      entire      algorithm      of      value-weighted      sampling      is      outlined      in      Algo-  \\n \\n  rithm      4.\\nThis      method      doesn’t      involve      updating      parameters      in   \\n \\na      diffusion      model;      hence,      it’s      not   \\n \\na       fine-tuning      method.</p><p> \\n \\n  Note      that      in      this      algorithm,      we      require   \\n \\na      form      of      v;(-)      to      compute      gradients.\\nThis      value      function  \\n \\n  can      be      estimated      through      regression      using      the      characterization      described      in      Lemma      2.\\nHere,      inspired  \\n \\n  by      Lemma      2,      we      use      the      following      loss      function:</p><p>1\\n \\n   om      (i,t)      (i,t)   \\n \\n°       o(2)   \\n \\n=      argmin      S-      S-      {es      (me)   \\n \\n—      exp   \\n \\n|   \\n \\n,      (28)</p><p>h:[R4,{0,T]]-R      t=T+1      i=1</p><p>where      the      data      is      collected      in      an      off-policy      way.\\nLater,      we      explain      two      alternative      approaches      in  \\n \\n  Section      6.2.1      and      Section      6.2.2.</p><p> \\n \\n  Advantages      and      potential      disadvantages.\\nThe      value-weighted      sampling      is      straightforward      and       less      memory-intensive      since      it      avoids      fine-tuning.\\nTherefore,      it      presents      an      appealing      simple      option  \\n \\n  if      it      performs      well.\\nIndeed,      in      various      inverse      problems,      such      as      inpainting,      super-resolution,      and       colorization,      setting      r()      as   \\n \\na      likelihood      of      the      measurement      model   \\n \\ny   \\n \\n=      g(a)      +e      (where   \\n \\ny      represents  \\n \\n  actual      measurements,   \\n \\n€      denotes      noise,      and      g      defines      the      measurement      function)      has      proven      highly  \\n \\n  successful      (Chung      et      al.,      2022;      Bansal      et      al.,      2023;      Chung      et      al.,      2022).</p><p> \\n \\n  The      potential      drawback      compared      to      fine-tuning      algorithms      so      far      (i.e.,      PPO      and      reward-  \\n \\n  weighted      MLE)      is      the      necessity      to      learn      differetiable      soft      value      functions      like      direct      reward      back-  \\n \\n  propagation.\\nThis      learning      process      is      often      not      straightforward      as      explained      in      Section      5.2.\\nThe  \\n \\n  previously      discussed      fine-tuning      algorithms      (i.e.,      PPO,      reward-weighted      MLE)      circumvent      this  \\n \\n  requirement      by      obtaining      rewards      to      go      via      Monte      Carlo      approaches,      thereby      avoiding      the      need      for  \\n \\n  soft      value      functions.</p><h4>Hence,      we      can      approximate:</h4><p> \\n \\n  o°(t)      Vu;      (a4)  \\n \\n  a   \\n \\n+      p(      xz,      ti      Ore):      (27)  \\n \\n  P(x,      t)      od</p><p>Remark      5.\\nNote      while      the      above      derivation      is      heuristic,      it      is      formalized      in      Uehara      et      al.\\n(2024,  \\n \\n  Lemma      2)      in      the      continuous-time      formulation.</p><p> \\n \\n  Algorithm.\\nUtilizing      (27),      the      entire      algorithm      of      value-weighted      sampling      is      outlined      in      Algo-  \\n \\n  rithm      4.\\nThis      method      doesn’t      involve      updating      parameters      in   \\n \\na      diffusion      model;      hence,      it’s      not   \\n \\na       fine-tuning      method.</p><p> \\n \\n  Note      that      in      this      algorithm,      we      require   \\n \\na      form      of      v;(-)      to      compute      gradients.\\nThis      value      function  \\n \\n  can      be      estimated      through      regression      using      the      characterization      described      in      Lemma      2.\\nHere,      inspired  \\n \\n  by      Lemma      2,      we      use      the      following      loss      function:</p><p>1\\n \\n   om      (i,t)      (i,t)   \\n \\n°       o(2)   \\n \\n=      argmin      S-      S-      {es      (me)   \\n \\n—      exp   \\n \\n|   \\n \\n,      (28)</p><p>h:[R4,{0,T]]-R      t=T+1      i=1</p><p>where      the      data      is      collected      in      an      off-policy      way.\\nLater,      we      explain      two      alternative      approaches      in  \\n \\n  Section      6.2.1      and      Section      6.2.2.</p><p> \\n \\n  Advantages      and      potential      disadvantages.\\nThe      value-weighted      sampling      is      straightforward      and       less      memory-intensive      since      it      avoids      fine-tuning.\\nTherefore,      it      presents      an      appealing      simple      option  \\n \\n  if      it      performs      well.\\nIndeed,      in      various      inverse      problems,      such      as      inpainting,      super-resolution,      and       colorization,      setting      r()      as   \\n \\na      likelihood      of      the      measurement      model   \\n \\ny   \\n \\n=      g(a)      +e      (where   \\n \\ny      represents  \\n \\n  actual      measurements,   \\n \\n€      denotes      noise,      and      g      defines      the      measurement      function)      has      proven      highly  \\n \\n  successful      (Chung      et      al.,      2022;      Bansal      et      al.,      2023;      Chung      et      al.,      2022).</p><p> \\n \\n  The      potential      drawback      compared      to      fine-tuning      algorithms      so      far      (i.e.,      PPO      and      reward-  \\n \\n  weighted      MLE)      is      the      necessity      to      learn      differetiable      soft      value      functions      like      direct      reward      back-  \\n \\n  propagation.\\nThis      learning      process      is      often      not      straightforward      as      explained      in      Section      5.2.\\nThe  \\n \\n  previously      discussed      fine-tuning      algorithms      (i.e.,      PPO,      reward-weighted      MLE)      circumvent      this  \\n \\n  requirement      by      obtaining      rewards      to      go      via      Monte      Carlo      approaches,      thereby      avoiding      the      need      for  \\n \\n  soft      value      functions.</p><h2>6.2.1      Soft      Q-learning</h2><p>ning  \\n \\n  We      have      elucidated      that      leveraging      Lemma      2,      we      can      estimate      soft      value      functions      v;(-)      based  \\n \\n  on      Equation      (28)      in   \\n \\na      Monte      Carlo      way.\\nSubsequently,      these      soft      value      functions      are      used      in  \\n \\n  value-weighted      sampling      to      sample      from      the      target      distribution      p,.\\nAlternatively,      there      is      another  \\n \\n  method      that      involves      using      soft      Bellman      equations      to      estimate      soft      value      functions      v;(-).\\nThis  \\n \\n  technique      is      commonly      called      soft      Q-learning      in      the      context      of      standard      RL.</p><p> \\n \\n  First,      recalling      the      soft      Bellman      equations      in      (16),      we      have en      (22)   \\n \\n=      fey      (=)      pa      nd</p><p>Taking      the      logarithm,      we      obtain u(ay)   \\n \\n=      alog   \\n \\nf      exp      (eaen)      De      (@4-1   \\n \\n|      @4)dxy-1.</p><p>a</p><h3>Hence,</h3><p> \\n \\n \\n_   \\n \\n:      (xt)      Up-1(Lt-1)   \\n \\n°       vu   \\n \\n=      argmin      Ey,.u,   \\n \\n|   \\n \\n4      ——   \\n \\n—      log   \\n \\n|      exp   \\n \\n|      ————   \\n \\n]      Dpre(@e-1]      21)      da4_1  \\n \\n  a   \\n \\na h:X->R  \\n \\n  where      u,   \\n \\n€      A(%)      is      any      roll-in      distribution      that      covers      the      entire      space      V.      Using      this      relation      and       replacing      the      expectation      E,,,.,,,      with      empirical      approximation,      we      are      able      to      estimate      soft      value  \\n \\n  functions      v;      in   \\n \\na      recursive      manner:</p><p> \\n \\n \\n1   \\n \\nm      ~(j—1)   \\n \\n°       aj   \\n \\n;   \\n \\ni      Ur      Ut   \\n \\ny       {6      (x)},   \\n \\n—       argmin   \\n \\ny      S      {ht   \\n \\n)   \\n \\n—      log   \\n \\n/      exp      (eee)      Ppre(@1—1|}      yan] h:      [R¢,[0,7]]|-R      t=T+1      i=1</p><p>where      the      data      is      collected      in      an      off-policy      way.</p><p> \\n \\n  Remark      6.\\nAlthough      soft      Q-learning      is      widely      used      in      standard      RL      (Schulman      et      al.,      2017),      it  \\n \\n  cannot      be      directly      applied      to      our      fine-tuning      context      without      resorting      to      value-weighted      sampling  \\n \\n  or      value-weighted      MLE.\\nThis      is      because,      even      if      we      estimate      soft-value      functions      as      %;,      substituting  \\n \\n  Uz      with      0,      in      the      soft-optimal      policy      results      in      an      unnormalized      policy.<ul><li>6.2.2.\\nApproximation      using      Tweedie’s      formula</li></ul></p><p> \\n \\n  So      far,      we      have      explained      two      approaches:   \\n \\na      Monte      Carlo      approach      and   \\n \\na      value      iteration      approach      to  \\n \\n  estimate      soft      value      functions.\\nHowever,      learning      value      functions      in      (28)      can      still      be      often      challenging  \\n \\n  in      practice.\\nTherefore,      we      can      employ      approximation      strategies      inspired      by      recent      literature      on  \\n \\n  classifier      guidance      (e.g.,      reconstruction      guidance      (Ho      et      al.,      2022),      manifold      constrained      gradients  \\n \\n  (Chung      et      al.,      2022),      universal      guidance      (Bansal      et      al.,      2023),      and      diffusion      posterior      sampling  \\n \\n  (Chung      et      al.,      2022)).</p><p> \\n \\n  Specifically,      we      adopt      the      following      approximation:</p><p>ur(xz)   \\n \\n=      alog      Egpry      lex      (“)      oy   \\n \\n=      alog      (/      exp      (om)   \\n \\np      (colar      (30)</p><p>~\\n \\n   alog      (exw      (ceo)   \\n \\n)   \\n \\n»       £o(t1)   \\n \\n=      Egprey|xo   \\n \\n|      xe],      (31) a\\n \\n    =\\n \\n   1r(Xo(2t)).</p><p> \\n \\n  Here,      we      replace      the      integration      in      (30)      with   \\n \\na      Dirac      delta      distribution      with      the      posterior      mean.\\n \\n \\n  Importantly,      we      can      calculate      Zo      (x;)   \\n \\n=      E,»*-[xo   \\n \\n|      x4]      using      the      pre-trained      (score-based)      diffusion  \\n \\n  model      based      on      Tweedie’s      formula:</p><table><tr><td colSpan=1>      O12</td><td colSpan=1><p>]\\n \\n \\n  E,pre      [xo   \\n \\n|      x1   \\n \\n—      Ut   \\n \\n+      {or}   \\n \\nV      0g      Ge(&e)  \\n \\n  Le</p></td></tr></table><p>Recall      that      the      notation      p/?,      0?,      q,      are      defined      in      (3).\\nFinally,      by      recalling   \\n \\nV      log   \\n \\n@   \\n \\n=      Si      ore      (x;,t)      in  \\n \\n  score-based      diffusion      models,      we      can      approximate      Vv;(a)      with</p><h4>Le</h4><p> \\n \\n  and      plug      it      into      Algorithm   \\n \\n4      (i.e.,      value-weighted      sampling).\\n \\n \\n  As      mentioned      earlier,      this      approach      has      been      practically      widely      used      in      the      context      of      classifier  \\n \\n  guidance.\\nDespite      its      simplicity,      the      approximation      error      can      be      significant,      as      it      can      not      be  \\n \\n  diminished      even      with   \\n \\na      large      sample      size      because      the      discrepancy      from      (30)      to      (31)      is      not      merely   \\n \\na       statistical      error      that      diminishes      with      increasing      sample      size.</p><li>6.2.3.\\nZeroth-Order      Guidance      using      Path      Integral      Control</li><p> \\n \\n  In      the      previous      subsection      (Section      6.2.2),      we      discussed      an      approximation      technique      to      bypass      the  \\n \\n  necessity      of      learning      explicit      value      functions.\\nAnother      approach      to      bypass      this      requirement      is      to      use  \\n \\n  control-based      techniques      for      obtaining      soft-optimal      policies,      a.k.a.,      path      integral      control      (Theodorou  \\n \\n  et      al.,      2010;      Williams      et      al.,      2017;      Kazim      et      al.,      2024).</p><p> \\n \\n  Recall      that      we      have      p(x;,t;      6)   \\n \\n=      2;   \\n \\n+      (dt)g(x+,      t)      and      o°(t)   \\n \\n=      g(t)(dt),      the      motivation      behind this      approach      is      as      follows.\\nInitially,      we      have:</p><h5>Vo,      exp(v(#1)/@)</h5><p>=\\n \\n   Egppreyexp(vr_1      (22-1)      /@)   \\n \\n|      24]      (Soft      Bellman      equation) _\\n \\n   (1   \\n \\n—   \\n \\nx   \\n \\n—      (Ot)      G(a2,      t))?\\n=\\n \\n   Vz»,   \\n \\n{   \\n \\n[      explatena/ay)      exp      (-      0-5a(t)      (ot)   \\n \\n)      in,-}</p><p>=\\n \\n   Byes      [ese      a(era/a))      {ea      ~~      (61)g(nn,)}      EE      Cah</p><p>~\\n \\n   arp      Buty      lexr(era(ea/a))      Gera   \\n \\n—      21   \\n \\n~      (8t)ale      plod</p><p>_t   \\n \\n1       Ht)      ot)      Eg,prey      [E      ppp}      [exp(r(2o)/a)   \\n \\n|      T-1]      e|24|   \\n \\n=      Woon)      [exp(r(xo/a))e|x¢]      ;</p><h5>Now,      we      obtain:</h5><p>o7(t)Vnv(ae)   \\n \\n1      Eqppry      lexp(r(wo)/a)      eel  \\n \\n  Q   \\n \\n~   \\n \\na   \\n \\n*      Egyprey      lexp(r(2o)/a))      |x]      (32)</p><p> \\n \\n  Importantly,      this      approach      does      not      require      any      differentiation.\\nTherefore,      by      running      policies  \\n \\n  from      pre-trained      models      and      simply      using      Monte      Carlo      approximation      in      both      the      denominator  \\n \\n  and      numerator,      we      can      estimate      the      above      quantity      (the      right-hand      side      of      (32))      without      making  \\n \\n  any      models      (classifiers),      unlike      classifier      guidance.\\nNote      while      this      approach      is      widely      used      in      the  \\n \\n  control      community,      it      may      not      be      feasible      for      diffusion      models      due      to      the      high      dimensionality      of  \\n \\n  the      input      space.</p><h3>6.3      Path      Consistency      Learning      (Losses      Often      Used      in      Gflownets)</h3><p>ets)  \\n \\n  Now,      we      explain      how      to      apply      path      consistency      learning      (PCL)      (Nachum      et      al.,      2017)      to      fine-  \\n \\n  tune      diffusion      models.\\nIn      the      Gflownets      literature      (Bengio      et      al.,      2023),      it      seems      that      this      variant      is  \\n \\n  utilized      as      either   \\n \\na      detailed      balance      or   \\n \\na      trajectory      balance      loss,      as      discussed      in      Mohammadpour  \\n \\n  et      al.\\n(2023);      Tiapkin      et      al.\\n(2023);      Deleu      et      al.\\n(2024).\\nHowever,      to      the      best      of      our      knowledge,  \\n \\n  the      precise      formulation      of      path      consistency      learning      in      the      context      of      fine-tuning      diffusion      models  \\n \\n  has      not      been      established.\\nTherefore,      we      start      by      elucidating      the      rationale      of      PCL.\\nSubsequently,  \\n \\n  we      provide   \\n \\na      comprehensive      explanation      of      the      PCL.\\nFinally,      we      discuss      its      connection      with      the  \\n \\n  literature      on      Gflownets.</p><p>Motivation.\\nHere,      we      present      the      fundamental      principles      of      the      PCL.\\nTo      start      with,      we      prove      the  \\n \\n  following      lemma,      which      characterizes      soft-value      functions      and      soft-optimal      policies      recursively.</p><p>Algorithm   \\n \\n5      Path      Consistency      Learning      (Training      with      detailed      balance      loss)  \\n \\n  1:      Require:      Diffusion-model      {N(p(2,t;),07(t))}i-74,,  \\n \\n  -pre-trained   \\n \\n=      model  \\n \\n  {N      (p(t,      t;      Opre),      07      (t))      }i_744,      batch      size      m,      parameter   \\n \\na   \\n \\n€      Rt,      learning      rate   \\n \\n7       2:      Set   \\n \\na      model      {v;(-;@)}      to      learn      optimal      soft      value      function,      and   \\n \\na      model      {p;(-|-;@)}      to      learn  \\n \\n  optimal      polices.\\n \\n \\n  3:      Initialize:      0;   \\n \\n=      Opre  \\n \\n  4:      for      s   \\n \\n=      {1,---      ,S}do  \\n \\n  5:      Collect   \\n \\nm      samples      {a      (0)      }?_7,,      from   \\n \\na      current      diffusion      model      (i.e.,      generating      by      sequen- tially      running      polices      {N(p(2z,      t;      8),      07(t))      }i_-74,      from      t   \\n \\n=      T      to      t   \\n \\n=      0)  \\n \\n  6:      Setuo=r</p><li>(i)   \\n \\n?       a!\\n \\n \\n3   \\n \\na   \\n \\n4      Up—-1(@      13      ds)      re  \\n \\n  bs41   \\n \\n—      bs   \\n \\n—      Vo   \\n \\n3   \\n \\na      9)   \\n \\n+      log      p:(a\\\\\\\\\\\\\\\\,      |a\\\\\\\\\\\\\\\\;0,)   \\n \\n-      EP   \\n \\n—      Jog      rl      >}      Ibe:</li><p>a\\n \\n \\n  t=T+1      i=1  \\n \\n  (c}      ite)      ()      via)      43s)   \\n \\n;       Os41   \\n \\n<      Os   \\n \\n—      Vo   \\n \\n3      TE      velee      ids)      —S<   \\n \\n+      log      pe(ay      2s      |p      0)      —§      log      pee      (ar?\\nifort?\\nJe      lass</p><p>t=T+1      i=1 7:      end      for  \\n \\n  8:      Output:      {p,(x1-1      [215      Os)      fe</p><h3>Lemma      4      (1-step      Consistency      Equation).</h3><li>(2)   \\n \\n+      log      pi      (aa      |a1)   \\n \\n=      (eaeen)   \\n \\n+      log      pp      (x+-1|2+)      (33)</li><p> \\n \\n  Proof.\\nWe      consider      the      marginal      distribution      with      respect      to      x,      and      2;_,      induced      by      the      soft-  \\n \\n  optimal      policy,      and      denote      it      [(z;,      2:1)   \\n \\n€      A(X   \\n \\nx      X).\\nThen,      it      is      clearly      I(x,)l(a,_1   \\n \\n|      x)   \\n \\n=       I(x4-1)l      (xz   \\n \\n|      a1).\\nNow,      from      Theorem   \\n \\n2      that      characterizes      marginal      distributions,      and      Theorem   \\n \\n3       that      characterizes      posterior      distributions,      this      results      in      :</p><p>1\\n \\n   UE      L      re   \\n \\nx   \\n \\n1      Otte      me  \\n \\n  exp   \\n \\n(   \\n \\ni      2)      Pe      (xt)   \\n \\nx      pe      (®e-1|@e)   \\n \\n=   \\n \\nB      exp      (faeev)      Pra      (@1-1)   \\n \\nX      Pe      (1/01) a\\n \\n   —_-_——”’      C   \\n \\na      ~~  \\n \\n  C       we  \\n \\n  Optimal      policy  \\n \\n  “~   \\n \\n“      Posterior      distribution  \\n \\n  Marginal      distribution      at      t      Marginal      distribution      at      t-1 Rearranging      yields:</p><p>1\\n \\n   UL(&   \\n \\n1      Up—1      (Le      re  \\n \\n  G      exp   \\n \\n(      ))   \\n \\nx      Di      (@-1|%4)   \\n \\n=      G      exp      (eaeen)   \\n \\nx      pe      (@4-1|24)</p><p>Taking      the      logarithm,      the      statement      is      concluded.\\nO</p><p>Algorithm.\\nBeing      motivated      by      the      relation      in      (33),      after      initializing      v9   \\n \\n=      r,      we      obtain      the  \\n \\n  recursive      equation:</p><p>2 Olen      U_-1      (Lt  \\n \\n  (pp)   \\n \\n=      argmin   \\n \\n—      Egany   \\n \\n|      oO   \\n \\n4      tog      g      (arp      ng)   \\n \\n—      PEED)   \\n \\n—      dog      pP\"      (ya      er) g):%3R,g2):¥      A(X)   \\n \\na   \\n \\no U   \\n \\n_   \\n \\nx   \\n \\n_      re      re  \\n \\n  =      (MEY)      tog      peal      altiansa)      +++      Flog      pe      (esl),  \\n \\n  which      is      an      extension      of      (33).\\nThe      loss      function      based      on      the      above      k-step      consistency      equation  \\n \\n  could      make      training      faster      without      learning      value      functions      at      every      time      point,      as      noted      in      the  \\n \\n  literature      in      PCL.\\nIn      the      extreme      case      (i.e.,      when      we      recursively      apply      it      with      t   \\n \\n=      7’),      we      obtain      the  \\n \\n  following.</p><li>(34)  \\n \\n  where      u,   \\n \\n€      A(X)      is      any      exploratory      roll-in      distribution.\\nBased      on      this      algorithm,      we      outline      the  \\n \\n  entire      algorithm      in      Algorithm      5.  \\n \\n  We      make      several      important      remarks      regarding      Algorithm      5.\\nFirstly,      while      we      use      on-policy      data  \\n \\n  collection,      technically,      any      policy      can      be      used      in      this      off-policy      algorithm,      like      reward-weighted  \\n \\n  MLE.\\nSecondly,      in      practice,      it      might      be      preferable      to      utilize   \\n \\na      sub-trajectory      from      x;      to      x;_,      based  \\n \\n  on      the      following      expression:</li><p>UFZ  \\n \\n  log      pi_      pa      (Gee      |Vt—ng1)   \\n \\n+      +++   \\n \\n+      log      pi      (4-1      |a4)   \\n \\n+      (2)</p><h3>Corollary      1      (7-step      consistency).</h3><p>log      pf      (xo|21)   \\n \\n+      +++   \\n \\n+      log      pp(wr-a|er)   \\n \\n+      log      pp      (x7)      (35) r(x      re      re      re  \\n \\n  —      (=)   \\n \\n+      log      p?\\n*(xo|v1)      +--+   \\n \\n+      log      pr\"      (ar-i|er)   \\n \\n+      log      pe      (xr).</p><p>Proof.\\nWe      consider      the      marginal      distribution      with      respect      to      7,      +--+   \\n \\n,   \\n \\n%      induced      by      the      soft-optimal  \\n \\n  policy,      and      denote      it      I(vp,---      ,--+      ,%o)   \\n \\n€   \\n \\n¥      X--+   \\n \\nx      X.      We      have Uar)l(ar—1   \\n \\n|      Lr)      tee      I(xo|21)   \\n \\n=      U(x      )l(a4   \\n \\n|      Xo)      te      (ar   \\n \\n|      LT-1)      (36) From      Theorem   \\n \\n2      that      characterized      marginal      distributions,      and      Theorem   \\n \\n3      that      characterize      posterior  \\n \\n  distributions,      the      left      hand      side      of      (36)      is      equal      to *\\n \\n   *\\n \\n   *</p><p>Pr      (xr)   \\n \\nx      pp      (r|er-1)      X-++   \\n \\nx      pi      (Xo|@1) Marginal      distribution      atT      Optimal      policy      at      T   \\n \\n—   \\n \\n1      Optimal      policy      at   \\n \\n1 and      the      right-hand      side      in      (36)      is      equal      to exp(r      (xo)      /a)      pre      pre      pre  \\n \\n  aq      Po      (xo)   \\n \\nX      po      (a1   \\n \\n|      Zo)      X+++   \\n \\nX      prey      (@r   \\n \\n|      er-1)-.\\n \\n \\n|      ~~  \\n \\n  Marginal      distribution      at   \\n \\n0      Posterior      distribution      Posterior      distribution By      rearranging      the      term,      we      obtain      (35).\\nO</p><p> \\n \\n  Comparison      with      Gflownets.\\nIn      the      Gflownets      literature,      similar      losses      are      used.\\nFor      instance,  \\n \\n  the      loss      derived      from      (33)      or      (35)      is      commonly      known      as   \\n \\na      detailed      balance      loss      (Bengio      et      al.,  \\n \\n  2023)      or   \\n \\na      trajectory      loss      (Malkin      et      al.,      2022),      respectively.</p><p> \\n \\n  Note      in      general,      the      literature      in      Gflownets      primarily      focuses      on      sampling      from      unnormalized  \\n \\n  models      (distributions      proportional      to      exp(r(a))).\\nHence,      reference      policies      (i.e,      {p?\"\"}\\n \\n \\n)      or      latent  \\n \\n  states      (1.e.,      %7.;      before      x9)      are      introduced      without      relying      on      pre-trained      diffusion      models.\\nIn  \\n \\n  contrast,      in      our      context,      we      use      policies      derived      from      pre-trained      diffusion      models      as      reference  \\n \\n  policies,      leveraging      them      as      our      prior      knowledge.</p><p>7\\n \\n   Fine-Tuning      Settings      Taxonomy</p><p> \\n \\n  So      far,      we      implicitly      assume      we      have      access      to      reward      functions.\\nHowever,      these      functions      are      often  \\n \\n  unknown      and      need      to      be      learned      from      data.\\nWe      classify      several      settings      in      terms      of      whether      reward  \\n \\n  functions      are      available      or,      if      not,      how      they      could      be      learned.\\nThis      section      is      summarized      in      Figure      3.</p><h4>Direct      backpropagation      (Sec.      5.2)       _       Differentiable</h4><p>_\\n \\n   ——_      Non-differentiable PP      5.1</p><h4>Accurate      rewards      O      (Sec.      5.1)</h4><p> \\n \\n  feedback   \\n \\n;    \\n \\n—      Reward-weighted      MLE      (Sec.      6.1  \\n \\n  Xo      Unknown      reward      Memory      effi-      oe      eward-welgnte      (Sec.      6.1) unctions      —_\\\\\\\\\\\\\\\\_—      ciency</p><h4>No      fine-tuning      Computational</h4><p>efficiency —\\n \\n    NN      Direct      backpropagation      (Sec.      5.2)</p><h5>Value-weighted      sampling      (Sec.      6.2)</h5><p>Figure      3:      Practical      recommendation      of      RL-based      algorithms      to      optimize      downstream      reward  \\n \\n  functions.</p><h6>7.1      Fine-Tuning      with      Known,      Differentiable      Reward      Functions</h6><p>ions  \\n \\n  When      accurate      differentiable      reward      functions      are      available      (e.g.,      standard      in      computer      vision      tasks),  \\n \\n  our      primary      focus      is      typically      on      computational      and      memory      efficiency.\\nIn      general,      we      advocate      for  \\n \\n  employing      reward      backpropagation      (c.f.      Algorithm      2)      for      its      computational      efficiency.\\nAlternatively,  \\n \\n  if      memory      efficiency      is   \\n \\na      significant      concern,      we      suggest      using      PPO      due      to      its      stability.</p><h6>7.2      Fine-Tuning      with      Black-Box      Reward      Feedback</h6><p>back  \\n \\n  Here,      we      explore      scenarios      where      we      have      access      to      accurate      but      non-differentiable      (black-box)  \\n \\n  reward      feedback,      often      found      in      scientific      simulations.\\nThe      emphasis      still      remains      primarily      on  \\n \\n  computational      and      memory      efficiency.\\nIn      such      scenarios,      due      to      the      non-differentiability      of      reward  \\n \\n  feedback,      we      recommend      using      PPO      or      reward-weighted      MLE      for      the      following      reasons.</p><p> \\n \\n  In      the      preceding      section,      we      advocated      for      the      use      of      reward      backpropagation      due      to      its      com-  \\n \\n  putational      advantages.\\nHowever,      when      dealing      with      non-differentiable      feedback,      the      advantage  \\n \\n  of      reward      backpropagation      may      diminish.\\nThis      is      because      learning      from      such      feedback      requires  \\n \\n  learning      differentiable      reward      functions      to      make      the      algorithm      work      as      we      discuss      in      Section      5.2.  \\n \\n  However,      obtaining   \\n \\na      differentiable      reward      function      can      be      challenging      in      many      domains.\\nFor  \\n \\n  example,      in      molecular      property      prediction,      molecular      fingerprints      are      informative      features,      and       accurate      reward      functions      can      be      derived      by      training      simple      neural      networks      on      these      features  \\n \\n  (Pattanaik      and      Coley,      2020).\\nYet,      these      mappings,      grounded      in      prior      scientific      understanding,      are  \\n \\n  non-differentiable,      thereby      restricting      their      applicability.</p><p> \\n \\n  In      contrast,      PPO      and      reward-weighted      MLE      allow      for      policy      updates      without      the      explicit  \\n \\n  requirement      to      learn   \\n \\na      differentiable      reward      function,      offering   \\n \\na      significant      advantage      over      reward  \\n \\n  backpropagation.</p><h6>7.3      Fine-Tuning      with      Unknown      Rewards      Functions</h6><p>ions  \\n \\n  When      reward      functions      (or      computational      proxies      in      Section      7.2)      are      unavailable,      we      must      learn  \\n \\n  from      data      with      reward      feedback.\\nIn      such      cases,      compared      to      the      previous      scenarios,      two      important  \\n \\n  considerations      arise:</p><p>¢\\n \\n   Not      only      computational      or      memory      efficiency      but      also      feedback      efficiency      (i.e.,      sample  \\n \\n  efficiency      regarding      reward      feedback)      is      crucial.</p><p> \\n \\n \\n¢      Given      that      learned      reward      feedback      may      not      generalize      well      outside      the      training      data      distri-  \\n \\n  butions,      it      is      essential      to      constrain      fine-tuned      models      to      prevent      significant      divergence      from  \\n \\n  diffusion      models.\\nIn      these      situations,      not      only      soft      PPO      and      direct      backpropagation      but      also  \\n \\n  methods      discussed      in      Section      6,      such      as      reward-weighted      MLE,      can      be      particularly      effective.</p><p>Further,      the      scenario      involving      unknown      reward      functions      can      be      broadly      categorized      into      two      cases,  \\n \\n  which      we      will      discuss      in      more      detail.</p><p>Offline      scenario      (Figure      4).\\nIn      many      scenarios,      even  \\n \\n  in      cases      where      reward      functions      are      unknown,      we      of-  \\n \\n.     .</p><p>Offline      data      with ten      have      access      to      offline      data      with      reward      feedback      reward      feedback</p><li>(i)      (i)   \\n \\nO   \\n \\n;   \\n \\ni       a”      ra  \\n \\n.      ne      straightforward      approach      is      to</li><p> \\n \\n \\n{   \\n \\n,   \\n \\n(   \\n \\n)    \\n.\\n    .\\n \\n   g      PP  \\n \\n.      <=>      Learning      reward  \\n \\n  perform      regression      with      neural      networks      and      build   \\n \\na      models  \\n \\n  learned      regressor      *.\\nHowever,      this      learned      reward      func-      SS  \\n \\n  tion   \\n \\n*      might      not      generalize      well      beyond      the      distribution      x)  \\n \\n  of      the      offline      data.\\nIn      regions      outside      this      distribution,   \\n \\n7       could      assign   \\n \\na      high      value      even      when      the      actual      reward  \\n \\n  r      is      low      due      to      high      uncertainty.\\nConsequently,      we  \\n \\n  could      be      easily      misled      by      out-of-distribution      samples  \\n \\n  if      we      solely      optimize      7.\\n|Fine-tuning      diffusion      models   \\n \\n|       One      approach      to      alleviate      this      issue      is      to      adopt   \\n \\na       pessimistic      (i.e.,      conservative)      strategy,      as      proposed      in      Figure      4:      Offline      scenario  \\n \\n  Uehara      et      al.\\n(2024).\\nThis      technique      has      been      widely      Collecting      data]   \\n \\nY      Learning      reward  \\n \\n  applied      in      the      field      of      offline      RL      (Levine,      2018).\\nThe      aa  \\n \\n  main      idea      is      to      penalize   \\n \\n7      outside      the      distributions      us-  \\n \\n  ing      techniques      such      as      adding      an      explicit      penalty      term  \\n \\n  (Yu      et      al.,      2020;      Chang      et      al.,      2021),      bootstrapping,  \\n \\n  or      more      sophisticated      methods      (Kumar      et      al.,      2020;  \\n \\n  Xie      et      al.,      2021;      Rigter      et      al.,      2022;      Uehara      and      Sun,  \\n \\n  2021).\\nBy      doing      so,      we      can      avoid      being      fooled      by  \\n \\n  out-to-distribution      regions      while      still      benefiting      from  \\n \\n  the      extrapolation      capabilities      of      reward      models.</p><p>S—      diffusion      models      |</p><p>Figure      5:      Online      scenario</p><p> \\n \\n  Online      scenario      (Figure      5).\\nConsider      situations  \\n \\n  where      reward      functions      are      unknown,      but      we      can      gather      data      online.\\nThese      scenarios      are      often  \\n \\n  referred      to      as   \\n \\na      lab-in-the-loop      setting.</p><p> \\n \\n  In      such      scenarios,      Uehara      et      al.\\n(2024)      proposes      an      iterative      procedure      comprising      three      steps:</p><li>(1)      collecting      feedback      data      by      exploring      new      areas      (i.e.,      obtaining   \\n \\nx      from      current      diffusion      models  \\n \\n  and      corresponding      r(x)),      (2)      learning      the      reward      function      from      the      collected      feedback      data,      and</li><li>(3)      fine-tuning      current      diffusion      models      by      maximizing      the      learned      reward      function      enhanced      by  \\n \\n  an      optimistic      bonus      term,      using      algorithms      discussed      in      Section      4.2.\\nIn      contrast      to      standard      online  \\n \\n  PPO,      where      feedback      data      is      directly      used      in      the      fine-tuning      process,      actual      reward      feedback      is  \\n \\n  only      used      in      step      (2)      and      not      in      step      (3).\\nAn      additional      key      aspect      is      the      use      of      an      optimistic  \\n \\n  reward      function      that      encourages      exploration      beyond      the      distributions      of      the      current      diffusion      model.\\n \\n \\n  This      deliberate      separation      between      reward      learning      and      fine-tuning      steps,      coupled      with      the      use      of  \\n \\n  optimism,      significantly      enhances      feedback      efficiency.</li><p>8\\n \\n   Connection      with      Classifier      Guidance</p><p> \\n \\n  Classifier      guidance      (Dhariwal      and      Nichol,      2021)      is      commonly      used      for      conditional      generation      in  \\n \\n  diffusion      models.\\nInterestingly,      RL-based      methods      share   \\n \\na      close      connection      with      classifier      guidance.\\n \\n \\n  More      specifically,      in      this      section,      following      Zhao      et      al.\\n(2024),      we      elucidate      how      RL-based      methods  \\n \\n  can      be      applied      to      conditional      generation.\\nFurthermore,      from      this      unified      viewpoint,      we      highlight  \\n \\n  that      classifier      guidance      is      seen      as   \\n \\na      value-weighted      sampling      in      Section      4.2.</p><h6>8.1      Classfier      Guidance</h6><p>ance  \\n \\n  In      this      section,      we      first      introduce      classifier      guidance.\\nClassifier      guidance      utilizes      an      unconditional  \\n \\n  pre-trained      model      for      conditional      generation.\\nMore      specifically,      we      consider   \\n \\na      scenario      where      we  \\n \\n  have   \\n \\na      pre-trained      diffusion      model,      which      enables      us      to      sample      from      p?\\n\"*(x),      and      data      with      feedback  \\n \\n  {x,y},      where   \\n \\ny      represents      the      new      label      we      want      to      condition      on.\\nOur      objective      here      is      to      sample  \\n \\n  from      p(-|y)   \\n \\n€      [Y   \\n \\n>      ACR), P(-   \\n \\n|      y)   \\n \\nx      py(yl-)pP*(-).\\n(37)</p><p>where      p,   \\n \\n:   \\n \\nX      —>      A(Y)      denotes      the      conditional      distribution      of   \\n \\ny      given      x.</p><p>Motivation.\\nIn      classifier      guidance,      we      consider      the      following      policy:</p><p>pe      (-lare,      y)   \\n \\n=   \\n \\nN      (p(22,      t;      Opre)   \\n \\n+      a(t)      Ve      log      Vy      (Xt;      t),      a*(t))   \\n \\n’ dy(x,t)   \\n \\n=      Egy.\\nyap,      (-leo)      L(Y   \\n \\n=      y)|a,   \\n \\n=      2].</p><p>Under      the      continuous-time      formulation      of      diffusion      models,      applying      Doob’s      h-transform      (Rogers  \\n \\n  and      Williams,      2000),      it      is      shown      that      we      can      sample      from      p(x|y)      by      sequentially      running      {p7\"\\\\\\\\\\\\\\'(-|7+,      y)      }i-r41-</p><p>Algorithm.\\nSpecifically,      the      algorithm      comprises      two      steps.\\nIt’s      important      to      note      that      this      is      not  \\n \\n  a      fine-tuning      method      but      rather      an      inference-time      technique      as      we      freeze      the      pre-trained      model.</p><li>1. Learning      q,(x,t)      through      regression      by      gathering      data      containing      {z;,      y}.\\nHere,      for      each  \\n \\n  (Xo,      y),      using   \\n \\na      forward      policy      for      the      pre-training      (from   \\n \\n0      to      T),      we      obtain      2;      starting      from</li><p>Lo.</p><li>2. During      inference,      sequentially      execute      {p2\"(-|a,,      y)      }1_>   \\n \\n1      by      leverage      the      fact      that      compared</li><p>gui to      the      policy      in      pre-trained      diffusion      models,      the      mean      of      {p?\\n\"\\\\\\\\\\\\\\'(-|71,      y)      }i-74,      is      just      shifted  \\n \\n  with      an      additional      term      V,,      log      q,(2,      t).</p><p> \\n \\n  Notably,      the      method      described      above      can      be      reduced      to      value-weighted      sampling      (cf.      Sec-  \\n \\n  tion      6.2),      when      r(x,      y)   \\n \\n=      log      p(y|x).\\nInspired      by      such      an      observation,      we      apply      an      RL-based  \\n \\n  fine-tuning      method      for      conditional      generation      as      below.</p><h7>8.2.      RL-Based      Fine-Tuning      for      Conditional      Generation</h7><p>tion  \\n \\n  In      this      section,      following      Zhao      et      al.\\n(2024),      we      explain      an      RL-based      fine-tuning      approach      for  \\n \\n  conditional      generation.\\nThe      core      idea      is      that      setting      r(x,      y)   \\n \\n=      log      p(y|x)      allows      us      to      sample      from  \\n \\n  the      target      conditional      distribution      (37),      drawing      on      the      insights      discussed      in      Section      4.2.  \\n \\n  More      precisely,      introducing   \\n \\na      distribution      over   \\n \\ny      as      q(-),      we      formulate      the      following      RL      problem:</p><p>{Di      ft   \\n \\n=      argmax      Egy,      (-\\\\\\\\\\\\\\\\-y)}.yott      log      p(y|xo)   \\n \\n—      NeopyaKL(pe(-|e,      y)      ||P      (late).\\n \\n \\n  {prE[(V,.X)PA(A)]      Heyy (38)</p><p> \\n \\n  Compared      to      the      objective      function      (18)      in      Section      4.2,      the      main      differences      are:      (1)      policy      space  \\n \\n  is      expanded      to      accommodate      additional      control      (i.e.,      y):      pz(-|-,-)      C      [R¢   \\n \\nx   \\n \\nY   \\n \\n+      A(R®)],      (2)      the  \\n \\n  fine-tuning      objectives      are      set      as      log-likelihoods:      log      p(y|z).\\nThen,      following      Theorem      1,      we      can  \\n \\n  establish      the      following      result.</p><p>Corollary      2.\\nThe      distribution      induced      by      {p*}1_74      (ue.   \\n \\n[      {Teri      Ph      (ae-1   \\n \\n|      te,      y)}deur      is      the target      conditional      distribution      (37).</p><p> \\n \\n  The      above      corollary      suggests      that      for      conditional      generation,      we      can      utilize      various      off-the-shelf  \\n \\n  algorithms      discussed      in      Section      4.2      and      6,      such      as      PPO,      reward      backpropagation,      and      reward-  \\n \\n  weighted      MLE,      as      well      as      value-weighted      sampling.</p><p>9\\n \\n   Connection      with      Flow-Based      Diffusion      Models</p><p> \\n \\n  We      elucidate      the      close      relationship      between      bridge      matching      and      fine-tuning,      as      summarized      in  \\n \\n  Table      2.\\nBridge      (flow)      matching      (Liu      et      al.,      2022;      Tong      et      al.,      2023;      Lipman      et      al.,      2023;      Liu      et      al.,  \\n \\n  2022;      Shi      et      al.,      2024;      Albergo      and      Vanden-Eijnden,      2022),      has      recently      gained      popularity      as   \\n \\na       distinct      type      of      diffusion      model      from      score-matching-based      ones.\\nThese      works      suggest      that      training  \\n \\n  and      inference      speed      will      be      accelerated      due      to      the      ability      to      define   \\n \\na      more      direct      trajectory      from  \\n \\n  noise      to      data      distribution      than      score-matching-based      diffusion      models.</p><p> \\n \\n  The      connection      between      bridge      matching      and      fine-tuning      becomes      evident      in      the      continuous-  \\n \\n  time      formulation.\\nTo      begin,      we      first      provide   \\n \\na      brief      overview      of      the      continuous-time      formulation      of  \\n \\n  score-matching-based      diffusion      models.</p><p> \\n \\n  Table      2:      Comparison      between      training      score/flow-based      diffusion      models      and      fine-tuning.\\nWe  \\n \\n  optimize      the      parameter   \\n \\n6      in      P’,      which      represents      the      induced      distribution      over      trajectories      associated  \\n \\n  with      6.</p><h8>Loss      Note      of      Q</h8><p>Score-based      KL(Q\\\\\\\\\\\\\\'™¢|P?)      Qtime:      Time-reversal      of      reference      SDE      (reference training      SDE      is      defined      from   \\n \\n0      to      T’      where      the      initial      dis-  \\n \\n  tribute      at   \\n \\n0      is      data      distribution)</p><p> \\n \\n  Flow-based      KL(Q*°>      ||P?)      @Q*°°h:      Coupling      between      the      reference      SDE      from  \\n \\n  training      T      to   \\n \\n0      conditioning      at   \\n \\n0      (dist.      over      x.;|7o)      and    \\n \\na      data      distribution      at      time   \\n \\n0      (dis.      over      x).\\nNote  \\n \\n  that      reference      SDE      is      defined      from      7’      to      0.</p><p> \\n \\n  Fine-tuning      KL(P?||Q*°)      @*:      Coupling      between      the      pre-trained      SDE  \\n \\n  from      T      to   \\n \\n0      conditioned      on   \\n \\n0      (dist.      over      x7.1|X0)  \\n \\n  and   \\n \\na      data      distribution      at      time   \\n \\n0      (dis.      over      Xo) 9.1      Continuous      Time      Formulation      of      Score-Based      diffusion      models</p><p> \\n \\n  We      provide   \\n \\na      brief      overview      of      the      continuous-time      formulation      of      diffusion      models      (Song      et      al.,  \\n \\n  2021),      further      expanding      Section      1.1.1.\\nIn      this      formulation,      we      initially      define      the      forward      reference  \\n \\n  SDE,      spanning      from      time   \\n \\n0      to      7’      (e.g.,      variance      exploding      (VE)      process      or      variance      preserving      (VP)  \\n \\n  process),      by      setting      an      initial      distribution      at   \\n \\n0      as      the      data      distribution.\\nAdditionally,      the      reference  \\n \\n  SDE      is      tailored      to      converge      to      an      easy      distribution      at      time      7’,      such      as      the      normal      distribution.</p><p> \\n \\n  Subsequently,      we      consider      the      time-reversal      SDE      (Anderson,      1982),      progressing      from      T\\\\\\\\\\\\\\'      to      0.\\nIf  \\n \\n  we      could      learn      this      time-reversal      SDE,      it      would      enable      us      to      sample      from      the      data      distribution      by  \\n \\n  starting      with      the      easy      initial      distribution      (at      7’)      and      following      the      time-reversal      SDE      from      T      to   \\n \\n0      at  \\n \\n  inference      time.</p><p> \\n \\n  Training.\\nNow,      the      remaining      question      is      how      to      learn      this      time-reversal      SDE.\\nHere,      by      repre-  \\n \\n  senting      the      induced      distribution      over      trajectories      from      T      to   \\n \\n0      with      this      time-reversal      SDE      as      Q\\\\\\\\\\\\\\'™®,  \\n \\n  our      goal      is      to      train   \\n \\na      new      SDE,      parameterized      by   \\n \\n@      in      the      neural      network,      such      that      the      induced  \\n \\n  distribution      P®      over      trajectories      associated      with   \\n \\n6      aligns      with      Q\"™*.\\nSpecifically,      the      objective      is      to minimize      the      following      losses:</p><p> \\n \\n  argmin      KL(Q\"™||P*).\\n(39)  \\n \\n  0  \\n \\n  With      some      algebra,      leveraging      the      observation      that      the      time-reversal      SDE      comprises      the      original  \\n \\n  forward      SDE      and      the      scoring      term      of      the      marginal      distribution      (Anderson,      1982)      as      in      (2),      Song  \\n \\n  et      al.\\n(2021)      demonstrates      that      this      KL      loss      (39)      is      approximated      as Dore   \\n \\n=      argmin      Eeuni((0,7))01~qyo0(weleo)co~pP\"e      Fe      Fl      Vey      log      jo      (Xt   \\n \\n|      Xo)   \\n \\n_      so(Xt,      t)|I?\\n],      (40)  \\n \\n  which      is      equal      to      (4)      where      the      weight      A(t)      is      {o?\\n}?\\n(for      notation,      refer      to      Section      1.1.1).</p><h9>9.2      Flow-Based      (Bridge-Based)      Diffusion      Models</h9><p>dels We      adopt      the      explanation      outlined      in      Liu      et      al.\\n(2022).\\nIn      this      framework,      we      begin      by      considering  \\n \\n  a      reference      SDE      (from      T\\\\\\\\\\\\\\'      to      0),      x;   \\n \\n=      zr_;      where      z;      is      defined      as      Brownian      motion:</p><p>dz,   \\n \\n=      dwt,      te      (0,      7).\\n(41)</p><p> \\n \\n  Subsequently,      we      consider      the      coupling      of      the      reference      SDE      conditioned      on      state      at      time   \\n \\n0      (i.e.,  \\n \\n  distribution      of      xo.7      conditional      on      x)      and      the      data      distribution      at      time      0,      denoting      the      resulting  \\n \\n  distribution      as      Q*°?,      Informally,      this      Q*°°?\\nis      characterized      as      follows.</p><p>Informal      characterization      of      Q*”.\\nWhen      discretizing      the      time      step,      by      denoting      the      distribution  \\n \\n  induced      by      the      reference      SDE      as      p\"(x7,--+   \\n \\n,      29),      we      define QF      (a7,      tae   \\n \\n,      20)   \\n \\n=      p(x,      ay   \\n \\n|      r9)pP\"°      (x9).</p><p>The      distribution      Q*”      above      is   \\n \\na      limiting      distribution      obtained      by      making      the      discretization      step small.</p><p>Training.\\nIn      bridge-based      diffusion      models,      our      objective      is      to      train   \\n \\na      new      SDE,      parametrized      by  \\n \\n  6      (e.g.,   \\n \\na      neural      network),      to      align      the      induced      distribution      P®      with      Q*°?.\\nSpecifically,      bridge-based training      aims      to      minimize aremin      KL(Q*>||P’).\\n(42)  \\n \\n  0  \\n \\n  While      the      method      to      minimize      the      above      is      currently      unclear,      we      can      derive   \\n \\na      trainable      objective  \\n \\n  through      algebraic      manipulation.\\nInitially,      we      observe      that      the      reference      SDE      (41),      conditioned      on  \\n \\n  state      xo      at      time   \\n \\n0      (zr   \\n \\n=      D),      is      expressed      as:</p><p>b\\n \\n \\n  b—      2;</p><p>t   \\n \\n€      [0,T];dz?\\n \\n \\n=      g?(z?,t)dt      +du,,  \\n \\n  g(z?,t)   \\n \\n=   \\n \\na       (43)</p><p>using      the      seminal      “Doob’s      h-transform”      (Rogers      and      Williams,      2000).\\nThis      SDE      is      commonly  \\n \\n  known      as      the      Brownian      bridge.\\nNow,      we      consider   \\n \\na      new      SDE      parameterized      by      0:</p><p>t   \\n \\n€      [0,7];      dz   \\n \\n=      s(z,t;      @)dt   \\n \\n+      dw.\\n(44)</p><p>Then,      we      can      demonstrate      that      minimizing      the      KL      loss      in      (42)      is      equivalent      to argmin      E,W      unil0,7],1~4\"°      cop?\\n[Ilg”      (2,      t)   \\n \\n_      s(21,      t;      0)      M3]      (45)</p><p>where      q;°      denotes      the      induced      distribution      at      time      t      following      SDE      (43)      conditioned      on      xo,      and       s:      Xx      |0,T]   \\n \\n>   \\n \\n&      is      the      model      we      aim      to      optimize.</p><p> \\n \\n  Flow-based      diffusion      models.\\nThe      aforementioned      formulation      has      been      originally      proposed      as   \\n \\na       bridge-based      SDE      (Liu      et      al.,      2022)      because      the      conditional      SDE      is      often      referred      to      as   \\n \\na      Brownian  \\n \\n  bridge.\\nFlow-based      diffusion      models      proposed      in      Lipman      et      al.\\n(2023);      Tong      et      al.\\n(2023)      share   \\n \\na       close      relationship      with      bridge-based      diffusion      models.\\nThese      flow-based      models      use   \\n \\na      target      SDE (43)      and      an      SDE      with   \\n \\na      parameter      (44)      without      stochastic      terms      (1.e.,      no      dw,)      and      aim      to      minimize  \\n \\n  the      loss      function      (45).</p><p> \\n \\n  Comparison      with      score-based      diffusion      models.\\nBoth      score-based      diffusion      models      and      bridge-  \\n \\n  based      diffusion      models      aim      to      minimize      KL      divergences.\\nDespite      its      similarity,      in      contrast      to  \\n \\n  score-based      diffusion      models,      bridge-based      diffusion      models      target      Q*°>      rather      than      the      time-  \\n \\n  reversal      SDE      Q\"™®.\\nThe      distribution      Q*”      is      often      considered      preferable      because      it      can      impose      an  \\n \\n  SDE      that      navigates      efficiently      from   \\n \\n0      to      7’      (e.g.,   \\n \\na      Brownian      bridge      with      minimal      noise).\\nThis      can  \\n \\n  expedite      the      learning      process      since      we      lack      direct      control      over      the      time-reversal      SDE      in      score-based  \\n \\n  diffusion      models.</p><h10>9.3.      Connection      with      RL-Based      Fine-Tuning</h10><p>ning  \\n \\n  RL-based      fine-tuning      shares   \\n \\na      close      relationship      with      flow-based      training.\\nTo      illustrate      this,      consider  \\n \\n  the      reference      SDE      (from      7’      to      0)      following      the      pre-trained      diffusion      model.\\nThen,      akin      to      flow-  \\n \\n  based      training,      we      consider      the      coupling      of      the      reference      SDE      conditioned      on      state      at      time   \\n \\n0       (i.e.,      distribution      over      xo-r      conditioned      on      x9)      and      the      target      distribution      at      time   \\n \\n0      (i.e.,      p,(-)   \\n \\n«       exp(r(-)/a)p?\\n\"°(-)),      denoting      the      induced      distribution      as      Q*™.</p><p>Informal      characterization      of      Q™°*.\\nInformally,      Q*\"°      is      introduced      as      follows.\\nWhen      discretizing  \\n \\n  the      time      step,      by      denoting      the      distribution      induced      by      the      reference      SDE      as      p™\\\\\\\\\\\\\\'(xp,-+-   \\n \\n,      9),      we define Qi\"      (xr,   \\n \\na   \\n \\n,      0)   \\n \\n=      p(x,      nr      Ly   \\n \\n|      L0)Pr      (Xo).</p><p>The      distribution      Q*°      above      is   \\n \\na      limiting      distribution      obtained      by      making      the      discretization      step small.</p><p>Training.\\nNow,      we      introduce   \\n \\na      new      SDE      parameterized      by   \\n \\n@      such      as      an      neural      network      to      align  \\n \\n  the      induced      distribution      P’      with      Q®\"*.\\nActually,      in      the      continuous      formulation,      the      RL-problem      (18) in      Section      4.2      is      equal      to      solving argmin      KL(P?||Q\"™).\\n \\n \\n  0 This      formalization      has      been      presented      in      Uehara      et      al.\\n(2024).\\nIntuitively,      as      we      see      in      Theorem      3,  \\n \\n  this      is      expected      because      we      see      that      the      posterior      distribution      induced      by      pre-trained      models:</p><p>ref   \\n \\n(      PD      (ar,-++      21   \\n \\n|      to)      (=      vey      (er|er_-1)p      Pp      (ar-1   \\n \\n|      er_2)      +++      pp\"      (x1   \\n \\n|      20), remains      preserved      after      fine-tuning      based      on      the      RL-formulation      (18).</p><p> \\n \\n  Comparison      with      flow-based      training.\\nIn      contrast      to      flow-based      training,      RL-based      fine-tuning  \\n \\n  minimizes      the      inverse      KL      divergence      rather      than      the      KL      divergence.\\nIntuitively,      this      is      because,  \\n \\n  unlike      Qt™*      or      Q*>   \\n \\n,      we      cannot      sample      from      Q*”      (recall      that   \\n \\na      marginal      distribution      at      time   \\n \\n0      in  \\n \\n  Qi,      i.e.,      p,,      is      unnormalized);      on      the      other      hand,      the      marginal      distributions      at      time   \\n \\n0      in      both      Q\\\\\\\\\\\\\\'™*  \\n \\n  and      Q*°°>      are      data      distributions.</p><p> \\n \\n  Remark   \\n \\n7      (Extension      to      f-divergence).\\nEach      of      these      methods      can      be      technically      extended      by  \\n \\n  incorporating      more      general      divergences      beyond      KL      divergence.\\nFor      instance,      in      the      context      of  \\n \\n  fine-tuning,      please      see      Tang      (2024).</p><p>10      Connection      with      Sampling      from      Unnormalized      Distributions</p><p> \\n \\n  Numerous      studies      delve      into      sampling      from      an      unnormalized      distribution      proportional      to      exp(r()),  \\n \\n  commonly      known      as      the      Gibbs      distribution,      extensively      discussed      in      the      computational      statistics  \\n \\n  and      statistical      physics      community      (Robert,      2014).\\nThis      issue      is      pertinent      to      our      work,      as      during  \\n \\n  fine-tuning,      the      target      distribution      is      also      formulated      in      the      Gibbs      distribution      form      in      (10),      which      is  \\n \\n  proportional      to      exp(r(a))p?\\n\"*      (x).</p><p> \\n \\n  The      literature      employs      two      main      approaches      to      tackle      this      challenge:      Markov      Chain      Monte  \\n \\n  Carlo      (MCMC)      and      RL-based      methods.\\nIn      particular,      the      RL-based      diffusion      model      fine-tuning  \\n \\n  we      have      discussed      so      far      is      closely      related      to      the      latter      approach.\\nIn      this      section,      we      explain      and       compare      these      two      approaches.</p><h10>10.1      Markov      Chain      Monte      Carlo      (MCMC)</h10><p>CMC)  \\n \\n  In      MCMC,   \\n \\na      Markov      chain      is      constructed      to      approximate   \\n \\na      target      distribution      such      that      the      equilib-  \\n \\n  rium      distribution      of      the      Markov      chain      converges      to      the      target      distribution.\\nHowever,      dealing      with  \\n \\n  high-dimensional      domain      spaces      is      challenging      for      MCMC.\\n \\n \\nA      common      strategy      involves      leveraging  \\n \\n  gradient      information,      such      as      the      Metropolis-adjusted      Langevin      algorithm      (MALA)      (Besag,      1994)  \\n \\n  or      Hamiltonian      Monte      Carlo      (HMC)      (Neal      et      al.,      2011;      Girolami      and      Calderhead,      2011).\\nMALA  \\n \\n  shares      similarities      with      classifier      guidance      and      value-weighted      sampling,      as      both      methods      depend  \\n \\n  on      the      first-order      information      from      reward      (or      value)      functions.</p><p> \\n \\n  Can      we      use      MCMC      for      fine-tuning      diffusion      models?\\nIn      the      context      of      fine-tuning      dif-  \\n \\n  fusion      models,      while      it      may      seem      intuitive      to      sample      from      the      target      distribution      (10)      (i.e.,  \\n \\n  Dy   \\n \\n&      exp(r(x))pP\"*(x))      using      MCMC,      it’s      not      straightforward      for      the      following      reasons:</p><p>¢\\n \\n   We      lack      of      an      analytical      form      of      p?\\n\"*(-).<ul><li>*\\n \\n   Additionally,      even      if      we      can      estimate      p?\\n’°(-)      in      an      unbiased      manner,      as      in      Chen      et      al.\\n(2018),  \\n \\n  the      mixing      time      for      obtaining   \\n \\na      single      sample      in      MCMC      might      be      lengthy.</li></ul></p><p> \\n \\n  Therefore,      we      explore      creating   \\n \\na      generative      model      (i.e.,      diffusion      model)      for      this      task      using      RL-  \\n \\n  based      fine-tuning      so      that      we      can      easily      sample      from      p,(-)      during      inference      (i.e.,      simulating      the  \\n \\n  time-reversal      SDE      (2)      with      policies      from   \\n \\n¢   \\n \\n=      T\\\\\\\\\\\\\\'+   \\n \\n1      to      t   \\n \\n=      1),      without      relying      on      MCMC.\\nThis  \\n \\n  addresses      the      concern      of      MCMC      by      avoiding      the      need      to      estimate      p?\\n\"*(-)      and      minimizing      inference  \\n \\n  time,      albeit      at      the      expense      of      increased      training      time.</p><p>10.2}      RL-Based      Approaches  \\n \\n  While      MCMC      has      historically      been      widely      used      for      this      purpose,      recent      works      have      proposed      an  \\n \\n  RL-based      approach      or      its      variant      (e.g.,      Bernton      et      al.      (2019);      Heng      et      al.\\n(2020);      Huang      et      al.\\n(2023);</p><p> \\n \\n  Vargas      et      al.\\n(2023)).\\nFor      example,   \\n \\na      seminal      study      (Zhang      and      Chen,      2021)      introduced   \\n \\na      method  \\n \\n  very      similar      to      reward      backpropagation.\\nIn      their      work,      they      initially      define      Brownian      motion      as   \\n \\na       reference      SDE      (from      7’      to      0),      similar      to      the      pre-trained      diffusion      models      in      our      context.\\nThen,      by  \\n \\n  appropriately      setting      rewards,      Zhang      and      Chen      (2021)      aims      to      learn   \\n \\na      new      SDE      (from      T\\\\\\\\\\\\\\'      to      0)      such  \\n \\n  that      we      can      sample      from      the      target      distribution      at      the      endpoint      0.</p><p> \\n \\n  These      approaches      offer      advantages      over      MCMC      due      to      their      ability      to      minimize      inference      time  \\n \\n  (amortization)      by      constructing   \\n \\na      generative      model      comprising      policies      in      the      training      time      and      solely  \\n \\n  executing      learned      policies      during      inference.\\nUnlike      MCMC,      these      methods      significantly      reduce      the  \\n \\n  inference      cost      as      we      don’t      need      to      be      concerned      about      the      mixing      time.\\nAdditionally,      RL-based  \\n \\n  approaches      have      the      potential      to      effectively      handle      high-dimensional,      complex      (i.e.,      multi-modal)  \\n \\n  distributions      by      harnessing      the      considerable      expressive      power      arising      from      latent      states.</p><p> \\n \\n  Despite      these      advantages,      it      is      often      unclear      how      to      select   \\n \\na      reference      SDE      in      these      works.\\nIn  \\n \\n  contrast,      in      RL-based      fine-tuning      of      diffusion      models,      we      directly      leverage      pre-trained      diffusion  \\n \\n  models      as      the      reference      SDEs,      i.e.,      as      our      prior      knowledge.</p><p> \\n \\n  Key      message.\\nFinally,      it      is      worth      noting      that      we      can      technically      utilize      any      off-the-shelf      RL      algo-  \\n \\n  rithms      for      training.\\nFor      example,      Zhang      and      Chen      (2021)      employs      reward      backpropagation,      while  \\n \\n  works      in      Gflownets      typically      leverage      PCL,      as      we      mention      in      Section      6.3.\\nHowever,      additionally,      it  \\n \\n  is      possible      to      use      PPO      or      reward-weighted      MLE      mentioned      in      Section      4.2.</p><p>11      Closely      Related      Directions</p><p>Lastly,      we      highlight      several      closely      related      directions      we      have      not      yet      discussed.</p><p> \\n \\n  Aligning      text-to-image      models.\\nWe      recognize      that      current      efforts      in      fine-tuning      diffusion      models  \\n \\n  primarily      revolve      around      aligning      text-to-image      models      using      human      feedback      (Lee      et      al.,      2023;  \\n \\n  Wallace      et      al.,      2023;      Wu      et      al.,      2023;      Yang      et      al.,      2023).\\nIn      this      paper,      our      goal      is      to      consider   \\n \\na      more  \\n \\n  general      and      fundamental      formulation      that      is      not      tailored      to      any      particular      task.</p><p> \\n \\n  Diffusion      models      for      RL.\\nDiffusion      models      are      well-suited      for      specific      reinforcement      learning  \\n \\n  applications      due      to      their      ability      to      model      complex      and      multimodal      distributions      as      polices      (Janner  \\n \\n  et      al.,      2022;      Ajay      et      al.,      2023;      Wang      et      al.,      2022;      Hansen-Estruch      et      al.,      2023;      Du      et      al.,      2024;      Zhu  \\n \\n  et      al.,      2023).\\nFor      example,      Wang      et      al.\\n(2022)      use      conditional      diffusion      models      as      policies      and       demonstrate      their      effectiveness      in      typical      offline      RL      benchmarks.\\nWhile      RL-based      fine-tuning      also  \\n \\n  aims      to      maximize      certain      reward      functions,      the      primary      focus      of      this      tutorial      covers      methods      that  \\n \\n  leverage      pre-trained      diffusion      models      for      this      purpose.</p><p> \\n \\n  RL      from      human      feedback      (RLHF)      for      language      models.\\nRLHF      is      widely      discussed      in      the  \\n \\n  context      of      language      models,      where      many      algorithms      share      similarities      with      those      used      in      diffusion  \\n \\n  models      (Ouyang      et      al.,      2022;      Bai      et      al.,      2022;      Casper      et      al.,      2023).\\nFor      instance,      both      domains  \\n \\n  commonly      employ      PPO      as   \\n \\na      standard      approach,      and      reward-weighted      training      (or      reward-weighted  \\n \\n  MLE)      often      serves      as   \\n \\na      basic      baseline      approach.\\nDespite      these      similarities,      nuanced      differences  \\n \\n  exist      mainly      because      diffusion      models      typically      operate      in   \\n \\na      continuous      input      space      rather      than  \\n \\n  a      discrete      one.\\nMoreover,      techniques      like      value-weighted      sampling      emerge      uniquely      within      the  \\n \\n  context      of      diffusion      models.</p><p>12      Summary</p><p> \\n \\n  In      this      article,      we      comprehensively      explain      how      fine-tuning      diffusion      models      to      maximize      down-  \\n \\n  stream      reward      functions      can      be      formalized      as   \\n \\na      reinforcement      learning      (RL)      problem      in      entropy-  \\n \\n  regularized      Markov      decision      processes      (MDPs).\\nBased      on      this      viewpoint,      we      elaborate      on      the  \\n \\n  application      of      various      RL      algorithms,      such      as      PPO      and      reward-weighted      MLE,      specifically      tailored  \\n \\n  for      fine-tuning      diffusion      models.\\nAdditionally,      we      categorize      different      scenarios      based      on      how  \\n \\n  reward      feedback      is      obtained.\\nWhile      this      categorization      is      not      always      explicitly      mentioned      in      many  \\n \\n  existing      works,      it      is      crucial      when      selecting      appropriate      algorithms.\\nFinally,      we      discuss      the      rela-  \\n \\n  tionships      with      several      related      topics,      including      classifier      guidance,      Gflownets,      path      integral      control  \\n \\n  theory,      and      sampling      from      unnormalized      distributions.</p><h4>References</h4><p>Agarwal,      A.,      N.      Jiang,      S.      M.      Kakade,      and      W.      Sun      (2019).\\nReinforcement      learning:      Theory      and       algorithms.\\nCS      Dept.,      UW      Seattle,      Seattle,      WA,      USA,      Tech.\\nRep,      10-4.</p><p>Agarwal,      V.      and      D.      R.      Kelley      (2022).\\nThe      genetic      and      biochemical      determinants      of      mrna      degradation  \\n \\n  rates      in      mammals.\\nGenome      biology      23(1),      245.</p><p> \\n \\n  Ajay,      A.,      Y.      Du,      A.      Gupta,      J.      B.      Tenenbaum,      T.      S.      Jaakkola,      and      P.      Agrawal      (2023).\\nIs      conditional  \\n \\n  generative      modeling      all      you      need      for      decision      making?\\nIn      The      Eleventh      International      Conference  \\n \\n  on      Learning      Representations.</p><p>Albergo,      M.      S.      and      E.      Vanden-Eijnden      (2022).\\nBuilding      normalizing      flows      with      stochastic      inter-  \\n \\n  polants.\\narXiv      preprint      arXiv:2209.15571.</p><p>Anderson,      B.      D.      (1982).\\nReverse-time      diffusion      equation      models.\\nStochastic      Processes      and      their  \\n \\n  Applications      12(3),      313-326.</p><p> \\n \\n  Austin,      J.,      D.      D.      Johnson,      J.      Ho,      D.      Tarlow,      and      R.      Van      Den      Berg      (2021).\\nStructured      denoising  \\n \\n  diffusion      models      in      discrete      state-spaces.\\nAdvances      in      Neural      Information      Processing      Systems      34,  \\n \\n  17981-17993.</p><p>Avdeyey,      P.,      C.      Shi,      Y.      Tan,      K.      Dudnyk,      and      J.      Zhou      (2023).\\nDirichlet      diffusion      score      model      for  \\n \\n  biological      sequence      generation.\\narXiv      preprint      arXiv:2305.\\n10699.</p><p>Bai,      Y.,      S.      Kadavath,      S.      Kundu,      A.      Askell,      J.      Kernion,      A.      Jones,      A.      Chen,      A.      Goldie,      A.      Mirhoseini, C.      McKinnon,      et      al.\\n(2022).\\nConstitutional      ai:      Harmlessness      from      ai      feedback.\\narXiv      preprint  \\n \\n  arXiv:2212.08073.</p><p> \\n \\n  Bansal,      A.,      H.-M.\\nChu,      A.      Schwarzschild,      S.      Sengupta,      M.      Goldblum,      J.      Geiping,      and      T.      Goldstein  \\n \\n  (2023).\\nUniversal      guidance      for      diffusion      models.\\nIn      Proceedings      of      the      IEEE/CVF      Conference  \\n \\n  on      Computer      Vision      and      Pattern      Recognition,      pp.\\n843-852.</p><p>Bengio,      Y.,      S.      Lahlou,      T.      Deleu,      E.      J.      Hu,      M.      Tiwari,      and      E.      Bengio      (2023).\\nGflownet      foundations.\\n \\n \\n  Journal      of      Machine      Learning      Research      24(210),      1-55.</p><p> \\n \\n  Benton,      J.,      Y.      Shi,      V.      De      Bortoli,      G.      Deligiannidis,      and      A.      Doucet      (2024).\\nFrom      denoising      diffusions  \\n \\n  to      denoising      markov      models.\\nJournal      of      the      Royal      Statistical      Society      Series      B:      Statistical  \\n \\n  Methodology      86(2),      286-301.</p><p>Bernton,      E.,      J.      Heng,      A.      Doucet,      and      P.      E.      Jacob      (2019).\\nSchr\\\\\\\\\\\\\\\\””      odinger      bridge      samplers.\\narXiv  \\n \\n  preprint      arXiv:      1912.13170.</p><p>Besag,      J.\\n(1994).\\nComments      on      “representations      of      knowledge      in      complex      systems”      by      u.      grenander  \\n \\n  and      mi      miller.\\nJ.      Roy.\\nStatist.\\nSoc.      Ser.\\n \\n \\nB      56(591-592),      4.</p><p>Black,      K.,      M.      Janner,      Y.      Du,      I.      Kostrikov,      and      S.      Levine      (2023).\\nTraining      diffusion      models      with  \\n \\n  reinforcement      learning.\\narXiv      preprint      arXiv:2305.13301.</p><p> \\n \\n  Campbell,      A.,      J.      Benton,      V.      De      Bortoli,      T.      Rainforth,      G.      Deligiannidis,      and      A.      Doucet      (2022).\\n \\n \\nA       continuous      time      framework      for      discrete      denoising      models.\\nAdvances      in      Neural      Information  \\n \\n  Processing      Systems      35,      28266-28279.</p><p> \\n \\n  Campbell,      A.,      J.      Yim,      R.      Barzilay,      T.      Rainforth,      and      T.      Jaakkola      (2024).\\nGenerative      flows      on  \\n \\n  discrete      state-spaces:      Enabling      multimodal      flows      with      applications      to      protein      co-design.\\narXiv  \\n \\n  preprint      arXiv:2402.04997.</p><p>Cao,      H.,      C.      Tan,      Z.      Gao,      Y.      Xu,      G.      Chen,      P.-A.\\nHeng,      and      S.      Z.      Li      (2024).\\n \\n \\nA      survey      on      generative  \\n \\n  diffusion      models.\\n[EEE      Transactions      on      Knowledge      and      Data      Engineering.</p><p>Casper,      S.,      X.      Davies,      C.      Shi,      T.      K.      Gilbert,      J.      Scheurer,      J.      Rando,      R.      Freedman,      T.      Korbak, D.      Lindner,   \\n \\nP P.      Freire,      et      al.\\n(2023).\\nOpen      problems      and      fundamental      limitations      of      reinforcement  \\n \\n  learning      from      human      feedback.\\narXiv      preprint      arXiv:2307.15217.</p><p>Castillo-Hair,      S.      M.      and      G.      Seelig      (2021).\\nMachine      learning      for      designing      next-generation      mrna  \\n \\n  therapeutics.\\nAccounts      of      Chemical      Research      55(1),      24-34.</p><p>Chang,      J.      D.,      M.      Uehara,      D.      Sreenivas,      R.      Kidambi,      and      W.      Sun      (2021).\\nMitigating      covariate      shift  \\n \\n  in      imitation      learning      via      offline      data      without      great      coverage.\\narXiv      preprint      arXiv:2106.03207.</p><p>Chen,      M.,      S.      Mei,      J.      Fan,      and      M.      Wang      (2024).\\nAn      overview      of      diffusion      models:      Applications,  \\n \\n  guided      generation,      statistical      rates      and      optimization.\\narXiv      preprint      arXiv:2404.07771.</p><p>Chen,      R.      T.,      Y.      Rubanova,      J.      Bettencourt,      and      D.      K.      Duvenaud      (2018).\\nNeural      ordinary      differential  \\n \\n  equations.\\nAdvances      in      neural      information      processing      systems      31.</p><p>Chung,      H.,      J.      Kim,      M.      T.      Mccann,      M.      L.      Klasky,      and      J.      C.      Ye      (2022).\\nDiffusion      posterior      sampling  \\n \\n  for      general      noisy      inverse      problems.\\narXiv      preprint      arXiv:2209.\\n14687.</p><p> \\n \\n  Chung,      H.,      B.      Sim,      D.      Ryu,      and      J.      C.      Ye      (2022).\\nImproving      diffusion      models      for      inverse      problems  \\n \\n  using      manifold      constraints.\\nAdvances      in      Neural      Information      Processing      Systems      35,      25683-—  \\n \\n  25696.</p><p>Clark,      K.,      P.      Vicol,      K.      Swersky,      and      D.      J.      Fleet      (2023).\\nDirectly      fine-tuning      diffusion      models      on  \\n \\n  differentiable      rewards.\\narXiv      preprint      arXiv:2309.\\n17400.</p><p> \\n \\n  De      Bortoli,      V.,      E.      Mathieu,      M.      Hutchinson,      J.      Thornton,      Y.      W.      Teh,      and      A.      Doucet      (2022).\\nRieman-  \\n \\n  nian      score-based      generative      modelling.\\nAdvances      in      Neural      Information      Processing      Systems      35,  \\n \\n  2406-2422.</p><p>Deleu,      T.,      P.      Nouri,      N.      Malkin,      D.      Precup,      and      Y.      Bengio      (2024).\\nDiscrete      probabilistic      inference      as  \\n \\n  control      in      multi-path      environments.\\narXiv      preprint      arXiv:2402.10309.</p><p>Dhariwal,      P.      and      A.      Nichol      (2021).\\nDiffusion      models      beat      gans      on      image      synthesis.\\nAdvances      in  \\n \\n  neural      information      processing      systems      34,      8780-8794.</p><p> \\n \\n  Du,      Y.,      S.      Yang,      B.      Dai,      H.      Dai,      O.      Nachum,      J.      Tenenbaum,      D.      Schuurmans,      and      P.      Abbeel      (2024).\\n \\n \\n  Learning      universal      policies      via      text-guided      video      generation.\\nAdvances      in      Neural      Information  \\n \\n  Processing      Systems      36.</p><p>Fan,      Y.,      O.      Watkins,      Y.      Du,      H.      Liu,      M.      Ryu,      C.      Boutilier,      P.      Abbeel,      M.      Ghavamzadeh,      K.      Lee,      and K.      Lee      (2023) 023).\\nDPOK:      Reinforcement      learning      for      fine-tuning      text-to-image      diffusion      models.\\n \\n \\n  arXiv      preprint      arXiv:2305.16381.</p><p>Finn,      C.,      S.      Levine,      and      P.      Abbeel      (2016).\\nGuided      cost      learning:      Deep      inverse      optimal      control      via  \\n \\n  policy      optimization.\\nIn      International      conference      on      machine      learning,      pp.\\n49-58.\\nPMLR.</p><p>Fox,      R.,      A.      Pakman,      and      N.      Tishby      (2015).\\nTaming      the      noise      in      reinforcement      learning      via      soft  \\n \\n  updates.\\narXiv      preprint      arXiv:      1512.08562.</p><p>Frey,      N.      C.,      D.      Berenberg,      K.      Zadorozhny,      J.      Kleinhenz,      J.      Lafrance-Vanasse,      I.      Hotzel,      Y.      Wu, S.      Ra,   \\n \\nR R.      Bonneau,      K.      Cho,      et      al.\\n(2023).\\nProtein      discovery      with      discrete      walk-jump      sampling.\\n \\n \\n  arXiv      preprint      arXiv:2306.\\n12360.</p><p>Geist,      M.,      B.      Scherrer,      and      O.      Pietquin      (2019).\\n \\n \\nA      theory      of      regularized      markov      decision      processes.\\n \\n \\n  In      International      Conference      on      Machine      Learning,      pp.\\n2160-2169.\\nPMLR.</p><p> \\n \\n  Girolami,      M.      and      B.      Calderhead      (2011).\\nRiemann      manifold      langevin      and      hamiltonian      monte  \\n \\n  carlo      methods.\\nJournal      of      the      Royal      Statistical      Society      Series      B:      Statistical      Methodology      73(2),  \\n \\n  123-214.</p><p>Gosai,      S.      J.,      R.      I.      Castro,      N.      Fuentes,      J.      C.      Butts,      S.      Kales,      R.      R.      Noche,      K.      Mouri,      P.      C.      Sabeti, S.   \\n \\nK K.      Reilly,      and      R.      Tewhey      (2023).\\nMachine-guided      design      of      synthetic      cell      type-specific  \\n \\n  cis-regulatory      elements.\\nbioRxiv.</p><p>Haarnoja,      T.,      H.      Tang,      P.      Abbeel,      and      S.      Levine      (2017).\\nReinforcement      learning      with      deep      energy-  \\n \\n  based      policies.\\nIn      International      conference      on      machine      learning,      pp.\\n1352-1361.\\nPMLR.</p><p>Hansen-Estruch,      P.,      I.      Kostrikov,      M.      Janner,      J.      G.      Kuba,      and      S.      Levine      (2023).\\nIdql:      Implicit  \\n \\n  q-learning      as      an      actor-critic      method      with      diffusion      policies.\\narXiv      preprint      arXiv:2304.10573.</p><p>Heng,      J.,      A.      N.      Bishop,      G.      Deligiannidis,      and      A.      Doucet      (2020).\\nControlled      sequential      monte      carlo.</p><p>Ho,      J.,      A.      Jain,      and      P.      Abbeel      (2020).\\nDenoising      diffusion      probabilistic      models.\\nAdvances      in      neural  \\n \\n  information      processing      systems      33,      6840-6851.</p><p>Ho,      J.      and      T.      Salimans      (2022).\\nClassifier-free      diffusion      guidance.\\narXiv      preprint      arXiv:2207.12598.</p><p>Ho,      J.,      T.      Salimans,      A.      Gritsenko,      W.      Chan,      M.      Norouzi,      and      D.      J.      Fleet      (2022).\\nVideo      diffusion  \\n \\n  models.\\nAdvances      in      Neural      Information      Processing      Systems      35,      8633-8646.</p><p> \\n \\n  Hoogeboom,      E.,      V.      G.      Satorras,      C.      Vignac,      and      M.      Welling      (2022).\\nEquivariant      diffusion      for  \\n \\n  molecule      generation      in      3d.\\nIn      International      conference      on      machine      learning,      pp.\\n8867-8887.</p><h4>      PMLR.</h4><p>Huang,      X.,      H.      Dong,      H.      Yifan,      Y.      Ma,      and      T.      Zhang      (2023).\\nReverse      diffusion      monte      carlo.\\nIn      The  \\n \\n  Twelfth      International      Conference      on      Learning      Representations.</p><p>Janner,      M.,      Y.      Du,      J.      B.      Tenenbaum,      and      S.      Levine      (2022).\\nPlanning      with      diffusion      for      flexible  \\n \\n  behavior      synthesis.\\narXiv      preprint      arXiv:2205.09991.</p><p> \\n \\n  Jo,      J.,      S.      Lee,      and      S.      J.      Hwang      (2022).\\nScore-based      generative      modeling      of      graphs      via      the      system  \\n \\n  of      stochastic      differential      equations.\\nIn      International      Conference      on      Machine      Learning,      pp.\\n \\n \\n  10362-10383.\\nPMLR.</p><p> \\n \\n  Kazim,      M.,      J.      Hong,      M.-G.\\nKim,      and      K.-K.\\nK.      Kim      (2024).\\nRecent      advances      in      path      integral      control  \\n \\n  for      trajectory      optimization:      An      overview      in      theoretical      and      algorithmic      perspectives.\\nAnnual  \\n \\n  Reviews      in      Control      57,      100931.</p><p>Krishnamoorthy,      S.,      S.      M.      Mashkaria,      and      A.      Grover      (2023).\\nDiffusion      models      for      black-box  \\n \\n  optimization.\\narXiv      preprint      arXiv:2306.07180.</p><p>Kumar,      A.,      A.      Zhou,      G.      Tucker,      and      S.      Levine      (2020).\\nConservative      q-learning      for      offline      reinforce-  \\n \\n  ment      learning.\\nAdvances      in      Neural      Information      Processing      Systems      33,      1179-1191.</p><p>Lal,      A.,      D.      Garfield,      T.      Biancalani,      and      G.      Eraslan      (2024).\\nreglm:      Designing      realistic      regulatory  \\n \\n  dna      with      autoregressive      language      models.\\nbioRxiv,      2024-02.</p><p>Lee,      K.,      H.      Liu,      M.      Ryu,      O.      Watkins,      Y.      Du,      C.      Boutilier,      P.      Abbeel,      M.      Ghavamzadeh,      and      S.      S.      Gu  \\n \\n  (2023).\\nAligning      text-to-image      models      using      human      feedback.\\narXiv      preprint      arXiv:2302.12192.</p><p>Levine,      S.      (2018).\\nReinforcement      learning      and      control      as      probabilistic      inference:      Tutorial      and       review.\\narXiv      preprint      arXiv:      1805.00909.</p><p>Li,      Z.,      Y.      Ni,      T.      A.      B.      Huygelen,      A.      Das,      G.      Xia,      G.-B.\\nStan,      and      Y.      Zhao      (2023).\\nLatent      diffusion  \\n \\n  model      for      dna      sequence      generation.\\narXiv      preprint      arXiv:2310.06150.</p><p>Lipman,      Y.,      R.      T.      Chen,      H.      Ben-Hamu,      M.      Nickel,      and      M.      Le      (2023).\\nFlow      matching      for      generative  \\n \\n  modeling.\\nJCLR      2023.</p><p>Liu,      X.,      C.      Gong,      and      Q.      Liu      (2022).\\nFlow      straight      and      fast:      Learning      to      generate      and      transfer      data  \\n \\n  with      rectified      flow.\\narXiv      preprint      arXiv:2209.03003.</p><p>Liu,      X.,      L.      Wu,      M.      Ye,      and      Q.      Liu      (2022).\\nLet      us      build      bridges:      Understanding      and      extending  \\n \\n  diffusion      generative      models.\\narXiv      preprint      arXiv:2208.14699.</p><p>Lou,      A.,      C.      Meng,      and      S.      Ermon      (2023).\\nDiscrete      diffusion      language      modeling      by      estimating      the  \\n \\n  ratios      of      the      data      distribution.\\narXiv      preprint      arXiv:2310.16834.</p><p>Malkin,      N.,      M.      Jain,      E.      Bengio,      C.      Sun,      and      Y.      Bengio      (2022).\\nTrajectory      balance:      Improved      credit  \\n \\n  assignment      in      gflownets.\\nAdvances      in      Neural      Information      Processing      Systems      35,      5955-5967.</p><p>Mohammadpour,      S.,      E.      Bengio,      E.      Frejinger,      and      P.-L.\\nBacon      (2023).\\nMaximum      entropy      gflownets  \\n \\n  with      soft      q-learning.\\narXiv      preprint      arXiv:2312.14331.</p><p>Nachum,      O.,      M.      Norouzi,      K.      Xu,      and      D.      Schuurmans      (2017).\\nBridging      the      gap      between      value      and       policy      based      reinforcement      learning.\\nAdvances      in      neural      information      processing      systems      30.</p><p>Neal,      R.      M.      et      al.\\n(2011).\\nMcmce      using      hamiltonian      dynamics.\\nHandbook      of      markov      chain      monte  \\n \\n  carlo      2(11),      2.</p><p>Neu,      G.,      A.      Jonsson,      and      V.      Gémez      (2017).\\n \\n \\nA      unified      view      of      entropy-regularized      markov      decision  \\n \\n  processes.\\narXiv      preprint      arXiv:      1705.07798.</p><p>Ouyang,      L.,      J.      Wu,      X.      Jiang,      D.      Almeida,      C.      Wainwright,      P.      Mishkin,      C.      Zhang,      S.      Agarwal, K.      Slama,   \\n \\nA A.      Ray,      et      al.\\n(2022).\\nTraining      language      models      to      follow      instructions      with      human  \\n \\n  feedback.\\nAdvances      in      Neural      Information      Processing      Systems      35,      27730-27744.</p><p>Pattanaik,      L.      and      C.      W.      Coley      (2020).\\nMolecular      representation:      going      long      on      fingerprints.\\n \\n \\n  Chem      6(6),      1204-1207.</p><p>Peng,      X.      B.,      A.      Kumar,      G.      Zhang,      and      S.      Levine      (2019).\\nAdvantage-weighted      regression:      Simple  \\n \\n  and      scalable      off-policy      reinforcement      learning.\\narXiv      preprint      arXiv:1910.00177.</p><p>Peters,      J.,      K.      Mulling,      and      Y.      Altun      (2010).\\nRelative      entropy      policy      search.\\nIn      Proceedings      of      the  \\n \\n  AAAI      Conference      on      Artificial      Intelligence,      Volume      24,      pp.\\n1607-1612.</p><p> \\n \\n  Podell,      D.,      Z.      English,      K.      Lacey,      A.      Blattmann,      T.      Dockhorn,      J.      Miiller,      J.      Penna,      and      R.      Rombach  \\n \\n  (2023).\\nSdxl:      Improving      latent      diffusion      models      for      high-resolution      image      synthesis.\\narXiv  \\n \\n  preprint      arXiv:2307.01952.</p><p>Prabhudesai,      M.,      A.      Goyal,      D.      Pathak,      and      K.      Fragkiadaki      (2023).\\nAligning      text-to-image      diffusion  \\n \\n  models      with      reward      backpropagation.\\narXiv      preprint      arXiv:2310.03739.</p><p>Ramesh,      A.,      P.      Dhariwal,      A.      Nichol,      C.      Chu,      and      M.      Chen      (2022).\\nHierarchical      text-conditional  \\n \\n  image      generation      with      clip      latents.\\narXiv      preprint      arXiv:2204.06125      1(2),      3.</p><p>Rigter,      M.,      B.      Lacerda,      and      N.      Hawes      (2022).\\nRambo-rl:      Robust      adversarial      model-based      offline  \\n \\n  reinforcement      learning.\\nAdvances      in      neural      information      processing      systems      35,      16082-16097.</p><p>Robert,      C.      (2014).\\nStatistical      modeling      and      computation.</p><p>Rogers,      L.      C.      G.      and      D.      Williams      (2000).\\nDiffusions,      Markov      processes      and      martingales:      Volume      2,  \\n \\n  It6é      calculus,      Volume      2.\\nCambridge      university      press.</p><p> \\n \\n  Rombach,      R.,      A.      Blattmann,      D.      Lorenz,      P.      Esser,      and      B.      Ommer      (2022,      June).\\nHigh-resolution  \\n \\n  image      synthesis      with      latent      diffusion      models.\\nIn      Proceedings      of      the      IEEE/CVF      Conference      on  \\n \\n  Computer      Vision      and      Pattern      Recognition      (CVPR),      pp.\\n10684-10695.</p><p>Sarkar,      A.,      Z.      Tang,      C.      Zhao,      and      P.      Koo      (2024).\\nDesigning      dna      with      tunable      regulatory      activity  \\n \\n  using      discrete      diffusion.\\nbioRxiv,      2024-05.</p><p>Schulman,      J.,      X.      Chen,      and      P.      Abbeel      (2017).\\nEquivalence      between      policy      gradients      and      soft  \\n \\n  q-learning.\\narXiv      preprint      arXiv:      1704.06440.</p><p>Schulman,      J.,      S.      Levine,      P.      Abbeel,      M.      Jordan,      and      P.      Moritz      (2015).\\nTrust      region      policy      optimization.\\n \\n \\n  In      International      conference      on      machine      learning,      pp.\\n1889-1897.\\nPMLR.</p><p>Schulman,      J.,      F.      Wolski,      P.      Dhariwal,      A.      Radford,      and      O.      Klimov      (2017).\\nProximal      policy      optimiza-  \\n \\n  tion      algorithms.\\narXiv      preprint      arXiv:      1707.06347.</p><p>Shi,      Y.,      V.      De      Bortoli,      A.      Campbell,      and      A.      Doucet      (2024).\\nDiffusion      schrédinger      bridge      matching.\\n \\n \\n  Advances      in      Neural      Information      Processing      Systems      36.</p><p> \\n \\n  Sohl-Dickstein,      J.,      E.      Weiss,      N.      Maheswaranathan,      and      S.      Ganguli      (2015).\\nDeep      unsupervised  \\n \\n  learning      using      nonequilibrium      thermodynamics.\\nIn      /nternational      conference      on      machine      learning,  \\n \\n  pp.\\n2256-2265.\\nPMLR.</p><p>Song,      J.,      C.      Meng,      and      S.      Ermon      (2020).\\nDenoising      diffusion      implicit      models.\\narXiv      preprint  \\n \\n  arXiv:2010.02502.</p><p>Song,      Y.,      C.      Durkan,      I.      Murray,      and      S.      Ermon      (2021).\\nMaximum      likelihood      training      of      score-based  \\n \\n  diffusion      models.\\nAdvances      in      neural      information      processing      systems      34,      1415-1428.</p><p>Stark,      H.,      B.      Jing,      C.      Wang,      G.      Corso,      B.      Berger,      R.      Barzilay,      and      T.      Jaakkola      (2024).\\nDirichlet  \\n \\n  flow      matching      with      applications      to      dna      sequence      design.\\narXiv      preprint      arXiv:2402.05841.</p><p>Sutton,      R.      S.      and      A.      G.      Barto      (2018).\\nReinforcement      learning:      An      introduction.\\nMIT      press.</p><p>Tang,      W.      (2024).\\nFine-tuning      of      diffusion      models      via      stochastic      control:      entropy      regularization      and       beyond.\\narXiv      preprint      arXiv:2403.06279.</p><p>Tang,      W.      and      H.      Zhao      (2024).\\nScore-based      diffusion      models      via      stochastic      differential      equations—a  \\n \\n  technical      tutorial.\\narXiv      preprint      arXiv:2402.07487.</p><p>Theodorou,      E.,      J.      Buchli,      and      S.      Schaal      (2010).\\n \\n \\nA      generalized      path      integral      control      approach      to  \\n \\n  reinforcement      learning.\\nThe      Journal      of      Machine      Learning      Research      11,      3137-3181.</p><p>Tiapkin,      D.,      N.      Morozov,      A.      Naumov,      and      D.      Vetrov      (2023).\\nGenerative      flow      networks      as      entropy-  \\n \\n  regularized      rl.\\narXiv      preprint      arXiv:2310.12934.</p><p> \\n \\n  Tong,      A.,      N.      Malkin,      G.      Huguet,      Y.      Zhang,      J.      Rector-Brooks,      K.      Fatras,      G.      Wolf,      and      Y.      Bengio  \\n \\n  (2023).\\nConditional      flow      matching:      Simulation-free      dynamic      optimal      transport.\\narXiv      preprint  \\n \\n  arXiv:2302.00482.</p><p>Uehara,      M.      and      W.      Sun      (2021).\\nPessimistic      model-based      offline      reinforcement      learning      under  \\n \\n  partial      coverage.\\narXiv      preprint      arXiv:2107.06226.</p><p>Uehara,      M.,      Y.      Zhao,      K.      Black,      E.      Hajiramezanali,      G.      Scalia,      N.      L.      Diamant,      A.      M.      Tseng, T.      Biancalani,      and      S S.      Levine      (2024).\\nFine-tuning      of      continuous-time      diffusion      models      as      entropy-  \\n \\n  regularized      control.\\narXiv      preprint      arXiv:2402.15194.</p><p> \\n \\n  Uehara,      M.,      Y.      Zhao,      K.      Black,      E.      Hajiramezanali,      G.      Scalia,      N.      L.      Diamant,      A.      M.      Tseng,      S.      Levine,  \\n \\n  and      T.      Biancalani      (2024).\\nFeedback      efficient      online      fine-tuning      of      diffusion      models.\\narXiv      preprint  \\n \\n  arXiv:2402.\\n16359.</p><p> \\n \\n  Uehara,      M.,      Y.      Zhao,      E.      Hajiramezanali,      G.      Scalia,      G.      Eraslan,      A.      Lal,      S.      Levine,      and      T.      Biancalani  \\n \\n  (2024).\\nBridging      model-based      optimization      and      generative      modeling      via      conservative      fine-tuning  \\n \\n  of      diffusion      models.\\narXiv      preprint      arXiv:2405.19673.</p><p>Vargas,      F.,      W.      Grathwohl,      and      A.      Doucet      (2023).\\nDenoising      diffusion      samplers.\\narXiv      preprint  \\n \\n  arXiv:2302.\\n13834.</p><p> \\n \\n  Wallace,      B.,      M.      Dang,      R.      Rafailov,      L.      Zhou,      A.      Lou,      S.      Purushwalkam,      S.      Ermon,      C.      Xiong,      S.      Joty,  \\n \\n  and      N.      Naik      (2023).\\nDiffusion      model      alignment      using      direct      preference      optimization.\\narXiv  \\n \\n  preprint      arXiv:2311.12908.</p><p>Wang,      Z.,      J.      J.      Hunt,      and      M.      Zhou      (2022).\\nDiffusion      policies      as      an      expressive      policy      class      for      offline  \\n \\n  reinforcement      learning.\\narXiv      preprint      arXiv:2208.06193.</p><p>Widatalla,      T.,      R.      Rafailov,      and      B.      Hie      (2024).\\nAligning      protein      generative      models      with      experimental  \\n \\n  fitness      via      direct      preference      optimization.\\nbioRxiv,      2024-05.</p><p>Williams,      G.,      A.      Aldrich,      and      E.      A.      Theodorou      (2017).\\nModel      predictive      path      integral      control:      From  \\n \\n  theory      to      parallel      computation.\\nJournal      of      Guidance,      Control,      and      Dynamics      40(2),      344-357.</p><p>Wu,      X.,      K.      Sun,      F.      Zhu,      R.      Zhao,      and      H.      Li      (2023).\\nBetter      aligning      text-to-image      models      with  \\n \\n  human      preference.\\narXiv      preprint      arXiv:2303.14420.</p><p>Wu,      Y.,      G.      Tucker,      and      O.      Nachum      (2019).\\nBehavior      regularized      offline      reinforcement      learning.\\n \\n \\n  arXiv      preprint      arXiv:      1911.11361.</p><p>Wulfmeier,      M.,      P.      Ondruska,      and      I.      Posner      (2015).\\nMaximum      entropy      deep      inverse      reinforcement  \\n \\n  learning.\\narXiv      preprint      arXiv:      1507.04888.</p><p>Xie,      T.,      C.-A.\\nCheng,      N.      Jiang,      P.      Mineiro,      and      A.      Agarwal      (2021).\\nBellman-consistent      pessimism  \\n \\n  for      offline      reinforcement      learning.\\nAdvances      in      neural      information      processing      systems      34.</p><p>Xu,      M.,      L.      Yu,      Y.      Song,      C.      Shi,      S.      Ermon,      and      J.      Tang      (2022).\\nGeodiff:   \\n \\nA      geometric      diffusion  \\n \\n  model      for      molecular      conformation      generation.\\narXiv      preprint      arXiv:2203.02923.</p><p> \\n \\n  Yang,      L.,      Z.      Zhang,      Y.      Song,      S.      Hong,      R.      Xu,      Y.      Zhao,      W.      Zhang,      B.      Cui,      and      M.-H.\\nYang      (2023).\\n \\n \\n  Diffusion      models:   \\n \\nA      comprehensive      survey      of      methods      and      applications.\\nACM      Computing  \\n \\n  Surveys      56(4),      1-39.</p><p> \\n \\n  Yu,      T.,      G.      Thomas,      L.      Yu,      S.      Ermon,      J.      Y.      Zou,      S.      Levine,      C.      Finn,      and      T.      Ma      (2020).\\nMopo:  \\n \\n  Model-based      offline      policy      optimization.\\nAdvances      in      Neural      Information      Processing      Systems      33,  \\n \\n  14129-14142.</p><p> \\n \\n  Yuan,      H.,      K.      Huang,      C.      Ni,      M.      Chen,      and      M.      Wang      (2023).\\nReward-directed      conditional      diffusion:  \\n \\n  Provable      distribution      estimation      and      reward      improvement.\\nIn      Thirty-seventh      Conference      on  \\n \\n  Neural      Information      Processing      Systems.</p><p>Zhang,      H.      and      T.      Xu      (2023).\\nTowards      controllable      diffusion      models      via      reward-guided      exploration.\\n \\n \\n  arXiv      preprint      arXiv:2304.07132.</p><p>Zhang,      Q.      and      Y.      Chen      (2021).\\nPath      integral      sampler:   \\n \\na      stochastic      control      approach      for      sampling.\\n \\n \\n  arXiv      preprint      arXiv:2111,15141.</p><p> \\n \\n  Zhao,      Y.,      M.      Uehara,      G.      Scalia,      T.      Biancalani,      S.      Levine,      and      E.      Hajiramezanali      (2024).\\n \\n \\n  Adding      conditional      control      to      diffusion      models      with      reinforcement      learning.\\narXiv      preprint  \\n \\n  arXiv:2406.\\n12120.</p><p>Zhou,      Z.,      S.      Kearnes,      L.      Li,      R.      N.      Zare,      and      P.      Riley      (2019).\\nOptimization      of      molecules      via      deep  \\n \\n  reinforcement      learning.\\nScientific      reports      9(1),      10752.</p><p>Zhu,      Z.,      H.      Zhao,      H.      He,      Y.      Zhong,      S.      Zhang,      Y.      Yu,      and      W.      Zhang      (2023).\\nDiffusion      models      for  \\n \\n  reinforcement      learning:   \\n \\nA      survey.\\narXiv      preprint      arXiv:2311.01223.</p><p>Ziebart,      B.      D.,      A.      L.      Maas,      J.      A.      Bagnell,      A.      K.      Dey,      et      al.\\n(2008).\\nMaximum      entropy      inverse  \\n \\n  reinforcement      learning.\\nIn      Aaai,      Volume      8,      pp.\\n1433-1438.\\nChicago,      IL,      USA.</p><h3>Hence,</h3><p> \\n \\n \\n_   \\n \\n:      (xt)      Up-1(Lt-1)   \\n \\n°       vu   \\n \\n=      argmin      Ey,.u,   \\n \\n|   \\n \\n4      ——   \\n \\n—      log   \\n \\n|      exp   \\n \\n|      ————   \\n \\n]      Dpre(@e-1]      21)      da4_1  \\n \\n  a   \\n \\na h:X->R  \\n \\n  where      u,   \\n \\n€      A(%)      is      any      roll-in      distribution      that      covers      the      entire      space      V.      Using      this      relation      and       replacing      the      expectation      E,,,.,,,      with      empirical      approximation,      we      are      able      to      estimate      soft      value  \\n \\n  functions      v;      in   \\n \\na      recursive      manner:</p><p> \\n \\n \\n1   \\n \\nm      ~(j—1)   \\n \\n°       aj   \\n \\n;   \\n \\ni      Ur      Ut   \\n \\ny       {6      (x)},   \\n \\n—       argmin   \\n \\ny      S      {ht   \\n \\n)   \\n \\n—      log   \\n \\n/      exp      (eee)      Ppre(@1—1|}      yan] h:      [R¢,[0,7]]|-R      t=T+1      i=1</p><p>where      the      data      is      collected      in      an      off-policy      way.</p><p> \\n \\n  Remark      6.\\nAlthough      soft      Q-learning      is      widely      used      in      standard      RL      (Schulman      et      al.,      2017),      it  \\n \\n  cannot      be      directly      applied      to      our      fine-tuning      context      without      resorting      to      value-weighted      sampling  \\n \\n  or      value-weighted      MLE.\\nThis      is      because,      even      if      we      estimate      soft-value      functions      as      %;,      substituting  \\n \\n  Uz      with      0,      in      the      soft-optimal      policy      results      in      an      unnormalized      policy.<ul><li>6.2.2.\\nApproximation      using      Tweedie’s      formula</li></ul></p><p> \\n \\n  So      far,      we      have      explained      two      approaches:   \\n \\na      Monte      Carlo      approach      and   \\n \\na      value      iteration      approach      to  \\n \\n  estimate      soft      value      functions.\\nHowever,      learning      value      functions      in      (28)      can      still      be      often      challenging  \\n \\n  in      practice.\\nTherefore,      we      can      employ      approximation      strategies      inspired      by      recent      literature      on  \\n \\n  classifier      guidance      (e.g.,      reconstruction      guidance      (Ho      et      al.,      2022),      manifold      constrained      gradients  \\n \\n  (Chung      et      al.,      2022),      universal      guidance      (Bansal      et      al.,      2023),      and      diffusion      posterior      sampling  \\n \\n  (Chung      et      al.,      2022)).</p><p> \\n \\n  Specifically,      we      adopt      the      following      approximation:</p><p>ur(xz)   \\n \\n=      alog      Egpry      lex      (“)      oy   \\n \\n=      alog      (/      exp      (om)   \\n \\np      (colar      (30)</p><p>~\\n \\n   alog      (exw      (ceo)   \\n \\n)   \\n \\n»       £o(t1)   \\n \\n=      Egprey|xo   \\n \\n|      xe],      (31) a\\n \\n    =\\n \\n   1r(Xo(2t)).</p><p> \\n \\n  Here,      we      replace      the      integration      in      (30)      with   \\n \\na      Dirac      delta      distribution      with      the      posterior      mean.\\n \\n \\n  Importantly,      we      can      calculate      Zo      (x;)   \\n \\n=      E,»*-[xo   \\n \\n|      x4]      using      the      pre-trained      (score-based)      diffusion  \\n \\n  model      based      on      Tweedie’s      formula:</p><table><tr><td colSpan=1>      O12</td><td colSpan=1><p>]\\n \\n \\n  E,pre      [xo   \\n \\n|      x1   \\n \\n—      Ut   \\n \\n+      {or}   \\n \\nV      0g      Ge(&e)  \\n \\n  Le</p></td></tr></table><p>Recall      that      the      notation      p/?,      0?,      q,      are      defined      in      (3).\\nFinally,      by      recalling   \\n \\nV      log   \\n \\n@   \\n \\n=      Si      ore      (x;,t)      in  \\n \\n  score-based      diffusion      models,      we      can      approximate      Vv;(a)      with</p><h4>Le</h4><p> \\n \\n  and      plug      it      into      Algorithm   \\n \\n4      (i.e.,      value-weighted      sampling).\\n \\n \\n  As      mentioned      earlier,      this      approach      has      been      practically      widely      used      in      the      context      of      classifier  \\n \\n  guidance.\\nDespite      its      simplicity,      the      approximation      error      can      be      significant,      as      it      can      not      be  \\n \\n  diminished      even      with   \\n \\na      large      sample      size      because      the      discrepancy      from      (30)      to      (31)      is      not      merely   \\n \\na       statistical      error      that      diminishes      with      increasing      sample      size.</p><li>6.2.3.\\nZeroth-Order      Guidance      using      Path      Integral      Control</li><p> \\n \\n  In      the      previous      subsection      (Section      6.2.2),      we      discussed      an      approximation      technique      to      bypass      the  \\n \\n  necessity      of      learning      explicit      value      functions.\\nAnother      approach      to      bypass      this      requirement      is      to      use  \\n \\n  control-based      techniques      for      obtaining      soft-optimal      policies,      a.k.a.,      path      integral      control      (Theodorou  \\n \\n  et      al.,      2010;      Williams      et      al.,      2017;      Kazim      et      al.,      2024).</p><p> \\n \\n  Recall      that      we      have      p(x;,t;      6)   \\n \\n=      2;   \\n \\n+      (dt)g(x+,      t)      and      o°(t)   \\n \\n=      g(t)(dt),      the      motivation      behind this      approach      is      as      follows.\\nInitially,      we      have:</p><h5>Vo,      exp(v(#1)/@)</h5><p>=\\n \\n   Egppreyexp(vr_1      (22-1)      /@)   \\n \\n|      24]      (Soft      Bellman      equation) _\\n \\n   (1   \\n \\n—   \\n \\nx   \\n \\n—      (Ot)      G(a2,      t))?\\n=\\n \\n   Vz»,   \\n \\n{   \\n \\n[      explatena/ay)      exp      (-      0-5a(t)      (ot)   \\n \\n)      in,-}</p><p>=\\n \\n   Byes      [ese      a(era/a))      {ea      ~~      (61)g(nn,)}      EE      Cah</p><p>~\\n \\n   arp      Buty      lexr(era(ea/a))      Gera   \\n \\n—      21   \\n \\n~      (8t)ale      plod</p><p>_t   \\n \\n1       Ht)      ot)      Eg,prey      [E      ppp}      [exp(r(2o)/a)   \\n \\n|      T-1]      e|24|   \\n \\n=      Woon)      [exp(r(xo/a))e|x¢]      ;</p><h5>Now,      we      obtain:</h5><p>o7(t)Vnv(ae)   \\n \\n1      Eqppry      lexp(r(wo)/a)      eel  \\n \\n  Q   \\n \\n~   \\n \\na   \\n \\n*      Egyprey      lexp(r(2o)/a))      |x]      (32)</p><p> \\n \\n  Importantly,      this      approach      does      not      require      any      differentiation.\\nTherefore,      by      running      policies  \\n \\n  from      pre-trained      models      and      simply      using      Monte      Carlo      approximation      in      both      the      denominator  \\n \\n  and      numerator,      we      can      estimate      the      above      quantity      (the      right-hand      side      of      (32))      without      making  \\n \\n  any      models      (classifiers),      unlike      classifier      guidance.\\nNote      while      this      approach      is      widely      used      in      the  \\n \\n  control      community,      it      may      not      be      feasible      for      diffusion      models      due      to      the      high      dimensionality      of  \\n \\n  the      input      space.</p><h4>Le</h4><p> \\n \\n  and      plug      it      into      Algorithm   \\n \\n4      (i.e.,      value-weighted      sampling).\\n \\n \\n  As      mentioned      earlier,      this      approach      has      been      practically      widely      used      in      the      context      of      classifier  \\n \\n  guidance.\\nDespite      its      simplicity,      the      approximation      error      can      be      significant,      as      it      can      not      be  \\n \\n  diminished      even      with   \\n \\na      large      sample      size      because      the      discrepancy      from      (30)      to      (31)      is      not      merely   \\n \\na       statistical      error      that      diminishes      with      increasing      sample      size.</p><li>6.2.3.\\nZeroth-Order      Guidance      using      Path      Integral      Control</li><p> \\n \\n  In      the      previous      subsection      (Section      6.2.2),      we      discussed      an      approximation      technique      to      bypass      the  \\n \\n  necessity      of      learning      explicit      value      functions.\\nAnother      approach      to      bypass      this      requirement      is      to      use  \\n \\n  control-based      techniques      for      obtaining      soft-optimal      policies,      a.k.a.,      path      integral      control      (Theodorou  \\n \\n  et      al.,      2010;      Williams      et      al.,      2017;      Kazim      et      al.,      2024).</p><p> \\n \\n  Recall      that      we      have      p(x;,t;      6)   \\n \\n=      2;   \\n \\n+      (dt)g(x+,      t)      and      o°(t)   \\n \\n=      g(t)(dt),      the      motivation      behind this      approach      is      as      follows.\\nInitially,      we      have:</p><h5>Vo,      exp(v(#1)/@)</h5><p>=\\n \\n   Egppreyexp(vr_1      (22-1)      /@)   \\n \\n|      24]      (Soft      Bellman      equation) _\\n \\n   (1   \\n \\n—   \\n \\nx   \\n \\n—      (Ot)      G(a2,      t))?\\n=\\n \\n   Vz»,   \\n \\n{   \\n \\n[      explatena/ay)      exp      (-      0-5a(t)      (ot)   \\n \\n)      in,-}</p><p>=\\n \\n   Byes      [ese      a(era/a))      {ea      ~~      (61)g(nn,)}      EE      Cah</p><p>~\\n \\n   arp      Buty      lexr(era(ea/a))      Gera   \\n \\n—      21   \\n \\n~      (8t)ale      plod</p><p>_t   \\n \\n1       Ht)      ot)      Eg,prey      [E      ppp}      [exp(r(2o)/a)   \\n \\n|      T-1]      e|24|   \\n \\n=      Woon)      [exp(r(xo/a))e|x¢]      ;</p><h5>Now,      we      obtain:</h5><p>o7(t)Vnv(ae)   \\n \\n1      Eqppry      lexp(r(wo)/a)      eel  \\n \\n  Q   \\n \\n~   \\n \\na   \\n \\n*      Egyprey      lexp(r(2o)/a))      |x]      (32)</p><p> \\n \\n  Importantly,      this      approach      does      not      require      any      differentiation.\\nTherefore,      by      running      policies  \\n \\n  from      pre-trained      models      and      simply      using      Monte      Carlo      approximation      in      both      the      denominator  \\n \\n  and      numerator,      we      can      estimate      the      above      quantity      (the      right-hand      side      of      (32))      without      making  \\n \\n  any      models      (classifiers),      unlike      classifier      guidance.\\nNote      while      this      approach      is      widely      used      in      the  \\n \\n  control      community,      it      may      not      be      feasible      for      diffusion      models      due      to      the      high      dimensionality      of  \\n \\n  the      input      space.</p><h5>Vo,      exp(v(#1)/@)</h5><p>=\\n \\n   Egppreyexp(vr_1      (22-1)      /@)   \\n \\n|      24]      (Soft      Bellman      equation) _\\n \\n   (1   \\n \\n—   \\n \\nx   \\n \\n—      (Ot)      G(a2,      t))?\\n=\\n \\n   Vz»,   \\n \\n{   \\n \\n[      explatena/ay)      exp      (-      0-5a(t)      (ot)   \\n \\n)      in,-}</p><p>=\\n \\n   Byes      [ese      a(era/a))      {ea      ~~      (61)g(nn,)}      EE      Cah</p><p>~\\n \\n   arp      Buty      lexr(era(ea/a))      Gera   \\n \\n—      21   \\n \\n~      (8t)ale      plod</p><p>_t   \\n \\n1       Ht)      ot)      Eg,prey      [E      ppp}      [exp(r(2o)/a)   \\n \\n|      T-1]      e|24|   \\n \\n=      Woon)      [exp(r(xo/a))e|x¢]      ;</p><h5>Now,      we      obtain:</h5><p>o7(t)Vnv(ae)   \\n \\n1      Eqppry      lexp(r(wo)/a)      eel  \\n \\n  Q   \\n \\n~   \\n \\na   \\n \\n*      Egyprey      lexp(r(2o)/a))      |x]      (32)</p><p> \\n \\n  Importantly,      this      approach      does      not      require      any      differentiation.\\nTherefore,      by      running      policies  \\n \\n  from      pre-trained      models      and      simply      using      Monte      Carlo      approximation      in      both      the      denominator  \\n \\n  and      numerator,      we      can      estimate      the      above      quantity      (the      right-hand      side      of      (32))      without      making  \\n \\n  any      models      (classifiers),      unlike      classifier      guidance.\\nNote      while      this      approach      is      widely      used      in      the  \\n \\n  control      community,      it      may      not      be      feasible      for      diffusion      models      due      to      the      high      dimensionality      of  \\n \\n  the      input      space.</p><h3>6.3      Path      Consistency      Learning      (Losses      Often      Used      in      Gflownets)</h3><p>ets)  \\n \\n  Now,      we      explain      how      to      apply      path      consistency      learning      (PCL)      (Nachum      et      al.,      2017)      to      fine-  \\n \\n  tune      diffusion      models.\\nIn      the      Gflownets      literature      (Bengio      et      al.,      2023),      it      seems      that      this      variant      is  \\n \\n  utilized      as      either   \\n \\na      detailed      balance      or   \\n \\na      trajectory      balance      loss,      as      discussed      in      Mohammadpour  \\n \\n  et      al.\\n(2023);      Tiapkin      et      al.\\n(2023);      Deleu      et      al.\\n(2024).\\nHowever,      to      the      best      of      our      knowledge,  \\n \\n  the      precise      formulation      of      path      consistency      learning      in      the      context      of      fine-tuning      diffusion      models  \\n \\n  has      not      been      established.\\nTherefore,      we      start      by      elucidating      the      rationale      of      PCL.\\nSubsequently,  \\n \\n  we      provide   \\n \\na      comprehensive      explanation      of      the      PCL.\\nFinally,      we      discuss      its      connection      with      the  \\n \\n  literature      on      Gflownets.</p><p>Motivation.\\nHere,      we      present      the      fundamental      principles      of      the      PCL.\\nTo      start      with,      we      prove      the  \\n \\n  following      lemma,      which      characterizes      soft-value      functions      and      soft-optimal      policies      recursively.</p><p>Algorithm   \\n \\n5      Path      Consistency      Learning      (Training      with      detailed      balance      loss)  \\n \\n  1:      Require:      Diffusion-model      {N(p(2,t;),07(t))}i-74,,  \\n \\n  -pre-trained   \\n \\n=      model  \\n \\n  {N      (p(t,      t;      Opre),      07      (t))      }i_744,      batch      size      m,      parameter   \\n \\na   \\n \\n€      Rt,      learning      rate   \\n \\n7       2:      Set   \\n \\na      model      {v;(-;@)}      to      learn      optimal      soft      value      function,      and   \\n \\na      model      {p;(-|-;@)}      to      learn  \\n \\n  optimal      polices.\\n \\n \\n  3:      Initialize:      0;   \\n \\n=      Opre  \\n \\n  4:      for      s   \\n \\n=      {1,---      ,S}do  \\n \\n  5:      Collect   \\n \\nm      samples      {a      (0)      }?_7,,      from   \\n \\na      current      diffusion      model      (i.e.,      generating      by      sequen- tially      running      polices      {N(p(2z,      t;      8),      07(t))      }i_-74,      from      t   \\n \\n=      T      to      t   \\n \\n=      0)  \\n \\n  6:      Setuo=r</p><li>(i)   \\n \\n?       a!\\n \\n \\n3   \\n \\na   \\n \\n4      Up—-1(@      13      ds)      re  \\n \\n  bs41   \\n \\n—      bs   \\n \\n—      Vo   \\n \\n3   \\n \\na      9)   \\n \\n+      log      p:(a\\\\\\\\\\\\\\\\,      |a\\\\\\\\\\\\\\\\;0,)   \\n \\n-      EP   \\n \\n—      Jog      rl      >}      Ibe:</li><p>a\\n \\n \\n  t=T+1      i=1  \\n \\n  (c}      ite)      ()      via)      43s)   \\n \\n;       Os41   \\n \\n<      Os   \\n \\n—      Vo   \\n \\n3      TE      velee      ids)      —S<   \\n \\n+      log      pe(ay      2s      |p      0)      —§      log      pee      (ar?\\nifort?\\nJe      lass</p><p>t=T+1      i=1 7:      end      for  \\n \\n  8:      Output:      {p,(x1-1      [215      Os)      fe</p><h3>Lemma      4      (1-step      Consistency      Equation).</h3><li>(2)   \\n \\n+      log      pi      (aa      |a1)   \\n \\n=      (eaeen)   \\n \\n+      log      pp      (x+-1|2+)      (33)</li><p> \\n \\n  Proof.\\nWe      consider      the      marginal      distribution      with      respect      to      x,      and      2;_,      induced      by      the      soft-  \\n \\n  optimal      policy,      and      denote      it      [(z;,      2:1)   \\n \\n€      A(X   \\n \\nx      X).\\nThen,      it      is      clearly      I(x,)l(a,_1   \\n \\n|      x)   \\n \\n=       I(x4-1)l      (xz   \\n \\n|      a1).\\nNow,      from      Theorem   \\n \\n2      that      characterizes      marginal      distributions,      and      Theorem   \\n \\n3       that      characterizes      posterior      distributions,      this      results      in      :</p><p>1\\n \\n   UE      L      re   \\n \\nx   \\n \\n1      Otte      me  \\n \\n  exp   \\n \\n(   \\n \\ni      2)      Pe      (xt)   \\n \\nx      pe      (®e-1|@e)   \\n \\n=   \\n \\nB      exp      (faeev)      Pra      (@1-1)   \\n \\nX      Pe      (1/01) a\\n \\n   —_-_——”’      C   \\n \\na      ~~  \\n \\n  C       we  \\n \\n  Optimal      policy  \\n \\n  “~   \\n \\n“      Posterior      distribution  \\n \\n  Marginal      distribution      at      t      Marginal      distribution      at      t-1 Rearranging      yields:</p><p>1\\n \\n   UL(&   \\n \\n1      Up—1      (Le      re  \\n \\n  G      exp   \\n \\n(      ))   \\n \\nx      Di      (@-1|%4)   \\n \\n=      G      exp      (eaeen)   \\n \\nx      pe      (@4-1|24)</p><p>Taking      the      logarithm,      the      statement      is      concluded.\\nO</p><p>Algorithm.\\nBeing      motivated      by      the      relation      in      (33),      after      initializing      v9   \\n \\n=      r,      we      obtain      the  \\n \\n  recursive      equation:</p><p>2 Olen      U_-1      (Lt  \\n \\n  (pp)   \\n \\n=      argmin   \\n \\n—      Egany   \\n \\n|      oO   \\n \\n4      tog      g      (arp      ng)   \\n \\n—      PEED)   \\n \\n—      dog      pP\"      (ya      er) g):%3R,g2):¥      A(X)   \\n \\na   \\n \\no U   \\n \\n_   \\n \\nx   \\n \\n_      re      re  \\n \\n  =      (MEY)      tog      peal      altiansa)      +++      Flog      pe      (esl),  \\n \\n  which      is      an      extension      of      (33).\\nThe      loss      function      based      on      the      above      k-step      consistency      equation  \\n \\n  could      make      training      faster      without      learning      value      functions      at      every      time      point,      as      noted      in      the  \\n \\n  literature      in      PCL.\\nIn      the      extreme      case      (i.e.,      when      we      recursively      apply      it      with      t   \\n \\n=      7’),      we      obtain      the  \\n \\n  following.</p><li>(34)  \\n \\n  where      u,   \\n \\n€      A(X)      is      any      exploratory      roll-in      distribution.\\nBased      on      this      algorithm,      we      outline      the  \\n \\n  entire      algorithm      in      Algorithm      5.  \\n \\n  We      make      several      important      remarks      regarding      Algorithm      5.\\nFirstly,      while      we      use      on-policy      data  \\n \\n  collection,      technically,      any      policy      can      be      used      in      this      off-policy      algorithm,      like      reward-weighted  \\n \\n  MLE.\\nSecondly,      in      practice,      it      might      be      preferable      to      utilize   \\n \\na      sub-trajectory      from      x;      to      x;_,      based  \\n \\n  on      the      following      expression:</li><p>UFZ  \\n \\n  log      pi_      pa      (Gee      |Vt—ng1)   \\n \\n+      +++   \\n \\n+      log      pi      (4-1      |a4)   \\n \\n+      (2)</p><h3>Corollary      1      (7-step      consistency).</h3><p>log      pf      (xo|21)   \\n \\n+      +++   \\n \\n+      log      pp(wr-a|er)   \\n \\n+      log      pp      (x7)      (35) r(x      re      re      re  \\n \\n  —      (=)   \\n \\n+      log      p?\\n*(xo|v1)      +--+   \\n \\n+      log      pr\"      (ar-i|er)   \\n \\n+      log      pe      (xr).</p><p>Proof.\\nWe      consider      the      marginal      distribution      with      respect      to      7,      +--+   \\n \\n,   \\n \\n%      induced      by      the      soft-optimal  \\n \\n  policy,      and      denote      it      I(vp,---      ,--+      ,%o)   \\n \\n€   \\n \\n¥      X--+   \\n \\nx      X.      We      have Uar)l(ar—1   \\n \\n|      Lr)      tee      I(xo|21)   \\n \\n=      U(x      )l(a4   \\n \\n|      Xo)      te      (ar   \\n \\n|      LT-1)      (36) From      Theorem   \\n \\n2      that      characterized      marginal      distributions,      and      Theorem   \\n \\n3      that      characterize      posterior  \\n \\n  distributions,      the      left      hand      side      of      (36)      is      equal      to *\\n \\n   *\\n \\n   *</p><p>Pr      (xr)   \\n \\nx      pp      (r|er-1)      X-++   \\n \\nx      pi      (Xo|@1) Marginal      distribution      atT      Optimal      policy      at      T   \\n \\n—   \\n \\n1      Optimal      policy      at   \\n \\n1 and      the      right-hand      side      in      (36)      is      equal      to exp(r      (xo)      /a)      pre      pre      pre  \\n \\n  aq      Po      (xo)   \\n \\nX      po      (a1   \\n \\n|      Zo)      X+++   \\n \\nX      prey      (@r   \\n \\n|      er-1)-.\\n \\n \\n|      ~~  \\n \\n  Marginal      distribution      at   \\n \\n0      Posterior      distribution      Posterior      distribution By      rearranging      the      term,      we      obtain      (35).\\nO</p><p> \\n \\n  Comparison      with      Gflownets.\\nIn      the      Gflownets      literature,      similar      losses      are      used.\\nFor      instance,  \\n \\n  the      loss      derived      from      (33)      or      (35)      is      commonly      known      as   \\n \\na      detailed      balance      loss      (Bengio      et      al.,  \\n \\n  2023)      or   \\n \\na      trajectory      loss      (Malkin      et      al.,      2022),      respectively.</p><p> \\n \\n  Note      in      general,      the      literature      in      Gflownets      primarily      focuses      on      sampling      from      unnormalized  \\n \\n  models      (distributions      proportional      to      exp(r(a))).\\nHence,      reference      policies      (i.e,      {p?\"\"}\\n \\n \\n)      or      latent  \\n \\n  states      (1.e.,      %7.;      before      x9)      are      introduced      without      relying      on      pre-trained      diffusion      models.\\nIn  \\n \\n  contrast,      in      our      context,      we      use      policies      derived      from      pre-trained      diffusion      models      as      reference  \\n \\n  policies,      leveraging      them      as      our      prior      knowledge.</p><p>7\\n \\n   Fine-Tuning      Settings      Taxonomy</p><p> \\n \\n  So      far,      we      implicitly      assume      we      have      access      to      reward      functions.\\nHowever,      these      functions      are      often  \\n \\n  unknown      and      need      to      be      learned      from      data.\\nWe      classify      several      settings      in      terms      of      whether      reward  \\n \\n  functions      are      available      or,      if      not,      how      they      could      be      learned.\\nThis      section      is      summarized      in      Figure      3.</p><h4>Direct      backpropagation      (Sec.      5.2)       _       Differentiable</h4><p>_\\n \\n   ——_      Non-differentiable PP      5.1</p><h4>Accurate      rewards      O      (Sec.      5.1)</h4><p> \\n \\n  feedback   \\n \\n;    \\n \\n—      Reward-weighted      MLE      (Sec.      6.1  \\n \\n  Xo      Unknown      reward      Memory      effi-      oe      eward-welgnte      (Sec.      6.1) unctions      —_\\\\\\\\\\\\\\\\_—      ciency</p><h4>No      fine-tuning      Computational</h4><p>efficiency —\\n \\n    NN      Direct      backpropagation      (Sec.      5.2)</p><h5>Value-weighted      sampling      (Sec.      6.2)</h5><p>Figure      3:      Practical      recommendation      of      RL-based      algorithms      to      optimize      downstream      reward  \\n \\n  functions.</p><h6>7.1      Fine-Tuning      with      Known,      Differentiable      Reward      Functions</h6><p>ions  \\n \\n  When      accurate      differentiable      reward      functions      are      available      (e.g.,      standard      in      computer      vision      tasks),  \\n \\n  our      primary      focus      is      typically      on      computational      and      memory      efficiency.\\nIn      general,      we      advocate      for  \\n \\n  employing      reward      backpropagation      (c.f.      Algorithm      2)      for      its      computational      efficiency.\\nAlternatively,  \\n \\n  if      memory      efficiency      is   \\n \\na      significant      concern,      we      suggest      using      PPO      due      to      its      stability.</p><h6>7.2      Fine-Tuning      with      Black-Box      Reward      Feedback</h6><p>back  \\n \\n  Here,      we      explore      scenarios      where      we      have      access      to      accurate      but      non-differentiable      (black-box)  \\n \\n  reward      feedback,      often      found      in      scientific      simulations.\\nThe      emphasis      still      remains      primarily      on  \\n \\n  computational      and      memory      efficiency.\\nIn      such      scenarios,      due      to      the      non-differentiability      of      reward  \\n \\n  feedback,      we      recommend      using      PPO      or      reward-weighted      MLE      for      the      following      reasons.</p><p> \\n \\n  In      the      preceding      section,      we      advocated      for      the      use      of      reward      backpropagation      due      to      its      com-  \\n \\n  putational      advantages.\\nHowever,      when      dealing      with      non-differentiable      feedback,      the      advantage  \\n \\n  of      reward      backpropagation      may      diminish.\\nThis      is      because      learning      from      such      feedback      requires  \\n \\n  learning      differentiable      reward      functions      to      make      the      algorithm      work      as      we      discuss      in      Section      5.2.  \\n \\n  However,      obtaining   \\n \\na      differentiable      reward      function      can      be      challenging      in      many      domains.\\nFor  \\n \\n  example,      in      molecular      property      prediction,      molecular      fingerprints      are      informative      features,      and       accurate      reward      functions      can      be      derived      by      training      simple      neural      networks      on      these      features  \\n \\n  (Pattanaik      and      Coley,      2020).\\nYet,      these      mappings,      grounded      in      prior      scientific      understanding,      are  \\n \\n  non-differentiable,      thereby      restricting      their      applicability.</p><p> \\n \\n  In      contrast,      PPO      and      reward-weighted      MLE      allow      for      policy      updates      without      the      explicit  \\n \\n  requirement      to      learn   \\n \\na      differentiable      reward      function,      offering   \\n \\na      significant      advantage      over      reward  \\n \\n  backpropagation.</p><h6>7.3      Fine-Tuning      with      Unknown      Rewards      Functions</h6><p>ions  \\n \\n  When      reward      functions      (or      computational      proxies      in      Section      7.2)      are      unavailable,      we      must      learn  \\n \\n  from      data      with      reward      feedback.\\nIn      such      cases,      compared      to      the      previous      scenarios,      two      important  \\n \\n  considerations      arise:</p><p>¢\\n \\n   Not      only      computational      or      memory      efficiency      but      also      feedback      efficiency      (i.e.,      sample  \\n \\n  efficiency      regarding      reward      feedback)      is      crucial.</p><p> \\n \\n \\n¢      Given      that      learned      reward      feedback      may      not      generalize      well      outside      the      training      data      distri-  \\n \\n  butions,      it      is      essential      to      constrain      fine-tuned      models      to      prevent      significant      divergence      from  \\n \\n  diffusion      models.\\nIn      these      situations,      not      only      soft      PPO      and      direct      backpropagation      but      also  \\n \\n  methods      discussed      in      Section      6,      such      as      reward-weighted      MLE,      can      be      particularly      effective.</p><p>Further,      the      scenario      involving      unknown      reward      functions      can      be      broadly      categorized      into      two      cases,  \\n \\n  which      we      will      discuss      in      more      detail.</p><p>Offline      scenario      (Figure      4).\\nIn      many      scenarios,      even  \\n \\n  in      cases      where      reward      functions      are      unknown,      we      of-  \\n \\n.     .</p><p>Offline      data      with ten      have      access      to      offline      data      with      reward      feedback      reward      feedback</p><li>(i)      (i)   \\n \\nO   \\n \\n;   \\n \\ni       a”      ra  \\n \\n.      ne      straightforward      approach      is      to</li><p> \\n \\n \\n{   \\n \\n,   \\n \\n(   \\n \\n)    \\n.\\n    .\\n \\n   g      PP  \\n \\n.      <=>      Learning      reward  \\n \\n  perform      regression      with      neural      networks      and      build   \\n \\na      models  \\n \\n  learned      regressor      *.\\nHowever,      this      learned      reward      func-      SS  \\n \\n  tion   \\n \\n*      might      not      generalize      well      beyond      the      distribution      x)  \\n \\n  of      the      offline      data.\\nIn      regions      outside      this      distribution,   \\n \\n7       could      assign   \\n \\na      high      value      even      when      the      actual      reward  \\n \\n  r      is      low      due      to      high      uncertainty.\\nConsequently,      we  \\n \\n  could      be      easily      misled      by      out-of-distribution      samples  \\n \\n  if      we      solely      optimize      7.\\n|Fine-tuning      diffusion      models   \\n \\n|       One      approach      to      alleviate      this      issue      is      to      adopt   \\n \\na       pessimistic      (i.e.,      conservative)      strategy,      as      proposed      in      Figure      4:      Offline      scenario  \\n \\n  Uehara      et      al.\\n(2024).\\nThis      technique      has      been      widely      Collecting      data]   \\n \\nY      Learning      reward  \\n \\n  applied      in      the      field      of      offline      RL      (Levine,      2018).\\nThe      aa  \\n \\n  main      idea      is      to      penalize   \\n \\n7      outside      the      distributions      us-  \\n \\n  ing      techniques      such      as      adding      an      explicit      penalty      term  \\n \\n  (Yu      et      al.,      2020;      Chang      et      al.,      2021),      bootstrapping,  \\n \\n  or      more      sophisticated      methods      (Kumar      et      al.,      2020;  \\n \\n  Xie      et      al.,      2021;      Rigter      et      al.,      2022;      Uehara      and      Sun,  \\n \\n  2021).\\nBy      doing      so,      we      can      avoid      being      fooled      by  \\n \\n  out-to-distribution      regions      while      still      benefiting      from  \\n \\n  the      extrapolation      capabilities      of      reward      models.</p><p>S—      diffusion      models      |</p><p>Figure      5:      Online      scenario</p><p> \\n \\n  Online      scenario      (Figure      5).\\nConsider      situations  \\n \\n  where      reward      functions      are      unknown,      but      we      can      gather      data      online.\\nThese      scenarios      are      often  \\n \\n  referred      to      as   \\n \\na      lab-in-the-loop      setting.</p><p> \\n \\n  In      such      scenarios,      Uehara      et      al.\\n(2024)      proposes      an      iterative      procedure      comprising      three      steps:</p><li>(1)      collecting      feedback      data      by      exploring      new      areas      (i.e.,      obtaining   \\n \\nx      from      current      diffusion      models  \\n \\n  and      corresponding      r(x)),      (2)      learning      the      reward      function      from      the      collected      feedback      data,      and</li><li>(3)      fine-tuning      current      diffusion      models      by      maximizing      the      learned      reward      function      enhanced      by  \\n \\n  an      optimistic      bonus      term,      using      algorithms      discussed      in      Section      4.2.\\nIn      contrast      to      standard      online  \\n \\n  PPO,      where      feedback      data      is      directly      used      in      the      fine-tuning      process,      actual      reward      feedback      is  \\n \\n  only      used      in      step      (2)      and      not      in      step      (3).\\nAn      additional      key      aspect      is      the      use      of      an      optimistic  \\n \\n  reward      function      that      encourages      exploration      beyond      the      distributions      of      the      current      diffusion      model.\\n \\n \\n  This      deliberate      separation      between      reward      learning      and      fine-tuning      steps,      coupled      with      the      use      of  \\n \\n  optimism,      significantly      enhances      feedback      efficiency.</li><p>8\\n \\n   Connection      with      Classifier      Guidance</p><p> \\n \\n  Classifier      guidance      (Dhariwal      and      Nichol,      2021)      is      commonly      used      for      conditional      generation      in  \\n \\n  diffusion      models.\\nInterestingly,      RL-based      methods      share   \\n \\na      close      connection      with      classifier      guidance.\\n \\n \\n  More      specifically,      in      this      section,      following      Zhao      et      al.\\n(2024),      we      elucidate      how      RL-based      methods  \\n \\n  can      be      applied      to      conditional      generation.\\nFurthermore,      from      this      unified      viewpoint,      we      highlight  \\n \\n  that      classifier      guidance      is      seen      as   \\n \\na      value-weighted      sampling      in      Section      4.2.</p><h6>8.1      Classfier      Guidance</h6><p>ance  \\n \\n  In      this      section,      we      first      introduce      classifier      guidance.\\nClassifier      guidance      utilizes      an      unconditional  \\n \\n  pre-trained      model      for      conditional      generation.\\nMore      specifically,      we      consider   \\n \\na      scenario      where      we  \\n \\n  have   \\n \\na      pre-trained      diffusion      model,      which      enables      us      to      sample      from      p?\\n\"*(x),      and      data      with      feedback  \\n \\n  {x,y},      where   \\n \\ny      represents      the      new      label      we      want      to      condition      on.\\nOur      objective      here      is      to      sample  \\n \\n  from      p(-|y)   \\n \\n€      [Y   \\n \\n>      ACR), P(-   \\n \\n|      y)   \\n \\nx      py(yl-)pP*(-).\\n(37)</p><p>where      p,   \\n \\n:   \\n \\nX      —>      A(Y)      denotes      the      conditional      distribution      of   \\n \\ny      given      x.</p><p>Motivation.\\nIn      classifier      guidance,      we      consider      the      following      policy:</p><p>pe      (-lare,      y)   \\n \\n=   \\n \\nN      (p(22,      t;      Opre)   \\n \\n+      a(t)      Ve      log      Vy      (Xt;      t),      a*(t))   \\n \\n’ dy(x,t)   \\n \\n=      Egy.\\nyap,      (-leo)      L(Y   \\n \\n=      y)|a,   \\n \\n=      2].</p><p>Under      the      continuous-time      formulation      of      diffusion      models,      applying      Doob’s      h-transform      (Rogers  \\n \\n  and      Williams,      2000),      it      is      shown      that      we      can      sample      from      p(x|y)      by      sequentially      running      {p7\"\\\\\\\\\\\\\\'(-|7+,      y)      }i-r41-</p><p>Algorithm.\\nSpecifically,      the      algorithm      comprises      two      steps.\\nIt’s      important      to      note      that      this      is      not  \\n \\n  a      fine-tuning      method      but      rather      an      inference-time      technique      as      we      freeze      the      pre-trained      model.</p><li>1. Learning      q,(x,t)      through      regression      by      gathering      data      containing      {z;,      y}.\\nHere,      for      each  \\n \\n  (Xo,      y),      using   \\n \\na      forward      policy      for      the      pre-training      (from   \\n \\n0      to      T),      we      obtain      2;      starting      from</li><p>Lo.</p><li>2. During      inference,      sequentially      execute      {p2\"(-|a,,      y)      }1_>   \\n \\n1      by      leverage      the      fact      that      compared</li><p>gui to      the      policy      in      pre-trained      diffusion      models,      the      mean      of      {p?\\n\"\\\\\\\\\\\\\\'(-|71,      y)      }i-74,      is      just      shifted  \\n \\n  with      an      additional      term      V,,      log      q,(2,      t).</p><p> \\n \\n  Notably,      the      method      described      above      can      be      reduced      to      value-weighted      sampling      (cf.      Sec-  \\n \\n  tion      6.2),      when      r(x,      y)   \\n \\n=      log      p(y|x).\\nInspired      by      such      an      observation,      we      apply      an      RL-based  \\n \\n  fine-tuning      method      for      conditional      generation      as      below.</p><h7>8.2.      RL-Based      Fine-Tuning      for      Conditional      Generation</h7><p>tion  \\n \\n  In      this      section,      following      Zhao      et      al.\\n(2024),      we      explain      an      RL-based      fine-tuning      approach      for  \\n \\n  conditional      generation.\\nThe      core      idea      is      that      setting      r(x,      y)   \\n \\n=      log      p(y|x)      allows      us      to      sample      from  \\n \\n  the      target      conditional      distribution      (37),      drawing      on      the      insights      discussed      in      Section      4.2.  \\n \\n  More      precisely,      introducing   \\n \\na      distribution      over   \\n \\ny      as      q(-),      we      formulate      the      following      RL      problem:</p><p>{Di      ft   \\n \\n=      argmax      Egy,      (-\\\\\\\\\\\\\\\\-y)}.yott      log      p(y|xo)   \\n \\n—      NeopyaKL(pe(-|e,      y)      ||P      (late).\\n \\n \\n  {prE[(V,.X)PA(A)]      Heyy (38)</p><p> \\n \\n  Compared      to      the      objective      function      (18)      in      Section      4.2,      the      main      differences      are:      (1)      policy      space  \\n \\n  is      expanded      to      accommodate      additional      control      (i.e.,      y):      pz(-|-,-)      C      [R¢   \\n \\nx   \\n \\nY   \\n \\n+      A(R®)],      (2)      the  \\n \\n  fine-tuning      objectives      are      set      as      log-likelihoods:      log      p(y|z).\\nThen,      following      Theorem      1,      we      can  \\n \\n  establish      the      following      result.</p><p>Corollary      2.\\nThe      distribution      induced      by      {p*}1_74      (ue.   \\n \\n[      {Teri      Ph      (ae-1   \\n \\n|      te,      y)}deur      is      the target      conditional      distribution      (37).</p><p> \\n \\n  The      above      corollary      suggests      that      for      conditional      generation,      we      can      utilize      various      off-the-shelf  \\n \\n  algorithms      discussed      in      Section      4.2      and      6,      such      as      PPO,      reward      backpropagation,      and      reward-  \\n \\n  weighted      MLE,      as      well      as      value-weighted      sampling.</p><p>9\\n \\n   Connection      with      Flow-Based      Diffusion      Models</p><p> \\n \\n  We      elucidate      the      close      relationship      between      bridge      matching      and      fine-tuning,      as      summarized      in  \\n \\n  Table      2.\\nBridge      (flow)      matching      (Liu      et      al.,      2022;      Tong      et      al.,      2023;      Lipman      et      al.,      2023;      Liu      et      al.,  \\n \\n  2022;      Shi      et      al.,      2024;      Albergo      and      Vanden-Eijnden,      2022),      has      recently      gained      popularity      as   \\n \\na       distinct      type      of      diffusion      model      from      score-matching-based      ones.\\nThese      works      suggest      that      training  \\n \\n  and      inference      speed      will      be      accelerated      due      to      the      ability      to      define   \\n \\na      more      direct      trajectory      from  \\n \\n  noise      to      data      distribution      than      score-matching-based      diffusion      models.</p><p> \\n \\n  The      connection      between      bridge      matching      and      fine-tuning      becomes      evident      in      the      continuous-  \\n \\n  time      formulation.\\nTo      begin,      we      first      provide   \\n \\na      brief      overview      of      the      continuous-time      formulation      of  \\n \\n  score-matching-based      diffusion      models.</p><p> \\n \\n  Table      2:      Comparison      between      training      score/flow-based      diffusion      models      and      fine-tuning.\\nWe  \\n \\n  optimize      the      parameter   \\n \\n6      in      P’,      which      represents      the      induced      distribution      over      trajectories      associated  \\n \\n  with      6.</p><h8>Loss      Note      of      Q</h8><p>Score-based      KL(Q\\\\\\\\\\\\\\'™¢|P?)      Qtime:      Time-reversal      of      reference      SDE      (reference training      SDE      is      defined      from   \\n \\n0      to      T’      where      the      initial      dis-  \\n \\n  tribute      at   \\n \\n0      is      data      distribution)</p><p> \\n \\n  Flow-based      KL(Q*°>      ||P?)      @Q*°°h:      Coupling      between      the      reference      SDE      from  \\n \\n  training      T      to   \\n \\n0      conditioning      at   \\n \\n0      (dist.      over      x.;|7o)      and    \\n \\na      data      distribution      at      time   \\n \\n0      (dis.      over      x).\\nNote  \\n \\n  that      reference      SDE      is      defined      from      7’      to      0.</p><p> \\n \\n  Fine-tuning      KL(P?||Q*°)      @*:      Coupling      between      the      pre-trained      SDE  \\n \\n  from      T      to   \\n \\n0      conditioned      on   \\n \\n0      (dist.      over      x7.1|X0)  \\n \\n  and   \\n \\na      data      distribution      at      time   \\n \\n0      (dis.      over      Xo) 9.1      Continuous      Time      Formulation      of      Score-Based      diffusion      models</p><p> \\n \\n  We      provide   \\n \\na      brief      overview      of      the      continuous-time      formulation      of      diffusion      models      (Song      et      al.,  \\n \\n  2021),      further      expanding      Section      1.1.1.\\nIn      this      formulation,      we      initially      define      the      forward      reference  \\n \\n  SDE,      spanning      from      time   \\n \\n0      to      7’      (e.g.,      variance      exploding      (VE)      process      or      variance      preserving      (VP)  \\n \\n  process),      by      setting      an      initial      distribution      at   \\n \\n0      as      the      data      distribution.\\nAdditionally,      the      reference  \\n \\n  SDE      is      tailored      to      converge      to      an      easy      distribution      at      time      7’,      such      as      the      normal      distribution.</p><p> \\n \\n  Subsequently,      we      consider      the      time-reversal      SDE      (Anderson,      1982),      progressing      from      T\\\\\\\\\\\\\\'      to      0.\\nIf  \\n \\n  we      could      learn      this      time-reversal      SDE,      it      would      enable      us      to      sample      from      the      data      distribution      by  \\n \\n  starting      with      the      easy      initial      distribution      (at      7’)      and      following      the      time-reversal      SDE      from      T      to   \\n \\n0      at  \\n \\n  inference      time.</p><p> \\n \\n  Training.\\nNow,      the      remaining      question      is      how      to      learn      this      time-reversal      SDE.\\nHere,      by      repre-  \\n \\n  senting      the      induced      distribution      over      trajectories      from      T      to   \\n \\n0      with      this      time-reversal      SDE      as      Q\\\\\\\\\\\\\\'™®,  \\n \\n  our      goal      is      to      train   \\n \\na      new      SDE,      parameterized      by   \\n \\n@      in      the      neural      network,      such      that      the      induced  \\n \\n  distribution      P®      over      trajectories      associated      with   \\n \\n6      aligns      with      Q\"™*.\\nSpecifically,      the      objective      is      to minimize      the      following      losses:</p><p> \\n \\n  argmin      KL(Q\"™||P*).\\n(39)  \\n \\n  0  \\n \\n  With      some      algebra,      leveraging      the      observation      that      the      time-reversal      SDE      comprises      the      original  \\n \\n  forward      SDE      and      the      scoring      term      of      the      marginal      distribution      (Anderson,      1982)      as      in      (2),      Song  \\n \\n  et      al.\\n(2021)      demonstrates      that      this      KL      loss      (39)      is      approximated      as Dore   \\n \\n=      argmin      Eeuni((0,7))01~qyo0(weleo)co~pP\"e      Fe      Fl      Vey      log      jo      (Xt   \\n \\n|      Xo)   \\n \\n_      so(Xt,      t)|I?\\n],      (40)  \\n \\n  which      is      equal      to      (4)      where      the      weight      A(t)      is      {o?\\n}?\\n(for      notation,      refer      to      Section      1.1.1).</p><h9>9.2      Flow-Based      (Bridge-Based)      Diffusion      Models</h9><p>dels We      adopt      the      explanation      outlined      in      Liu      et      al.\\n(2022).\\nIn      this      framework,      we      begin      by      considering  \\n \\n  a      reference      SDE      (from      T\\\\\\\\\\\\\\'      to      0),      x;   \\n \\n=      zr_;      where      z;      is      defined      as      Brownian      motion:</p><p>dz,   \\n \\n=      dwt,      te      (0,      7).\\n(41)</p><p> \\n \\n  Subsequently,      we      consider      the      coupling      of      the      reference      SDE      conditioned      on      state      at      time   \\n \\n0      (i.e.,  \\n \\n  distribution      of      xo.7      conditional      on      x)      and      the      data      distribution      at      time      0,      denoting      the      resulting  \\n \\n  distribution      as      Q*°?,      Informally,      this      Q*°°?\\nis      characterized      as      follows.</p><p>Informal      characterization      of      Q*”.\\nWhen      discretizing      the      time      step,      by      denoting      the      distribution  \\n \\n  induced      by      the      reference      SDE      as      p\"(x7,--+   \\n \\n,      29),      we      define QF      (a7,      tae   \\n \\n,      20)   \\n \\n=      p(x,      ay   \\n \\n|      r9)pP\"°      (x9).</p><p>The      distribution      Q*”      above      is   \\n \\na      limiting      distribution      obtained      by      making      the      discretization      step small.</p><p>Training.\\nIn      bridge-based      diffusion      models,      our      objective      is      to      train   \\n \\na      new      SDE,      parametrized      by  \\n \\n  6      (e.g.,   \\n \\na      neural      network),      to      align      the      induced      distribution      P®      with      Q*°?.\\nSpecifically,      bridge-based training      aims      to      minimize aremin      KL(Q*>||P’).\\n(42)  \\n \\n  0  \\n \\n  While      the      method      to      minimize      the      above      is      currently      unclear,      we      can      derive   \\n \\na      trainable      objective  \\n \\n  through      algebraic      manipulation.\\nInitially,      we      observe      that      the      reference      SDE      (41),      conditioned      on  \\n \\n  state      xo      at      time   \\n \\n0      (zr   \\n \\n=      D),      is      expressed      as:</p><p>b\\n \\n \\n  b—      2;</p><p>t   \\n \\n€      [0,T];dz?\\n \\n \\n=      g?(z?,t)dt      +du,,  \\n \\n  g(z?,t)   \\n \\n=   \\n \\na       (43)</p><p>using      the      seminal      “Doob’s      h-transform”      (Rogers      and      Williams,      2000).\\nThis      SDE      is      commonly  \\n \\n  known      as      the      Brownian      bridge.\\nNow,      we      consider   \\n \\na      new      SDE      parameterized      by      0:</p><p>t   \\n \\n€      [0,7];      dz   \\n \\n=      s(z,t;      @)dt   \\n \\n+      dw.\\n(44)</p><p>Then,      we      can      demonstrate      that      minimizing      the      KL      loss      in      (42)      is      equivalent      to argmin      E,W      unil0,7],1~4\"°      cop?\\n[Ilg”      (2,      t)   \\n \\n_      s(21,      t;      0)      M3]      (45)</p><p>where      q;°      denotes      the      induced      distribution      at      time      t      following      SDE      (43)      conditioned      on      xo,      and       s:      Xx      |0,T]   \\n \\n>   \\n \\n&      is      the      model      we      aim      to      optimize.</p><p> \\n \\n  Flow-based      diffusion      models.\\nThe      aforementioned      formulation      has      been      originally      proposed      as   \\n \\na       bridge-based      SDE      (Liu      et      al.,      2022)      because      the      conditional      SDE      is      often      referred      to      as   \\n \\na      Brownian  \\n \\n  bridge.\\nFlow-based      diffusion      models      proposed      in      Lipman      et      al.\\n(2023);      Tong      et      al.\\n(2023)      share   \\n \\na       close      relationship      with      bridge-based      diffusion      models.\\nThese      flow-based      models      use   \\n \\na      target      SDE (43)      and      an      SDE      with   \\n \\na      parameter      (44)      without      stochastic      terms      (1.e.,      no      dw,)      and      aim      to      minimize  \\n \\n  the      loss      function      (45).</p><p> \\n \\n  Comparison      with      score-based      diffusion      models.\\nBoth      score-based      diffusion      models      and      bridge-  \\n \\n  based      diffusion      models      aim      to      minimize      KL      divergences.\\nDespite      its      similarity,      in      contrast      to  \\n \\n  score-based      diffusion      models,      bridge-based      diffusion      models      target      Q*°>      rather      than      the      time-  \\n \\n  reversal      SDE      Q\"™®.\\nThe      distribution      Q*”      is      often      considered      preferable      because      it      can      impose      an  \\n \\n  SDE      that      navigates      efficiently      from   \\n \\n0      to      7’      (e.g.,   \\n \\na      Brownian      bridge      with      minimal      noise).\\nThis      can  \\n \\n  expedite      the      learning      process      since      we      lack      direct      control      over      the      time-reversal      SDE      in      score-based  \\n \\n  diffusion      models.</p><h10>9.3.      Connection      with      RL-Based      Fine-Tuning</h10><p>ning  \\n \\n  RL-based      fine-tuning      shares   \\n \\na      close      relationship      with      flow-based      training.\\nTo      illustrate      this,      consider  \\n \\n  the      reference      SDE      (from      7’      to      0)      following      the      pre-trained      diffusion      model.\\nThen,      akin      to      flow-  \\n \\n  based      training,      we      consider      the      coupling      of      the      reference      SDE      conditioned      on      state      at      time   \\n \\n0       (i.e.,      distribution      over      xo-r      conditioned      on      x9)      and      the      target      distribution      at      time   \\n \\n0      (i.e.,      p,(-)   \\n \\n«       exp(r(-)/a)p?\\n\"°(-)),      denoting      the      induced      distribution      as      Q*™.</p><p>Informal      characterization      of      Q™°*.\\nInformally,      Q*\"°      is      introduced      as      follows.\\nWhen      discretizing  \\n \\n  the      time      step,      by      denoting      the      distribution      induced      by      the      reference      SDE      as      p™\\\\\\\\\\\\\\'(xp,-+-   \\n \\n,      9),      we define Qi\"      (xr,   \\n \\na   \\n \\n,      0)   \\n \\n=      p(x,      nr      Ly   \\n \\n|      L0)Pr      (Xo).</p><p>The      distribution      Q*°      above      is   \\n \\na      limiting      distribution      obtained      by      making      the      discretization      step small.</p><p>Training.\\nNow,      we      introduce   \\n \\na      new      SDE      parameterized      by   \\n \\n@      such      as      an      neural      network      to      align  \\n \\n  the      induced      distribution      P’      with      Q®\"*.\\nActually,      in      the      continuous      formulation,      the      RL-problem      (18) in      Section      4.2      is      equal      to      solving argmin      KL(P?||Q\"™).\\n \\n \\n  0 This      formalization      has      been      presented      in      Uehara      et      al.\\n(2024).\\nIntuitively,      as      we      see      in      Theorem      3,  \\n \\n  this      is      expected      because      we      see      that      the      posterior      distribution      induced      by      pre-trained      models:</p><p>ref   \\n \\n(      PD      (ar,-++      21   \\n \\n|      to)      (=      vey      (er|er_-1)p      Pp      (ar-1   \\n \\n|      er_2)      +++      pp\"      (x1   \\n \\n|      20), remains      preserved      after      fine-tuning      based      on      the      RL-formulation      (18).</p><p> \\n \\n  Comparison      with      flow-based      training.\\nIn      contrast      to      flow-based      training,      RL-based      fine-tuning  \\n \\n  minimizes      the      inverse      KL      divergence      rather      than      the      KL      divergence.\\nIntuitively,      this      is      because,  \\n \\n  unlike      Qt™*      or      Q*>   \\n \\n,      we      cannot      sample      from      Q*”      (recall      that   \\n \\na      marginal      distribution      at      time   \\n \\n0      in  \\n \\n  Qi,      i.e.,      p,,      is      unnormalized);      on      the      other      hand,      the      marginal      distributions      at      time   \\n \\n0      in      both      Q\\\\\\\\\\\\\\'™*  \\n \\n  and      Q*°°>      are      data      distributions.</p><p> \\n \\n  Remark   \\n \\n7      (Extension      to      f-divergence).\\nEach      of      these      methods      can      be      technically      extended      by  \\n \\n  incorporating      more      general      divergences      beyond      KL      divergence.\\nFor      instance,      in      the      context      of  \\n \\n  fine-tuning,      please      see      Tang      (2024).</p><p>10      Connection      with      Sampling      from      Unnormalized      Distributions</p><p> \\n \\n  Numerous      studies      delve      into      sampling      from      an      unnormalized      distribution      proportional      to      exp(r()),  \\n \\n  commonly      known      as      the      Gibbs      distribution,      extensively      discussed      in      the      computational      statistics  \\n \\n  and      statistical      physics      community      (Robert,      2014).\\nThis      issue      is      pertinent      to      our      work,      as      during  \\n \\n  fine-tuning,      the      target      distribution      is      also      formulated      in      the      Gibbs      distribution      form      in      (10),      which      is  \\n \\n  proportional      to      exp(r(a))p?\\n\"*      (x).</p><p> \\n \\n  The      literature      employs      two      main      approaches      to      tackle      this      challenge:      Markov      Chain      Monte  \\n \\n  Carlo      (MCMC)      and      RL-based      methods.\\nIn      particular,      the      RL-based      diffusion      model      fine-tuning  \\n \\n  we      have      discussed      so      far      is      closely      related      to      the      latter      approach.\\nIn      this      section,      we      explain      and       compare      these      two      approaches.</p><h10>10.1      Markov      Chain      Monte      Carlo      (MCMC)</h10><p>CMC)  \\n \\n  In      MCMC,   \\n \\na      Markov      chain      is      constructed      to      approximate   \\n \\na      target      distribution      such      that      the      equilib-  \\n \\n  rium      distribution      of      the      Markov      chain      converges      to      the      target      distribution.\\nHowever,      dealing      with  \\n \\n  high-dimensional      domain      spaces      is      challenging      for      MCMC.\\n \\n \\nA      common      strategy      involves      leveraging  \\n \\n  gradient      information,      such      as      the      Metropolis-adjusted      Langevin      algorithm      (MALA)      (Besag,      1994)  \\n \\n  or      Hamiltonian      Monte      Carlo      (HMC)      (Neal      et      al.,      2011;      Girolami      and      Calderhead,      2011).\\nMALA  \\n \\n  shares      similarities      with      classifier      guidance      and      value-weighted      sampling,      as      both      methods      depend  \\n \\n  on      the      first-order      information      from      reward      (or      value)      functions.</p><p> \\n \\n  Can      we      use      MCMC      for      fine-tuning      diffusion      models?\\nIn      the      context      of      fine-tuning      dif-  \\n \\n  fusion      models,      while      it      may      seem      intuitive      to      sample      from      the      target      distribution      (10)      (i.e.,  \\n \\n  Dy   \\n \\n&      exp(r(x))pP\"*(x))      using      MCMC,      it’s      not      straightforward      for      the      following      reasons:</p><p>¢\\n \\n   We      lack      of      an      analytical      form      of      p?\\n\"*(-).<ul><li>*\\n \\n   Additionally,      even      if      we      can      estimate      p?\\n’°(-)      in      an      unbiased      manner,      as      in      Chen      et      al.\\n(2018),  \\n \\n  the      mixing      time      for      obtaining   \\n \\na      single      sample      in      MCMC      might      be      lengthy.</li></ul></p><p> \\n \\n  Therefore,      we      explore      creating   \\n \\na      generative      model      (i.e.,      diffusion      model)      for      this      task      using      RL-  \\n \\n  based      fine-tuning      so      that      we      can      easily      sample      from      p,(-)      during      inference      (i.e.,      simulating      the  \\n \\n  time-reversal      SDE      (2)      with      policies      from   \\n \\n¢   \\n \\n=      T\\\\\\\\\\\\\\'+   \\n \\n1      to      t   \\n \\n=      1),      without      relying      on      MCMC.\\nThis  \\n \\n  addresses      the      concern      of      MCMC      by      avoiding      the      need      to      estimate      p?\\n\"*(-)      and      minimizing      inference  \\n \\n  time,      albeit      at      the      expense      of      increased      training      time.</p><p>10.2}      RL-Based      Approaches  \\n \\n  While      MCMC      has      historically      been      widely      used      for      this      purpose,      recent      works      have      proposed      an  \\n \\n  RL-based      approach      or      its      variant      (e.g.,      Bernton      et      al.      (2019);      Heng      et      al.\\n(2020);      Huang      et      al.\\n(2023);</p><p> \\n \\n  Vargas      et      al.\\n(2023)).\\nFor      example,   \\n \\na      seminal      study      (Zhang      and      Chen,      2021)      introduced   \\n \\na      method  \\n \\n  very      similar      to      reward      backpropagation.\\nIn      their      work,      they      initially      define      Brownian      motion      as   \\n \\na       reference      SDE      (from      7’      to      0),      similar      to      the      pre-trained      diffusion      models      in      our      context.\\nThen,      by  \\n \\n  appropriately      setting      rewards,      Zhang      and      Chen      (2021)      aims      to      learn   \\n \\na      new      SDE      (from      T\\\\\\\\\\\\\\'      to      0)      such  \\n \\n  that      we      can      sample      from      the      target      distribution      at      the      endpoint      0.</p><p> \\n \\n  These      approaches      offer      advantages      over      MCMC      due      to      their      ability      to      minimize      inference      time  \\n \\n  (amortization)      by      constructing   \\n \\na      generative      model      comprising      policies      in      the      training      time      and      solely  \\n \\n  executing      learned      policies      during      inference.\\nUnlike      MCMC,      these      methods      significantly      reduce      the  \\n \\n  inference      cost      as      we      don’t      need      to      be      concerned      about      the      mixing      time.\\nAdditionally,      RL-based  \\n \\n  approaches      have      the      potential      to      effectively      handle      high-dimensional,      complex      (i.e.,      multi-modal)  \\n \\n  distributions      by      harnessing      the      considerable      expressive      power      arising      from      latent      states.</p><p> \\n \\n  Despite      these      advantages,      it      is      often      unclear      how      to      select   \\n \\na      reference      SDE      in      these      works.\\nIn  \\n \\n  contrast,      in      RL-based      fine-tuning      of      diffusion      models,      we      directly      leverage      pre-trained      diffusion  \\n \\n  models      as      the      reference      SDEs,      i.e.,      as      our      prior      knowledge.</p><p> \\n \\n  Key      message.\\nFinally,      it      is      worth      noting      that      we      can      technically      utilize      any      off-the-shelf      RL      algo-  \\n \\n  rithms      for      training.\\nFor      example,      Zhang      and      Chen      (2021)      employs      reward      backpropagation,      while  \\n \\n  works      in      Gflownets      typically      leverage      PCL,      as      we      mention      in      Section      6.3.\\nHowever,      additionally,      it  \\n \\n  is      possible      to      use      PPO      or      reward-weighted      MLE      mentioned      in      Section      4.2.</p><p>11      Closely      Related      Directions</p><p>Lastly,      we      highlight      several      closely      related      directions      we      have      not      yet      discussed.</p><p> \\n \\n  Aligning      text-to-image      models.\\nWe      recognize      that      current      efforts      in      fine-tuning      diffusion      models  \\n \\n  primarily      revolve      around      aligning      text-to-image      models      using      human      feedback      (Lee      et      al.,      2023;  \\n \\n  Wallace      et      al.,      2023;      Wu      et      al.,      2023;      Yang      et      al.,      2023).\\nIn      this      paper,      our      goal      is      to      consider   \\n \\na      more  \\n \\n  general      and      fundamental      formulation      that      is      not      tailored      to      any      particular      task.</p><p> \\n \\n  Diffusion      models      for      RL.\\nDiffusion      models      are      well-suited      for      specific      reinforcement      learning  \\n \\n  applications      due      to      their      ability      to      model      complex      and      multimodal      distributions      as      polices      (Janner  \\n \\n  et      al.,      2022;      Ajay      et      al.,      2023;      Wang      et      al.,      2022;      Hansen-Estruch      et      al.,      2023;      Du      et      al.,      2024;      Zhu  \\n \\n  et      al.,      2023).\\nFor      example,      Wang      et      al.\\n(2022)      use      conditional      diffusion      models      as      policies      and       demonstrate      their      effectiveness      in      typical      offline      RL      benchmarks.\\nWhile      RL-based      fine-tuning      also  \\n \\n  aims      to      maximize      certain      reward      functions,      the      primary      focus      of      this      tutorial      covers      methods      that  \\n \\n  leverage      pre-trained      diffusion      models      for      this      purpose.</p><p> \\n \\n  RL      from      human      feedback      (RLHF)      for      language      models.\\nRLHF      is      widely      discussed      in      the  \\n \\n  context      of      language      models,      where      many      algorithms      share      similarities      with      those      used      in      diffusion  \\n \\n  models      (Ouyang      et      al.,      2022;      Bai      et      al.,      2022;      Casper      et      al.,      2023).\\nFor      instance,      both      domains  \\n \\n  commonly      employ      PPO      as   \\n \\na      standard      approach,      and      reward-weighted      training      (or      reward-weighted  \\n \\n  MLE)      often      serves      as   \\n \\na      basic      baseline      approach.\\nDespite      these      similarities,      nuanced      differences  \\n \\n  exist      mainly      because      diffusion      models      typically      operate      in   \\n \\na      continuous      input      space      rather      than  \\n \\n  a      discrete      one.\\nMoreover,      techniques      like      value-weighted      sampling      emerge      uniquely      within      the  \\n \\n  context      of      diffusion      models.</p><p>12      Summary</p><p> \\n \\n  In      this      article,      we      comprehensively      explain      how      fine-tuning      diffusion      models      to      maximize      down-  \\n \\n  stream      reward      functions      can      be      formalized      as   \\n \\na      reinforcement      learning      (RL)      problem      in      entropy-  \\n \\n  regularized      Markov      decision      processes      (MDPs).\\nBased      on      this      viewpoint,      we      elaborate      on      the  \\n \\n  application      of      various      RL      algorithms,      such      as      PPO      and      reward-weighted      MLE,      specifically      tailored  \\n \\n  for      fine-tuning      diffusion      models.\\nAdditionally,      we      categorize      different      scenarios      based      on      how  \\n \\n  reward      feedback      is      obtained.\\nWhile      this      categorization      is      not      always      explicitly      mentioned      in      many  \\n \\n  existing      works,      it      is      crucial      when      selecting      appropriate      algorithms.\\nFinally,      we      discuss      the      rela-  \\n \\n  tionships      with      several      related      topics,      including      classifier      guidance,      Gflownets,      path      integral      control  \\n \\n  theory,      and      sampling      from      unnormalized      distributions.</p><h4>References</h4><p>Agarwal,      A.,      N.      Jiang,      S.      M.      Kakade,      and      W.      Sun      (2019).\\nReinforcement      learning:      Theory      and       algorithms.\\nCS      Dept.,      UW      Seattle,      Seattle,      WA,      USA,      Tech.\\nRep,      10-4.</p><p>Agarwal,      V.      and      D.      R.      Kelley      (2022).\\nThe      genetic      and      biochemical      determinants      of      mrna      degradation  \\n \\n  rates      in      mammals.\\nGenome      biology      23(1),      245.</p><p> \\n \\n  Ajay,      A.,      Y.      Du,      A.      Gupta,      J.      B.      Tenenbaum,      T.      S.      Jaakkola,      and      P.      Agrawal      (2023).\\nIs      conditional  \\n \\n  generative      modeling      all      you      need      for      decision      making?\\nIn      The      Eleventh      International      Conference  \\n \\n  on      Learning      Representations.</p><p>Albergo,      M.      S.      and      E.      Vanden-Eijnden      (2022).\\nBuilding      normalizing      flows      with      stochastic      inter-  \\n \\n  polants.\\narXiv      preprint      arXiv:2209.15571.</p><p>Anderson,      B.      D.      (1982).\\nReverse-time      diffusion      equation      models.\\nStochastic      Processes      and      their  \\n \\n  Applications      12(3),      313-326.</p><p> \\n \\n  Austin,      J.,      D.      D.      Johnson,      J.      Ho,      D.      Tarlow,      and      R.      Van      Den      Berg      (2021).\\nStructured      denoising  \\n \\n  diffusion      models      in      discrete      state-spaces.\\nAdvances      in      Neural      Information      Processing      Systems      34,  \\n \\n  17981-17993.</p><p>Avdeyey,      P.,      C.      Shi,      Y.      Tan,      K.      Dudnyk,      and      J.      Zhou      (2023).\\nDirichlet      diffusion      score      model      for  \\n \\n  biological      sequence      generation.\\narXiv      preprint      arXiv:2305.\\n10699.</p><p>Bai,      Y.,      S.      Kadavath,      S.      Kundu,      A.      Askell,      J.      Kernion,      A.      Jones,      A.      Chen,      A.      Goldie,      A.      Mirhoseini, C.      McKinnon,      et      al.\\n(2022).\\nConstitutional      ai:      Harmlessness      from      ai      feedback.\\narXiv      preprint  \\n \\n  arXiv:2212.08073.</p><p> \\n \\n  Bansal,      A.,      H.-M.\\nChu,      A.      Schwarzschild,      S.      Sengupta,      M.      Goldblum,      J.      Geiping,      and      T.      Goldstein  \\n \\n  (2023).\\nUniversal      guidance      for      diffusion      models.\\nIn      Proceedings      of      the      IEEE/CVF      Conference  \\n \\n  on      Computer      Vision      and      Pattern      Recognition,      pp.\\n843-852.</p><p>Bengio,      Y.,      S.      Lahlou,      T.      Deleu,      E.      J.      Hu,      M.      Tiwari,      and      E.      Bengio      (2023).\\nGflownet      foundations.\\n \\n \\n  Journal      of      Machine      Learning      Research      24(210),      1-55.</p><p> \\n \\n  Benton,      J.,      Y.      Shi,      V.      De      Bortoli,      G.      Deligiannidis,      and      A.      Doucet      (2024).\\nFrom      denoising      diffusions  \\n \\n  to      denoising      markov      models.\\nJournal      of      the      Royal      Statistical      Society      Series      B:      Statistical  \\n \\n  Methodology      86(2),      286-301.</p><p>Bernton,      E.,      J.      Heng,      A.      Doucet,      and      P.      E.      Jacob      (2019).\\nSchr\\\\\\\\\\\\\\\\””      odinger      bridge      samplers.\\narXiv  \\n \\n  preprint      arXiv:      1912.13170.</p><p>Besag,      J.\\n(1994).\\nComments      on      “representations      of      knowledge      in      complex      systems”      by      u.      grenander  \\n \\n  and      mi      miller.\\nJ.      Roy.\\nStatist.\\nSoc.      Ser.\\n \\n \\nB      56(591-592),      4.</p><p>Black,      K.,      M.      Janner,      Y.      Du,      I.      Kostrikov,      and      S.      Levine      (2023).\\nTraining      diffusion      models      with  \\n \\n  reinforcement      learning.\\narXiv      preprint      arXiv:2305.13301.</p><p> \\n \\n  Campbell,      A.,      J.      Benton,      V.      De      Bortoli,      T.      Rainforth,      G.      Deligiannidis,      and      A.      Doucet      (2022).\\n \\n \\nA       continuous      time      framework      for      discrete      denoising      models.\\nAdvances      in      Neural      Information  \\n \\n  Processing      Systems      35,      28266-28279.</p><p> \\n \\n  Campbell,      A.,      J.      Yim,      R.      Barzilay,      T.      Rainforth,      and      T.      Jaakkola      (2024).\\nGenerative      flows      on  \\n \\n  discrete      state-spaces:      Enabling      multimodal      flows      with      applications      to      protein      co-design.\\narXiv  \\n \\n  preprint      arXiv:2402.04997.</p><p>Cao,      H.,      C.      Tan,      Z.      Gao,      Y.      Xu,      G.      Chen,      P.-A.\\nHeng,      and      S.      Z.      Li      (2024).\\n \\n \\nA      survey      on      generative  \\n \\n  diffusion      models.\\n[EEE      Transactions      on      Knowledge      and      Data      Engineering.</p><p>Casper,      S.,      X.      Davies,      C.      Shi,      T.      K.      Gilbert,      J.      Scheurer,      J.      Rando,      R.      Freedman,      T.      Korbak, D.      Lindner,   \\n \\nP P.      Freire,      et      al.\\n(2023).\\nOpen      problems      and      fundamental      limitations      of      reinforcement  \\n \\n  learning      from      human      feedback.\\narXiv      preprint      arXiv:2307.15217.</p><p>Castillo-Hair,      S.      M.      and      G.      Seelig      (2021).\\nMachine      learning      for      designing      next-generation      mrna  \\n \\n  therapeutics.\\nAccounts      of      Chemical      Research      55(1),      24-34.</p><p>Chang,      J.      D.,      M.      Uehara,      D.      Sreenivas,      R.      Kidambi,      and      W.      Sun      (2021).\\nMitigating      covariate      shift  \\n \\n  in      imitation      learning      via      offline      data      without      great      coverage.\\narXiv      preprint      arXiv:2106.03207.</p><p>Chen,      M.,      S.      Mei,      J.      Fan,      and      M.      Wang      (2024).\\nAn      overview      of      diffusion      models:      Applications,  \\n \\n  guided      generation,      statistical      rates      and      optimization.\\narXiv      preprint      arXiv:2404.07771.</p><p>Chen,      R.      T.,      Y.      Rubanova,      J.      Bettencourt,      and      D.      K.      Duvenaud      (2018).\\nNeural      ordinary      differential  \\n \\n  equations.\\nAdvances      in      neural      information      processing      systems      31.</p><p>Chung,      H.,      J.      Kim,      M.      T.      Mccann,      M.      L.      Klasky,      and      J.      C.      Ye      (2022).\\nDiffusion      posterior      sampling  \\n \\n  for      general      noisy      inverse      problems.\\narXiv      preprint      arXiv:2209.\\n14687.</p><p> \\n \\n  Chung,      H.,      B.      Sim,      D.      Ryu,      and      J.      C.      Ye      (2022).\\nImproving      diffusion      models      for      inverse      problems  \\n \\n  using      manifold      constraints.\\nAdvances      in      Neural      Information      Processing      Systems      35,      25683-—  \\n \\n  25696.</p><p>Clark,      K.,      P.      Vicol,      K.      Swersky,      and      D.      J.      Fleet      (2023).\\nDirectly      fine-tuning      diffusion      models      on  \\n \\n  differentiable      rewards.\\narXiv      preprint      arXiv:2309.\\n17400.</p><p> \\n \\n  De      Bortoli,      V.,      E.      Mathieu,      M.      Hutchinson,      J.      Thornton,      Y.      W.      Teh,      and      A.      Doucet      (2022).\\nRieman-  \\n \\n  nian      score-based      generative      modelling.\\nAdvances      in      Neural      Information      Processing      Systems      35,  \\n \\n  2406-2422.</p><p>Deleu,      T.,      P.      Nouri,      N.      Malkin,      D.      Precup,      and      Y.      Bengio      (2024).\\nDiscrete      probabilistic      inference      as  \\n \\n  control      in      multi-path      environments.\\narXiv      preprint      arXiv:2402.10309.</p><p>Dhariwal,      P.      and      A.      Nichol      (2021).\\nDiffusion      models      beat      gans      on      image      synthesis.\\nAdvances      in  \\n \\n  neural      information      processing      systems      34,      8780-8794.</p><p> \\n \\n  Du,      Y.,      S.      Yang,      B.      Dai,      H.      Dai,      O.      Nachum,      J.      Tenenbaum,      D.      Schuurmans,      and      P.      Abbeel      (2024).\\n \\n \\n  Learning      universal      policies      via      text-guided      video      generation.\\nAdvances      in      Neural      Information  \\n \\n  Processing      Systems      36.</p><p>Fan,      Y.,      O.      Watkins,      Y.      Du,      H.      Liu,      M.      Ryu,      C.      Boutilier,      P.      Abbeel,      M.      Ghavamzadeh,      K.      Lee,      and K.      Lee      (2023) 023).\\nDPOK:      Reinforcement      learning      for      fine-tuning      text-to-image      diffusion      models.\\n \\n \\n  arXiv      preprint      arXiv:2305.16381.</p><p>Finn,      C.,      S.      Levine,      and      P.      Abbeel      (2016).\\nGuided      cost      learning:      Deep      inverse      optimal      control      via  \\n \\n  policy      optimization.\\nIn      International      conference      on      machine      learning,      pp.\\n49-58.\\nPMLR.</p><p>Fox,      R.,      A.      Pakman,      and      N.      Tishby      (2015).\\nTaming      the      noise      in      reinforcement      learning      via      soft  \\n \\n  updates.\\narXiv      preprint      arXiv:      1512.08562.</p><p>Frey,      N.      C.,      D.      Berenberg,      K.      Zadorozhny,      J.      Kleinhenz,      J.      Lafrance-Vanasse,      I.      Hotzel,      Y.      Wu, S.      Ra,   \\n \\nR R.      Bonneau,      K.      Cho,      et      al.\\n(2023).\\nProtein      discovery      with      discrete      walk-jump      sampling.\\n \\n \\n  arXiv      preprint      arXiv:2306.\\n12360.</p><p>Geist,      M.,      B.      Scherrer,      and      O.      Pietquin      (2019).\\n \\n \\nA      theory      of      regularized      markov      decision      processes.\\n \\n \\n  In      International      Conference      on      Machine      Learning,      pp.\\n2160-2169.\\nPMLR.</p><p> \\n \\n  Girolami,      M.      and      B.      Calderhead      (2011).\\nRiemann      manifold      langevin      and      hamiltonian      monte  \\n \\n  carlo      methods.\\nJournal      of      the      Royal      Statistical      Society      Series      B:      Statistical      Methodology      73(2),  \\n \\n  123-214.</p><p>Gosai,      S.      J.,      R.      I.      Castro,      N.      Fuentes,      J.      C.      Butts,      S.      Kales,      R.      R.      Noche,      K.      Mouri,      P.      C.      Sabeti, S.   \\n \\nK K.      Reilly,      and      R.      Tewhey      (2023).\\nMachine-guided      design      of      synthetic      cell      type-specific  \\n \\n  cis-regulatory      elements.\\nbioRxiv.</p><p>Haarnoja,      T.,      H.      Tang,      P.      Abbeel,      and      S.      Levine      (2017).\\nReinforcement      learning      with      deep      energy-  \\n \\n  based      policies.\\nIn      International      conference      on      machine      learning,      pp.\\n1352-1361.\\nPMLR.</p><p>Hansen-Estruch,      P.,      I.      Kostrikov,      M.      Janner,      J.      G.      Kuba,      and      S.      Levine      (2023).\\nIdql:      Implicit  \\n \\n  q-learning      as      an      actor-critic      method      with      diffusion      policies.\\narXiv      preprint      arXiv:2304.10573.</p><p>Heng,      J.,      A.      N.      Bishop,      G.      Deligiannidis,      and      A.      Doucet      (2020).\\nControlled      sequential      monte      carlo.</p><p>Ho,      J.,      A.      Jain,      and      P.      Abbeel      (2020).\\nDenoising      diffusion      probabilistic      models.\\nAdvances      in      neural  \\n \\n  information      processing      systems      33,      6840-6851.</p><p>Ho,      J.      and      T.      Salimans      (2022).\\nClassifier-free      diffusion      guidance.\\narXiv      preprint      arXiv:2207.12598.</p><p>Ho,      J.,      T.      Salimans,      A.      Gritsenko,      W.      Chan,      M.      Norouzi,      and      D.      J.      Fleet      (2022).\\nVideo      diffusion  \\n \\n  models.\\nAdvances      in      Neural      Information      Processing      Systems      35,      8633-8646.</p><p> \\n \\n  Hoogeboom,      E.,      V.      G.      Satorras,      C.      Vignac,      and      M.      Welling      (2022).\\nEquivariant      diffusion      for  \\n \\n  molecule      generation      in      3d.\\nIn      International      conference      on      machine      learning,      pp.\\n8867-8887.</p><h4>      PMLR.</h4><p>Huang,      X.,      H.      Dong,      H.      Yifan,      Y.      Ma,      and      T.      Zhang      (2023).\\nReverse      diffusion      monte      carlo.\\nIn      The  \\n \\n  Twelfth      International      Conference      on      Learning      Representations.</p><p>Janner,      M.,      Y.      Du,      J.      B.      Tenenbaum,      and      S.      Levine      (2022).\\nPlanning      with      diffusion      for      flexible  \\n \\n  behavior      synthesis.\\narXiv      preprint      arXiv:2205.09991.</p><p> \\n \\n  Jo,      J.,      S.      Lee,      and      S.      J.      Hwang      (2022).\\nScore-based      generative      modeling      of      graphs      via      the      system  \\n \\n  of      stochastic      differential      equations.\\nIn      International      Conference      on      Machine      Learning,      pp.\\n \\n \\n  10362-10383.\\nPMLR.</p><p> \\n \\n  Kazim,      M.,      J.      Hong,      M.-G.\\nKim,      and      K.-K.\\nK.      Kim      (2024).\\nRecent      advances      in      path      integral      control  \\n \\n  for      trajectory      optimization:      An      overview      in      theoretical      and      algorithmic      perspectives.\\nAnnual  \\n \\n  Reviews      in      Control      57,      100931.</p><p>Krishnamoorthy,      S.,      S.      M.      Mashkaria,      and      A.      Grover      (2023).\\nDiffusion      models      for      black-box  \\n \\n  optimization.\\narXiv      preprint      arXiv:2306.07180.</p><p>Kumar,      A.,      A.      Zhou,      G.      Tucker,      and      S.      Levine      (2020).\\nConservative      q-learning      for      offline      reinforce-  \\n \\n  ment      learning.\\nAdvances      in      Neural      Information      Processing      Systems      33,      1179-1191.</p><p>Lal,      A.,      D.      Garfield,      T.      Biancalani,      and      G.      Eraslan      (2024).\\nreglm:      Designing      realistic      regulatory  \\n \\n  dna      with      autoregressive      language      models.\\nbioRxiv,      2024-02.</p><p>Lee,      K.,      H.      Liu,      M.      Ryu,      O.      Watkins,      Y.      Du,      C.      Boutilier,      P.      Abbeel,      M.      Ghavamzadeh,      and      S.      S.      Gu  \\n \\n  (2023).\\nAligning      text-to-image      models      using      human      feedback.\\narXiv      preprint      arXiv:2302.12192.</p><p>Levine,      S.      (2018).\\nReinforcement      learning      and      control      as      probabilistic      inference:      Tutorial      and       review.\\narXiv      preprint      arXiv:      1805.00909.</p><p>Li,      Z.,      Y.      Ni,      T.      A.      B.      Huygelen,      A.      Das,      G.      Xia,      G.-B.\\nStan,      and      Y.      Zhao      (2023).\\nLatent      diffusion  \\n \\n  model      for      dna      sequence      generation.\\narXiv      preprint      arXiv:2310.06150.</p><p>Lipman,      Y.,      R.      T.      Chen,      H.      Ben-Hamu,      M.      Nickel,      and      M.      Le      (2023).\\nFlow      matching      for      generative  \\n \\n  modeling.\\nJCLR      2023.</p><p>Liu,      X.,      C.      Gong,      and      Q.      Liu      (2022).\\nFlow      straight      and      fast:      Learning      to      generate      and      transfer      data  \\n \\n  with      rectified      flow.\\narXiv      preprint      arXiv:2209.03003.</p><p>Liu,      X.,      L.      Wu,      M.      Ye,      and      Q.      Liu      (2022).\\nLet      us      build      bridges:      Understanding      and      extending  \\n \\n  diffusion      generative      models.\\narXiv      preprint      arXiv:2208.14699.</p><p>Lou,      A.,      C.      Meng,      and      S.      Ermon      (2023).\\nDiscrete      diffusion      language      modeling      by      estimating      the  \\n \\n  ratios      of      the      data      distribution.\\narXiv      preprint      arXiv:2310.16834.</p><p>Malkin,      N.,      M.      Jain,      E.      Bengio,      C.      Sun,      and      Y.      Bengio      (2022).\\nTrajectory      balance:      Improved      credit  \\n \\n  assignment      in      gflownets.\\nAdvances      in      Neural      Information      Processing      Systems      35,      5955-5967.</p><p>Mohammadpour,      S.,      E.      Bengio,      E.      Frejinger,      and      P.-L.\\nBacon      (2023).\\nMaximum      entropy      gflownets  \\n \\n  with      soft      q-learning.\\narXiv      preprint      arXiv:2312.14331.</p><p>Nachum,      O.,      M.      Norouzi,      K.      Xu,      and      D.      Schuurmans      (2017).\\nBridging      the      gap      between      value      and       policy      based      reinforcement      learning.\\nAdvances      in      neural      information      processing      systems      30.</p><p>Neal,      R.      M.      et      al.\\n(2011).\\nMcmce      using      hamiltonian      dynamics.\\nHandbook      of      markov      chain      monte  \\n \\n  carlo      2(11),      2.</p><p>Neu,      G.,      A.      Jonsson,      and      V.      Gémez      (2017).\\n \\n \\nA      unified      view      of      entropy-regularized      markov      decision  \\n \\n  processes.\\narXiv      preprint      arXiv:      1705.07798.</p><p>Ouyang,      L.,      J.      Wu,      X.      Jiang,      D.      Almeida,      C.      Wainwright,      P.      Mishkin,      C.      Zhang,      S.      Agarwal, K.      Slama,   \\n \\nA A.      Ray,      et      al.\\n(2022).\\nTraining      language      models      to      follow      instructions      with      human  \\n \\n  feedback.\\nAdvances      in      Neural      Information      Processing      Systems      35,      27730-27744.</p><p>Pattanaik,      L.      and      C.      W.      Coley      (2020).\\nMolecular      representation:      going      long      on      fingerprints.\\n \\n \\n  Chem      6(6),      1204-1207.</p><p>Peng,      X.      B.,      A.      Kumar,      G.      Zhang,      and      S.      Levine      (2019).\\nAdvantage-weighted      regression:      Simple  \\n \\n  and      scalable      off-policy      reinforcement      learning.\\narXiv      preprint      arXiv:1910.00177.</p><p>Peters,      J.,      K.      Mulling,      and      Y.      Altun      (2010).\\nRelative      entropy      policy      search.\\nIn      Proceedings      of      the  \\n \\n  AAAI      Conference      on      Artificial      Intelligence,      Volume      24,      pp.\\n1607-1612.</p><p> \\n \\n  Podell,      D.,      Z.      English,      K.      Lacey,      A.      Blattmann,      T.      Dockhorn,      J.      Miiller,      J.      Penna,      and      R.      Rombach  \\n \\n  (2023).\\nSdxl:      Improving      latent      diffusion      models      for      high-resolution      image      synthesis.\\narXiv  \\n \\n  preprint      arXiv:2307.01952.</p><p>Prabhudesai,      M.,      A.      Goyal,      D.      Pathak,      and      K.      Fragkiadaki      (2023).\\nAligning      text-to-image      diffusion  \\n \\n  models      with      reward      backpropagation.\\narXiv      preprint      arXiv:2310.03739.</p><p>Ramesh,      A.,      P.      Dhariwal,      A.      Nichol,      C.      Chu,      and      M.      Chen      (2022).\\nHierarchical      text-conditional  \\n \\n  image      generation      with      clip      latents.\\narXiv      preprint      arXiv:2204.06125      1(2),      3.</p><p>Rigter,      M.,      B.      Lacerda,      and      N.      Hawes      (2022).\\nRambo-rl:      Robust      adversarial      model-based      offline  \\n \\n  reinforcement      learning.\\nAdvances      in      neural      information      processing      systems      35,      16082-16097.</p><p>Robert,      C.      (2014).\\nStatistical      modeling      and      computation.</p><p>Rogers,      L.      C.      G.      and      D.      Williams      (2000).\\nDiffusions,      Markov      processes      and      martingales:      Volume      2,  \\n \\n  It6é      calculus,      Volume      2.\\nCambridge      university      press.</p><p> \\n \\n  Rombach,      R.,      A.      Blattmann,      D.      Lorenz,      P.      Esser,      and      B.      Ommer      (2022,      June).\\nHigh-resolution  \\n \\n  image      synthesis      with      latent      diffusion      models.\\nIn      Proceedings      of      the      IEEE/CVF      Conference      on  \\n \\n  Computer      Vision      and      Pattern      Recognition      (CVPR),      pp.\\n10684-10695.</p><p>Sarkar,      A.,      Z.      Tang,      C.      Zhao,      and      P.      Koo      (2024).\\nDesigning      dna      with      tunable      regulatory      activity  \\n \\n  using      discrete      diffusion.\\nbioRxiv,      2024-05.</p><p>Schulman,      J.,      X.      Chen,      and      P.      Abbeel      (2017).\\nEquivalence      between      policy      gradients      and      soft  \\n \\n  q-learning.\\narXiv      preprint      arXiv:      1704.06440.</p><p>Schulman,      J.,      S.      Levine,      P.      Abbeel,      M.      Jordan,      and      P.      Moritz      (2015).\\nTrust      region      policy      optimization.\\n \\n \\n  In      International      conference      on      machine      learning,      pp.\\n1889-1897.\\nPMLR.</p><p>Schulman,      J.,      F.      Wolski,      P.      Dhariwal,      A.      Radford,      and      O.      Klimov      (2017).\\nProximal      policy      optimiza-  \\n \\n  tion      algorithms.\\narXiv      preprint      arXiv:      1707.06347.</p><p>Shi,      Y.,      V.      De      Bortoli,      A.      Campbell,      and      A.      Doucet      (2024).\\nDiffusion      schrédinger      bridge      matching.\\n \\n \\n  Advances      in      Neural      Information      Processing      Systems      36.</p><p> \\n \\n  Sohl-Dickstein,      J.,      E.      Weiss,      N.      Maheswaranathan,      and      S.      Ganguli      (2015).\\nDeep      unsupervised  \\n \\n  learning      using      nonequilibrium      thermodynamics.\\nIn      /nternational      conference      on      machine      learning,  \\n \\n  pp.\\n2256-2265.\\nPMLR.</p><p>Song,      J.,      C.      Meng,      and      S.      Ermon      (2020).\\nDenoising      diffusion      implicit      models.\\narXiv      preprint  \\n \\n  arXiv:2010.02502.</p><p>Song,      Y.,      C.      Durkan,      I.      Murray,      and      S.      Ermon      (2021).\\nMaximum      likelihood      training      of      score-based  \\n \\n  diffusion      models.\\nAdvances      in      neural      information      processing      systems      34,      1415-1428.</p><p>Stark,      H.,      B.      Jing,      C.      Wang,      G.      Corso,      B.      Berger,      R.      Barzilay,      and      T.      Jaakkola      (2024).\\nDirichlet  \\n \\n  flow      matching      with      applications      to      dna      sequence      design.\\narXiv      preprint      arXiv:2402.05841.</p><p>Sutton,      R.      S.      and      A.      G.      Barto      (2018).\\nReinforcement      learning:      An      introduction.\\nMIT      press.</p><p>Tang,      W.      (2024).\\nFine-tuning      of      diffusion      models      via      stochastic      control:      entropy      regularization      and       beyond.\\narXiv      preprint      arXiv:2403.06279.</p><p>Tang,      W.      and      H.      Zhao      (2024).\\nScore-based      diffusion      models      via      stochastic      differential      equations—a  \\n \\n  technical      tutorial.\\narXiv      preprint      arXiv:2402.07487.</p><p>Theodorou,      E.,      J.      Buchli,      and      S.      Schaal      (2010).\\n \\n \\nA      generalized      path      integral      control      approach      to  \\n \\n  reinforcement      learning.\\nThe      Journal      of      Machine      Learning      Research      11,      3137-3181.</p><p>Tiapkin,      D.,      N.      Morozov,      A.      Naumov,      and      D.      Vetrov      (2023).\\nGenerative      flow      networks      as      entropy-  \\n \\n  regularized      rl.\\narXiv      preprint      arXiv:2310.12934.</p><p> \\n \\n  Tong,      A.,      N.      Malkin,      G.      Huguet,      Y.      Zhang,      J.      Rector-Brooks,      K.      Fatras,      G.      Wolf,      and      Y.      Bengio  \\n \\n  (2023).\\nConditional      flow      matching:      Simulation-free      dynamic      optimal      transport.\\narXiv      preprint  \\n \\n  arXiv:2302.00482.</p><p>Uehara,      M.      and      W.      Sun      (2021).\\nPessimistic      model-based      offline      reinforcement      learning      under  \\n \\n  partial      coverage.\\narXiv      preprint      arXiv:2107.06226.</p><p>Uehara,      M.,      Y.      Zhao,      K.      Black,      E.      Hajiramezanali,      G.      Scalia,      N.      L.      Diamant,      A.      M.      Tseng, T.      Biancalani,      and      S S.      Levine      (2024).\\nFine-tuning      of      continuous-time      diffusion      models      as      entropy-  \\n \\n  regularized      control.\\narXiv      preprint      arXiv:2402.15194.</p><p> \\n \\n  Uehara,      M.,      Y.      Zhao,      K.      Black,      E.      Hajiramezanali,      G.      Scalia,      N.      L.      Diamant,      A.      M.      Tseng,      S.      Levine,  \\n \\n  and      T.      Biancalani      (2024).\\nFeedback      efficient      online      fine-tuning      of      diffusion      models.\\narXiv      preprint  \\n \\n  arXiv:2402.\\n16359.</p><p> \\n \\n  Uehara,      M.,      Y.      Zhao,      E.      Hajiramezanali,      G.      Scalia,      G.      Eraslan,      A.      Lal,      S.      Levine,      and      T.      Biancalani  \\n \\n  (2024).\\nBridging      model-based      optimization      and      generative      modeling      via      conservative      fine-tuning  \\n \\n  of      diffusion      models.\\narXiv      preprint      arXiv:2405.19673.</p><p>Vargas,      F.,      W.      Grathwohl,      and      A.      Doucet      (2023).\\nDenoising      diffusion      samplers.\\narXiv      preprint  \\n \\n  arXiv:2302.\\n13834.</p><p> \\n \\n  Wallace,      B.,      M.      Dang,      R.      Rafailov,      L.      Zhou,      A.      Lou,      S.      Purushwalkam,      S.      Ermon,      C.      Xiong,      S.      Joty,  \\n \\n  and      N.      Naik      (2023).\\nDiffusion      model      alignment      using      direct      preference      optimization.\\narXiv  \\n \\n  preprint      arXiv:2311.12908.</p><p>Wang,      Z.,      J.      J.      Hunt,      and      M.      Zhou      (2022).\\nDiffusion      policies      as      an      expressive      policy      class      for      offline  \\n \\n  reinforcement      learning.\\narXiv      preprint      arXiv:2208.06193.</p><p>Widatalla,      T.,      R.      Rafailov,      and      B.      Hie      (2024).\\nAligning      protein      generative      models      with      experimental  \\n \\n  fitness      via      direct      preference      optimization.\\nbioRxiv,      2024-05.</p><p>Williams,      G.,      A.      Aldrich,      and      E.      A.      Theodorou      (2017).\\nModel      predictive      path      integral      control:      From  \\n \\n  theory      to      parallel      computation.\\nJournal      of      Guidance,      Control,      and      Dynamics      40(2),      344-357.</p><p>Wu,      X.,      K.      Sun,      F.      Zhu,      R.      Zhao,      and      H.      Li      (2023).\\nBetter      aligning      text-to-image      models      with  \\n \\n  human      preference.\\narXiv      preprint      arXiv:2303.14420.</p><p>Wu,      Y.,      G.      Tucker,      and      O.      Nachum      (2019).\\nBehavior      regularized      offline      reinforcement      learning.\\n \\n \\n  arXiv      preprint      arXiv:      1911.11361.</p><p>Wulfmeier,      M.,      P.      Ondruska,      and      I.      Posner      (2015).\\nMaximum      entropy      deep      inverse      reinforcement  \\n \\n  learning.\\narXiv      preprint      arXiv:      1507.04888.</p><p>Xie,      T.,      C.-A.\\nCheng,      N.      Jiang,      P.      Mineiro,      and      A.      Agarwal      (2021).\\nBellman-consistent      pessimism  \\n \\n  for      offline      reinforcement      learning.\\nAdvances      in      neural      information      processing      systems      34.</p><p>Xu,      M.,      L.      Yu,      Y.      Song,      C.      Shi,      S.      Ermon,      and      J.      Tang      (2022).\\nGeodiff:   \\n \\nA      geometric      diffusion  \\n \\n  model      for      molecular      conformation      generation.\\narXiv      preprint      arXiv:2203.02923.</p><p> \\n \\n  Yang,      L.,      Z.      Zhang,      Y.      Song,      S.      Hong,      R.      Xu,      Y.      Zhao,      W.      Zhang,      B.      Cui,      and      M.-H.\\nYang      (2023).\\n \\n \\n  Diffusion      models:   \\n \\nA      comprehensive      survey      of      methods      and      applications.\\nACM      Computing  \\n \\n  Surveys      56(4),      1-39.</p><p> \\n \\n  Yu,      T.,      G.      Thomas,      L.      Yu,      S.      Ermon,      J.      Y.      Zou,      S.      Levine,      C.      Finn,      and      T.      Ma      (2020).\\nMopo:  \\n \\n  Model-based      offline      policy      optimization.\\nAdvances      in      Neural      Information      Processing      Systems      33,  \\n \\n  14129-14142.</p><p> \\n \\n  Yuan,      H.,      K.      Huang,      C.      Ni,      M.      Chen,      and      M.      Wang      (2023).\\nReward-directed      conditional      diffusion:  \\n \\n  Provable      distribution      estimation      and      reward      improvement.\\nIn      Thirty-seventh      Conference      on  \\n \\n  Neural      Information      Processing      Systems.</p><p>Zhang,      H.      and      T.      Xu      (2023).\\nTowards      controllable      diffusion      models      via      reward-guided      exploration.\\n \\n \\n  arXiv      preprint      arXiv:2304.07132.</p><p>Zhang,      Q.      and      Y.      Chen      (2021).\\nPath      integral      sampler:   \\n \\na      stochastic      control      approach      for      sampling.\\n \\n \\n  arXiv      preprint      arXiv:2111,15141.</p><p> \\n \\n  Zhao,      Y.,      M.      Uehara,      G.      Scalia,      T.      Biancalani,      S.      Levine,      and      E.      Hajiramezanali      (2024).\\n \\n \\n  Adding      conditional      control      to      diffusion      models      with      reinforcement      learning.\\narXiv      preprint  \\n \\n  arXiv:2406.\\n12120.</p><p>Zhou,      Z.,      S.      Kearnes,      L.      Li,      R.      N.      Zare,      and      P.      Riley      (2019).\\nOptimization      of      molecules      via      deep  \\n \\n  reinforcement      learning.\\nScientific      reports      9(1),      10752.</p><p>Zhu,      Z.,      H.      Zhao,      H.      He,      Y.      Zhong,      S.      Zhang,      Y.      Yu,      and      W.      Zhang      (2023).\\nDiffusion      models      for  \\n \\n  reinforcement      learning:   \\n \\nA      survey.\\narXiv      preprint      arXiv:2311.01223.</p><p>Ziebart,      B.      D.,      A.      L.      Maas,      J.      A.      Bagnell,      A.      K.      Dey,      et      al.\\n(2008).\\nMaximum      entropy      inverse  \\n \\n  reinforcement      learning.\\nIn      Aaai,      Volume      8,      pp.\\n1433-1438.\\nChicago,      IL,      USA.</p><h4>Direct      backpropagation      (Sec.      5.2)       _       Differentiable</h4><p>_\\n \\n   ——_      Non-differentiable PP      5.1</p><h4>Accurate      rewards      O      (Sec.      5.1)</h4><p> \\n \\n  feedback   \\n \\n;    \\n \\n—      Reward-weighted      MLE      (Sec.      6.1  \\n \\n  Xo      Unknown      reward      Memory      effi-      oe      eward-welgnte      (Sec.      6.1) unctions      —_\\\\\\\\\\\\\\\\_—      ciency</p><h4>No      fine-tuning      Computational</h4><p>efficiency —\\n \\n    NN      Direct      backpropagation      (Sec.      5.2)</p><h5>Value-weighted      sampling      (Sec.      6.2)</h5><p>Figure      3:      Practical      recommendation      of      RL-based      algorithms      to      optimize      downstream      reward  \\n \\n  functions.</p><h6>7.1      Fine-Tuning      with      Known,      Differentiable      Reward      Functions</h6><p>ions  \\n \\n  When      accurate      differentiable      reward      functions      are      available      (e.g.,      standard      in      computer      vision      tasks),  \\n \\n  our      primary      focus      is      typically      on      computational      and      memory      efficiency.\\nIn      general,      we      advocate      for  \\n \\n  employing      reward      backpropagation      (c.f.      Algorithm      2)      for      its      computational      efficiency.\\nAlternatively,  \\n \\n  if      memory      efficiency      is   \\n \\na      significant      concern,      we      suggest      using      PPO      due      to      its      stability.</p><h6>7.2      Fine-Tuning      with      Black-Box      Reward      Feedback</h6><p>back  \\n \\n  Here,      we      explore      scenarios      where      we      have      access      to      accurate      but      non-differentiable      (black-box)  \\n \\n  reward      feedback,      often      found      in      scientific      simulations.\\nThe      emphasis      still      remains      primarily      on  \\n \\n  computational      and      memory      efficiency.\\nIn      such      scenarios,      due      to      the      non-differentiability      of      reward  \\n \\n  feedback,      we      recommend      using      PPO      or      reward-weighted      MLE      for      the      following      reasons.</p><p> \\n \\n  In      the      preceding      section,      we      advocated      for      the      use      of      reward      backpropagation      due      to      its      com-  \\n \\n  putational      advantages.\\nHowever,      when      dealing      with      non-differentiable      feedback,      the      advantage  \\n \\n  of      reward      backpropagation      may      diminish.\\nThis      is      because      learning      from      such      feedback      requires  \\n \\n  learning      differentiable      reward      functions      to      make      the      algorithm      work      as      we      discuss      in      Section      5.2.  \\n \\n  However,      obtaining   \\n \\na      differentiable      reward      function      can      be      challenging      in      many      domains.\\nFor  \\n \\n  example,      in      molecular      property      prediction,      molecular      fingerprints      are      informative      features,      and       accurate      reward      functions      can      be      derived      by      training      simple      neural      networks      on      these      features  \\n \\n  (Pattanaik      and      Coley,      2020).\\nYet,      these      mappings,      grounded      in      prior      scientific      understanding,      are  \\n \\n  non-differentiable,      thereby      restricting      their      applicability.</p><p> \\n \\n  In      contrast,      PPO      and      reward-weighted      MLE      allow      for      policy      updates      without      the      explicit  \\n \\n  requirement      to      learn   \\n \\na      differentiable      reward      function,      offering   \\n \\na      significant      advantage      over      reward  \\n \\n  backpropagation.</p><h6>7.3      Fine-Tuning      with      Unknown      Rewards      Functions</h6><p>ions  \\n \\n  When      reward      functions      (or      computational      proxies      in      Section      7.2)      are      unavailable,      we      must      learn  \\n \\n  from      data      with      reward      feedback.\\nIn      such      cases,      compared      to      the      previous      scenarios,      two      important  \\n \\n  considerations      arise:</p><p>¢\\n \\n   Not      only      computational      or      memory      efficiency      but      also      feedback      efficiency      (i.e.,      sample  \\n \\n  efficiency      regarding      reward      feedback)      is      crucial.</p><p> \\n \\n \\n¢      Given      that      learned      reward      feedback      may      not      generalize      well      outside      the      training      data      distri-  \\n \\n  butions,      it      is      essential      to      constrain      fine-tuned      models      to      prevent      significant      divergence      from  \\n \\n  diffusion      models.\\nIn      these      situations,      not      only      soft      PPO      and      direct      backpropagation      but      also  \\n \\n  methods      discussed      in      Section      6,      such      as      reward-weighted      MLE,      can      be      particularly      effective.</p><p>Further,      the      scenario      involving      unknown      reward      functions      can      be      broadly      categorized      into      two      cases,  \\n \\n  which      we      will      discuss      in      more      detail.</p><p>Offline      scenario      (Figure      4).\\nIn      many      scenarios,      even  \\n \\n  in      cases      where      reward      functions      are      unknown,      we      of-  \\n \\n.     .</p><p>Offline      data      with ten      have      access      to      offline      data      with      reward      feedback      reward      feedback</p><li>(i)      (i)   \\n \\nO   \\n \\n;   \\n \\ni       a”      ra  \\n \\n.      ne      straightforward      approach      is      to</li><p> \\n \\n \\n{   \\n \\n,   \\n \\n(   \\n \\n)    \\n.\\n    .\\n \\n   g      PP  \\n \\n.      <=>      Learning      reward  \\n \\n  perform      regression      with      neural      networks      and      build   \\n \\na      models  \\n \\n  learned      regressor      *.\\nHowever,      this      learned      reward      func-      SS  \\n \\n  tion   \\n \\n*      might      not      generalize      well      beyond      the      distribution      x)  \\n \\n  of      the      offline      data.\\nIn      regions      outside      this      distribution,   \\n \\n7       could      assign   \\n \\na      high      value      even      when      the      actual      reward  \\n \\n  r      is      low      due      to      high      uncertainty.\\nConsequently,      we  \\n \\n  could      be      easily      misled      by      out-of-distribution      samples  \\n \\n  if      we      solely      optimize      7.\\n|Fine-tuning      diffusion      models   \\n \\n|       One      approach      to      alleviate      this      issue      is      to      adopt   \\n \\na       pessimistic      (i.e.,      conservative)      strategy,      as      proposed      in      Figure      4:      Offline      scenario  \\n \\n  Uehara      et      al.\\n(2024).\\nThis      technique      has      been      widely      Collecting      data]   \\n \\nY      Learning      reward  \\n \\n  applied      in      the      field      of      offline      RL      (Levine,      2018).\\nThe      aa  \\n \\n  main      idea      is      to      penalize   \\n \\n7      outside      the      distributions      us-  \\n \\n  ing      techniques      such      as      adding      an      explicit      penalty      term  \\n \\n  (Yu      et      al.,      2020;      Chang      et      al.,      2021),      bootstrapping,  \\n \\n  or      more      sophisticated      methods      (Kumar      et      al.,      2020;  \\n \\n  Xie      et      al.,      2021;      Rigter      et      al.,      2022;      Uehara      and      Sun,  \\n \\n  2021).\\nBy      doing      so,      we      can      avoid      being      fooled      by  \\n \\n  out-to-distribution      regions      while      still      benefiting      from  \\n \\n  the      extrapolation      capabilities      of      reward      models.</p><p>S—      diffusion      models      |</p><p>Figure      5:      Online      scenario</p><p> \\n \\n  Online      scenario      (Figure      5).\\nConsider      situations  \\n \\n  where      reward      functions      are      unknown,      but      we      can      gather      data      online.\\nThese      scenarios      are      often  \\n \\n  referred      to      as   \\n \\na      lab-in-the-loop      setting.</p><p> \\n \\n  In      such      scenarios,      Uehara      et      al.\\n(2024)      proposes      an      iterative      procedure      comprising      three      steps:</p><li>(1)      collecting      feedback      data      by      exploring      new      areas      (i.e.,      obtaining   \\n \\nx      from      current      diffusion      models  \\n \\n  and      corresponding      r(x)),      (2)      learning      the      reward      function      from      the      collected      feedback      data,      and</li><li>(3)      fine-tuning      current      diffusion      models      by      maximizing      the      learned      reward      function      enhanced      by  \\n \\n  an      optimistic      bonus      term,      using      algorithms      discussed      in      Section      4.2.\\nIn      contrast      to      standard      online  \\n \\n  PPO,      where      feedback      data      is      directly      used      in      the      fine-tuning      process,      actual      reward      feedback      is  \\n \\n  only      used      in      step      (2)      and      not      in      step      (3).\\nAn      additional      key      aspect      is      the      use      of      an      optimistic  \\n \\n  reward      function      that      encourages      exploration      beyond      the      distributions      of      the      current      diffusion      model.\\n \\n \\n  This      deliberate      separation      between      reward      learning      and      fine-tuning      steps,      coupled      with      the      use      of  \\n \\n  optimism,      significantly      enhances      feedback      efficiency.</li><p>8\\n \\n   Connection      with      Classifier      Guidance</p><p> \\n \\n  Classifier      guidance      (Dhariwal      and      Nichol,      2021)      is      commonly      used      for      conditional      generation      in  \\n \\n  diffusion      models.\\nInterestingly,      RL-based      methods      share   \\n \\na      close      connection      with      classifier      guidance.\\n \\n \\n  More      specifically,      in      this      section,      following      Zhao      et      al.\\n(2024),      we      elucidate      how      RL-based      methods  \\n \\n  can      be      applied      to      conditional      generation.\\nFurthermore,      from      this      unified      viewpoint,      we      highlight  \\n \\n  that      classifier      guidance      is      seen      as   \\n \\na      value-weighted      sampling      in      Section      4.2.</p><h6>8.1      Classfier      Guidance</h6><p>ance  \\n \\n  In      this      section,      we      first      introduce      classifier      guidance.\\nClassifier      guidance      utilizes      an      unconditional  \\n \\n  pre-trained      model      for      conditional      generation.\\nMore      specifically,      we      consider   \\n \\na      scenario      where      we  \\n \\n  have   \\n \\na      pre-trained      diffusion      model,      which      enables      us      to      sample      from      p?\\n\"*(x),      and      data      with      feedback  \\n \\n  {x,y},      where   \\n \\ny      represents      the      new      label      we      want      to      condition      on.\\nOur      objective      here      is      to      sample  \\n \\n  from      p(-|y)   \\n \\n€      [Y   \\n \\n>      ACR), P(-   \\n \\n|      y)   \\n \\nx      py(yl-)pP*(-).\\n(37)</p><p>where      p,   \\n \\n:   \\n \\nX      —>      A(Y)      denotes      the      conditional      distribution      of   \\n \\ny      given      x.</p><p>Motivation.\\nIn      classifier      guidance,      we      consider      the      following      policy:</p><p>pe      (-lare,      y)   \\n \\n=   \\n \\nN      (p(22,      t;      Opre)   \\n \\n+      a(t)      Ve      log      Vy      (Xt;      t),      a*(t))   \\n \\n’ dy(x,t)   \\n \\n=      Egy.\\nyap,      (-leo)      L(Y   \\n \\n=      y)|a,   \\n \\n=      2].</p><p>Under      the      continuous-time      formulation      of      diffusion      models,      applying      Doob’s      h-transform      (Rogers  \\n \\n  and      Williams,      2000),      it      is      shown      that      we      can      sample      from      p(x|y)      by      sequentially      running      {p7\"\\\\\\\\\\\\\\'(-|7+,      y)      }i-r41-</p><p>Algorithm.\\nSpecifically,      the      algorithm      comprises      two      steps.\\nIt’s      important      to      note      that      this      is      not  \\n \\n  a      fine-tuning      method      but      rather      an      inference-time      technique      as      we      freeze      the      pre-trained      model.</p><li>1. Learning      q,(x,t)      through      regression      by      gathering      data      containing      {z;,      y}.\\nHere,      for      each  \\n \\n  (Xo,      y),      using   \\n \\na      forward      policy      for      the      pre-training      (from   \\n \\n0      to      T),      we      obtain      2;      starting      from</li><p>Lo.</p><li>2. During      inference,      sequentially      execute      {p2\"(-|a,,      y)      }1_>   \\n \\n1      by      leverage      the      fact      that      compared</li><p>gui to      the      policy      in      pre-trained      diffusion      models,      the      mean      of      {p?\\n\"\\\\\\\\\\\\\\'(-|71,      y)      }i-74,      is      just      shifted  \\n \\n  with      an      additional      term      V,,      log      q,(2,      t).</p><p> \\n \\n  Notably,      the      method      described      above      can      be      reduced      to      value-weighted      sampling      (cf.      Sec-  \\n \\n  tion      6.2),      when      r(x,      y)   \\n \\n=      log      p(y|x).\\nInspired      by      such      an      observation,      we      apply      an      RL-based  \\n \\n  fine-tuning      method      for      conditional      generation      as      below.</p><h7>8.2.      RL-Based      Fine-Tuning      for      Conditional      Generation</h7><p>tion  \\n \\n  In      this      section,      following      Zhao      et      al.\\n(2024),      we      explain      an      RL-based      fine-tuning      approach      for  \\n \\n  conditional      generation.\\nThe      core      idea      is      that      setting      r(x,      y)   \\n \\n=      log      p(y|x)      allows      us      to      sample      from  \\n \\n  the      target      conditional      distribution      (37),      drawing      on      the      insights      discussed      in      Section      4.2.  \\n \\n  More      precisely,      introducing   \\n \\na      distribution      over   \\n \\ny      as      q(-),      we      formulate      the      following      RL      problem:</p><p>{Di      ft   \\n \\n=      argmax      Egy,      (-\\\\\\\\\\\\\\\\-y)}.yott      log      p(y|xo)   \\n \\n—      NeopyaKL(pe(-|e,      y)      ||P      (late).\\n \\n \\n  {prE[(V,.X)PA(A)]      Heyy (38)</p><p> \\n \\n  Compared      to      the      objective      function      (18)      in      Section      4.2,      the      main      differences      are:      (1)      policy      space  \\n \\n  is      expanded      to      accommodate      additional      control      (i.e.,      y):      pz(-|-,-)      C      [R¢   \\n \\nx   \\n \\nY   \\n \\n+      A(R®)],      (2)      the  \\n \\n  fine-tuning      objectives      are      set      as      log-likelihoods:      log      p(y|z).\\nThen,      following      Theorem      1,      we      can  \\n \\n  establish      the      following      result.</p><p>Corollary      2.\\nThe      distribution      induced      by      {p*}1_74      (ue.   \\n \\n[      {Teri      Ph      (ae-1   \\n \\n|      te,      y)}deur      is      the target      conditional      distribution      (37).</p><p> \\n \\n  The      above      corollary      suggests      that      for      conditional      generation,      we      can      utilize      various      off-the-shelf  \\n \\n  algorithms      discussed      in      Section      4.2      and      6,      such      as      PPO,      reward      backpropagation,      and      reward-  \\n \\n  weighted      MLE,      as      well      as      value-weighted      sampling.</p><p>9\\n \\n   Connection      with      Flow-Based      Diffusion      Models</p><p> \\n \\n  We      elucidate      the      close      relationship      between      bridge      matching      and      fine-tuning,      as      summarized      in  \\n \\n  Table      2.\\nBridge      (flow)      matching      (Liu      et      al.,      2022;      Tong      et      al.,      2023;      Lipman      et      al.,      2023;      Liu      et      al.,  \\n \\n  2022;      Shi      et      al.,      2024;      Albergo      and      Vanden-Eijnden,      2022),      has      recently      gained      popularity      as   \\n \\na       distinct      type      of      diffusion      model      from      score-matching-based      ones.\\nThese      works      suggest      that      training  \\n \\n  and      inference      speed      will      be      accelerated      due      to      the      ability      to      define   \\n \\na      more      direct      trajectory      from  \\n \\n  noise      to      data      distribution      than      score-matching-based      diffusion      models.</p><p> \\n \\n  The      connection      between      bridge      matching      and      fine-tuning      becomes      evident      in      the      continuous-  \\n \\n  time      formulation.\\nTo      begin,      we      first      provide   \\n \\na      brief      overview      of      the      continuous-time      formulation      of  \\n \\n  score-matching-based      diffusion      models.</p><p> \\n \\n  Table      2:      Comparison      between      training      score/flow-based      diffusion      models      and      fine-tuning.\\nWe  \\n \\n  optimize      the      parameter   \\n \\n6      in      P’,      which      represents      the      induced      distribution      over      trajectories      associated  \\n \\n  with      6.</p><h8>Loss      Note      of      Q</h8><p>Score-based      KL(Q\\\\\\\\\\\\\\'™¢|P?)      Qtime:      Time-reversal      of      reference      SDE      (reference training      SDE      is      defined      from   \\n \\n0      to      T’      where      the      initial      dis-  \\n \\n  tribute      at   \\n \\n0      is      data      distribution)</p><p> \\n \\n  Flow-based      KL(Q*°>      ||P?)      @Q*°°h:      Coupling      between      the      reference      SDE      from  \\n \\n  training      T      to   \\n \\n0      conditioning      at   \\n \\n0      (dist.      over      x.;|7o)      and    \\n \\na      data      distribution      at      time   \\n \\n0      (dis.      over      x).\\nNote  \\n \\n  that      reference      SDE      is      defined      from      7’      to      0.</p><p> \\n \\n  Fine-tuning      KL(P?||Q*°)      @*:      Coupling      between      the      pre-trained      SDE  \\n \\n  from      T      to   \\n \\n0      conditioned      on   \\n \\n0      (dist.      over      x7.1|X0)  \\n \\n  and   \\n \\na      data      distribution      at      time   \\n \\n0      (dis.      over      Xo) 9.1      Continuous      Time      Formulation      of      Score-Based      diffusion      models</p><p> \\n \\n  We      provide   \\n \\na      brief      overview      of      the      continuous-time      formulation      of      diffusion      models      (Song      et      al.,  \\n \\n  2021),      further      expanding      Section      1.1.1.\\nIn      this      formulation,      we      initially      define      the      forward      reference  \\n \\n  SDE,      spanning      from      time   \\n \\n0      to      7’      (e.g.,      variance      exploding      (VE)      process      or      variance      preserving      (VP)  \\n \\n  process),      by      setting      an      initial      distribution      at   \\n \\n0      as      the      data      distribution.\\nAdditionally,      the      reference  \\n \\n  SDE      is      tailored      to      converge      to      an      easy      distribution      at      time      7’,      such      as      the      normal      distribution.</p><p> \\n \\n  Subsequently,      we      consider      the      time-reversal      SDE      (Anderson,      1982),      progressing      from      T\\\\\\\\\\\\\\'      to      0.\\nIf  \\n \\n  we      could      learn      this      time-reversal      SDE,      it      would      enable      us      to      sample      from      the      data      distribution      by  \\n \\n  starting      with      the      easy      initial      distribution      (at      7’)      and      following      the      time-reversal      SDE      from      T      to   \\n \\n0      at  \\n \\n  inference      time.</p><p> \\n \\n  Training.\\nNow,      the      remaining      question      is      how      to      learn      this      time-reversal      SDE.\\nHere,      by      repre-  \\n \\n  senting      the      induced      distribution      over      trajectories      from      T      to   \\n \\n0      with      this      time-reversal      SDE      as      Q\\\\\\\\\\\\\\'™®,  \\n \\n  our      goal      is      to      train   \\n \\na      new      SDE,      parameterized      by   \\n \\n@      in      the      neural      network,      such      that      the      induced  \\n \\n  distribution      P®      over      trajectories      associated      with   \\n \\n6      aligns      with      Q\"™*.\\nSpecifically,      the      objective      is      to minimize      the      following      losses:</p><p> \\n \\n  argmin      KL(Q\"™||P*).\\n(39)  \\n \\n  0  \\n \\n  With      some      algebra,      leveraging      the      observation      that      the      time-reversal      SDE      comprises      the      original  \\n \\n  forward      SDE      and      the      scoring      term      of      the      marginal      distribution      (Anderson,      1982)      as      in      (2),      Song  \\n \\n  et      al.\\n(2021)      demonstrates      that      this      KL      loss      (39)      is      approximated      as Dore   \\n \\n=      argmin      Eeuni((0,7))01~qyo0(weleo)co~pP\"e      Fe      Fl      Vey      log      jo      (Xt   \\n \\n|      Xo)   \\n \\n_      so(Xt,      t)|I?\\n],      (40)  \\n \\n  which      is      equal      to      (4)      where      the      weight      A(t)      is      {o?\\n}?\\n(for      notation,      refer      to      Section      1.1.1).</p><h9>9.2      Flow-Based      (Bridge-Based)      Diffusion      Models</h9><p>dels We      adopt      the      explanation      outlined      in      Liu      et      al.\\n(2022).\\nIn      this      framework,      we      begin      by      considering  \\n \\n  a      reference      SDE      (from      T\\\\\\\\\\\\\\'      to      0),      x;   \\n \\n=      zr_;      where      z;      is      defined      as      Brownian      motion:</p><p>dz,   \\n \\n=      dwt,      te      (0,      7).\\n(41)</p><p> \\n \\n  Subsequently,      we      consider      the      coupling      of      the      reference      SDE      conditioned      on      state      at      time   \\n \\n0      (i.e.,  \\n \\n  distribution      of      xo.7      conditional      on      x)      and      the      data      distribution      at      time      0,      denoting      the      resulting  \\n \\n  distribution      as      Q*°?,      Informally,      this      Q*°°?\\nis      characterized      as      follows.</p><p>Informal      characterization      of      Q*”.\\nWhen      discretizing      the      time      step,      by      denoting      the      distribution  \\n \\n  induced      by      the      reference      SDE      as      p\"(x7,--+   \\n \\n,      29),      we      define QF      (a7,      tae   \\n \\n,      20)   \\n \\n=      p(x,      ay   \\n \\n|      r9)pP\"°      (x9).</p><p>The      distribution      Q*”      above      is   \\n \\na      limiting      distribution      obtained      by      making      the      discretization      step small.</p><p>Training.\\nIn      bridge-based      diffusion      models,      our      objective      is      to      train   \\n \\na      new      SDE,      parametrized      by  \\n \\n  6      (e.g.,   \\n \\na      neural      network),      to      align      the      induced      distribution      P®      with      Q*°?.\\nSpecifically,      bridge-based training      aims      to      minimize aremin      KL(Q*>||P’).\\n(42)  \\n \\n  0  \\n \\n  While      the      method      to      minimize      the      above      is      currently      unclear,      we      can      derive   \\n \\na      trainable      objective  \\n \\n  through      algebraic      manipulation.\\nInitially,      we      observe      that      the      reference      SDE      (41),      conditioned      on  \\n \\n  state      xo      at      time   \\n \\n0      (zr   \\n \\n=      D),      is      expressed      as:</p><p>b\\n \\n \\n  b—      2;</p><p>t   \\n \\n€      [0,T];dz?\\n \\n \\n=      g?(z?,t)dt      +du,,  \\n \\n  g(z?,t)   \\n \\n=   \\n \\na       (43)</p><p>using      the      seminal      “Doob’s      h-transform”      (Rogers      and      Williams,      2000).\\nThis      SDE      is      commonly  \\n \\n  known      as      the      Brownian      bridge.\\nNow,      we      consider   \\n \\na      new      SDE      parameterized      by      0:</p><p>t   \\n \\n€      [0,7];      dz   \\n \\n=      s(z,t;      @)dt   \\n \\n+      dw.\\n(44)</p><p>Then,      we      can      demonstrate      that      minimizing      the      KL      loss      in      (42)      is      equivalent      to argmin      E,W      unil0,7],1~4\"°      cop?\\n[Ilg”      (2,      t)   \\n \\n_      s(21,      t;      0)      M3]      (45)</p><p>where      q;°      denotes      the      induced      distribution      at      time      t      following      SDE      (43)      conditioned      on      xo,      and       s:      Xx      |0,T]   \\n \\n>   \\n \\n&      is      the      model      we      aim      to      optimize.</p><p> \\n \\n  Flow-based      diffusion      models.\\nThe      aforementioned      formulation      has      been      originally      proposed      as   \\n \\na       bridge-based      SDE      (Liu      et      al.,      2022)      because      the      conditional      SDE      is      often      referred      to      as   \\n \\na      Brownian  \\n \\n  bridge.\\nFlow-based      diffusion      models      proposed      in      Lipman      et      al.\\n(2023);      Tong      et      al.\\n(2023)      share   \\n \\na       close      relationship      with      bridge-based      diffusion      models.\\nThese      flow-based      models      use   \\n \\na      target      SDE (43)      and      an      SDE      with   \\n \\na      parameter      (44)      without      stochastic      terms      (1.e.,      no      dw,)      and      aim      to      minimize  \\n \\n  the      loss      function      (45).</p><p> \\n \\n  Comparison      with      score-based      diffusion      models.\\nBoth      score-based      diffusion      models      and      bridge-  \\n \\n  based      diffusion      models      aim      to      minimize      KL      divergences.\\nDespite      its      similarity,      in      contrast      to  \\n \\n  score-based      diffusion      models,      bridge-based      diffusion      models      target      Q*°>      rather      than      the      time-  \\n \\n  reversal      SDE      Q\"™®.\\nThe      distribution      Q*”      is      often      considered      preferable      because      it      can      impose      an  \\n \\n  SDE      that      navigates      efficiently      from   \\n \\n0      to      7’      (e.g.,   \\n \\na      Brownian      bridge      with      minimal      noise).\\nThis      can  \\n \\n  expedite      the      learning      process      since      we      lack      direct      control      over      the      time-reversal      SDE      in      score-based  \\n \\n  diffusion      models.</p><h10>9.3.      Connection      with      RL-Based      Fine-Tuning</h10><p>ning  \\n \\n  RL-based      fine-tuning      shares   \\n \\na      close      relationship      with      flow-based      training.\\nTo      illustrate      this,      consider  \\n \\n  the      reference      SDE      (from      7’      to      0)      following      the      pre-trained      diffusion      model.\\nThen,      akin      to      flow-  \\n \\n  based      training,      we      consider      the      coupling      of      the      reference      SDE      conditioned      on      state      at      time   \\n \\n0       (i.e.,      distribution      over      xo-r      conditioned      on      x9)      and      the      target      distribution      at      time   \\n \\n0      (i.e.,      p,(-)   \\n \\n«       exp(r(-)/a)p?\\n\"°(-)),      denoting      the      induced      distribution      as      Q*™.</p><p>Informal      characterization      of      Q™°*.\\nInformally,      Q*\"°      is      introduced      as      follows.\\nWhen      discretizing  \\n \\n  the      time      step,      by      denoting      the      distribution      induced      by      the      reference      SDE      as      p™\\\\\\\\\\\\\\'(xp,-+-   \\n \\n,      9),      we define Qi\"      (xr,   \\n \\na   \\n \\n,      0)   \\n \\n=      p(x,      nr      Ly   \\n \\n|      L0)Pr      (Xo).</p><p>The      distribution      Q*°      above      is   \\n \\na      limiting      distribution      obtained      by      making      the      discretization      step small.</p><p>Training.\\nNow,      we      introduce   \\n \\na      new      SDE      parameterized      by   \\n \\n@      such      as      an      neural      network      to      align  \\n \\n  the      induced      distribution      P’      with      Q®\"*.\\nActually,      in      the      continuous      formulation,      the      RL-problem      (18) in      Section      4.2      is      equal      to      solving argmin      KL(P?||Q\"™).\\n \\n \\n  0 This      formalization      has      been      presented      in      Uehara      et      al.\\n(2024).\\nIntuitively,      as      we      see      in      Theorem      3,  \\n \\n  this      is      expected      because      we      see      that      the      posterior      distribution      induced      by      pre-trained      models:</p><p>ref   \\n \\n(      PD      (ar,-++      21   \\n \\n|      to)      (=      vey      (er|er_-1)p      Pp      (ar-1   \\n \\n|      er_2)      +++      pp\"      (x1   \\n \\n|      20), remains      preserved      after      fine-tuning      based      on      the      RL-formulation      (18).</p><p> \\n \\n  Comparison      with      flow-based      training.\\nIn      contrast      to      flow-based      training,      RL-based      fine-tuning  \\n \\n  minimizes      the      inverse      KL      divergence      rather      than      the      KL      divergence.\\nIntuitively,      this      is      because,  \\n \\n  unlike      Qt™*      or      Q*>   \\n \\n,      we      cannot      sample      from      Q*”      (recall      that   \\n \\na      marginal      distribution      at      time   \\n \\n0      in  \\n \\n  Qi,      i.e.,      p,,      is      unnormalized);      on      the      other      hand,      the      marginal      distributions      at      time   \\n \\n0      in      both      Q\\\\\\\\\\\\\\'™*  \\n \\n  and      Q*°°>      are      data      distributions.</p><p> \\n \\n  Remark   \\n \\n7      (Extension      to      f-divergence).\\nEach      of      these      methods      can      be      technically      extended      by  \\n \\n  incorporating      more      general      divergences      beyond      KL      divergence.\\nFor      instance,      in      the      context      of  \\n \\n  fine-tuning,      please      see      Tang      (2024).</p><p>10      Connection      with      Sampling      from      Unnormalized      Distributions</p><p> \\n \\n  Numerous      studies      delve      into      sampling      from      an      unnormalized      distribution      proportional      to      exp(r()),  \\n \\n  commonly      known      as      the      Gibbs      distribution,      extensively      discussed      in      the      computational      statistics  \\n \\n  and      statistical      physics      community      (Robert,      2014).\\nThis      issue      is      pertinent      to      our      work,      as      during  \\n \\n  fine-tuning,      the      target      distribution      is      also      formulated      in      the      Gibbs      distribution      form      in      (10),      which      is  \\n \\n  proportional      to      exp(r(a))p?\\n\"*      (x).</p><p> \\n \\n  The      literature      employs      two      main      approaches      to      tackle      this      challenge:      Markov      Chain      Monte  \\n \\n  Carlo      (MCMC)      and      RL-based      methods.\\nIn      particular,      the      RL-based      diffusion      model      fine-tuning  \\n \\n  we      have      discussed      so      far      is      closely      related      to      the      latter      approach.\\nIn      this      section,      we      explain      and       compare      these      two      approaches.</p><h10>10.1      Markov      Chain      Monte      Carlo      (MCMC)</h10><p>CMC)  \\n \\n  In      MCMC,   \\n \\na      Markov      chain      is      constructed      to      approximate   \\n \\na      target      distribution      such      that      the      equilib-  \\n \\n  rium      distribution      of      the      Markov      chain      converges      to      the      target      distribution.\\nHowever,      dealing      with  \\n \\n  high-dimensional      domain      spaces      is      challenging      for      MCMC.\\n \\n \\nA      common      strategy      involves      leveraging  \\n \\n  gradient      information,      such      as      the      Metropolis-adjusted      Langevin      algorithm      (MALA)      (Besag,      1994)  \\n \\n  or      Hamiltonian      Monte      Carlo      (HMC)      (Neal      et      al.,      2011;      Girolami      and      Calderhead,      2011).\\nMALA  \\n \\n  shares      similarities      with      classifier      guidance      and      value-weighted      sampling,      as      both      methods      depend  \\n \\n  on      the      first-order      information      from      reward      (or      value)      functions.</p><p> \\n \\n  Can      we      use      MCMC      for      fine-tuning      diffusion      models?\\nIn      the      context      of      fine-tuning      dif-  \\n \\n  fusion      models,      while      it      may      seem      intuitive      to      sample      from      the      target      distribution      (10)      (i.e.,  \\n \\n  Dy   \\n \\n&      exp(r(x))pP\"*(x))      using      MCMC,      it’s      not      straightforward      for      the      following      reasons:</p><p>¢\\n \\n   We      lack      of      an      analytical      form      of      p?\\n\"*(-).<ul><li>*\\n \\n   Additionally,      even      if      we      can      estimate      p?\\n’°(-)      in      an      unbiased      manner,      as      in      Chen      et      al.\\n(2018),  \\n \\n  the      mixing      time      for      obtaining   \\n \\na      single      sample      in      MCMC      might      be      lengthy.</li></ul></p><p> \\n \\n  Therefore,      we      explore      creating   \\n \\na      generative      model      (i.e.,      diffusion      model)      for      this      task      using      RL-  \\n \\n  based      fine-tuning      so      that      we      can      easily      sample      from      p,(-)      during      inference      (i.e.,      simulating      the  \\n \\n  time-reversal      SDE      (2)      with      policies      from   \\n \\n¢   \\n \\n=      T\\\\\\\\\\\\\\'+   \\n \\n1      to      t   \\n \\n=      1),      without      relying      on      MCMC.\\nThis  \\n \\n  addresses      the      concern      of      MCMC      by      avoiding      the      need      to      estimate      p?\\n\"*(-)      and      minimizing      inference  \\n \\n  time,      albeit      at      the      expense      of      increased      training      time.</p><p>10.2}      RL-Based      Approaches  \\n \\n  While      MCMC      has      historically      been      widely      used      for      this      purpose,      recent      works      have      proposed      an  \\n \\n  RL-based      approach      or      its      variant      (e.g.,      Bernton      et      al.      (2019);      Heng      et      al.\\n(2020);      Huang      et      al.\\n(2023);</p><p> \\n \\n  Vargas      et      al.\\n(2023)).\\nFor      example,   \\n \\na      seminal      study      (Zhang      and      Chen,      2021)      introduced   \\n \\na      method  \\n \\n  very      similar      to      reward      backpropagation.\\nIn      their      work,      they      initially      define      Brownian      motion      as   \\n \\na       reference      SDE      (from      7’      to      0),      similar      to      the      pre-trained      diffusion      models      in      our      context.\\nThen,      by  \\n \\n  appropriately      setting      rewards,      Zhang      and      Chen      (2021)      aims      to      learn   \\n \\na      new      SDE      (from      T\\\\\\\\\\\\\\'      to      0)      such  \\n \\n  that      we      can      sample      from      the      target      distribution      at      the      endpoint      0.</p><p> \\n \\n  These      approaches      offer      advantages      over      MCMC      due      to      their      ability      to      minimize      inference      time  \\n \\n  (amortization)      by      constructing   \\n \\na      generative      model      comprising      policies      in      the      training      time      and      solely  \\n \\n  executing      learned      policies      during      inference.\\nUnlike      MCMC,      these      methods      significantly      reduce      the  \\n \\n  inference      cost      as      we      don’t      need      to      be      concerned      about      the      mixing      time.\\nAdditionally,      RL-based  \\n \\n  approaches      have      the      potential      to      effectively      handle      high-dimensional,      complex      (i.e.,      multi-modal)  \\n \\n  distributions      by      harnessing      the      considerable      expressive      power      arising      from      latent      states.</p><p> \\n \\n  Despite      these      advantages,      it      is      often      unclear      how      to      select   \\n \\na      reference      SDE      in      these      works.\\nIn  \\n \\n  contrast,      in      RL-based      fine-tuning      of      diffusion      models,      we      directly      leverage      pre-trained      diffusion  \\n \\n  models      as      the      reference      SDEs,      i.e.,      as      our      prior      knowledge.</p><p> \\n \\n  Key      message.\\nFinally,      it      is      worth      noting      that      we      can      technically      utilize      any      off-the-shelf      RL      algo-  \\n \\n  rithms      for      training.\\nFor      example,      Zhang      and      Chen      (2021)      employs      reward      backpropagation,      while  \\n \\n  works      in      Gflownets      typically      leverage      PCL,      as      we      mention      in      Section      6.3.\\nHowever,      additionally,      it  \\n \\n  is      possible      to      use      PPO      or      reward-weighted      MLE      mentioned      in      Section      4.2.</p><p>11      Closely      Related      Directions</p><p>Lastly,      we      highlight      several      closely      related      directions      we      have      not      yet      discussed.</p><p> \\n \\n  Aligning      text-to-image      models.\\nWe      recognize      that      current      efforts      in      fine-tuning      diffusion      models  \\n \\n  primarily      revolve      around      aligning      text-to-image      models      using      human      feedback      (Lee      et      al.,      2023;  \\n \\n  Wallace      et      al.,      2023;      Wu      et      al.,      2023;      Yang      et      al.,      2023).\\nIn      this      paper,      our      goal      is      to      consider   \\n \\na      more  \\n \\n  general      and      fundamental      formulation      that      is      not      tailored      to      any      particular      task.</p><p> \\n \\n  Diffusion      models      for      RL.\\nDiffusion      models      are      well-suited      for      specific      reinforcement      learning  \\n \\n  applications      due      to      their      ability      to      model      complex      and      multimodal      distributions      as      polices      (Janner  \\n \\n  et      al.,      2022;      Ajay      et      al.,      2023;      Wang      et      al.,      2022;      Hansen-Estruch      et      al.,      2023;      Du      et      al.,      2024;      Zhu  \\n \\n  et      al.,      2023).\\nFor      example,      Wang      et      al.\\n(2022)      use      conditional      diffusion      models      as      policies      and       demonstrate      their      effectiveness      in      typical      offline      RL      benchmarks.\\nWhile      RL-based      fine-tuning      also  \\n \\n  aims      to      maximize      certain      reward      functions,      the      primary      focus      of      this      tutorial      covers      methods      that  \\n \\n  leverage      pre-trained      diffusion      models      for      this      purpose.</p><p> \\n \\n  RL      from      human      feedback      (RLHF)      for      language      models.\\nRLHF      is      widely      discussed      in      the  \\n \\n  context      of      language      models,      where      many      algorithms      share      similarities      with      those      used      in      diffusion  \\n \\n  models      (Ouyang      et      al.,      2022;      Bai      et      al.,      2022;      Casper      et      al.,      2023).\\nFor      instance,      both      domains  \\n \\n  commonly      employ      PPO      as   \\n \\na      standard      approach,      and      reward-weighted      training      (or      reward-weighted  \\n \\n  MLE)      often      serves      as   \\n \\na      basic      baseline      approach.\\nDespite      these      similarities,      nuanced      differences  \\n \\n  exist      mainly      because      diffusion      models      typically      operate      in   \\n \\na      continuous      input      space      rather      than  \\n \\n  a      discrete      one.\\nMoreover,      techniques      like      value-weighted      sampling      emerge      uniquely      within      the  \\n \\n  context      of      diffusion      models.</p><p>12      Summary</p><p> \\n \\n  In      this      article,      we      comprehensively      explain      how      fine-tuning      diffusion      models      to      maximize      down-  \\n \\n  stream      reward      functions      can      be      formalized      as   \\n \\na      reinforcement      learning      (RL)      problem      in      entropy-  \\n \\n  regularized      Markov      decision      processes      (MDPs).\\nBased      on      this      viewpoint,      we      elaborate      on      the  \\n \\n  application      of      various      RL      algorithms,      such      as      PPO      and      reward-weighted      MLE,      specifically      tailored  \\n \\n  for      fine-tuning      diffusion      models.\\nAdditionally,      we      categorize      different      scenarios      based      on      how  \\n \\n  reward      feedback      is      obtained.\\nWhile      this      categorization      is      not      always      explicitly      mentioned      in      many  \\n \\n  existing      works,      it      is      crucial      when      selecting      appropriate      algorithms.\\nFinally,      we      discuss      the      rela-  \\n \\n  tionships      with      several      related      topics,      including      classifier      guidance,      Gflownets,      path      integral      control  \\n \\n  theory,      and      sampling      from      unnormalized      distributions.</p><h5>Value-weighted      sampling      (Sec.      6.2)</h5><p>Figure      3:      Practical      recommendation      of      RL-based      algorithms      to      optimize      downstream      reward  \\n \\n  functions.</p><h6>7.1      Fine-Tuning      with      Known,      Differentiable      Reward      Functions</h6><p>ions  \\n \\n  When      accurate      differentiable      reward      functions      are      available      (e.g.,      standard      in      computer      vision      tasks),  \\n \\n  our      primary      focus      is      typically      on      computational      and      memory      efficiency.\\nIn      general,      we      advocate      for  \\n \\n  employing      reward      backpropagation      (c.f.      Algorithm      2)      for      its      computational      efficiency.\\nAlternatively,  \\n \\n  if      memory      efficiency      is   \\n \\na      significant      concern,      we      suggest      using      PPO      due      to      its      stability.</p><h6>7.2      Fine-Tuning      with      Black-Box      Reward      Feedback</h6><p>back  \\n \\n  Here,      we      explore      scenarios      where      we      have      access      to      accurate      but      non-differentiable      (black-box)  \\n \\n  reward      feedback,      often      found      in      scientific      simulations.\\nThe      emphasis      still      remains      primarily      on  \\n \\n  computational      and      memory      efficiency.\\nIn      such      scenarios,      due      to      the      non-differentiability      of      reward  \\n \\n  feedback,      we      recommend      using      PPO      or      reward-weighted      MLE      for      the      following      reasons.</p><p> \\n \\n  In      the      preceding      section,      we      advocated      for      the      use      of      reward      backpropagation      due      to      its      com-  \\n \\n  putational      advantages.\\nHowever,      when      dealing      with      non-differentiable      feedback,      the      advantage  \\n \\n  of      reward      backpropagation      may      diminish.\\nThis      is      because      learning      from      such      feedback      requires  \\n \\n  learning      differentiable      reward      functions      to      make      the      algorithm      work      as      we      discuss      in      Section      5.2.  \\n \\n  However,      obtaining   \\n \\na      differentiable      reward      function      can      be      challenging      in      many      domains.\\nFor  \\n \\n  example,      in      molecular      property      prediction,      molecular      fingerprints      are      informative      features,      and       accurate      reward      functions      can      be      derived      by      training      simple      neural      networks      on      these      features  \\n \\n  (Pattanaik      and      Coley,      2020).\\nYet,      these      mappings,      grounded      in      prior      scientific      understanding,      are  \\n \\n  non-differentiable,      thereby      restricting      their      applicability.</p><p> \\n \\n  In      contrast,      PPO      and      reward-weighted      MLE      allow      for      policy      updates      without      the      explicit  \\n \\n  requirement      to      learn   \\n \\na      differentiable      reward      function,      offering   \\n \\na      significant      advantage      over      reward  \\n \\n  backpropagation.</p><h6>7.3      Fine-Tuning      with      Unknown      Rewards      Functions</h6><p>ions  \\n \\n  When      reward      functions      (or      computational      proxies      in      Section      7.2)      are      unavailable,      we      must      learn  \\n \\n  from      data      with      reward      feedback.\\nIn      such      cases,      compared      to      the      previous      scenarios,      two      important  \\n \\n  considerations      arise:</p><p>¢\\n \\n   Not      only      computational      or      memory      efficiency      but      also      feedback      efficiency      (i.e.,      sample  \\n \\n  efficiency      regarding      reward      feedback)      is      crucial.</p><p> \\n \\n \\n¢      Given      that      learned      reward      feedback      may      not      generalize      well      outside      the      training      data      distri-  \\n \\n  butions,      it      is      essential      to      constrain      fine-tuned      models      to      prevent      significant      divergence      from  \\n \\n  diffusion      models.\\nIn      these      situations,      not      only      soft      PPO      and      direct      backpropagation      but      also  \\n \\n  methods      discussed      in      Section      6,      such      as      reward-weighted      MLE,      can      be      particularly      effective.</p><p>Further,      the      scenario      involving      unknown      reward      functions      can      be      broadly      categorized      into      two      cases,  \\n \\n  which      we      will      discuss      in      more      detail.</p><p>Offline      scenario      (Figure      4).\\nIn      many      scenarios,      even  \\n \\n  in      cases      where      reward      functions      are      unknown,      we      of-  \\n \\n.     .</p><p>Offline      data      with ten      have      access      to      offline      data      with      reward      feedback      reward      feedback</p><li>(i)      (i)   \\n \\nO   \\n \\n;   \\n \\ni       a”      ra  \\n \\n.      ne      straightforward      approach      is      to</li><p> \\n \\n \\n{   \\n \\n,   \\n \\n(   \\n \\n)    \\n.\\n    .\\n \\n   g      PP  \\n \\n.      <=>      Learning      reward  \\n \\n  perform      regression      with      neural      networks      and      build   \\n \\na      models  \\n \\n  learned      regressor      *.\\nHowever,      this      learned      reward      func-      SS  \\n \\n  tion   \\n \\n*      might      not      generalize      well      beyond      the      distribution      x)  \\n \\n  of      the      offline      data.\\nIn      regions      outside      this      distribution,   \\n \\n7       could      assign   \\n \\na      high      value      even      when      the      actual      reward  \\n \\n  r      is      low      due      to      high      uncertainty.\\nConsequently,      we  \\n \\n  could      be      easily      misled      by      out-of-distribution      samples  \\n \\n  if      we      solely      optimize      7.\\n|Fine-tuning      diffusion      models   \\n \\n|       One      approach      to      alleviate      this      issue      is      to      adopt   \\n \\na       pessimistic      (i.e.,      conservative)      strategy,      as      proposed      in      Figure      4:      Offline      scenario  \\n \\n  Uehara      et      al.\\n(2024).\\nThis      technique      has      been      widely      Collecting      data]   \\n \\nY      Learning      reward  \\n \\n  applied      in      the      field      of      offline      RL      (Levine,      2018).\\nThe      aa  \\n \\n  main      idea      is      to      penalize   \\n \\n7      outside      the      distributions      us-  \\n \\n  ing      techniques      such      as      adding      an      explicit      penalty      term  \\n \\n  (Yu      et      al.,      2020;      Chang      et      al.,      2021),      bootstrapping,  \\n \\n  or      more      sophisticated      methods      (Kumar      et      al.,      2020;  \\n \\n  Xie      et      al.,      2021;      Rigter      et      al.,      2022;      Uehara      and      Sun,  \\n \\n  2021).\\nBy      doing      so,      we      can      avoid      being      fooled      by  \\n \\n  out-to-distribution      regions      while      still      benefiting      from  \\n \\n  the      extrapolation      capabilities      of      reward      models.</p><p>S—      diffusion      models      |</p><p>Figure      5:      Online      scenario</p><p> \\n \\n  Online      scenario      (Figure      5).\\nConsider      situations  \\n \\n  where      reward      functions      are      unknown,      but      we      can      gather      data      online.\\nThese      scenarios      are      often  \\n \\n  referred      to      as   \\n \\na      lab-in-the-loop      setting.</p><p> \\n \\n  In      such      scenarios,      Uehara      et      al.\\n(2024)      proposes      an      iterative      procedure      comprising      three      steps:</p><li>(1)      collecting      feedback      data      by      exploring      new      areas      (i.e.,      obtaining   \\n \\nx      from      current      diffusion      models  \\n \\n  and      corresponding      r(x)),      (2)      learning      the      reward      function      from      the      collected      feedback      data,      and</li><li>(3)      fine-tuning      current      diffusion      models      by      maximizing      the      learned      reward      function      enhanced      by  \\n \\n  an      optimistic      bonus      term,      using      algorithms      discussed      in      Section      4.2.\\nIn      contrast      to      standard      online  \\n \\n  PPO,      where      feedback      data      is      directly      used      in      the      fine-tuning      process,      actual      reward      feedback      is  \\n \\n  only      used      in      step      (2)      and      not      in      step      (3).\\nAn      additional      key      aspect      is      the      use      of      an      optimistic  \\n \\n  reward      function      that      encourages      exploration      beyond      the      distributions      of      the      current      diffusion      model.\\n \\n \\n  This      deliberate      separation      between      reward      learning      and      fine-tuning      steps,      coupled      with      the      use      of  \\n \\n  optimism,      significantly      enhances      feedback      efficiency.</li><p>8\\n \\n   Connection      with      Classifier      Guidance</p><p> \\n \\n  Classifier      guidance      (Dhariwal      and      Nichol,      2021)      is      commonly      used      for      conditional      generation      in  \\n \\n  diffusion      models.\\nInterestingly,      RL-based      methods      share   \\n \\na      close      connection      with      classifier      guidance.\\n \\n \\n  More      specifically,      in      this      section,      following      Zhao      et      al.\\n(2024),      we      elucidate      how      RL-based      methods  \\n \\n  can      be      applied      to      conditional      generation.\\nFurthermore,      from      this      unified      viewpoint,      we      highlight  \\n \\n  that      classifier      guidance      is      seen      as   \\n \\na      value-weighted      sampling      in      Section      4.2.</p><h6>8.1      Classfier      Guidance</h6><p>ance  \\n \\n  In      this      section,      we      first      introduce      classifier      guidance.\\nClassifier      guidance      utilizes      an      unconditional  \\n \\n  pre-trained      model      for      conditional      generation.\\nMore      specifically,      we      consider   \\n \\na      scenario      where      we  \\n \\n  have   \\n \\na      pre-trained      diffusion      model,      which      enables      us      to      sample      from      p?\\n\"*(x),      and      data      with      feedback  \\n \\n  {x,y},      where   \\n \\ny      represents      the      new      label      we      want      to      condition      on.\\nOur      objective      here      is      to      sample  \\n \\n  from      p(-|y)   \\n \\n€      [Y   \\n \\n>      ACR), P(-   \\n \\n|      y)   \\n \\nx      py(yl-)pP*(-).\\n(37)</p><p>where      p,   \\n \\n:   \\n \\nX      —>      A(Y)      denotes      the      conditional      distribution      of   \\n \\ny      given      x.</p><p>Motivation.\\nIn      classifier      guidance,      we      consider      the      following      policy:</p><p>pe      (-lare,      y)   \\n \\n=   \\n \\nN      (p(22,      t;      Opre)   \\n \\n+      a(t)      Ve      log      Vy      (Xt;      t),      a*(t))   \\n \\n’ dy(x,t)   \\n \\n=      Egy.\\nyap,      (-leo)      L(Y   \\n \\n=      y)|a,   \\n \\n=      2].</p><p>Under      the      continuous-time      formulation      of      diffusion      models,      applying      Doob’s      h-transform      (Rogers  \\n \\n  and      Williams,      2000),      it      is      shown      that      we      can      sample      from      p(x|y)      by      sequentially      running      {p7\"\\\\\\\\\\\\\\'(-|7+,      y)      }i-r41-</p><p>Algorithm.\\nSpecifically,      the      algorithm      comprises      two      steps.\\nIt’s      important      to      note      that      this      is      not  \\n \\n  a      fine-tuning      method      but      rather      an      inference-time      technique      as      we      freeze      the      pre-trained      model.</p><li>1. Learning      q,(x,t)      through      regression      by      gathering      data      containing      {z;,      y}.\\nHere,      for      each  \\n \\n  (Xo,      y),      using   \\n \\na      forward      policy      for      the      pre-training      (from   \\n \\n0      to      T),      we      obtain      2;      starting      from</li><p>Lo.</p><li>2. During      inference,      sequentially      execute      {p2\"(-|a,,      y)      }1_>   \\n \\n1      by      leverage      the      fact      that      compared</li><p>gui to      the      policy      in      pre-trained      diffusion      models,      the      mean      of      {p?\\n\"\\\\\\\\\\\\\\'(-|71,      y)      }i-74,      is      just      shifted  \\n \\n  with      an      additional      term      V,,      log      q,(2,      t).</p><p> \\n \\n  Notably,      the      method      described      above      can      be      reduced      to      value-weighted      sampling      (cf.      Sec-  \\n \\n  tion      6.2),      when      r(x,      y)   \\n \\n=      log      p(y|x).\\nInspired      by      such      an      observation,      we      apply      an      RL-based  \\n \\n  fine-tuning      method      for      conditional      generation      as      below.</p><h7>8.2.      RL-Based      Fine-Tuning      for      Conditional      Generation</h7><p>tion  \\n \\n  In      this      section,      following      Zhao      et      al.\\n(2024),      we      explain      an      RL-based      fine-tuning      approach      for  \\n \\n  conditional      generation.\\nThe      core      idea      is      that      setting      r(x,      y)   \\n \\n=      log      p(y|x)      allows      us      to      sample      from  \\n \\n  the      target      conditional      distribution      (37),      drawing      on      the      insights      discussed      in      Section      4.2.  \\n \\n  More      precisely,      introducing   \\n \\na      distribution      over   \\n \\ny      as      q(-),      we      formulate      the      following      RL      problem:</p><p>{Di      ft   \\n \\n=      argmax      Egy,      (-\\\\\\\\\\\\\\\\-y)}.yott      log      p(y|xo)   \\n \\n—      NeopyaKL(pe(-|e,      y)      ||P      (late).\\n \\n \\n  {prE[(V,.X)PA(A)]      Heyy (38)</p><p> \\n \\n  Compared      to      the      objective      function      (18)      in      Section      4.2,      the      main      differences      are:      (1)      policy      space  \\n \\n  is      expanded      to      accommodate      additional      control      (i.e.,      y):      pz(-|-,-)      C      [R¢   \\n \\nx   \\n \\nY   \\n \\n+      A(R®)],      (2)      the  \\n \\n  fine-tuning      objectives      are      set      as      log-likelihoods:      log      p(y|z).\\nThen,      following      Theorem      1,      we      can  \\n \\n  establish      the      following      result.</p><p>Corollary      2.\\nThe      distribution      induced      by      {p*}1_74      (ue.   \\n \\n[      {Teri      Ph      (ae-1   \\n \\n|      te,      y)}deur      is      the target      conditional      distribution      (37).</p><p> \\n \\n  The      above      corollary      suggests      that      for      conditional      generation,      we      can      utilize      various      off-the-shelf  \\n \\n  algorithms      discussed      in      Section      4.2      and      6,      such      as      PPO,      reward      backpropagation,      and      reward-  \\n \\n  weighted      MLE,      as      well      as      value-weighted      sampling.</p><p>9\\n \\n   Connection      with      Flow-Based      Diffusion      Models</p><p> \\n \\n  We      elucidate      the      close      relationship      between      bridge      matching      and      fine-tuning,      as      summarized      in  \\n \\n  Table      2.\\nBridge      (flow)      matching      (Liu      et      al.,      2022;      Tong      et      al.,      2023;      Lipman      et      al.,      2023;      Liu      et      al.,  \\n \\n  2022;      Shi      et      al.,      2024;      Albergo      and      Vanden-Eijnden,      2022),      has      recently      gained      popularity      as   \\n \\na       distinct      type      of      diffusion      model      from      score-matching-based      ones.\\nThese      works      suggest      that      training  \\n \\n  and      inference      speed      will      be      accelerated      due      to      the      ability      to      define   \\n \\na      more      direct      trajectory      from  \\n \\n  noise      to      data      distribution      than      score-matching-based      diffusion      models.</p><p> \\n \\n  The      connection      between      bridge      matching      and      fine-tuning      becomes      evident      in      the      continuous-  \\n \\n  time      formulation.\\nTo      begin,      we      first      provide   \\n \\na      brief      overview      of      the      continuous-time      formulation      of  \\n \\n  score-matching-based      diffusion      models.</p><p> \\n \\n  Table      2:      Comparison      between      training      score/flow-based      diffusion      models      and      fine-tuning.\\nWe  \\n \\n  optimize      the      parameter   \\n \\n6      in      P’,      which      represents      the      induced      distribution      over      trajectories      associated  \\n \\n  with      6.</p><h8>Loss      Note      of      Q</h8><p>Score-based      KL(Q\\\\\\\\\\\\\\'™¢|P?)      Qtime:      Time-reversal      of      reference      SDE      (reference training      SDE      is      defined      from   \\n \\n0      to      T’      where      the      initial      dis-  \\n \\n  tribute      at   \\n \\n0      is      data      distribution)</p><p> \\n \\n  Flow-based      KL(Q*°>      ||P?)      @Q*°°h:      Coupling      between      the      reference      SDE      from  \\n \\n  training      T      to   \\n \\n0      conditioning      at   \\n \\n0      (dist.      over      x.;|7o)      and    \\n \\na      data      distribution      at      time   \\n \\n0      (dis.      over      x).\\nNote  \\n \\n  that      reference      SDE      is      defined      from      7’      to      0.</p><p> \\n \\n  Fine-tuning      KL(P?||Q*°)      @*:      Coupling      between      the      pre-trained      SDE  \\n \\n  from      T      to   \\n \\n0      conditioned      on   \\n \\n0      (dist.      over      x7.1|X0)  \\n \\n  and   \\n \\na      data      distribution      at      time   \\n \\n0      (dis.      over      Xo) 9.1      Continuous      Time      Formulation      of      Score-Based      diffusion      models</p><p> \\n \\n  We      provide   \\n \\na      brief      overview      of      the      continuous-time      formulation      of      diffusion      models      (Song      et      al.,  \\n \\n  2021),      further      expanding      Section      1.1.1.\\nIn      this      formulation,      we      initially      define      the      forward      reference  \\n \\n  SDE,      spanning      from      time   \\n \\n0      to      7’      (e.g.,      variance      exploding      (VE)      process      or      variance      preserving      (VP)  \\n \\n  process),      by      setting      an      initial      distribution      at   \\n \\n0      as      the      data      distribution.\\nAdditionally,      the      reference  \\n \\n  SDE      is      tailored      to      converge      to      an      easy      distribution      at      time      7’,      such      as      the      normal      distribution.</p><p> \\n \\n  Subsequently,      we      consider      the      time-reversal      SDE      (Anderson,      1982),      progressing      from      T\\\\\\\\\\\\\\'      to      0.\\nIf  \\n \\n  we      could      learn      this      time-reversal      SDE,      it      would      enable      us      to      sample      from      the      data      distribution      by  \\n \\n  starting      with      the      easy      initial      distribution      (at      7’)      and      following      the      time-reversal      SDE      from      T      to   \\n \\n0      at  \\n \\n  inference      time.</p><p> \\n \\n  Training.\\nNow,      the      remaining      question      is      how      to      learn      this      time-reversal      SDE.\\nHere,      by      repre-  \\n \\n  senting      the      induced      distribution      over      trajectories      from      T      to   \\n \\n0      with      this      time-reversal      SDE      as      Q\\\\\\\\\\\\\\'™®,  \\n \\n  our      goal      is      to      train   \\n \\na      new      SDE,      parameterized      by   \\n \\n@      in      the      neural      network,      such      that      the      induced  \\n \\n  distribution      P®      over      trajectories      associated      with   \\n \\n6      aligns      with      Q\"™*.\\nSpecifically,      the      objective      is      to minimize      the      following      losses:</p><p> \\n \\n  argmin      KL(Q\"™||P*).\\n(39)  \\n \\n  0  \\n \\n  With      some      algebra,      leveraging      the      observation      that      the      time-reversal      SDE      comprises      the      original  \\n \\n  forward      SDE      and      the      scoring      term      of      the      marginal      distribution      (Anderson,      1982)      as      in      (2),      Song  \\n \\n  et      al.\\n(2021)      demonstrates      that      this      KL      loss      (39)      is      approximated      as Dore   \\n \\n=      argmin      Eeuni((0,7))01~qyo0(weleo)co~pP\"e      Fe      Fl      Vey      log      jo      (Xt   \\n \\n|      Xo)   \\n \\n_      so(Xt,      t)|I?\\n],      (40)  \\n \\n  which      is      equal      to      (4)      where      the      weight      A(t)      is      {o?\\n}?\\n(for      notation,      refer      to      Section      1.1.1).</p><h9>9.2      Flow-Based      (Bridge-Based)      Diffusion      Models</h9><p>dels We      adopt      the      explanation      outlined      in      Liu      et      al.\\n(2022).\\nIn      this      framework,      we      begin      by      considering  \\n \\n  a      reference      SDE      (from      T\\\\\\\\\\\\\\'      to      0),      x;   \\n \\n=      zr_;      where      z;      is      defined      as      Brownian      motion:</p><p>dz,   \\n \\n=      dwt,      te      (0,      7).\\n(41)</p><p> \\n \\n  Subsequently,      we      consider      the      coupling      of      the      reference      SDE      conditioned      on      state      at      time   \\n \\n0      (i.e.,  \\n \\n  distribution      of      xo.7      conditional      on      x)      and      the      data      distribution      at      time      0,      denoting      the      resulting  \\n \\n  distribution      as      Q*°?,      Informally,      this      Q*°°?\\nis      characterized      as      follows.</p><p>Informal      characterization      of      Q*”.\\nWhen      discretizing      the      time      step,      by      denoting      the      distribution  \\n \\n  induced      by      the      reference      SDE      as      p\"(x7,--+   \\n \\n,      29),      we      define QF      (a7,      tae   \\n \\n,      20)   \\n \\n=      p(x,      ay   \\n \\n|      r9)pP\"°      (x9).</p><p>The      distribution      Q*”      above      is   \\n \\na      limiting      distribution      obtained      by      making      the      discretization      step small.</p><p>Training.\\nIn      bridge-based      diffusion      models,      our      objective      is      to      train   \\n \\na      new      SDE,      parametrized      by  \\n \\n  6      (e.g.,   \\n \\na      neural      network),      to      align      the      induced      distribution      P®      with      Q*°?.\\nSpecifically,      bridge-based training      aims      to      minimize aremin      KL(Q*>||P’).\\n(42)  \\n \\n  0  \\n \\n  While      the      method      to      minimize      the      above      is      currently      unclear,      we      can      derive   \\n \\na      trainable      objective  \\n \\n  through      algebraic      manipulation.\\nInitially,      we      observe      that      the      reference      SDE      (41),      conditioned      on  \\n \\n  state      xo      at      time   \\n \\n0      (zr   \\n \\n=      D),      is      expressed      as:</p><p>b\\n \\n \\n  b—      2;</p><p>t   \\n \\n€      [0,T];dz?\\n \\n \\n=      g?(z?,t)dt      +du,,  \\n \\n  g(z?,t)   \\n \\n=   \\n \\na       (43)</p><p>using      the      seminal      “Doob’s      h-transform”      (Rogers      and      Williams,      2000).\\nThis      SDE      is      commonly  \\n \\n  known      as      the      Brownian      bridge.\\nNow,      we      consider   \\n \\na      new      SDE      parameterized      by      0:</p><p>t   \\n \\n€      [0,7];      dz   \\n \\n=      s(z,t;      @)dt   \\n \\n+      dw.\\n(44)</p><p>Then,      we      can      demonstrate      that      minimizing      the      KL      loss      in      (42)      is      equivalent      to argmin      E,W      unil0,7],1~4\"°      cop?\\n[Ilg”      (2,      t)   \\n \\n_      s(21,      t;      0)      M3]      (45)</p><p>where      q;°      denotes      the      induced      distribution      at      time      t      following      SDE      (43)      conditioned      on      xo,      and       s:      Xx      |0,T]   \\n \\n>   \\n \\n&      is      the      model      we      aim      to      optimize.</p><p> \\n \\n  Flow-based      diffusion      models.\\nThe      aforementioned      formulation      has      been      originally      proposed      as   \\n \\na       bridge-based      SDE      (Liu      et      al.,      2022)      because      the      conditional      SDE      is      often      referred      to      as   \\n \\na      Brownian  \\n \\n  bridge.\\nFlow-based      diffusion      models      proposed      in      Lipman      et      al.\\n(2023);      Tong      et      al.\\n(2023)      share   \\n \\na       close      relationship      with      bridge-based      diffusion      models.\\nThese      flow-based      models      use   \\n \\na      target      SDE (43)      and      an      SDE      with   \\n \\na      parameter      (44)      without      stochastic      terms      (1.e.,      no      dw,)      and      aim      to      minimize  \\n \\n  the      loss      function      (45).</p><p> \\n \\n  Comparison      with      score-based      diffusion      models.\\nBoth      score-based      diffusion      models      and      bridge-  \\n \\n  based      diffusion      models      aim      to      minimize      KL      divergences.\\nDespite      its      similarity,      in      contrast      to  \\n \\n  score-based      diffusion      models,      bridge-based      diffusion      models      target      Q*°>      rather      than      the      time-  \\n \\n  reversal      SDE      Q\"™®.\\nThe      distribution      Q*”      is      often      considered      preferable      because      it      can      impose      an  \\n \\n  SDE      that      navigates      efficiently      from   \\n \\n0      to      7’      (e.g.,   \\n \\na      Brownian      bridge      with      minimal      noise).\\nThis      can  \\n \\n  expedite      the      learning      process      since      we      lack      direct      control      over      the      time-reversal      SDE      in      score-based  \\n \\n  diffusion      models.</p><h10>9.3.      Connection      with      RL-Based      Fine-Tuning</h10><p>ning  \\n \\n  RL-based      fine-tuning      shares   \\n \\na      close      relationship      with      flow-based      training.\\nTo      illustrate      this,      consider  \\n \\n  the      reference      SDE      (from      7’      to      0)      following      the      pre-trained      diffusion      model.\\nThen,      akin      to      flow-  \\n \\n  based      training,      we      consider      the      coupling      of      the      reference      SDE      conditioned      on      state      at      time   \\n \\n0       (i.e.,      distribution      over      xo-r      conditioned      on      x9)      and      the      target      distribution      at      time   \\n \\n0      (i.e.,      p,(-)   \\n \\n«       exp(r(-)/a)p?\\n\"°(-)),      denoting      the      induced      distribution      as      Q*™.</p><p>Informal      characterization      of      Q™°*.\\nInformally,      Q*\"°      is      introduced      as      follows.\\nWhen      discretizing  \\n \\n  the      time      step,      by      denoting      the      distribution      induced      by      the      reference      SDE      as      p™\\\\\\\\\\\\\\'(xp,-+-   \\n \\n,      9),      we define Qi\"      (xr,   \\n \\na   \\n \\n,      0)   \\n \\n=      p(x,      nr      Ly   \\n \\n|      L0)Pr      (Xo).</p><p>The      distribution      Q*°      above      is   \\n \\na      limiting      distribution      obtained      by      making      the      discretization      step small.</p><p>Training.\\nNow,      we      introduce   \\n \\na      new      SDE      parameterized      by   \\n \\n@      such      as      an      neural      network      to      align  \\n \\n  the      induced      distribution      P’      with      Q®\"*.\\nActually,      in      the      continuous      formulation,      the      RL-problem      (18) in      Section      4.2      is      equal      to      solving argmin      KL(P?||Q\"™).\\n \\n \\n  0 This      formalization      has      been      presented      in      Uehara      et      al.\\n(2024).\\nIntuitively,      as      we      see      in      Theorem      3,  \\n \\n  this      is      expected      because      we      see      that      the      posterior      distribution      induced      by      pre-trained      models:</p><p>ref   \\n \\n(      PD      (ar,-++      21   \\n \\n|      to)      (=      vey      (er|er_-1)p      Pp      (ar-1   \\n \\n|      er_2)      +++      pp\"      (x1   \\n \\n|      20), remains      preserved      after      fine-tuning      based      on      the      RL-formulation      (18).</p><p> \\n \\n  Comparison      with      flow-based      training.\\nIn      contrast      to      flow-based      training,      RL-based      fine-tuning  \\n \\n  minimizes      the      inverse      KL      divergence      rather      than      the      KL      divergence.\\nIntuitively,      this      is      because,  \\n \\n  unlike      Qt™*      or      Q*>   \\n \\n,      we      cannot      sample      from      Q*”      (recall      that   \\n \\na      marginal      distribution      at      time   \\n \\n0      in  \\n \\n  Qi,      i.e.,      p,,      is      unnormalized);      on      the      other      hand,      the      marginal      distributions      at      time   \\n \\n0      in      both      Q\\\\\\\\\\\\\\'™*  \\n \\n  and      Q*°°>      are      data      distributions.</p><p> \\n \\n  Remark   \\n \\n7      (Extension      to      f-divergence).\\nEach      of      these      methods      can      be      technically      extended      by  \\n \\n  incorporating      more      general      divergences      beyond      KL      divergence.\\nFor      instance,      in      the      context      of  \\n \\n  fine-tuning,      please      see      Tang      (2024).</p><p>10      Connection      with      Sampling      from      Unnormalized      Distributions</p><p> \\n \\n  Numerous      studies      delve      into      sampling      from      an      unnormalized      distribution      proportional      to      exp(r()),  \\n \\n  commonly      known      as      the      Gibbs      distribution,      extensively      discussed      in      the      computational      statistics  \\n \\n  and      statistical      physics      community      (Robert,      2014).\\nThis      issue      is      pertinent      to      our      work,      as      during  \\n \\n  fine-tuning,      the      target      distribution      is      also      formulated      in      the      Gibbs      distribution      form      in      (10),      which      is  \\n \\n  proportional      to      exp(r(a))p?\\n\"*      (x).</p><p> \\n \\n  The      literature      employs      two      main      approaches      to      tackle      this      challenge:      Markov      Chain      Monte  \\n \\n  Carlo      (MCMC)      and      RL-based      methods.\\nIn      particular,      the      RL-based      diffusion      model      fine-tuning  \\n \\n  we      have      discussed      so      far      is      closely      related      to      the      latter      approach.\\nIn      this      section,      we      explain      and       compare      these      two      approaches.</p><h10>10.1      Markov      Chain      Monte      Carlo      (MCMC)</h10><p>CMC)  \\n \\n  In      MCMC,   \\n \\na      Markov      chain      is      constructed      to      approximate   \\n \\na      target      distribution      such      that      the      equilib-  \\n \\n  rium      distribution      of      the      Markov      chain      converges      to      the      target      distribution.\\nHowever,      dealing      with  \\n \\n  high-dimensional      domain      spaces      is      challenging      for      MCMC.\\n \\n \\nA      common      strategy      involves      leveraging  \\n \\n  gradient      information,      such      as      the      Metropolis-adjusted      Langevin      algorithm      (MALA)      (Besag,      1994)  \\n \\n  or      Hamiltonian      Monte      Carlo      (HMC)      (Neal      et      al.,      2011;      Girolami      and      Calderhead,      2011).\\nMALA  \\n \\n  shares      similarities      with      classifier      guidance      and      value-weighted      sampling,      as      both      methods      depend  \\n \\n  on      the      first-order      information      from      reward      (or      value)      functions.</p><p> \\n \\n  Can      we      use      MCMC      for      fine-tuning      diffusion      models?\\nIn      the      context      of      fine-tuning      dif-  \\n \\n  fusion      models,      while      it      may      seem      intuitive      to      sample      from      the      target      distribution      (10)      (i.e.,  \\n \\n  Dy   \\n \\n&      exp(r(x))pP\"*(x))      using      MCMC,      it’s      not      straightforward      for      the      following      reasons:</p><p>¢\\n \\n   We      lack      of      an      analytical      form      of      p?\\n\"*(-).<ul><li>*\\n \\n   Additionally,      even      if      we      can      estimate      p?\\n’°(-)      in      an      unbiased      manner,      as      in      Chen      et      al.\\n(2018),  \\n \\n  the      mixing      time      for      obtaining   \\n \\na      single      sample      in      MCMC      might      be      lengthy.</li></ul></p><p> \\n \\n  Therefore,      we      explore      creating   \\n \\na      generative      model      (i.e.,      diffusion      model)      for      this      task      using      RL-  \\n \\n  based      fine-tuning      so      that      we      can      easily      sample      from      p,(-)      during      inference      (i.e.,      simulating      the  \\n \\n  time-reversal      SDE      (2)      with      policies      from   \\n \\n¢   \\n \\n=      T\\\\\\\\\\\\\\'+   \\n \\n1      to      t   \\n \\n=      1),      without      relying      on      MCMC.\\nThis  \\n \\n  addresses      the      concern      of      MCMC      by      avoiding      the      need      to      estimate      p?\\n\"*(-)      and      minimizing      inference  \\n \\n  time,      albeit      at      the      expense      of      increased      training      time.</p><p>10.2}      RL-Based      Approaches  \\n \\n  While      MCMC      has      historically      been      widely      used      for      this      purpose,      recent      works      have      proposed      an  \\n \\n  RL-based      approach      or      its      variant      (e.g.,      Bernton      et      al.      (2019);      Heng      et      al.\\n(2020);      Huang      et      al.\\n(2023);</p><p> \\n \\n  Vargas      et      al.\\n(2023)).\\nFor      example,   \\n \\na      seminal      study      (Zhang      and      Chen,      2021)      introduced   \\n \\na      method  \\n \\n  very      similar      to      reward      backpropagation.\\nIn      their      work,      they      initially      define      Brownian      motion      as   \\n \\na       reference      SDE      (from      7’      to      0),      similar      to      the      pre-trained      diffusion      models      in      our      context.\\nThen,      by  \\n \\n  appropriately      setting      rewards,      Zhang      and      Chen      (2021)      aims      to      learn   \\n \\na      new      SDE      (from      T\\\\\\\\\\\\\\'      to      0)      such  \\n \\n  that      we      can      sample      from      the      target      distribution      at      the      endpoint      0.</p><p> \\n \\n  These      approaches      offer      advantages      over      MCMC      due      to      their      ability      to      minimize      inference      time  \\n \\n  (amortization)      by      constructing   \\n \\na      generative      model      comprising      policies      in      the      training      time      and      solely  \\n \\n  executing      learned      policies      during      inference.\\nUnlike      MCMC,      these      methods      significantly      reduce      the  \\n \\n  inference      cost      as      we      don’t      need      to      be      concerned      about      the      mixing      time.\\nAdditionally,      RL-based  \\n \\n  approaches      have      the      potential      to      effectively      handle      high-dimensional,      complex      (i.e.,      multi-modal)  \\n \\n  distributions      by      harnessing      the      considerable      expressive      power      arising      from      latent      states.</p><p> \\n \\n  Despite      these      advantages,      it      is      often      unclear      how      to      select   \\n \\na      reference      SDE      in      these      works.\\nIn  \\n \\n  contrast,      in      RL-based      fine-tuning      of      diffusion      models,      we      directly      leverage      pre-trained      diffusion  \\n \\n  models      as      the      reference      SDEs,      i.e.,      as      our      prior      knowledge.</p><p> \\n \\n  Key      message.\\nFinally,      it      is      worth      noting      that      we      can      technically      utilize      any      off-the-shelf      RL      algo-  \\n \\n  rithms      for      training.\\nFor      example,      Zhang      and      Chen      (2021)      employs      reward      backpropagation,      while  \\n \\n  works      in      Gflownets      typically      leverage      PCL,      as      we      mention      in      Section      6.3.\\nHowever,      additionally,      it  \\n \\n  is      possible      to      use      PPO      or      reward-weighted      MLE      mentioned      in      Section      4.2.</p><p>11      Closely      Related      Directions</p><p>Lastly,      we      highlight      several      closely      related      directions      we      have      not      yet      discussed.</p><p> \\n \\n  Aligning      text-to-image      models.\\nWe      recognize      that      current      efforts      in      fine-tuning      diffusion      models  \\n \\n  primarily      revolve      around      aligning      text-to-image      models      using      human      feedback      (Lee      et      al.,      2023;  \\n \\n  Wallace      et      al.,      2023;      Wu      et      al.,      2023;      Yang      et      al.,      2023).\\nIn      this      paper,      our      goal      is      to      consider   \\n \\na      more  \\n \\n  general      and      fundamental      formulation      that      is      not      tailored      to      any      particular      task.</p><p> \\n \\n  Diffusion      models      for      RL.\\nDiffusion      models      are      well-suited      for      specific      reinforcement      learning  \\n \\n  applications      due      to      their      ability      to      model      complex      and      multimodal      distributions      as      polices      (Janner  \\n \\n  et      al.,      2022;      Ajay      et      al.,      2023;      Wang      et      al.,      2022;      Hansen-Estruch      et      al.,      2023;      Du      et      al.,      2024;      Zhu  \\n \\n  et      al.,      2023).\\nFor      example,      Wang      et      al.\\n(2022)      use      conditional      diffusion      models      as      policies      and       demonstrate      their      effectiveness      in      typical      offline      RL      benchmarks.\\nWhile      RL-based      fine-tuning      also  \\n \\n  aims      to      maximize      certain      reward      functions,      the      primary      focus      of      this      tutorial      covers      methods      that  \\n \\n  leverage      pre-trained      diffusion      models      for      this      purpose.</p><p> \\n \\n  RL      from      human      feedback      (RLHF)      for      language      models.\\nRLHF      is      widely      discussed      in      the  \\n \\n  context      of      language      models,      where      many      algorithms      share      similarities      with      those      used      in      diffusion  \\n \\n  models      (Ouyang      et      al.,      2022;      Bai      et      al.,      2022;      Casper      et      al.,      2023).\\nFor      instance,      both      domains  \\n \\n  commonly      employ      PPO      as   \\n \\na      standard      approach,      and      reward-weighted      training      (or      reward-weighted  \\n \\n  MLE)      often      serves      as   \\n \\na      basic      baseline      approach.\\nDespite      these      similarities,      nuanced      differences  \\n \\n  exist      mainly      because      diffusion      models      typically      operate      in   \\n \\na      continuous      input      space      rather      than  \\n \\n  a      discrete      one.\\nMoreover,      techniques      like      value-weighted      sampling      emerge      uniquely      within      the  \\n \\n  context      of      diffusion      models.</p><p>12      Summary</p><p> \\n \\n  In      this      article,      we      comprehensively      explain      how      fine-tuning      diffusion      models      to      maximize      down-  \\n \\n  stream      reward      functions      can      be      formalized      as   \\n \\na      reinforcement      learning      (RL)      problem      in      entropy-  \\n \\n  regularized      Markov      decision      processes      (MDPs).\\nBased      on      this      viewpoint,      we      elaborate      on      the  \\n \\n  application      of      various      RL      algorithms,      such      as      PPO      and      reward-weighted      MLE,      specifically      tailored  \\n \\n  for      fine-tuning      diffusion      models.\\nAdditionally,      we      categorize      different      scenarios      based      on      how  \\n \\n  reward      feedback      is      obtained.\\nWhile      this      categorization      is      not      always      explicitly      mentioned      in      many  \\n \\n  existing      works,      it      is      crucial      when      selecting      appropriate      algorithms.\\nFinally,      we      discuss      the      rela-  \\n \\n  tionships      with      several      related      topics,      including      classifier      guidance,      Gflownets,      path      integral      control  \\n \\n  theory,      and      sampling      from      unnormalized      distributions.</p><h6>7.1      Fine-Tuning      with      Known,      Differentiable      Reward      Functions</h6><p>ions  \\n \\n  When      accurate      differentiable      reward      functions      are      available      (e.g.,      standard      in      computer      vision      tasks),  \\n \\n  our      primary      focus      is      typically      on      computational      and      memory      efficiency.\\nIn      general,      we      advocate      for  \\n \\n  employing      reward      backpropagation      (c.f.      Algorithm      2)      for      its      computational      efficiency.\\nAlternatively,  \\n \\n  if      memory      efficiency      is   \\n \\na      significant      concern,      we      suggest      using      PPO      due      to      its      stability.</p><h6>7.2      Fine-Tuning      with      Black-Box      Reward      Feedback</h6><p>back  \\n \\n  Here,      we      explore      scenarios      where      we      have      access      to      accurate      but      non-differentiable      (black-box)  \\n \\n  reward      feedback,      often      found      in      scientific      simulations.\\nThe      emphasis      still      remains      primarily      on  \\n \\n  computational      and      memory      efficiency.\\nIn      such      scenarios,      due      to      the      non-differentiability      of      reward  \\n \\n  feedback,      we      recommend      using      PPO      or      reward-weighted      MLE      for      the      following      reasons.</p><p> \\n \\n  In      the      preceding      section,      we      advocated      for      the      use      of      reward      backpropagation      due      to      its      com-  \\n \\n  putational      advantages.\\nHowever,      when      dealing      with      non-differentiable      feedback,      the      advantage  \\n \\n  of      reward      backpropagation      may      diminish.\\nThis      is      because      learning      from      such      feedback      requires  \\n \\n  learning      differentiable      reward      functions      to      make      the      algorithm      work      as      we      discuss      in      Section      5.2.  \\n \\n  However,      obtaining   \\n \\na      differentiable      reward      function      can      be      challenging      in      many      domains.\\nFor  \\n \\n  example,      in      molecular      property      prediction,      molecular      fingerprints      are      informative      features,      and       accurate      reward      functions      can      be      derived      by      training      simple      neural      networks      on      these      features  \\n \\n  (Pattanaik      and      Coley,      2020).\\nYet,      these      mappings,      grounded      in      prior      scientific      understanding,      are  \\n \\n  non-differentiable,      thereby      restricting      their      applicability.</p><p> \\n \\n  In      contrast,      PPO      and      reward-weighted      MLE      allow      for      policy      updates      without      the      explicit  \\n \\n  requirement      to      learn   \\n \\na      differentiable      reward      function,      offering   \\n \\na      significant      advantage      over      reward  \\n \\n  backpropagation.</p><h6>7.3      Fine-Tuning      with      Unknown      Rewards      Functions</h6><p>ions  \\n \\n  When      reward      functions      (or      computational      proxies      in      Section      7.2)      are      unavailable,      we      must      learn  \\n \\n  from      data      with      reward      feedback.\\nIn      such      cases,      compared      to      the      previous      scenarios,      two      important  \\n \\n  considerations      arise:</p><p>¢\\n \\n   Not      only      computational      or      memory      efficiency      but      also      feedback      efficiency      (i.e.,      sample  \\n \\n  efficiency      regarding      reward      feedback)      is      crucial.</p><p> \\n \\n \\n¢      Given      that      learned      reward      feedback      may      not      generalize      well      outside      the      training      data      distri-  \\n \\n  butions,      it      is      essential      to      constrain      fine-tuned      models      to      prevent      significant      divergence      from  \\n \\n  diffusion      models.\\nIn      these      situations,      not      only      soft      PPO      and      direct      backpropagation      but      also  \\n \\n  methods      discussed      in      Section      6,      such      as      reward-weighted      MLE,      can      be      particularly      effective.</p><p>Further,      the      scenario      involving      unknown      reward      functions      can      be      broadly      categorized      into      two      cases,  \\n \\n  which      we      will      discuss      in      more      detail.</p><p>Offline      scenario      (Figure      4).\\nIn      many      scenarios,      even  \\n \\n  in      cases      where      reward      functions      are      unknown,      we      of-  \\n \\n.     .</p><p>Offline      data      with ten      have      access      to      offline      data      with      reward      feedback      reward      feedback</p><li>(i)      (i)   \\n \\nO   \\n \\n;   \\n \\ni       a”      ra  \\n \\n.      ne      straightforward      approach      is      to</li><p> \\n \\n \\n{   \\n \\n,   \\n \\n(   \\n \\n)    \\n.\\n    .\\n \\n   g      PP  \\n \\n.      <=>      Learning      reward  \\n \\n  perform      regression      with      neural      networks      and      build   \\n \\na      models  \\n \\n  learned      regressor      *.\\nHowever,      this      learned      reward      func-      SS  \\n \\n  tion   \\n \\n*      might      not      generalize      well      beyond      the      distribution      x)  \\n \\n  of      the      offline      data.\\nIn      regions      outside      this      distribution,   \\n \\n7       could      assign   \\n \\na      high      value      even      when      the      actual      reward  \\n \\n  r      is      low      due      to      high      uncertainty.\\nConsequently,      we  \\n \\n  could      be      easily      misled      by      out-of-distribution      samples  \\n \\n  if      we      solely      optimize      7.\\n|Fine-tuning      diffusion      models   \\n \\n|       One      approach      to      alleviate      this      issue      is      to      adopt   \\n \\na       pessimistic      (i.e.,      conservative)      strategy,      as      proposed      in      Figure      4:      Offline      scenario  \\n \\n  Uehara      et      al.\\n(2024).\\nThis      technique      has      been      widely      Collecting      data]   \\n \\nY      Learning      reward  \\n \\n  applied      in      the      field      of      offline      RL      (Levine,      2018).\\nThe      aa  \\n \\n  main      idea      is      to      penalize   \\n \\n7      outside      the      distributions      us-  \\n \\n  ing      techniques      such      as      adding      an      explicit      penalty      term  \\n \\n  (Yu      et      al.,      2020;      Chang      et      al.,      2021),      bootstrapping,  \\n \\n  or      more      sophisticated      methods      (Kumar      et      al.,      2020;  \\n \\n  Xie      et      al.,      2021;      Rigter      et      al.,      2022;      Uehara      and      Sun,  \\n \\n  2021).\\nBy      doing      so,      we      can      avoid      being      fooled      by  \\n \\n  out-to-distribution      regions      while      still      benefiting      from  \\n \\n  the      extrapolation      capabilities      of      reward      models.</p><p>S—      diffusion      models      |</p><p>Figure      5:      Online      scenario</p><p> \\n \\n  Online      scenario      (Figure      5).\\nConsider      situations  \\n \\n  where      reward      functions      are      unknown,      but      we      can      gather      data      online.\\nThese      scenarios      are      often  \\n \\n  referred      to      as   \\n \\na      lab-in-the-loop      setting.</p><p> \\n \\n  In      such      scenarios,      Uehara      et      al.\\n(2024)      proposes      an      iterative      procedure      comprising      three      steps:</p><li>(1)      collecting      feedback      data      by      exploring      new      areas      (i.e.,      obtaining   \\n \\nx      from      current      diffusion      models  \\n \\n  and      corresponding      r(x)),      (2)      learning      the      reward      function      from      the      collected      feedback      data,      and</li><li>(3)      fine-tuning      current      diffusion      models      by      maximizing      the      learned      reward      function      enhanced      by  \\n \\n  an      optimistic      bonus      term,      using      algorithms      discussed      in      Section      4.2.\\nIn      contrast      to      standard      online  \\n \\n  PPO,      where      feedback      data      is      directly      used      in      the      fine-tuning      process,      actual      reward      feedback      is  \\n \\n  only      used      in      step      (2)      and      not      in      step      (3).\\nAn      additional      key      aspect      is      the      use      of      an      optimistic  \\n \\n  reward      function      that      encourages      exploration      beyond      the      distributions      of      the      current      diffusion      model.\\n \\n \\n  This      deliberate      separation      between      reward      learning      and      fine-tuning      steps,      coupled      with      the      use      of  \\n \\n  optimism,      significantly      enhances      feedback      efficiency.</li><p>8\\n \\n   Connection      with      Classifier      Guidance</p><p> \\n \\n  Classifier      guidance      (Dhariwal      and      Nichol,      2021)      is      commonly      used      for      conditional      generation      in  \\n \\n  diffusion      models.\\nInterestingly,      RL-based      methods      share   \\n \\na      close      connection      with      classifier      guidance.\\n \\n \\n  More      specifically,      in      this      section,      following      Zhao      et      al.\\n(2024),      we      elucidate      how      RL-based      methods  \\n \\n  can      be      applied      to      conditional      generation.\\nFurthermore,      from      this      unified      viewpoint,      we      highlight  \\n \\n  that      classifier      guidance      is      seen      as   \\n \\na      value-weighted      sampling      in      Section      4.2.</p><h6>8.1      Classfier      Guidance</h6><p>ance  \\n \\n  In      this      section,      we      first      introduce      classifier      guidance.\\nClassifier      guidance      utilizes      an      unconditional  \\n \\n  pre-trained      model      for      conditional      generation.\\nMore      specifically,      we      consider   \\n \\na      scenario      where      we  \\n \\n  have   \\n \\na      pre-trained      diffusion      model,      which      enables      us      to      sample      from      p?\\n\"*(x),      and      data      with      feedback  \\n \\n  {x,y},      where   \\n \\ny      represents      the      new      label      we      want      to      condition      on.\\nOur      objective      here      is      to      sample  \\n \\n  from      p(-|y)   \\n \\n€      [Y   \\n \\n>      ACR), P(-   \\n \\n|      y)   \\n \\nx      py(yl-)pP*(-).\\n(37)</p><p>where      p,   \\n \\n:   \\n \\nX      —>      A(Y)      denotes      the      conditional      distribution      of   \\n \\ny      given      x.</p><p>Motivation.\\nIn      classifier      guidance,      we      consider      the      following      policy:</p><p>pe      (-lare,      y)   \\n \\n=   \\n \\nN      (p(22,      t;      Opre)   \\n \\n+      a(t)      Ve      log      Vy      (Xt;      t),      a*(t))   \\n \\n’ dy(x,t)   \\n \\n=      Egy.\\nyap,      (-leo)      L(Y   \\n \\n=      y)|a,   \\n \\n=      2].</p><p>Under      the      continuous-time      formulation      of      diffusion      models,      applying      Doob’s      h-transform      (Rogers  \\n \\n  and      Williams,      2000),      it      is      shown      that      we      can      sample      from      p(x|y)      by      sequentially      running      {p7\"\\\\\\\\\\\\\\'(-|7+,      y)      }i-r41-</p><p>Algorithm.\\nSpecifically,      the      algorithm      comprises      two      steps.\\nIt’s      important      to      note      that      this      is      not  \\n \\n  a      fine-tuning      method      but      rather      an      inference-time      technique      as      we      freeze      the      pre-trained      model.</p><li>1. Learning      q,(x,t)      through      regression      by      gathering      data      containing      {z;,      y}.\\nHere,      for      each  \\n \\n  (Xo,      y),      using   \\n \\na      forward      policy      for      the      pre-training      (from   \\n \\n0      to      T),      we      obtain      2;      starting      from</li><p>Lo.</p><li>2. During      inference,      sequentially      execute      {p2\"(-|a,,      y)      }1_>   \\n \\n1      by      leverage      the      fact      that      compared</li><p>gui to      the      policy      in      pre-trained      diffusion      models,      the      mean      of      {p?\\n\"\\\\\\\\\\\\\\'(-|71,      y)      }i-74,      is      just      shifted  \\n \\n  with      an      additional      term      V,,      log      q,(2,      t).</p><p> \\n \\n  Notably,      the      method      described      above      can      be      reduced      to      value-weighted      sampling      (cf.      Sec-  \\n \\n  tion      6.2),      when      r(x,      y)   \\n \\n=      log      p(y|x).\\nInspired      by      such      an      observation,      we      apply      an      RL-based  \\n \\n  fine-tuning      method      for      conditional      generation      as      below.</p><h7>8.2.      RL-Based      Fine-Tuning      for      Conditional      Generation</h7><p>tion  \\n \\n  In      this      section,      following      Zhao      et      al.\\n(2024),      we      explain      an      RL-based      fine-tuning      approach      for  \\n \\n  conditional      generation.\\nThe      core      idea      is      that      setting      r(x,      y)   \\n \\n=      log      p(y|x)      allows      us      to      sample      from  \\n \\n  the      target      conditional      distribution      (37),      drawing      on      the      insights      discussed      in      Section      4.2.  \\n \\n  More      precisely,      introducing   \\n \\na      distribution      over   \\n \\ny      as      q(-),      we      formulate      the      following      RL      problem:</p><p>{Di      ft   \\n \\n=      argmax      Egy,      (-\\\\\\\\\\\\\\\\-y)}.yott      log      p(y|xo)   \\n \\n—      NeopyaKL(pe(-|e,      y)      ||P      (late).\\n \\n \\n  {prE[(V,.X)PA(A)]      Heyy (38)</p><p> \\n \\n  Compared      to      the      objective      function      (18)      in      Section      4.2,      the      main      differences      are:      (1)      policy      space  \\n \\n  is      expanded      to      accommodate      additional      control      (i.e.,      y):      pz(-|-,-)      C      [R¢   \\n \\nx   \\n \\nY   \\n \\n+      A(R®)],      (2)      the  \\n \\n  fine-tuning      objectives      are      set      as      log-likelihoods:      log      p(y|z).\\nThen,      following      Theorem      1,      we      can  \\n \\n  establish      the      following      result.</p><p>Corollary      2.\\nThe      distribution      induced      by      {p*}1_74      (ue.   \\n \\n[      {Teri      Ph      (ae-1   \\n \\n|      te,      y)}deur      is      the target      conditional      distribution      (37).</p><p> \\n \\n  The      above      corollary      suggests      that      for      conditional      generation,      we      can      utilize      various      off-the-shelf  \\n \\n  algorithms      discussed      in      Section      4.2      and      6,      such      as      PPO,      reward      backpropagation,      and      reward-  \\n \\n  weighted      MLE,      as      well      as      value-weighted      sampling.</p><p>9\\n \\n   Connection      with      Flow-Based      Diffusion      Models</p><p> \\n \\n  We      elucidate      the      close      relationship      between      bridge      matching      and      fine-tuning,      as      summarized      in  \\n \\n  Table      2.\\nBridge      (flow)      matching      (Liu      et      al.,      2022;      Tong      et      al.,      2023;      Lipman      et      al.,      2023;      Liu      et      al.,  \\n \\n  2022;      Shi      et      al.,      2024;      Albergo      and      Vanden-Eijnden,      2022),      has      recently      gained      popularity      as   \\n \\na       distinct      type      of      diffusion      model      from      score-matching-based      ones.\\nThese      works      suggest      that      training  \\n \\n  and      inference      speed      will      be      accelerated      due      to      the      ability      to      define   \\n \\na      more      direct      trajectory      from  \\n \\n  noise      to      data      distribution      than      score-matching-based      diffusion      models.</p><p> \\n \\n  The      connection      between      bridge      matching      and      fine-tuning      becomes      evident      in      the      continuous-  \\n \\n  time      formulation.\\nTo      begin,      we      first      provide   \\n \\na      brief      overview      of      the      continuous-time      formulation      of  \\n \\n  score-matching-based      diffusion      models.</p><p> \\n \\n  Table      2:      Comparison      between      training      score/flow-based      diffusion      models      and      fine-tuning.\\nWe  \\n \\n  optimize      the      parameter   \\n \\n6      in      P’,      which      represents      the      induced      distribution      over      trajectories      associated  \\n \\n  with      6.</p><h8>Loss      Note      of      Q</h8><p>Score-based      KL(Q\\\\\\\\\\\\\\'™¢|P?)      Qtime:      Time-reversal      of      reference      SDE      (reference training      SDE      is      defined      from   \\n \\n0      to      T’      where      the      initial      dis-  \\n \\n  tribute      at   \\n \\n0      is      data      distribution)</p><p> \\n \\n  Flow-based      KL(Q*°>      ||P?)      @Q*°°h:      Coupling      between      the      reference      SDE      from  \\n \\n  training      T      to   \\n \\n0      conditioning      at   \\n \\n0      (dist.      over      x.;|7o)      and    \\n \\na      data      distribution      at      time   \\n \\n0      (dis.      over      x).\\nNote  \\n \\n  that      reference      SDE      is      defined      from      7’      to      0.</p><p> \\n \\n  Fine-tuning      KL(P?||Q*°)      @*:      Coupling      between      the      pre-trained      SDE  \\n \\n  from      T      to   \\n \\n0      conditioned      on   \\n \\n0      (dist.      over      x7.1|X0)  \\n \\n  and   \\n \\na      data      distribution      at      time   \\n \\n0      (dis.      over      Xo) 9.1      Continuous      Time      Formulation      of      Score-Based      diffusion      models</p><p> \\n \\n  We      provide   \\n \\na      brief      overview      of      the      continuous-time      formulation      of      diffusion      models      (Song      et      al.,  \\n \\n  2021),      further      expanding      Section      1.1.1.\\nIn      this      formulation,      we      initially      define      the      forward      reference  \\n \\n  SDE,      spanning      from      time   \\n \\n0      to      7’      (e.g.,      variance      exploding      (VE)      process      or      variance      preserving      (VP)  \\n \\n  process),      by      setting      an      initial      distribution      at   \\n \\n0      as      the      data      distribution.\\nAdditionally,      the      reference  \\n \\n  SDE      is      tailored      to      converge      to      an      easy      distribution      at      time      7’,      such      as      the      normal      distribution.</p><p> \\n \\n  Subsequently,      we      consider      the      time-reversal      SDE      (Anderson,      1982),      progressing      from      T\\\\\\\\\\\\\\'      to      0.\\nIf  \\n \\n  we      could      learn      this      time-reversal      SDE,      it      would      enable      us      to      sample      from      the      data      distribution      by  \\n \\n  starting      with      the      easy      initial      distribution      (at      7’)      and      following      the      time-reversal      SDE      from      T      to   \\n \\n0      at  \\n \\n  inference      time.</p><p> \\n \\n  Training.\\nNow,      the      remaining      question      is      how      to      learn      this      time-reversal      SDE.\\nHere,      by      repre-  \\n \\n  senting      the      induced      distribution      over      trajectories      from      T      to   \\n \\n0      with      this      time-reversal      SDE      as      Q\\\\\\\\\\\\\\'™®,  \\n \\n  our      goal      is      to      train   \\n \\na      new      SDE,      parameterized      by   \\n \\n@      in      the      neural      network,      such      that      the      induced  \\n \\n  distribution      P®      over      trajectories      associated      with   \\n \\n6      aligns      with      Q\"™*.\\nSpecifically,      the      objective      is      to minimize      the      following      losses:</p><p> \\n \\n  argmin      KL(Q\"™||P*).\\n(39)  \\n \\n  0  \\n \\n  With      some      algebra,      leveraging      the      observation      that      the      time-reversal      SDE      comprises      the      original  \\n \\n  forward      SDE      and      the      scoring      term      of      the      marginal      distribution      (Anderson,      1982)      as      in      (2),      Song  \\n \\n  et      al.\\n(2021)      demonstrates      that      this      KL      loss      (39)      is      approximated      as Dore   \\n \\n=      argmin      Eeuni((0,7))01~qyo0(weleo)co~pP\"e      Fe      Fl      Vey      log      jo      (Xt   \\n \\n|      Xo)   \\n \\n_      so(Xt,      t)|I?\\n],      (40)  \\n \\n  which      is      equal      to      (4)      where      the      weight      A(t)      is      {o?\\n}?\\n(for      notation,      refer      to      Section      1.1.1).</p><h9>9.2      Flow-Based      (Bridge-Based)      Diffusion      Models</h9><p>dels We      adopt      the      explanation      outlined      in      Liu      et      al.\\n(2022).\\nIn      this      framework,      we      begin      by      considering  \\n \\n  a      reference      SDE      (from      T\\\\\\\\\\\\\\'      to      0),      x;   \\n \\n=      zr_;      where      z;      is      defined      as      Brownian      motion:</p><p>dz,   \\n \\n=      dwt,      te      (0,      7).\\n(41)</p><p> \\n \\n  Subsequently,      we      consider      the      coupling      of      the      reference      SDE      conditioned      on      state      at      time   \\n \\n0      (i.e.,  \\n \\n  distribution      of      xo.7      conditional      on      x)      and      the      data      distribution      at      time      0,      denoting      the      resulting  \\n \\n  distribution      as      Q*°?,      Informally,      this      Q*°°?\\nis      characterized      as      follows.</p><p>Informal      characterization      of      Q*”.\\nWhen      discretizing      the      time      step,      by      denoting      the      distribution  \\n \\n  induced      by      the      reference      SDE      as      p\"(x7,--+   \\n \\n,      29),      we      define QF      (a7,      tae   \\n \\n,      20)   \\n \\n=      p(x,      ay   \\n \\n|      r9)pP\"°      (x9).</p><p>The      distribution      Q*”      above      is   \\n \\na      limiting      distribution      obtained      by      making      the      discretization      step small.</p><p>Training.\\nIn      bridge-based      diffusion      models,      our      objective      is      to      train   \\n \\na      new      SDE,      parametrized      by  \\n \\n  6      (e.g.,   \\n \\na      neural      network),      to      align      the      induced      distribution      P®      with      Q*°?.\\nSpecifically,      bridge-based training      aims      to      minimize aremin      KL(Q*>||P’).\\n(42)  \\n \\n  0  \\n \\n  While      the      method      to      minimize      the      above      is      currently      unclear,      we      can      derive   \\n \\na      trainable      objective  \\n \\n  through      algebraic      manipulation.\\nInitially,      we      observe      that      the      reference      SDE      (41),      conditioned      on  \\n \\n  state      xo      at      time   \\n \\n0      (zr   \\n \\n=      D),      is      expressed      as:</p><p>b\\n \\n \\n  b—      2;</p><p>t   \\n \\n€      [0,T];dz?\\n \\n \\n=      g?(z?,t)dt      +du,,  \\n \\n  g(z?,t)   \\n \\n=   \\n \\na       (43)</p><p>using      the      seminal      “Doob’s      h-transform”      (Rogers      and      Williams,      2000).\\nThis      SDE      is      commonly  \\n \\n  known      as      the      Brownian      bridge.\\nNow,      we      consider   \\n \\na      new      SDE      parameterized      by      0:</p><p>t   \\n \\n€      [0,7];      dz   \\n \\n=      s(z,t;      @)dt   \\n \\n+      dw.\\n(44)</p><p>Then,      we      can      demonstrate      that      minimizing      the      KL      loss      in      (42)      is      equivalent      to argmin      E,W      unil0,7],1~4\"°      cop?\\n[Ilg”      (2,      t)   \\n \\n_      s(21,      t;      0)      M3]      (45)</p><p>where      q;°      denotes      the      induced      distribution      at      time      t      following      SDE      (43)      conditioned      on      xo,      and       s:      Xx      |0,T]   \\n \\n>   \\n \\n&      is      the      model      we      aim      to      optimize.</p><p> \\n \\n  Flow-based      diffusion      models.\\nThe      aforementioned      formulation      has      been      originally      proposed      as   \\n \\na       bridge-based      SDE      (Liu      et      al.,      2022)      because      the      conditional      SDE      is      often      referred      to      as   \\n \\na      Brownian  \\n \\n  bridge.\\nFlow-based      diffusion      models      proposed      in      Lipman      et      al.\\n(2023);      Tong      et      al.\\n(2023)      share   \\n \\na       close      relationship      with      bridge-based      diffusion      models.\\nThese      flow-based      models      use   \\n \\na      target      SDE (43)      and      an      SDE      with   \\n \\na      parameter      (44)      without      stochastic      terms      (1.e.,      no      dw,)      and      aim      to      minimize  \\n \\n  the      loss      function      (45).</p><p> \\n \\n  Comparison      with      score-based      diffusion      models.\\nBoth      score-based      diffusion      models      and      bridge-  \\n \\n  based      diffusion      models      aim      to      minimize      KL      divergences.\\nDespite      its      similarity,      in      contrast      to  \\n \\n  score-based      diffusion      models,      bridge-based      diffusion      models      target      Q*°>      rather      than      the      time-  \\n \\n  reversal      SDE      Q\"™®.\\nThe      distribution      Q*”      is      often      considered      preferable      because      it      can      impose      an  \\n \\n  SDE      that      navigates      efficiently      from   \\n \\n0      to      7’      (e.g.,   \\n \\na      Brownian      bridge      with      minimal      noise).\\nThis      can  \\n \\n  expedite      the      learning      process      since      we      lack      direct      control      over      the      time-reversal      SDE      in      score-based  \\n \\n  diffusion      models.</p><h10>9.3.      Connection      with      RL-Based      Fine-Tuning</h10><p>ning  \\n \\n  RL-based      fine-tuning      shares   \\n \\na      close      relationship      with      flow-based      training.\\nTo      illustrate      this,      consider  \\n \\n  the      reference      SDE      (from      7’      to      0)      following      the      pre-trained      diffusion      model.\\nThen,      akin      to      flow-  \\n \\n  based      training,      we      consider      the      coupling      of      the      reference      SDE      conditioned      on      state      at      time   \\n \\n0       (i.e.,      distribution      over      xo-r      conditioned      on      x9)      and      the      target      distribution      at      time   \\n \\n0      (i.e.,      p,(-)   \\n \\n«       exp(r(-)/a)p?\\n\"°(-)),      denoting      the      induced      distribution      as      Q*™.</p><p>Informal      characterization      of      Q™°*.\\nInformally,      Q*\"°      is      introduced      as      follows.\\nWhen      discretizing  \\n \\n  the      time      step,      by      denoting      the      distribution      induced      by      the      reference      SDE      as      p™\\\\\\\\\\\\\\'(xp,-+-   \\n \\n,      9),      we define Qi\"      (xr,   \\n \\na   \\n \\n,      0)   \\n \\n=      p(x,      nr      Ly   \\n \\n|      L0)Pr      (Xo).</p><p>The      distribution      Q*°      above      is   \\n \\na      limiting      distribution      obtained      by      making      the      discretization      step small.</p><p>Training.\\nNow,      we      introduce   \\n \\na      new      SDE      parameterized      by   \\n \\n@      such      as      an      neural      network      to      align  \\n \\n  the      induced      distribution      P’      with      Q®\"*.\\nActually,      in      the      continuous      formulation,      the      RL-problem      (18) in      Section      4.2      is      equal      to      solving argmin      KL(P?||Q\"™).\\n \\n \\n  0 This      formalization      has      been      presented      in      Uehara      et      al.\\n(2024).\\nIntuitively,      as      we      see      in      Theorem      3,  \\n \\n  this      is      expected      because      we      see      that      the      posterior      distribution      induced      by      pre-trained      models:</p><p>ref   \\n \\n(      PD      (ar,-++      21   \\n \\n|      to)      (=      vey      (er|er_-1)p      Pp      (ar-1   \\n \\n|      er_2)      +++      pp\"      (x1   \\n \\n|      20), remains      preserved      after      fine-tuning      based      on      the      RL-formulation      (18).</p><p> \\n \\n  Comparison      with      flow-based      training.\\nIn      contrast      to      flow-based      training,      RL-based      fine-tuning  \\n \\n  minimizes      the      inverse      KL      divergence      rather      than      the      KL      divergence.\\nIntuitively,      this      is      because,  \\n \\n  unlike      Qt™*      or      Q*>   \\n \\n,      we      cannot      sample      from      Q*”      (recall      that   \\n \\na      marginal      distribution      at      time   \\n \\n0      in  \\n \\n  Qi,      i.e.,      p,,      is      unnormalized);      on      the      other      hand,      the      marginal      distributions      at      time   \\n \\n0      in      both      Q\\\\\\\\\\\\\\'™*  \\n \\n  and      Q*°°>      are      data      distributions.</p><p> \\n \\n  Remark   \\n \\n7      (Extension      to      f-divergence).\\nEach      of      these      methods      can      be      technically      extended      by  \\n \\n  incorporating      more      general      divergences      beyond      KL      divergence.\\nFor      instance,      in      the      context      of  \\n \\n  fine-tuning,      please      see      Tang      (2024).</p><p>10      Connection      with      Sampling      from      Unnormalized      Distributions</p><p> \\n \\n  Numerous      studies      delve      into      sampling      from      an      unnormalized      distribution      proportional      to      exp(r()),  \\n \\n  commonly      known      as      the      Gibbs      distribution,      extensively      discussed      in      the      computational      statistics  \\n \\n  and      statistical      physics      community      (Robert,      2014).\\nThis      issue      is      pertinent      to      our      work,      as      during  \\n \\n  fine-tuning,      the      target      distribution      is      also      formulated      in      the      Gibbs      distribution      form      in      (10),      which      is  \\n \\n  proportional      to      exp(r(a))p?\\n\"*      (x).</p><p> \\n \\n  The      literature      employs      two      main      approaches      to      tackle      this      challenge:      Markov      Chain      Monte  \\n \\n  Carlo      (MCMC)      and      RL-based      methods.\\nIn      particular,      the      RL-based      diffusion      model      fine-tuning  \\n \\n  we      have      discussed      so      far      is      closely      related      to      the      latter      approach.\\nIn      this      section,      we      explain      and       compare      these      two      approaches.</p><h10>10.1      Markov      Chain      Monte      Carlo      (MCMC)</h10><p>CMC)  \\n \\n  In      MCMC,   \\n \\na      Markov      chain      is      constructed      to      approximate   \\n \\na      target      distribution      such      that      the      equilib-  \\n \\n  rium      distribution      of      the      Markov      chain      converges      to      the      target      distribution.\\nHowever,      dealing      with  \\n \\n  high-dimensional      domain      spaces      is      challenging      for      MCMC.\\n \\n \\nA      common      strategy      involves      leveraging  \\n \\n  gradient      information,      such      as      the      Metropolis-adjusted      Langevin      algorithm      (MALA)      (Besag,      1994)  \\n \\n  or      Hamiltonian      Monte      Carlo      (HMC)      (Neal      et      al.,      2011;      Girolami      and      Calderhead,      2011).\\nMALA  \\n \\n  shares      similarities      with      classifier      guidance      and      value-weighted      sampling,      as      both      methods      depend  \\n \\n  on      the      first-order      information      from      reward      (or      value)      functions.</p><p> \\n \\n  Can      we      use      MCMC      for      fine-tuning      diffusion      models?\\nIn      the      context      of      fine-tuning      dif-  \\n \\n  fusion      models,      while      it      may      seem      intuitive      to      sample      from      the      target      distribution      (10)      (i.e.,  \\n \\n  Dy   \\n \\n&      exp(r(x))pP\"*(x))      using      MCMC,      it’s      not      straightforward      for      the      following      reasons:</p><p>¢\\n \\n   We      lack      of      an      analytical      form      of      p?\\n\"*(-).<ul><li>*\\n \\n   Additionally,      even      if      we      can      estimate      p?\\n’°(-)      in      an      unbiased      manner,      as      in      Chen      et      al.\\n(2018),  \\n \\n  the      mixing      time      for      obtaining   \\n \\na      single      sample      in      MCMC      might      be      lengthy.</li></ul></p><p> \\n \\n  Therefore,      we      explore      creating   \\n \\na      generative      model      (i.e.,      diffusion      model)      for      this      task      using      RL-  \\n \\n  based      fine-tuning      so      that      we      can      easily      sample      from      p,(-)      during      inference      (i.e.,      simulating      the  \\n \\n  time-reversal      SDE      (2)      with      policies      from   \\n \\n¢   \\n \\n=      T\\\\\\\\\\\\\\'+   \\n \\n1      to      t   \\n \\n=      1),      without      relying      on      MCMC.\\nThis  \\n \\n  addresses      the      concern      of      MCMC      by      avoiding      the      need      to      estimate      p?\\n\"*(-)      and      minimizing      inference  \\n \\n  time,      albeit      at      the      expense      of      increased      training      time.</p><p>10.2}      RL-Based      Approaches  \\n \\n  While      MCMC      has      historically      been      widely      used      for      this      purpose,      recent      works      have      proposed      an  \\n \\n  RL-based      approach      or      its      variant      (e.g.,      Bernton      et      al.      (2019);      Heng      et      al.\\n(2020);      Huang      et      al.\\n(2023);</p><p> \\n \\n  Vargas      et      al.\\n(2023)).\\nFor      example,   \\n \\na      seminal      study      (Zhang      and      Chen,      2021)      introduced   \\n \\na      method  \\n \\n  very      similar      to      reward      backpropagation.\\nIn      their      work,      they      initially      define      Brownian      motion      as   \\n \\na       reference      SDE      (from      7’      to      0),      similar      to      the      pre-trained      diffusion      models      in      our      context.\\nThen,      by  \\n \\n  appropriately      setting      rewards,      Zhang      and      Chen      (2021)      aims      to      learn   \\n \\na      new      SDE      (from      T\\\\\\\\\\\\\\'      to      0)      such  \\n \\n  that      we      can      sample      from      the      target      distribution      at      the      endpoint      0.</p><p> \\n \\n  These      approaches      offer      advantages      over      MCMC      due      to      their      ability      to      minimize      inference      time  \\n \\n  (amortization)      by      constructing   \\n \\na      generative      model      comprising      policies      in      the      training      time      and      solely  \\n \\n  executing      learned      policies      during      inference.\\nUnlike      MCMC,      these      methods      significantly      reduce      the  \\n \\n  inference      cost      as      we      don’t      need      to      be      concerned      about      the      mixing      time.\\nAdditionally,      RL-based  \\n \\n  approaches      have      the      potential      to      effectively      handle      high-dimensional,      complex      (i.e.,      multi-modal)  \\n \\n  distributions      by      harnessing      the      considerable      expressive      power      arising      from      latent      states.</p><p> \\n \\n  Despite      these      advantages,      it      is      often      unclear      how      to      select   \\n \\na      reference      SDE      in      these      works.\\nIn  \\n \\n  contrast,      in      RL-based      fine-tuning      of      diffusion      models,      we      directly      leverage      pre-trained      diffusion  \\n \\n  models      as      the      reference      SDEs,      i.e.,      as      our      prior      knowledge.</p><p> \\n \\n  Key      message.\\nFinally,      it      is      worth      noting      that      we      can      technically      utilize      any      off-the-shelf      RL      algo-  \\n \\n  rithms      for      training.\\nFor      example,      Zhang      and      Chen      (2021)      employs      reward      backpropagation,      while  \\n \\n  works      in      Gflownets      typically      leverage      PCL,      as      we      mention      in      Section      6.3.\\nHowever,      additionally,      it  \\n \\n  is      possible      to      use      PPO      or      reward-weighted      MLE      mentioned      in      Section      4.2.</p><p>11      Closely      Related      Directions</p><p>Lastly,      we      highlight      several      closely      related      directions      we      have      not      yet      discussed.</p><p> \\n \\n  Aligning      text-to-image      models.\\nWe      recognize      that      current      efforts      in      fine-tuning      diffusion      models  \\n \\n  primarily      revolve      around      aligning      text-to-image      models      using      human      feedback      (Lee      et      al.,      2023;  \\n \\n  Wallace      et      al.,      2023;      Wu      et      al.,      2023;      Yang      et      al.,      2023).\\nIn      this      paper,      our      goal      is      to      consider   \\n \\na      more  \\n \\n  general      and      fundamental      formulation      that      is      not      tailored      to      any      particular      task.</p><p> \\n \\n  Diffusion      models      for      RL.\\nDiffusion      models      are      well-suited      for      specific      reinforcement      learning  \\n \\n  applications      due      to      their      ability      to      model      complex      and      multimodal      distributions      as      polices      (Janner  \\n \\n  et      al.,      2022;      Ajay      et      al.,      2023;      Wang      et      al.,      2022;      Hansen-Estruch      et      al.,      2023;      Du      et      al.,      2024;      Zhu  \\n \\n  et      al.,      2023).\\nFor      example,      Wang      et      al.\\n(2022)      use      conditional      diffusion      models      as      policies      and       demonstrate      their      effectiveness      in      typical      offline      RL      benchmarks.\\nWhile      RL-based      fine-tuning      also  \\n \\n  aims      to      maximize      certain      reward      functions,      the      primary      focus      of      this      tutorial      covers      methods      that  \\n \\n  leverage      pre-trained      diffusion      models      for      this      purpose.</p><p> \\n \\n  RL      from      human      feedback      (RLHF)      for      language      models.\\nRLHF      is      widely      discussed      in      the  \\n \\n  context      of      language      models,      where      many      algorithms      share      similarities      with      those      used      in      diffusion  \\n \\n  models      (Ouyang      et      al.,      2022;      Bai      et      al.,      2022;      Casper      et      al.,      2023).\\nFor      instance,      both      domains  \\n \\n  commonly      employ      PPO      as   \\n \\na      standard      approach,      and      reward-weighted      training      (or      reward-weighted  \\n \\n  MLE)      often      serves      as   \\n \\na      basic      baseline      approach.\\nDespite      these      similarities,      nuanced      differences  \\n \\n  exist      mainly      because      diffusion      models      typically      operate      in   \\n \\na      continuous      input      space      rather      than  \\n \\n  a      discrete      one.\\nMoreover,      techniques      like      value-weighted      sampling      emerge      uniquely      within      the  \\n \\n  context      of      diffusion      models.</p><p>12      Summary</p><p> \\n \\n  In      this      article,      we      comprehensively      explain      how      fine-tuning      diffusion      models      to      maximize      down-  \\n \\n  stream      reward      functions      can      be      formalized      as   \\n \\na      reinforcement      learning      (RL)      problem      in      entropy-  \\n \\n  regularized      Markov      decision      processes      (MDPs).\\nBased      on      this      viewpoint,      we      elaborate      on      the  \\n \\n  application      of      various      RL      algorithms,      such      as      PPO      and      reward-weighted      MLE,      specifically      tailored  \\n \\n  for      fine-tuning      diffusion      models.\\nAdditionally,      we      categorize      different      scenarios      based      on      how  \\n \\n  reward      feedback      is      obtained.\\nWhile      this      categorization      is      not      always      explicitly      mentioned      in      many  \\n \\n  existing      works,      it      is      crucial      when      selecting      appropriate      algorithms.\\nFinally,      we      discuss      the      rela-  \\n \\n  tionships      with      several      related      topics,      including      classifier      guidance,      Gflownets,      path      integral      control  \\n \\n  theory,      and      sampling      from      unnormalized      distributions.</p><h7>8.2.      RL-Based      Fine-Tuning      for      Conditional      Generation</h7><p>tion  \\n \\n  In      this      section,      following      Zhao      et      al.\\n(2024),      we      explain      an      RL-based      fine-tuning      approach      for  \\n \\n  conditional      generation.\\nThe      core      idea      is      that      setting      r(x,      y)   \\n \\n=      log      p(y|x)      allows      us      to      sample      from  \\n \\n  the      target      conditional      distribution      (37),      drawing      on      the      insights      discussed      in      Section      4.2.  \\n \\n  More      precisely,      introducing   \\n \\na      distribution      over   \\n \\ny      as      q(-),      we      formulate      the      following      RL      problem:</p><p>{Di      ft   \\n \\n=      argmax      Egy,      (-\\\\\\\\\\\\\\\\-y)}.yott      log      p(y|xo)   \\n \\n—      NeopyaKL(pe(-|e,      y)      ||P      (late).\\n \\n \\n  {prE[(V,.X)PA(A)]      Heyy (38)</p><p> \\n \\n  Compared      to      the      objective      function      (18)      in      Section      4.2,      the      main      differences      are:      (1)      policy      space  \\n \\n  is      expanded      to      accommodate      additional      control      (i.e.,      y):      pz(-|-,-)      C      [R¢   \\n \\nx   \\n \\nY   \\n \\n+      A(R®)],      (2)      the  \\n \\n  fine-tuning      objectives      are      set      as      log-likelihoods:      log      p(y|z).\\nThen,      following      Theorem      1,      we      can  \\n \\n  establish      the      following      result.</p><p>Corollary      2.\\nThe      distribution      induced      by      {p*}1_74      (ue.   \\n \\n[      {Teri      Ph      (ae-1   \\n \\n|      te,      y)}deur      is      the target      conditional      distribution      (37).</p><p> \\n \\n  The      above      corollary      suggests      that      for      conditional      generation,      we      can      utilize      various      off-the-shelf  \\n \\n  algorithms      discussed      in      Section      4.2      and      6,      such      as      PPO,      reward      backpropagation,      and      reward-  \\n \\n  weighted      MLE,      as      well      as      value-weighted      sampling.</p><p>9\\n \\n   Connection      with      Flow-Based      Diffusion      Models</p><p> \\n \\n  We      elucidate      the      close      relationship      between      bridge      matching      and      fine-tuning,      as      summarized      in  \\n \\n  Table      2.\\nBridge      (flow)      matching      (Liu      et      al.,      2022;      Tong      et      al.,      2023;      Lipman      et      al.,      2023;      Liu      et      al.,  \\n \\n  2022;      Shi      et      al.,      2024;      Albergo      and      Vanden-Eijnden,      2022),      has      recently      gained      popularity      as   \\n \\na       distinct      type      of      diffusion      model      from      score-matching-based      ones.\\nThese      works      suggest      that      training  \\n \\n  and      inference      speed      will      be      accelerated      due      to      the      ability      to      define   \\n \\na      more      direct      trajectory      from  \\n \\n  noise      to      data      distribution      than      score-matching-based      diffusion      models.</p><p> \\n \\n  The      connection      between      bridge      matching      and      fine-tuning      becomes      evident      in      the      continuous-  \\n \\n  time      formulation.\\nTo      begin,      we      first      provide   \\n \\na      brief      overview      of      the      continuous-time      formulation      of  \\n \\n  score-matching-based      diffusion      models.</p><p> \\n \\n  Table      2:      Comparison      between      training      score/flow-based      diffusion      models      and      fine-tuning.\\nWe  \\n \\n  optimize      the      parameter   \\n \\n6      in      P’,      which      represents      the      induced      distribution      over      trajectories      associated  \\n \\n  with      6.</p><h8>Loss      Note      of      Q</h8><p>Score-based      KL(Q\\\\\\\\\\\\\\'™¢|P?)      Qtime:      Time-reversal      of      reference      SDE      (reference training      SDE      is      defined      from   \\n \\n0      to      T’      where      the      initial      dis-  \\n \\n  tribute      at   \\n \\n0      is      data      distribution)</p><p> \\n \\n  Flow-based      KL(Q*°>      ||P?)      @Q*°°h:      Coupling      between      the      reference      SDE      from  \\n \\n  training      T      to   \\n \\n0      conditioning      at   \\n \\n0      (dist.      over      x.;|7o)      and    \\n \\na      data      distribution      at      time   \\n \\n0      (dis.      over      x).\\nNote  \\n \\n  that      reference      SDE      is      defined      from      7’      to      0.</p><p> \\n \\n  Fine-tuning      KL(P?||Q*°)      @*:      Coupling      between      the      pre-trained      SDE  \\n \\n  from      T      to   \\n \\n0      conditioned      on   \\n \\n0      (dist.      over      x7.1|X0)  \\n \\n  and   \\n \\na      data      distribution      at      time   \\n \\n0      (dis.      over      Xo) 9.1      Continuous      Time      Formulation      of      Score-Based      diffusion      models</p><p> \\n \\n  We      provide   \\n \\na      brief      overview      of      the      continuous-time      formulation      of      diffusion      models      (Song      et      al.,  \\n \\n  2021),      further      expanding      Section      1.1.1.\\nIn      this      formulation,      we      initially      define      the      forward      reference  \\n \\n  SDE,      spanning      from      time   \\n \\n0      to      7’      (e.g.,      variance      exploding      (VE)      process      or      variance      preserving      (VP)  \\n \\n  process),      by      setting      an      initial      distribution      at   \\n \\n0      as      the      data      distribution.\\nAdditionally,      the      reference  \\n \\n  SDE      is      tailored      to      converge      to      an      easy      distribution      at      time      7’,      such      as      the      normal      distribution.</p><p> \\n \\n  Subsequently,      we      consider      the      time-reversal      SDE      (Anderson,      1982),      progressing      from      T\\\\\\\\\\\\\\'      to      0.\\nIf  \\n \\n  we      could      learn      this      time-reversal      SDE,      it      would      enable      us      to      sample      from      the      data      distribution      by  \\n \\n  starting      with      the      easy      initial      distribution      (at      7’)      and      following      the      time-reversal      SDE      from      T      to   \\n \\n0      at  \\n \\n  inference      time.</p><p> \\n \\n  Training.\\nNow,      the      remaining      question      is      how      to      learn      this      time-reversal      SDE.\\nHere,      by      repre-  \\n \\n  senting      the      induced      distribution      over      trajectories      from      T      to   \\n \\n0      with      this      time-reversal      SDE      as      Q\\\\\\\\\\\\\\'™®,  \\n \\n  our      goal      is      to      train   \\n \\na      new      SDE,      parameterized      by   \\n \\n@      in      the      neural      network,      such      that      the      induced  \\n \\n  distribution      P®      over      trajectories      associated      with   \\n \\n6      aligns      with      Q\"™*.\\nSpecifically,      the      objective      is      to minimize      the      following      losses:</p><p> \\n \\n  argmin      KL(Q\"™||P*).\\n(39)  \\n \\n  0  \\n \\n  With      some      algebra,      leveraging      the      observation      that      the      time-reversal      SDE      comprises      the      original  \\n \\n  forward      SDE      and      the      scoring      term      of      the      marginal      distribution      (Anderson,      1982)      as      in      (2),      Song  \\n \\n  et      al.\\n(2021)      demonstrates      that      this      KL      loss      (39)      is      approximated      as Dore   \\n \\n=      argmin      Eeuni((0,7))01~qyo0(weleo)co~pP\"e      Fe      Fl      Vey      log      jo      (Xt   \\n \\n|      Xo)   \\n \\n_      so(Xt,      t)|I?\\n],      (40)  \\n \\n  which      is      equal      to      (4)      where      the      weight      A(t)      is      {o?\\n}?\\n(for      notation,      refer      to      Section      1.1.1).</p><h9>9.2      Flow-Based      (Bridge-Based)      Diffusion      Models</h9><p>dels We      adopt      the      explanation      outlined      in      Liu      et      al.\\n(2022).\\nIn      this      framework,      we      begin      by      considering  \\n \\n  a      reference      SDE      (from      T\\\\\\\\\\\\\\'      to      0),      x;   \\n \\n=      zr_;      where      z;      is      defined      as      Brownian      motion:</p><p>dz,   \\n \\n=      dwt,      te      (0,      7).\\n(41)</p><p> \\n \\n  Subsequently,      we      consider      the      coupling      of      the      reference      SDE      conditioned      on      state      at      time   \\n \\n0      (i.e.,  \\n \\n  distribution      of      xo.7      conditional      on      x)      and      the      data      distribution      at      time      0,      denoting      the      resulting  \\n \\n  distribution      as      Q*°?,      Informally,      this      Q*°°?\\nis      characterized      as      follows.</p><p>Informal      characterization      of      Q*”.\\nWhen      discretizing      the      time      step,      by      denoting      the      distribution  \\n \\n  induced      by      the      reference      SDE      as      p\"(x7,--+   \\n \\n,      29),      we      define QF      (a7,      tae   \\n \\n,      20)   \\n \\n=      p(x,      ay   \\n \\n|      r9)pP\"°      (x9).</p><p>The      distribution      Q*”      above      is   \\n \\na      limiting      distribution      obtained      by      making      the      discretization      step small.</p><p>Training.\\nIn      bridge-based      diffusion      models,      our      objective      is      to      train   \\n \\na      new      SDE,      parametrized      by  \\n \\n  6      (e.g.,   \\n \\na      neural      network),      to      align      the      induced      distribution      P®      with      Q*°?.\\nSpecifically,      bridge-based training      aims      to      minimize aremin      KL(Q*>||P’).\\n(42)  \\n \\n  0  \\n \\n  While      the      method      to      minimize      the      above      is      currently      unclear,      we      can      derive   \\n \\na      trainable      objective  \\n \\n  through      algebraic      manipulation.\\nInitially,      we      observe      that      the      reference      SDE      (41),      conditioned      on  \\n \\n  state      xo      at      time   \\n \\n0      (zr   \\n \\n=      D),      is      expressed      as:</p><p>b\\n \\n \\n  b—      2;</p><p>t   \\n \\n€      [0,T];dz?\\n \\n \\n=      g?(z?,t)dt      +du,,  \\n \\n  g(z?,t)   \\n \\n=   \\n \\na       (43)</p><p>using      the      seminal      “Doob’s      h-transform”      (Rogers      and      Williams,      2000).\\nThis      SDE      is      commonly  \\n \\n  known      as      the      Brownian      bridge.\\nNow,      we      consider   \\n \\na      new      SDE      parameterized      by      0:</p><p>t   \\n \\n€      [0,7];      dz   \\n \\n=      s(z,t;      @)dt   \\n \\n+      dw.\\n(44)</p><p>Then,      we      can      demonstrate      that      minimizing      the      KL      loss      in      (42)      is      equivalent      to argmin      E,W      unil0,7],1~4\"°      cop?\\n[Ilg”      (2,      t)   \\n \\n_      s(21,      t;      0)      M3]      (45)</p><p>where      q;°      denotes      the      induced      distribution      at      time      t      following      SDE      (43)      conditioned      on      xo,      and       s:      Xx      |0,T]   \\n \\n>   \\n \\n&      is      the      model      we      aim      to      optimize.</p><p> \\n \\n  Flow-based      diffusion      models.\\nThe      aforementioned      formulation      has      been      originally      proposed      as   \\n \\na       bridge-based      SDE      (Liu      et      al.,      2022)      because      the      conditional      SDE      is      often      referred      to      as   \\n \\na      Brownian  \\n \\n  bridge.\\nFlow-based      diffusion      models      proposed      in      Lipman      et      al.\\n(2023);      Tong      et      al.\\n(2023)      share   \\n \\na       close      relationship      with      bridge-based      diffusion      models.\\nThese      flow-based      models      use   \\n \\na      target      SDE (43)      and      an      SDE      with   \\n \\na      parameter      (44)      without      stochastic      terms      (1.e.,      no      dw,)      and      aim      to      minimize  \\n \\n  the      loss      function      (45).</p><p> \\n \\n  Comparison      with      score-based      diffusion      models.\\nBoth      score-based      diffusion      models      and      bridge-  \\n \\n  based      diffusion      models      aim      to      minimize      KL      divergences.\\nDespite      its      similarity,      in      contrast      to  \\n \\n  score-based      diffusion      models,      bridge-based      diffusion      models      target      Q*°>      rather      than      the      time-  \\n \\n  reversal      SDE      Q\"™®.\\nThe      distribution      Q*”      is      often      considered      preferable      because      it      can      impose      an  \\n \\n  SDE      that      navigates      efficiently      from   \\n \\n0      to      7’      (e.g.,   \\n \\na      Brownian      bridge      with      minimal      noise).\\nThis      can  \\n \\n  expedite      the      learning      process      since      we      lack      direct      control      over      the      time-reversal      SDE      in      score-based  \\n \\n  diffusion      models.</p><h10>9.3.      Connection      with      RL-Based      Fine-Tuning</h10><p>ning  \\n \\n  RL-based      fine-tuning      shares   \\n \\na      close      relationship      with      flow-based      training.\\nTo      illustrate      this,      consider  \\n \\n  the      reference      SDE      (from      7’      to      0)      following      the      pre-trained      diffusion      model.\\nThen,      akin      to      flow-  \\n \\n  based      training,      we      consider      the      coupling      of      the      reference      SDE      conditioned      on      state      at      time   \\n \\n0       (i.e.,      distribution      over      xo-r      conditioned      on      x9)      and      the      target      distribution      at      time   \\n \\n0      (i.e.,      p,(-)   \\n \\n«       exp(r(-)/a)p?\\n\"°(-)),      denoting      the      induced      distribution      as      Q*™.</p><p>Informal      characterization      of      Q™°*.\\nInformally,      Q*\"°      is      introduced      as      follows.\\nWhen      discretizing  \\n \\n  the      time      step,      by      denoting      the      distribution      induced      by      the      reference      SDE      as      p™\\\\\\\\\\\\\\'(xp,-+-   \\n \\n,      9),      we define Qi\"      (xr,   \\n \\na   \\n \\n,      0)   \\n \\n=      p(x,      nr      Ly   \\n \\n|      L0)Pr      (Xo).</p><p>The      distribution      Q*°      above      is   \\n \\na      limiting      distribution      obtained      by      making      the      discretization      step small.</p><p>Training.\\nNow,      we      introduce   \\n \\na      new      SDE      parameterized      by   \\n \\n@      such      as      an      neural      network      to      align  \\n \\n  the      induced      distribution      P’      with      Q®\"*.\\nActually,      in      the      continuous      formulation,      the      RL-problem      (18) in      Section      4.2      is      equal      to      solving argmin      KL(P?||Q\"™).\\n \\n \\n  0 This      formalization      has      been      presented      in      Uehara      et      al.\\n(2024).\\nIntuitively,      as      we      see      in      Theorem      3,  \\n \\n  this      is      expected      because      we      see      that      the      posterior      distribution      induced      by      pre-trained      models:</p><p>ref   \\n \\n(      PD      (ar,-++      21   \\n \\n|      to)      (=      vey      (er|er_-1)p      Pp      (ar-1   \\n \\n|      er_2)      +++      pp\"      (x1   \\n \\n|      20), remains      preserved      after      fine-tuning      based      on      the      RL-formulation      (18).</p><p> \\n \\n  Comparison      with      flow-based      training.\\nIn      contrast      to      flow-based      training,      RL-based      fine-tuning  \\n \\n  minimizes      the      inverse      KL      divergence      rather      than      the      KL      divergence.\\nIntuitively,      this      is      because,  \\n \\n  unlike      Qt™*      or      Q*>   \\n \\n,      we      cannot      sample      from      Q*”      (recall      that   \\n \\na      marginal      distribution      at      time   \\n \\n0      in  \\n \\n  Qi,      i.e.,      p,,      is      unnormalized);      on      the      other      hand,      the      marginal      distributions      at      time   \\n \\n0      in      both      Q\\\\\\\\\\\\\\'™*  \\n \\n  and      Q*°°>      are      data      distributions.</p><p> \\n \\n  Remark   \\n \\n7      (Extension      to      f-divergence).\\nEach      of      these      methods      can      be      technically      extended      by  \\n \\n  incorporating      more      general      divergences      beyond      KL      divergence.\\nFor      instance,      in      the      context      of  \\n \\n  fine-tuning,      please      see      Tang      (2024).</p><p>10      Connection      with      Sampling      from      Unnormalized      Distributions</p><p> \\n \\n  Numerous      studies      delve      into      sampling      from      an      unnormalized      distribution      proportional      to      exp(r()),  \\n \\n  commonly      known      as      the      Gibbs      distribution,      extensively      discussed      in      the      computational      statistics  \\n \\n  and      statistical      physics      community      (Robert,      2014).\\nThis      issue      is      pertinent      to      our      work,      as      during  \\n \\n  fine-tuning,      the      target      distribution      is      also      formulated      in      the      Gibbs      distribution      form      in      (10),      which      is  \\n \\n  proportional      to      exp(r(a))p?\\n\"*      (x).</p><p> \\n \\n  The      literature      employs      two      main      approaches      to      tackle      this      challenge:      Markov      Chain      Monte  \\n \\n  Carlo      (MCMC)      and      RL-based      methods.\\nIn      particular,      the      RL-based      diffusion      model      fine-tuning  \\n \\n  we      have      discussed      so      far      is      closely      related      to      the      latter      approach.\\nIn      this      section,      we      explain      and       compare      these      two      approaches.</p><h10>10.1      Markov      Chain      Monte      Carlo      (MCMC)</h10><p>CMC)  \\n \\n  In      MCMC,   \\n \\na      Markov      chain      is      constructed      to      approximate   \\n \\na      target      distribution      such      that      the      equilib-  \\n \\n  rium      distribution      of      the      Markov      chain      converges      to      the      target      distribution.\\nHowever,      dealing      with  \\n \\n  high-dimensional      domain      spaces      is      challenging      for      MCMC.\\n \\n \\nA      common      strategy      involves      leveraging  \\n \\n  gradient      information,      such      as      the      Metropolis-adjusted      Langevin      algorithm      (MALA)      (Besag,      1994)  \\n \\n  or      Hamiltonian      Monte      Carlo      (HMC)      (Neal      et      al.,      2011;      Girolami      and      Calderhead,      2011).\\nMALA  \\n \\n  shares      similarities      with      classifier      guidance      and      value-weighted      sampling,      as      both      methods      depend  \\n \\n  on      the      first-order      information      from      reward      (or      value)      functions.</p><p> \\n \\n  Can      we      use      MCMC      for      fine-tuning      diffusion      models?\\nIn      the      context      of      fine-tuning      dif-  \\n \\n  fusion      models,      while      it      may      seem      intuitive      to      sample      from      the      target      distribution      (10)      (i.e.,  \\n \\n  Dy   \\n \\n&      exp(r(x))pP\"*(x))      using      MCMC,      it’s      not      straightforward      for      the      following      reasons:</p><p>¢\\n \\n   We      lack      of      an      analytical      form      of      p?\\n\"*(-).<ul><li>*\\n \\n   Additionally,      even      if      we      can      estimate      p?\\n’°(-)      in      an      unbiased      manner,      as      in      Chen      et      al.\\n(2018),  \\n \\n  the      mixing      time      for      obtaining   \\n \\na      single      sample      in      MCMC      might      be      lengthy.</li></ul></p><p> \\n \\n  Therefore,      we      explore      creating   \\n \\na      generative      model      (i.e.,      diffusion      model)      for      this      task      using      RL-  \\n \\n  based      fine-tuning      so      that      we      can      easily      sample      from      p,(-)      during      inference      (i.e.,      simulating      the  \\n \\n  time-reversal      SDE      (2)      with      policies      from   \\n \\n¢   \\n \\n=      T\\\\\\\\\\\\\\'+   \\n \\n1      to      t   \\n \\n=      1),      without      relying      on      MCMC.\\nThis  \\n \\n  addresses      the      concern      of      MCMC      by      avoiding      the      need      to      estimate      p?\\n\"*(-)      and      minimizing      inference  \\n \\n  time,      albeit      at      the      expense      of      increased      training      time.</p><p>10.2}      RL-Based      Approaches  \\n \\n  While      MCMC      has      historically      been      widely      used      for      this      purpose,      recent      works      have      proposed      an  \\n \\n  RL-based      approach      or      its      variant      (e.g.,      Bernton      et      al.      (2019);      Heng      et      al.\\n(2020);      Huang      et      al.\\n(2023);</p><p> \\n \\n  Vargas      et      al.\\n(2023)).\\nFor      example,   \\n \\na      seminal      study      (Zhang      and      Chen,      2021)      introduced   \\n \\na      method  \\n \\n  very      similar      to      reward      backpropagation.\\nIn      their      work,      they      initially      define      Brownian      motion      as   \\n \\na       reference      SDE      (from      7’      to      0),      similar      to      the      pre-trained      diffusion      models      in      our      context.\\nThen,      by  \\n \\n  appropriately      setting      rewards,      Zhang      and      Chen      (2021)      aims      to      learn   \\n \\na      new      SDE      (from      T\\\\\\\\\\\\\\'      to      0)      such  \\n \\n  that      we      can      sample      from      the      target      distribution      at      the      endpoint      0.</p><p> \\n \\n  These      approaches      offer      advantages      over      MCMC      due      to      their      ability      to      minimize      inference      time  \\n \\n  (amortization)      by      constructing   \\n \\na      generative      model      comprising      policies      in      the      training      time      and      solely  \\n \\n  executing      learned      policies      during      inference.\\nUnlike      MCMC,      these      methods      significantly      reduce      the  \\n \\n  inference      cost      as      we      don’t      need      to      be      concerned      about      the      mixing      time.\\nAdditionally,      RL-based  \\n \\n  approaches      have      the      potential      to      effectively      handle      high-dimensional,      complex      (i.e.,      multi-modal)  \\n \\n  distributions      by      harnessing      the      considerable      expressive      power      arising      from      latent      states.</p><p> \\n \\n  Despite      these      advantages,      it      is      often      unclear      how      to      select   \\n \\na      reference      SDE      in      these      works.\\nIn  \\n \\n  contrast,      in      RL-based      fine-tuning      of      diffusion      models,      we      directly      leverage      pre-trained      diffusion  \\n \\n  models      as      the      reference      SDEs,      i.e.,      as      our      prior      knowledge.</p><p> \\n \\n  Key      message.\\nFinally,      it      is      worth      noting      that      we      can      technically      utilize      any      off-the-shelf      RL      algo-  \\n \\n  rithms      for      training.\\nFor      example,      Zhang      and      Chen      (2021)      employs      reward      backpropagation,      while  \\n \\n  works      in      Gflownets      typically      leverage      PCL,      as      we      mention      in      Section      6.3.\\nHowever,      additionally,      it  \\n \\n  is      possible      to      use      PPO      or      reward-weighted      MLE      mentioned      in      Section      4.2.</p><p>11      Closely      Related      Directions</p><p>Lastly,      we      highlight      several      closely      related      directions      we      have      not      yet      discussed.</p><p> \\n \\n  Aligning      text-to-image      models.\\nWe      recognize      that      current      efforts      in      fine-tuning      diffusion      models  \\n \\n  primarily      revolve      around      aligning      text-to-image      models      using      human      feedback      (Lee      et      al.,      2023;  \\n \\n  Wallace      et      al.,      2023;      Wu      et      al.,      2023;      Yang      et      al.,      2023).\\nIn      this      paper,      our      goal      is      to      consider   \\n \\na      more  \\n \\n  general      and      fundamental      formulation      that      is      not      tailored      to      any      particular      task.</p><p> \\n \\n  Diffusion      models      for      RL.\\nDiffusion      models      are      well-suited      for      specific      reinforcement      learning  \\n \\n  applications      due      to      their      ability      to      model      complex      and      multimodal      distributions      as      polices      (Janner  \\n \\n  et      al.,      2022;      Ajay      et      al.,      2023;      Wang      et      al.,      2022;      Hansen-Estruch      et      al.,      2023;      Du      et      al.,      2024;      Zhu  \\n \\n  et      al.,      2023).\\nFor      example,      Wang      et      al.\\n(2022)      use      conditional      diffusion      models      as      policies      and       demonstrate      their      effectiveness      in      typical      offline      RL      benchmarks.\\nWhile      RL-based      fine-tuning      also  \\n \\n  aims      to      maximize      certain      reward      functions,      the      primary      focus      of      this      tutorial      covers      methods      that  \\n \\n  leverage      pre-trained      diffusion      models      for      this      purpose.</p><p> \\n \\n  RL      from      human      feedback      (RLHF)      for      language      models.\\nRLHF      is      widely      discussed      in      the  \\n \\n  context      of      language      models,      where      many      algorithms      share      similarities      with      those      used      in      diffusion  \\n \\n  models      (Ouyang      et      al.,      2022;      Bai      et      al.,      2022;      Casper      et      al.,      2023).\\nFor      instance,      both      domains  \\n \\n  commonly      employ      PPO      as   \\n \\na      standard      approach,      and      reward-weighted      training      (or      reward-weighted  \\n \\n  MLE)      often      serves      as   \\n \\na      basic      baseline      approach.\\nDespite      these      similarities,      nuanced      differences  \\n \\n  exist      mainly      because      diffusion      models      typically      operate      in   \\n \\na      continuous      input      space      rather      than  \\n \\n  a      discrete      one.\\nMoreover,      techniques      like      value-weighted      sampling      emerge      uniquely      within      the  \\n \\n  context      of      diffusion      models.</p><p>12      Summary</p><p> \\n \\n  In      this      article,      we      comprehensively      explain      how      fine-tuning      diffusion      models      to      maximize      down-  \\n \\n  stream      reward      functions      can      be      formalized      as   \\n \\na      reinforcement      learning      (RL)      problem      in      entropy-  \\n \\n  regularized      Markov      decision      processes      (MDPs).\\nBased      on      this      viewpoint,      we      elaborate      on      the  \\n \\n  application      of      various      RL      algorithms,      such      as      PPO      and      reward-weighted      MLE,      specifically      tailored  \\n \\n  for      fine-tuning      diffusion      models.\\nAdditionally,      we      categorize      different      scenarios      based      on      how  \\n \\n  reward      feedback      is      obtained.\\nWhile      this      categorization      is      not      always      explicitly      mentioned      in      many  \\n \\n  existing      works,      it      is      crucial      when      selecting      appropriate      algorithms.\\nFinally,      we      discuss      the      rela-  \\n \\n  tionships      with      several      related      topics,      including      classifier      guidance,      Gflownets,      path      integral      control  \\n \\n  theory,      and      sampling      from      unnormalized      distributions.</p><h8>Loss      Note      of      Q</h8><p>Score-based      KL(Q\\\\\\\\\\\\\\'™¢|P?)      Qtime:      Time-reversal      of      reference      SDE      (reference training      SDE      is      defined      from   \\n \\n0      to      T’      where      the      initial      dis-  \\n \\n  tribute      at   \\n \\n0      is      data      distribution)</p><p> \\n \\n  Flow-based      KL(Q*°>      ||P?)      @Q*°°h:      Coupling      between      the      reference      SDE      from  \\n \\n  training      T      to   \\n \\n0      conditioning      at   \\n \\n0      (dist.      over      x.;|7o)      and    \\n \\na      data      distribution      at      time   \\n \\n0      (dis.      over      x).\\nNote  \\n \\n  that      reference      SDE      is      defined      from      7’      to      0.</p><p> \\n \\n  Fine-tuning      KL(P?||Q*°)      @*:      Coupling      between      the      pre-trained      SDE  \\n \\n  from      T      to   \\n \\n0      conditioned      on   \\n \\n0      (dist.      over      x7.1|X0)  \\n \\n  and   \\n \\na      data      distribution      at      time   \\n \\n0      (dis.      over      Xo) 9.1      Continuous      Time      Formulation      of      Score-Based      diffusion      models</p><p> \\n \\n  We      provide   \\n \\na      brief      overview      of      the      continuous-time      formulation      of      diffusion      models      (Song      et      al.,  \\n \\n  2021),      further      expanding      Section      1.1.1.\\nIn      this      formulation,      we      initially      define      the      forward      reference  \\n \\n  SDE,      spanning      from      time   \\n \\n0      to      7’      (e.g.,      variance      exploding      (VE)      process      or      variance      preserving      (VP)  \\n \\n  process),      by      setting      an      initial      distribution      at   \\n \\n0      as      the      data      distribution.\\nAdditionally,      the      reference  \\n \\n  SDE      is      tailored      to      converge      to      an      easy      distribution      at      time      7’,      such      as      the      normal      distribution.</p><p> \\n \\n  Subsequently,      we      consider      the      time-reversal      SDE      (Anderson,      1982),      progressing      from      T\\\\\\\\\\\\\\'      to      0.\\nIf  \\n \\n  we      could      learn      this      time-reversal      SDE,      it      would      enable      us      to      sample      from      the      data      distribution      by  \\n \\n  starting      with      the      easy      initial      distribution      (at      7’)      and      following      the      time-reversal      SDE      from      T      to   \\n \\n0      at  \\n \\n  inference      time.</p><p> \\n \\n  Training.\\nNow,      the      remaining      question      is      how      to      learn      this      time-reversal      SDE.\\nHere,      by      repre-  \\n \\n  senting      the      induced      distribution      over      trajectories      from      T      to   \\n \\n0      with      this      time-reversal      SDE      as      Q\\\\\\\\\\\\\\'™®,  \\n \\n  our      goal      is      to      train   \\n \\na      new      SDE,      parameterized      by   \\n \\n@      in      the      neural      network,      such      that      the      induced  \\n \\n  distribution      P®      over      trajectories      associated      with   \\n \\n6      aligns      with      Q\"™*.\\nSpecifically,      the      objective      is      to minimize      the      following      losses:</p><p> \\n \\n  argmin      KL(Q\"™||P*).\\n(39)  \\n \\n  0  \\n \\n  With      some      algebra,      leveraging      the      observation      that      the      time-reversal      SDE      comprises      the      original  \\n \\n  forward      SDE      and      the      scoring      term      of      the      marginal      distribution      (Anderson,      1982)      as      in      (2),      Song  \\n \\n  et      al.\\n(2021)      demonstrates      that      this      KL      loss      (39)      is      approximated      as Dore   \\n \\n=      argmin      Eeuni((0,7))01~qyo0(weleo)co~pP\"e      Fe      Fl      Vey      log      jo      (Xt   \\n \\n|      Xo)   \\n \\n_      so(Xt,      t)|I?\\n],      (40)  \\n \\n  which      is      equal      to      (4)      where      the      weight      A(t)      is      {o?\\n}?\\n(for      notation,      refer      to      Section      1.1.1).</p><h9>9.2      Flow-Based      (Bridge-Based)      Diffusion      Models</h9><p>dels We      adopt      the      explanation      outlined      in      Liu      et      al.\\n(2022).\\nIn      this      framework,      we      begin      by      considering  \\n \\n  a      reference      SDE      (from      T\\\\\\\\\\\\\\'      to      0),      x;   \\n \\n=      zr_;      where      z;      is      defined      as      Brownian      motion:</p><p>dz,   \\n \\n=      dwt,      te      (0,      7).\\n(41)</p><p> \\n \\n  Subsequently,      we      consider      the      coupling      of      the      reference      SDE      conditioned      on      state      at      time   \\n \\n0      (i.e.,  \\n \\n  distribution      of      xo.7      conditional      on      x)      and      the      data      distribution      at      time      0,      denoting      the      resulting  \\n \\n  distribution      as      Q*°?,      Informally,      this      Q*°°?\\nis      characterized      as      follows.</p><p>Informal      characterization      of      Q*”.\\nWhen      discretizing      the      time      step,      by      denoting      the      distribution  \\n \\n  induced      by      the      reference      SDE      as      p\"(x7,--+   \\n \\n,      29),      we      define QF      (a7,      tae   \\n \\n,      20)   \\n \\n=      p(x,      ay   \\n \\n|      r9)pP\"°      (x9).</p><p>The      distribution      Q*”      above      is   \\n \\na      limiting      distribution      obtained      by      making      the      discretization      step small.</p><p>Training.\\nIn      bridge-based      diffusion      models,      our      objective      is      to      train   \\n \\na      new      SDE,      parametrized      by  \\n \\n  6      (e.g.,   \\n \\na      neural      network),      to      align      the      induced      distribution      P®      with      Q*°?.\\nSpecifically,      bridge-based training      aims      to      minimize aremin      KL(Q*>||P’).\\n(42)  \\n \\n  0  \\n \\n  While      the      method      to      minimize      the      above      is      currently      unclear,      we      can      derive   \\n \\na      trainable      objective  \\n \\n  through      algebraic      manipulation.\\nInitially,      we      observe      that      the      reference      SDE      (41),      conditioned      on  \\n \\n  state      xo      at      time   \\n \\n0      (zr   \\n \\n=      D),      is      expressed      as:</p><p>b\\n \\n \\n  b—      2;</p><p>t   \\n \\n€      [0,T];dz?\\n \\n \\n=      g?(z?,t)dt      +du,,  \\n \\n  g(z?,t)   \\n \\n=   \\n \\na       (43)</p><p>using      the      seminal      “Doob’s      h-transform”      (Rogers      and      Williams,      2000).\\nThis      SDE      is      commonly  \\n \\n  known      as      the      Brownian      bridge.\\nNow,      we      consider   \\n \\na      new      SDE      parameterized      by      0:</p><p>t   \\n \\n€      [0,7];      dz   \\n \\n=      s(z,t;      @)dt   \\n \\n+      dw.\\n(44)</p><p>Then,      we      can      demonstrate      that      minimizing      the      KL      loss      in      (42)      is      equivalent      to argmin      E,W      unil0,7],1~4\"°      cop?\\n[Ilg”      (2,      t)   \\n \\n_      s(21,      t;      0)      M3]      (45)</p><p>where      q;°      denotes      the      induced      distribution      at      time      t      following      SDE      (43)      conditioned      on      xo,      and       s:      Xx      |0,T]   \\n \\n>   \\n \\n&      is      the      model      we      aim      to      optimize.</p><p> \\n \\n  Flow-based      diffusion      models.\\nThe      aforementioned      formulation      has      been      originally      proposed      as   \\n \\na       bridge-based      SDE      (Liu      et      al.,      2022)      because      the      conditional      SDE      is      often      referred      to      as   \\n \\na      Brownian  \\n \\n  bridge.\\nFlow-based      diffusion      models      proposed      in      Lipman      et      al.\\n(2023);      Tong      et      al.\\n(2023)      share   \\n \\na       close      relationship      with      bridge-based      diffusion      models.\\nThese      flow-based      models      use   \\n \\na      target      SDE (43)      and      an      SDE      with   \\n \\na      parameter      (44)      without      stochastic      terms      (1.e.,      no      dw,)      and      aim      to      minimize  \\n \\n  the      loss      function      (45).</p><p> \\n \\n  Comparison      with      score-based      diffusion      models.\\nBoth      score-based      diffusion      models      and      bridge-  \\n \\n  based      diffusion      models      aim      to      minimize      KL      divergences.\\nDespite      its      similarity,      in      contrast      to  \\n \\n  score-based      diffusion      models,      bridge-based      diffusion      models      target      Q*°>      rather      than      the      time-  \\n \\n  reversal      SDE      Q\"™®.\\nThe      distribution      Q*”      is      often      considered      preferable      because      it      can      impose      an  \\n \\n  SDE      that      navigates      efficiently      from   \\n \\n0      to      7’      (e.g.,   \\n \\na      Brownian      bridge      with      minimal      noise).\\nThis      can  \\n \\n  expedite      the      learning      process      since      we      lack      direct      control      over      the      time-reversal      SDE      in      score-based  \\n \\n  diffusion      models.</p><h10>9.3.      Connection      with      RL-Based      Fine-Tuning</h10><p>ning  \\n \\n  RL-based      fine-tuning      shares   \\n \\na      close      relationship      with      flow-based      training.\\nTo      illustrate      this,      consider  \\n \\n  the      reference      SDE      (from      7’      to      0)      following      the      pre-trained      diffusion      model.\\nThen,      akin      to      flow-  \\n \\n  based      training,      we      consider      the      coupling      of      the      reference      SDE      conditioned      on      state      at      time   \\n \\n0       (i.e.,      distribution      over      xo-r      conditioned      on      x9)      and      the      target      distribution      at      time   \\n \\n0      (i.e.,      p,(-)   \\n \\n«       exp(r(-)/a)p?\\n\"°(-)),      denoting      the      induced      distribution      as      Q*™.</p><p>Informal      characterization      of      Q™°*.\\nInformally,      Q*\"°      is      introduced      as      follows.\\nWhen      discretizing  \\n \\n  the      time      step,      by      denoting      the      distribution      induced      by      the      reference      SDE      as      p™\\\\\\\\\\\\\\'(xp,-+-   \\n \\n,      9),      we define Qi\"      (xr,   \\n \\na   \\n \\n,      0)   \\n \\n=      p(x,      nr      Ly   \\n \\n|      L0)Pr      (Xo).</p><p>The      distribution      Q*°      above      is   \\n \\na      limiting      distribution      obtained      by      making      the      discretization      step small.</p><p>Training.\\nNow,      we      introduce   \\n \\na      new      SDE      parameterized      by   \\n \\n@      such      as      an      neural      network      to      align  \\n \\n  the      induced      distribution      P’      with      Q®\"*.\\nActually,      in      the      continuous      formulation,      the      RL-problem      (18) in      Section      4.2      is      equal      to      solving argmin      KL(P?||Q\"™).\\n \\n \\n  0 This      formalization      has      been      presented      in      Uehara      et      al.\\n(2024).\\nIntuitively,      as      we      see      in      Theorem      3,  \\n \\n  this      is      expected      because      we      see      that      the      posterior      distribution      induced      by      pre-trained      models:</p><p>ref   \\n \\n(      PD      (ar,-++      21   \\n \\n|      to)      (=      vey      (er|er_-1)p      Pp      (ar-1   \\n \\n|      er_2)      +++      pp\"      (x1   \\n \\n|      20), remains      preserved      after      fine-tuning      based      on      the      RL-formulation      (18).</p><p> \\n \\n  Comparison      with      flow-based      training.\\nIn      contrast      to      flow-based      training,      RL-based      fine-tuning  \\n \\n  minimizes      the      inverse      KL      divergence      rather      than      the      KL      divergence.\\nIntuitively,      this      is      because,  \\n \\n  unlike      Qt™*      or      Q*>   \\n \\n,      we      cannot      sample      from      Q*”      (recall      that   \\n \\na      marginal      distribution      at      time   \\n \\n0      in  \\n \\n  Qi,      i.e.,      p,,      is      unnormalized);      on      the      other      hand,      the      marginal      distributions      at      time   \\n \\n0      in      both      Q\\\\\\\\\\\\\\'™*  \\n \\n  and      Q*°°>      are      data      distributions.</p><p> \\n \\n  Remark   \\n \\n7      (Extension      to      f-divergence).\\nEach      of      these      methods      can      be      technically      extended      by  \\n \\n  incorporating      more      general      divergences      beyond      KL      divergence.\\nFor      instance,      in      the      context      of  \\n \\n  fine-tuning,      please      see      Tang      (2024).</p><p>10      Connection      with      Sampling      from      Unnormalized      Distributions</p><p> \\n \\n  Numerous      studies      delve      into      sampling      from      an      unnormalized      distribution      proportional      to      exp(r()),  \\n \\n  commonly      known      as      the      Gibbs      distribution,      extensively      discussed      in      the      computational      statistics  \\n \\n  and      statistical      physics      community      (Robert,      2014).\\nThis      issue      is      pertinent      to      our      work,      as      during  \\n \\n  fine-tuning,      the      target      distribution      is      also      formulated      in      the      Gibbs      distribution      form      in      (10),      which      is  \\n \\n  proportional      to      exp(r(a))p?\\n\"*      (x).</p><p> \\n \\n  The      literature      employs      two      main      approaches      to      tackle      this      challenge:      Markov      Chain      Monte  \\n \\n  Carlo      (MCMC)      and      RL-based      methods.\\nIn      particular,      the      RL-based      diffusion      model      fine-tuning  \\n \\n  we      have      discussed      so      far      is      closely      related      to      the      latter      approach.\\nIn      this      section,      we      explain      and       compare      these      two      approaches.</p><h10>10.1      Markov      Chain      Monte      Carlo      (MCMC)</h10><p>CMC)  \\n \\n  In      MCMC,   \\n \\na      Markov      chain      is      constructed      to      approximate   \\n \\na      target      distribution      such      that      the      equilib-  \\n \\n  rium      distribution      of      the      Markov      chain      converges      to      the      target      distribution.\\nHowever,      dealing      with  \\n \\n  high-dimensional      domain      spaces      is      challenging      for      MCMC.\\n \\n \\nA      common      strategy      involves      leveraging  \\n \\n  gradient      information,      such      as      the      Metropolis-adjusted      Langevin      algorithm      (MALA)      (Besag,      1994)  \\n \\n  or      Hamiltonian      Monte      Carlo      (HMC)      (Neal      et      al.,      2011;      Girolami      and      Calderhead,      2011).\\nMALA  \\n \\n  shares      similarities      with      classifier      guidance      and      value-weighted      sampling,      as      both      methods      depend  \\n \\n  on      the      first-order      information      from      reward      (or      value)      functions.</p><p> \\n \\n  Can      we      use      MCMC      for      fine-tuning      diffusion      models?\\nIn      the      context      of      fine-tuning      dif-  \\n \\n  fusion      models,      while      it      may      seem      intuitive      to      sample      from      the      target      distribution      (10)      (i.e.,  \\n \\n  Dy   \\n \\n&      exp(r(x))pP\"*(x))      using      MCMC,      it’s      not      straightforward      for      the      following      reasons:</p><p>¢\\n \\n   We      lack      of      an      analytical      form      of      p?\\n\"*(-).<ul><li>*\\n \\n   Additionally,      even      if      we      can      estimate      p?\\n’°(-)      in      an      unbiased      manner,      as      in      Chen      et      al.\\n(2018),  \\n \\n  the      mixing      time      for      obtaining   \\n \\na      single      sample      in      MCMC      might      be      lengthy.</li></ul></p><p> \\n \\n  Therefore,      we      explore      creating   \\n \\na      generative      model      (i.e.,      diffusion      model)      for      this      task      using      RL-  \\n \\n  based      fine-tuning      so      that      we      can      easily      sample      from      p,(-)      during      inference      (i.e.,      simulating      the  \\n \\n  time-reversal      SDE      (2)      with      policies      from   \\n \\n¢   \\n \\n=      T\\\\\\\\\\\\\\'+   \\n \\n1      to      t   \\n \\n=      1),      without      relying      on      MCMC.\\nThis  \\n \\n  addresses      the      concern      of      MCMC      by      avoiding      the      need      to      estimate      p?\\n\"*(-)      and      minimizing      inference  \\n \\n  time,      albeit      at      the      expense      of      increased      training      time.</p><p>10.2}      RL-Based      Approaches  \\n \\n  While      MCMC      has      historically      been      widely      used      for      this      purpose,      recent      works      have      proposed      an  \\n \\n  RL-based      approach      or      its      variant      (e.g.,      Bernton      et      al.      (2019);      Heng      et      al.\\n(2020);      Huang      et      al.\\n(2023);</p><p> \\n \\n  Vargas      et      al.\\n(2023)).\\nFor      example,   \\n \\na      seminal      study      (Zhang      and      Chen,      2021)      introduced   \\n \\na      method  \\n \\n  very      similar      to      reward      backpropagation.\\nIn      their      work,      they      initially      define      Brownian      motion      as   \\n \\na       reference      SDE      (from      7’      to      0),      similar      to      the      pre-trained      diffusion      models      in      our      context.\\nThen,      by  \\n \\n  appropriately      setting      rewards,      Zhang      and      Chen      (2021)      aims      to      learn   \\n \\na      new      SDE      (from      T\\\\\\\\\\\\\\'      to      0)      such  \\n \\n  that      we      can      sample      from      the      target      distribution      at      the      endpoint      0.</p><p> \\n \\n  These      approaches      offer      advantages      over      MCMC      due      to      their      ability      to      minimize      inference      time  \\n \\n  (amortization)      by      constructing   \\n \\na      generative      model      comprising      policies      in      the      training      time      and      solely  \\n \\n  executing      learned      policies      during      inference.\\nUnlike      MCMC,      these      methods      significantly      reduce      the  \\n \\n  inference      cost      as      we      don’t      need      to      be      concerned      about      the      mixing      time.\\nAdditionally,      RL-based  \\n \\n  approaches      have      the      potential      to      effectively      handle      high-dimensional,      complex      (i.e.,      multi-modal)  \\n \\n  distributions      by      harnessing      the      considerable      expressive      power      arising      from      latent      states.</p><p> \\n \\n  Despite      these      advantages,      it      is      often      unclear      how      to      select   \\n \\na      reference      SDE      in      these      works.\\nIn  \\n \\n  contrast,      in      RL-based      fine-tuning      of      diffusion      models,      we      directly      leverage      pre-trained      diffusion  \\n \\n  models      as      the      reference      SDEs,      i.e.,      as      our      prior      knowledge.</p><p> \\n \\n  Key      message.\\nFinally,      it      is      worth      noting      that      we      can      technically      utilize      any      off-the-shelf      RL      algo-  \\n \\n  rithms      for      training.\\nFor      example,      Zhang      and      Chen      (2021)      employs      reward      backpropagation,      while  \\n \\n  works      in      Gflownets      typically      leverage      PCL,      as      we      mention      in      Section      6.3.\\nHowever,      additionally,      it  \\n \\n  is      possible      to      use      PPO      or      reward-weighted      MLE      mentioned      in      Section      4.2.</p><p>11      Closely      Related      Directions</p><p>Lastly,      we      highlight      several      closely      related      directions      we      have      not      yet      discussed.</p><p> \\n \\n  Aligning      text-to-image      models.\\nWe      recognize      that      current      efforts      in      fine-tuning      diffusion      models  \\n \\n  primarily      revolve      around      aligning      text-to-image      models      using      human      feedback      (Lee      et      al.,      2023;  \\n \\n  Wallace      et      al.,      2023;      Wu      et      al.,      2023;      Yang      et      al.,      2023).\\nIn      this      paper,      our      goal      is      to      consider   \\n \\na      more  \\n \\n  general      and      fundamental      formulation      that      is      not      tailored      to      any      particular      task.</p><p> \\n \\n  Diffusion      models      for      RL.\\nDiffusion      models      are      well-suited      for      specific      reinforcement      learning  \\n \\n  applications      due      to      their      ability      to      model      complex      and      multimodal      distributions      as      polices      (Janner  \\n \\n  et      al.,      2022;      Ajay      et      al.,      2023;      Wang      et      al.,      2022;      Hansen-Estruch      et      al.,      2023;      Du      et      al.,      2024;      Zhu  \\n \\n  et      al.,      2023).\\nFor      example,      Wang      et      al.\\n(2022)      use      conditional      diffusion      models      as      policies      and       demonstrate      their      effectiveness      in      typical      offline      RL      benchmarks.\\nWhile      RL-based      fine-tuning      also  \\n \\n  aims      to      maximize      certain      reward      functions,      the      primary      focus      of      this      tutorial      covers      methods      that  \\n \\n  leverage      pre-trained      diffusion      models      for      this      purpose.</p><p> \\n \\n  RL      from      human      feedback      (RLHF)      for      language      models.\\nRLHF      is      widely      discussed      in      the  \\n \\n  context      of      language      models,      where      many      algorithms      share      similarities      with      those      used      in      diffusion  \\n \\n  models      (Ouyang      et      al.,      2022;      Bai      et      al.,      2022;      Casper      et      al.,      2023).\\nFor      instance,      both      domains  \\n \\n  commonly      employ      PPO      as   \\n \\na      standard      approach,      and      reward-weighted      training      (or      reward-weighted  \\n \\n  MLE)      often      serves      as   \\n \\na      basic      baseline      approach.\\nDespite      these      similarities,      nuanced      differences  \\n \\n  exist      mainly      because      diffusion      models      typically      operate      in   \\n \\na      continuous      input      space      rather      than  \\n \\n  a      discrete      one.\\nMoreover,      techniques      like      value-weighted      sampling      emerge      uniquely      within      the  \\n \\n  context      of      diffusion      models.</p><p>12      Summary</p><p> \\n \\n  In      this      article,      we      comprehensively      explain      how      fine-tuning      diffusion      models      to      maximize      down-  \\n \\n  stream      reward      functions      can      be      formalized      as   \\n \\na      reinforcement      learning      (RL)      problem      in      entropy-  \\n \\n  regularized      Markov      decision      processes      (MDPs).\\nBased      on      this      viewpoint,      we      elaborate      on      the  \\n \\n  application      of      various      RL      algorithms,      such      as      PPO      and      reward-weighted      MLE,      specifically      tailored  \\n \\n  for      fine-tuning      diffusion      models.\\nAdditionally,      we      categorize      different      scenarios      based      on      how  \\n \\n  reward      feedback      is      obtained.\\nWhile      this      categorization      is      not      always      explicitly      mentioned      in      many  \\n \\n  existing      works,      it      is      crucial      when      selecting      appropriate      algorithms.\\nFinally,      we      discuss      the      rela-  \\n \\n  tionships      with      several      related      topics,      including      classifier      guidance,      Gflownets,      path      integral      control  \\n \\n  theory,      and      sampling      from      unnormalized      distributions.</p><h9>9.2      Flow-Based      (Bridge-Based)      Diffusion      Models</h9><p>dels We      adopt      the      explanation      outlined      in      Liu      et      al.\\n(2022).\\nIn      this      framework,      we      begin      by      considering  \\n \\n  a      reference      SDE      (from      T\\\\\\\\\\\\\\'      to      0),      x;   \\n \\n=      zr_;      where      z;      is      defined      as      Brownian      motion:</p><p>dz,   \\n \\n=      dwt,      te      (0,      7).\\n(41)</p><p> \\n \\n  Subsequently,      we      consider      the      coupling      of      the      reference      SDE      conditioned      on      state      at      time   \\n \\n0      (i.e.,  \\n \\n  distribution      of      xo.7      conditional      on      x)      and      the      data      distribution      at      time      0,      denoting      the      resulting  \\n \\n  distribution      as      Q*°?,      Informally,      this      Q*°°?\\nis      characterized      as      follows.</p><p>Informal      characterization      of      Q*”.\\nWhen      discretizing      the      time      step,      by      denoting      the      distribution  \\n \\n  induced      by      the      reference      SDE      as      p\"(x7,--+   \\n \\n,      29),      we      define QF      (a7,      tae   \\n \\n,      20)   \\n \\n=      p(x,      ay   \\n \\n|      r9)pP\"°      (x9).</p><p>The      distribution      Q*”      above      is   \\n \\na      limiting      distribution      obtained      by      making      the      discretization      step small.</p><p>Training.\\nIn      bridge-based      diffusion      models,      our      objective      is      to      train   \\n \\na      new      SDE,      parametrized      by  \\n \\n  6      (e.g.,   \\n \\na      neural      network),      to      align      the      induced      distribution      P®      with      Q*°?.\\nSpecifically,      bridge-based training      aims      to      minimize aremin      KL(Q*>||P’).\\n(42)  \\n \\n  0  \\n \\n  While      the      method      to      minimize      the      above      is      currently      unclear,      we      can      derive   \\n \\na      trainable      objective  \\n \\n  through      algebraic      manipulation.\\nInitially,      we      observe      that      the      reference      SDE      (41),      conditioned      on  \\n \\n  state      xo      at      time   \\n \\n0      (zr   \\n \\n=      D),      is      expressed      as:</p><p>b\\n \\n \\n  b—      2;</p><p>t   \\n \\n€      [0,T];dz?\\n \\n \\n=      g?(z?,t)dt      +du,,  \\n \\n  g(z?,t)   \\n \\n=   \\n \\na       (43)</p><p>using      the      seminal      “Doob’s      h-transform”      (Rogers      and      Williams,      2000).\\nThis      SDE      is      commonly  \\n \\n  known      as      the      Brownian      bridge.\\nNow,      we      consider   \\n \\na      new      SDE      parameterized      by      0:</p><p>t   \\n \\n€      [0,7];      dz   \\n \\n=      s(z,t;      @)dt   \\n \\n+      dw.\\n(44)</p><p>Then,      we      can      demonstrate      that      minimizing      the      KL      loss      in      (42)      is      equivalent      to argmin      E,W      unil0,7],1~4\"°      cop?\\n[Ilg”      (2,      t)   \\n \\n_      s(21,      t;      0)      M3]      (45)</p><p>where      q;°      denotes      the      induced      distribution      at      time      t      following      SDE      (43)      conditioned      on      xo,      and       s:      Xx      |0,T]   \\n \\n>   \\n \\n&      is      the      model      we      aim      to      optimize.</p><p> \\n \\n  Flow-based      diffusion      models.\\nThe      aforementioned      formulation      has      been      originally      proposed      as   \\n \\na       bridge-based      SDE      (Liu      et      al.,      2022)      because      the      conditional      SDE      is      often      referred      to      as   \\n \\na      Brownian  \\n \\n  bridge.\\nFlow-based      diffusion      models      proposed      in      Lipman      et      al.\\n(2023);      Tong      et      al.\\n(2023)      share   \\n \\na       close      relationship      with      bridge-based      diffusion      models.\\nThese      flow-based      models      use   \\n \\na      target      SDE (43)      and      an      SDE      with   \\n \\na      parameter      (44)      without      stochastic      terms      (1.e.,      no      dw,)      and      aim      to      minimize  \\n \\n  the      loss      function      (45).</p><p> \\n \\n  Comparison      with      score-based      diffusion      models.\\nBoth      score-based      diffusion      models      and      bridge-  \\n \\n  based      diffusion      models      aim      to      minimize      KL      divergences.\\nDespite      its      similarity,      in      contrast      to  \\n \\n  score-based      diffusion      models,      bridge-based      diffusion      models      target      Q*°>      rather      than      the      time-  \\n \\n  reversal      SDE      Q\"™®.\\nThe      distribution      Q*”      is      often      considered      preferable      because      it      can      impose      an  \\n \\n  SDE      that      navigates      efficiently      from   \\n \\n0      to      7’      (e.g.,   \\n \\na      Brownian      bridge      with      minimal      noise).\\nThis      can  \\n \\n  expedite      the      learning      process      since      we      lack      direct      control      over      the      time-reversal      SDE      in      score-based  \\n \\n  diffusion      models.</p><h10>9.3.      Connection      with      RL-Based      Fine-Tuning</h10><p>ning  \\n \\n  RL-based      fine-tuning      shares   \\n \\na      close      relationship      with      flow-based      training.\\nTo      illustrate      this,      consider  \\n \\n  the      reference      SDE      (from      7’      to      0)      following      the      pre-trained      diffusion      model.\\nThen,      akin      to      flow-  \\n \\n  based      training,      we      consider      the      coupling      of      the      reference      SDE      conditioned      on      state      at      time   \\n \\n0       (i.e.,      distribution      over      xo-r      conditioned      on      x9)      and      the      target      distribution      at      time   \\n \\n0      (i.e.,      p,(-)   \\n \\n«       exp(r(-)/a)p?\\n\"°(-)),      denoting      the      induced      distribution      as      Q*™.</p><p>Informal      characterization      of      Q™°*.\\nInformally,      Q*\"°      is      introduced      as      follows.\\nWhen      discretizing  \\n \\n  the      time      step,      by      denoting      the      distribution      induced      by      the      reference      SDE      as      p™\\\\\\\\\\\\\\'(xp,-+-   \\n \\n,      9),      we define Qi\"      (xr,   \\n \\na   \\n \\n,      0)   \\n \\n=      p(x,      nr      Ly   \\n \\n|      L0)Pr      (Xo).</p><p>The      distribution      Q*°      above      is   \\n \\na      limiting      distribution      obtained      by      making      the      discretization      step small.</p><p>Training.\\nNow,      we      introduce   \\n \\na      new      SDE      parameterized      by   \\n \\n@      such      as      an      neural      network      to      align  \\n \\n  the      induced      distribution      P’      with      Q®\"*.\\nActually,      in      the      continuous      formulation,      the      RL-problem      (18) in      Section      4.2      is      equal      to      solving argmin      KL(P?||Q\"™).\\n \\n \\n  0 This      formalization      has      been      presented      in      Uehara      et      al.\\n(2024).\\nIntuitively,      as      we      see      in      Theorem      3,  \\n \\n  this      is      expected      because      we      see      that      the      posterior      distribution      induced      by      pre-trained      models:</p><p>ref   \\n \\n(      PD      (ar,-++      21   \\n \\n|      to)      (=      vey      (er|er_-1)p      Pp      (ar-1   \\n \\n|      er_2)      +++      pp\"      (x1   \\n \\n|      20), remains      preserved      after      fine-tuning      based      on      the      RL-formulation      (18).</p><p> \\n \\n  Comparison      with      flow-based      training.\\nIn      contrast      to      flow-based      training,      RL-based      fine-tuning  \\n \\n  minimizes      the      inverse      KL      divergence      rather      than      the      KL      divergence.\\nIntuitively,      this      is      because,  \\n \\n  unlike      Qt™*      or      Q*>   \\n \\n,      we      cannot      sample      from      Q*”      (recall      that   \\n \\na      marginal      distribution      at      time   \\n \\n0      in  \\n \\n  Qi,      i.e.,      p,,      is      unnormalized);      on      the      other      hand,      the      marginal      distributions      at      time   \\n \\n0      in      both      Q\\\\\\\\\\\\\\'™*  \\n \\n  and      Q*°°>      are      data      distributions.</p><p> \\n \\n  Remark   \\n \\n7      (Extension      to      f-divergence).\\nEach      of      these      methods      can      be      technically      extended      by  \\n \\n  incorporating      more      general      divergences      beyond      KL      divergence.\\nFor      instance,      in      the      context      of  \\n \\n  fine-tuning,      please      see      Tang      (2024).</p><p>10      Connection      with      Sampling      from      Unnormalized      Distributions</p><p> \\n \\n  Numerous      studies      delve      into      sampling      from      an      unnormalized      distribution      proportional      to      exp(r()),  \\n \\n  commonly      known      as      the      Gibbs      distribution,      extensively      discussed      in      the      computational      statistics  \\n \\n  and      statistical      physics      community      (Robert,      2014).\\nThis      issue      is      pertinent      to      our      work,      as      during  \\n \\n  fine-tuning,      the      target      distribution      is      also      formulated      in      the      Gibbs      distribution      form      in      (10),      which      is  \\n \\n  proportional      to      exp(r(a))p?\\n\"*      (x).</p><p> \\n \\n  The      literature      employs      two      main      approaches      to      tackle      this      challenge:      Markov      Chain      Monte  \\n \\n  Carlo      (MCMC)      and      RL-based      methods.\\nIn      particular,      the      RL-based      diffusion      model      fine-tuning  \\n \\n  we      have      discussed      so      far      is      closely      related      to      the      latter      approach.\\nIn      this      section,      we      explain      and       compare      these      two      approaches.</p><h10>10.1      Markov      Chain      Monte      Carlo      (MCMC)</h10><p>CMC)  \\n \\n  In      MCMC,   \\n \\na      Markov      chain      is      constructed      to      approximate   \\n \\na      target      distribution      such      that      the      equilib-  \\n \\n  rium      distribution      of      the      Markov      chain      converges      to      the      target      distribution.\\nHowever,      dealing      with  \\n \\n  high-dimensional      domain      spaces      is      challenging      for      MCMC.\\n \\n \\nA      common      strategy      involves      leveraging  \\n \\n  gradient      information,      such      as      the      Metropolis-adjusted      Langevin      algorithm      (MALA)      (Besag,      1994)  \\n \\n  or      Hamiltonian      Monte      Carlo      (HMC)      (Neal      et      al.,      2011;      Girolami      and      Calderhead,      2011).\\nMALA  \\n \\n  shares      similarities      with      classifier      guidance      and      value-weighted      sampling,      as      both      methods      depend  \\n \\n  on      the      first-order      information      from      reward      (or      value)      functions.</p><p> \\n \\n  Can      we      use      MCMC      for      fine-tuning      diffusion      models?\\nIn      the      context      of      fine-tuning      dif-  \\n \\n  fusion      models,      while      it      may      seem      intuitive      to      sample      from      the      target      distribution      (10)      (i.e.,  \\n \\n  Dy   \\n \\n&      exp(r(x))pP\"*(x))      using      MCMC,      it’s      not      straightforward      for      the      following      reasons:</p><p>¢\\n \\n   We      lack      of      an      analytical      form      of      p?\\n\"*(-).<ul><li>*\\n \\n   Additionally,      even      if      we      can      estimate      p?\\n’°(-)      in      an      unbiased      manner,      as      in      Chen      et      al.\\n(2018),  \\n \\n  the      mixing      time      for      obtaining   \\n \\na      single      sample      in      MCMC      might      be      lengthy.</li></ul></p><p> \\n \\n  Therefore,      we      explore      creating   \\n \\na      generative      model      (i.e.,      diffusion      model)      for      this      task      using      RL-  \\n \\n  based      fine-tuning      so      that      we      can      easily      sample      from      p,(-)      during      inference      (i.e.,      simulating      the  \\n \\n  time-reversal      SDE      (2)      with      policies      from   \\n \\n¢   \\n \\n=      T\\\\\\\\\\\\\\'+   \\n \\n1      to      t   \\n \\n=      1),      without      relying      on      MCMC.\\nThis  \\n \\n  addresses      the      concern      of      MCMC      by      avoiding      the      need      to      estimate      p?\\n\"*(-)      and      minimizing      inference  \\n \\n  time,      albeit      at      the      expense      of      increased      training      time.</p><p>10.2}      RL-Based      Approaches  \\n \\n  While      MCMC      has      historically      been      widely      used      for      this      purpose,      recent      works      have      proposed      an  \\n \\n  RL-based      approach      or      its      variant      (e.g.,      Bernton      et      al.      (2019);      Heng      et      al.\\n(2020);      Huang      et      al.\\n(2023);</p><p> \\n \\n  Vargas      et      al.\\n(2023)).\\nFor      example,   \\n \\na      seminal      study      (Zhang      and      Chen,      2021)      introduced   \\n \\na      method  \\n \\n  very      similar      to      reward      backpropagation.\\nIn      their      work,      they      initially      define      Brownian      motion      as   \\n \\na       reference      SDE      (from      7’      to      0),      similar      to      the      pre-trained      diffusion      models      in      our      context.\\nThen,      by  \\n \\n  appropriately      setting      rewards,      Zhang      and      Chen      (2021)      aims      to      learn   \\n \\na      new      SDE      (from      T\\\\\\\\\\\\\\'      to      0)      such  \\n \\n  that      we      can      sample      from      the      target      distribution      at      the      endpoint      0.</p><p> \\n \\n  These      approaches      offer      advantages      over      MCMC      due      to      their      ability      to      minimize      inference      time  \\n \\n  (amortization)      by      constructing   \\n \\na      generative      model      comprising      policies      in      the      training      time      and      solely  \\n \\n  executing      learned      policies      during      inference.\\nUnlike      MCMC,      these      methods      significantly      reduce      the  \\n \\n  inference      cost      as      we      don’t      need      to      be      concerned      about      the      mixing      time.\\nAdditionally,      RL-based  \\n \\n  approaches      have      the      potential      to      effectively      handle      high-dimensional,      complex      (i.e.,      multi-modal)  \\n \\n  distributions      by      harnessing      the      considerable      expressive      power      arising      from      latent      states.</p><p> \\n \\n  Despite      these      advantages,      it      is      often      unclear      how      to      select   \\n \\na      reference      SDE      in      these      works.\\nIn  \\n \\n  contrast,      in      RL-based      fine-tuning      of      diffusion      models,      we      directly      leverage      pre-trained      diffusion  \\n \\n  models      as      the      reference      SDEs,      i.e.,      as      our      prior      knowledge.</p><p> \\n \\n  Key      message.\\nFinally,      it      is      worth      noting      that      we      can      technically      utilize      any      off-the-shelf      RL      algo-  \\n \\n  rithms      for      training.\\nFor      example,      Zhang      and      Chen      (2021)      employs      reward      backpropagation,      while  \\n \\n  works      in      Gflownets      typically      leverage      PCL,      as      we      mention      in      Section      6.3.\\nHowever,      additionally,      it  \\n \\n  is      possible      to      use      PPO      or      reward-weighted      MLE      mentioned      in      Section      4.2.</p><p>11      Closely      Related      Directions</p><p>Lastly,      we      highlight      several      closely      related      directions      we      have      not      yet      discussed.</p><p> \\n \\n  Aligning      text-to-image      models.\\nWe      recognize      that      current      efforts      in      fine-tuning      diffusion      models  \\n \\n  primarily      revolve      around      aligning      text-to-image      models      using      human      feedback      (Lee      et      al.,      2023;  \\n \\n  Wallace      et      al.,      2023;      Wu      et      al.,      2023;      Yang      et      al.,      2023).\\nIn      this      paper,      our      goal      is      to      consider   \\n \\na      more  \\n \\n  general      and      fundamental      formulation      that      is      not      tailored      to      any      particular      task.</p><p> \\n \\n  Diffusion      models      for      RL.\\nDiffusion      models      are      well-suited      for      specific      reinforcement      learning  \\n \\n  applications      due      to      their      ability      to      model      complex      and      multimodal      distributions      as      polices      (Janner  \\n \\n  et      al.,      2022;      Ajay      et      al.,      2023;      Wang      et      al.,      2022;      Hansen-Estruch      et      al.,      2023;      Du      et      al.,      2024;      Zhu  \\n \\n  et      al.,      2023).\\nFor      example,      Wang      et      al.\\n(2022)      use      conditional      diffusion      models      as      policies      and       demonstrate      their      effectiveness      in      typical      offline      RL      benchmarks.\\nWhile      RL-based      fine-tuning      also  \\n \\n  aims      to      maximize      certain      reward      functions,      the      primary      focus      of      this      tutorial      covers      methods      that  \\n \\n  leverage      pre-trained      diffusion      models      for      this      purpose.</p><p> \\n \\n  RL      from      human      feedback      (RLHF)      for      language      models.\\nRLHF      is      widely      discussed      in      the  \\n \\n  context      of      language      models,      where      many      algorithms      share      similarities      with      those      used      in      diffusion  \\n \\n  models      (Ouyang      et      al.,      2022;      Bai      et      al.,      2022;      Casper      et      al.,      2023).\\nFor      instance,      both      domains  \\n \\n  commonly      employ      PPO      as   \\n \\na      standard      approach,      and      reward-weighted      training      (or      reward-weighted  \\n \\n  MLE)      often      serves      as   \\n \\na      basic      baseline      approach.\\nDespite      these      similarities,      nuanced      differences  \\n \\n  exist      mainly      because      diffusion      models      typically      operate      in   \\n \\na      continuous      input      space      rather      than  \\n \\n  a      discrete      one.\\nMoreover,      techniques      like      value-weighted      sampling      emerge      uniquely      within      the  \\n \\n  context      of      diffusion      models.</p><p>12      Summary</p><p> \\n \\n  In      this      article,      we      comprehensively      explain      how      fine-tuning      diffusion      models      to      maximize      down-  \\n \\n  stream      reward      functions      can      be      formalized      as   \\n \\na      reinforcement      learning      (RL)      problem      in      entropy-  \\n \\n  regularized      Markov      decision      processes      (MDPs).\\nBased      on      this      viewpoint,      we      elaborate      on      the  \\n \\n  application      of      various      RL      algorithms,      such      as      PPO      and      reward-weighted      MLE,      specifically      tailored  \\n \\n  for      fine-tuning      diffusion      models.\\nAdditionally,      we      categorize      different      scenarios      based      on      how  \\n \\n  reward      feedback      is      obtained.\\nWhile      this      categorization      is      not      always      explicitly      mentioned      in      many  \\n \\n  existing      works,      it      is      crucial      when      selecting      appropriate      algorithms.\\nFinally,      we      discuss      the      rela-  \\n \\n  tionships      with      several      related      topics,      including      classifier      guidance,      Gflownets,      path      integral      control  \\n \\n  theory,      and      sampling      from      unnormalized      distributions.</p><h10>9.3.      Connection      with      RL-Based      Fine-Tuning</h10><p>ning  \\n \\n  RL-based      fine-tuning      shares   \\n \\na      close      relationship      with      flow-based      training.\\nTo      illustrate      this,      consider  \\n \\n  the      reference      SDE      (from      7’      to      0)      following      the      pre-trained      diffusion      model.\\nThen,      akin      to      flow-  \\n \\n  based      training,      we      consider      the      coupling      of      the      reference      SDE      conditioned      on      state      at      time   \\n \\n0       (i.e.,      distribution      over      xo-r      conditioned      on      x9)      and      the      target      distribution      at      time   \\n \\n0      (i.e.,      p,(-)   \\n \\n«       exp(r(-)/a)p?\\n\"°(-)),      denoting      the      induced      distribution      as      Q*™.</p><p>Informal      characterization      of      Q™°*.\\nInformally,      Q*\"°      is      introduced      as      follows.\\nWhen      discretizing  \\n \\n  the      time      step,      by      denoting      the      distribution      induced      by      the      reference      SDE      as      p™\\\\\\\\\\\\\\'(xp,-+-   \\n \\n,      9),      we define Qi\"      (xr,   \\n \\na   \\n \\n,      0)   \\n \\n=      p(x,      nr      Ly   \\n \\n|      L0)Pr      (Xo).</p><p>The      distribution      Q*°      above      is   \\n \\na      limiting      distribution      obtained      by      making      the      discretization      step small.</p><p>Training.\\nNow,      we      introduce   \\n \\na      new      SDE      parameterized      by   \\n \\n@      such      as      an      neural      network      to      align  \\n \\n  the      induced      distribution      P’      with      Q®\"*.\\nActually,      in      the      continuous      formulation,      the      RL-problem      (18) in      Section      4.2      is      equal      to      solving argmin      KL(P?||Q\"™).\\n \\n \\n  0 This      formalization      has      been      presented      in      Uehara      et      al.\\n(2024).\\nIntuitively,      as      we      see      in      Theorem      3,  \\n \\n  this      is      expected      because      we      see      that      the      posterior      distribution      induced      by      pre-trained      models:</p><p>ref   \\n \\n(      PD      (ar,-++      21   \\n \\n|      to)      (=      vey      (er|er_-1)p      Pp      (ar-1   \\n \\n|      er_2)      +++      pp\"      (x1   \\n \\n|      20), remains      preserved      after      fine-tuning      based      on      the      RL-formulation      (18).</p><p> \\n \\n  Comparison      with      flow-based      training.\\nIn      contrast      to      flow-based      training,      RL-based      fine-tuning  \\n \\n  minimizes      the      inverse      KL      divergence      rather      than      the      KL      divergence.\\nIntuitively,      this      is      because,  \\n \\n  unlike      Qt™*      or      Q*>   \\n \\n,      we      cannot      sample      from      Q*”      (recall      that   \\n \\na      marginal      distribution      at      time   \\n \\n0      in  \\n \\n  Qi,      i.e.,      p,,      is      unnormalized);      on      the      other      hand,      the      marginal      distributions      at      time   \\n \\n0      in      both      Q\\\\\\\\\\\\\\'™*  \\n \\n  and      Q*°°>      are      data      distributions.</p><p> \\n \\n  Remark   \\n \\n7      (Extension      to      f-divergence).\\nEach      of      these      methods      can      be      technically      extended      by  \\n \\n  incorporating      more      general      divergences      beyond      KL      divergence.\\nFor      instance,      in      the      context      of  \\n \\n  fine-tuning,      please      see      Tang      (2024).</p><p>10      Connection      with      Sampling      from      Unnormalized      Distributions</p><p> \\n \\n  Numerous      studies      delve      into      sampling      from      an      unnormalized      distribution      proportional      to      exp(r()),  \\n \\n  commonly      known      as      the      Gibbs      distribution,      extensively      discussed      in      the      computational      statistics  \\n \\n  and      statistical      physics      community      (Robert,      2014).\\nThis      issue      is      pertinent      to      our      work,      as      during  \\n \\n  fine-tuning,      the      target      distribution      is      also      formulated      in      the      Gibbs      distribution      form      in      (10),      which      is  \\n \\n  proportional      to      exp(r(a))p?\\n\"*      (x).</p><p> \\n \\n  The      literature      employs      two      main      approaches      to      tackle      this      challenge:      Markov      Chain      Monte  \\n \\n  Carlo      (MCMC)      and      RL-based      methods.\\nIn      particular,      the      RL-based      diffusion      model      fine-tuning  \\n \\n  we      have      discussed      so      far      is      closely      related      to      the      latter      approach.\\nIn      this      section,      we      explain      and       compare      these      two      approaches.</p><h10>10.1      Markov      Chain      Monte      Carlo      (MCMC)</h10><p>CMC)  \\n \\n  In      MCMC,   \\n \\na      Markov      chain      is      constructed      to      approximate   \\n \\na      target      distribution      such      that      the      equilib-  \\n \\n  rium      distribution      of      the      Markov      chain      converges      to      the      target      distribution.\\nHowever,      dealing      with  \\n \\n  high-dimensional      domain      spaces      is      challenging      for      MCMC.\\n \\n \\nA      common      strategy      involves      leveraging  \\n \\n  gradient      information,      such      as      the      Metropolis-adjusted      Langevin      algorithm      (MALA)      (Besag,      1994)  \\n \\n  or      Hamiltonian      Monte      Carlo      (HMC)      (Neal      et      al.,      2011;      Girolami      and      Calderhead,      2011).\\nMALA  \\n \\n  shares      similarities      with      classifier      guidance      and      value-weighted      sampling,      as      both      methods      depend  \\n \\n  on      the      first-order      information      from      reward      (or      value)      functions.</p><p> \\n \\n  Can      we      use      MCMC      for      fine-tuning      diffusion      models?\\nIn      the      context      of      fine-tuning      dif-  \\n \\n  fusion      models,      while      it      may      seem      intuitive      to      sample      from      the      target      distribution      (10)      (i.e.,  \\n \\n  Dy   \\n \\n&      exp(r(x))pP\"*(x))      using      MCMC,      it’s      not      straightforward      for      the      following      reasons:</p><p>¢\\n \\n   We      lack      of      an      analytical      form      of      p?\\n\"*(-).<ul><li>*\\n \\n   Additionally,      even      if      we      can      estimate      p?\\n’°(-)      in      an      unbiased      manner,      as      in      Chen      et      al.\\n(2018),  \\n \\n  the      mixing      time      for      obtaining   \\n \\na      single      sample      in      MCMC      might      be      lengthy.</li></ul></p><p> \\n \\n  Therefore,      we      explore      creating   \\n \\na      generative      model      (i.e.,      diffusion      model)      for      this      task      using      RL-  \\n \\n  based      fine-tuning      so      that      we      can      easily      sample      from      p,(-)      during      inference      (i.e.,      simulating      the  \\n \\n  time-reversal      SDE      (2)      with      policies      from   \\n \\n¢   \\n \\n=      T\\\\\\\\\\\\\\'+   \\n \\n1      to      t   \\n \\n=      1),      without      relying      on      MCMC.\\nThis  \\n \\n  addresses      the      concern      of      MCMC      by      avoiding      the      need      to      estimate      p?\\n\"*(-)      and      minimizing      inference  \\n \\n  time,      albeit      at      the      expense      of      increased      training      time.</p><p>10.2}      RL-Based      Approaches  \\n \\n  While      MCMC      has      historically      been      widely      used      for      this      purpose,      recent      works      have      proposed      an  \\n \\n  RL-based      approach      or      its      variant      (e.g.,      Bernton      et      al.      (2019);      Heng      et      al.\\n(2020);      Huang      et      al.\\n(2023);</p><p> \\n \\n  Vargas      et      al.\\n(2023)).\\nFor      example,   \\n \\na      seminal      study      (Zhang      and      Chen,      2021)      introduced   \\n \\na      method  \\n \\n  very      similar      to      reward      backpropagation.\\nIn      their      work,      they      initially      define      Brownian      motion      as   \\n \\na       reference      SDE      (from      7’      to      0),      similar      to      the      pre-trained      diffusion      models      in      our      context.\\nThen,      by  \\n \\n  appropriately      setting      rewards,      Zhang      and      Chen      (2021)      aims      to      learn   \\n \\na      new      SDE      (from      T\\\\\\\\\\\\\\'      to      0)      such  \\n \\n  that      we      can      sample      from      the      target      distribution      at      the      endpoint      0.</p><p> \\n \\n  These      approaches      offer      advantages      over      MCMC      due      to      their      ability      to      minimize      inference      time  \\n \\n  (amortization)      by      constructing   \\n \\na      generative      model      comprising      policies      in      the      training      time      and      solely  \\n \\n  executing      learned      policies      during      inference.\\nUnlike      MCMC,      these      methods      significantly      reduce      the  \\n \\n  inference      cost      as      we      don’t      need      to      be      concerned      about      the      mixing      time.\\nAdditionally,      RL-based  \\n \\n  approaches      have      the      potential      to      effectively      handle      high-dimensional,      complex      (i.e.,      multi-modal)  \\n \\n  distributions      by      harnessing      the      considerable      expressive      power      arising      from      latent      states.</p><p> \\n \\n  Despite      these      advantages,      it      is      often      unclear      how      to      select   \\n \\na      reference      SDE      in      these      works.\\nIn  \\n \\n  contrast,      in      RL-based      fine-tuning      of      diffusion      models,      we      directly      leverage      pre-trained      diffusion  \\n \\n  models      as      the      reference      SDEs,      i.e.,      as      our      prior      knowledge.</p><p> \\n \\n  Key      message.\\nFinally,      it      is      worth      noting      that      we      can      technically      utilize      any      off-the-shelf      RL      algo-  \\n \\n  rithms      for      training.\\nFor      example,      Zhang      and      Chen      (2021)      employs      reward      backpropagation,      while  \\n \\n  works      in      Gflownets      typically      leverage      PCL,      as      we      mention      in      Section      6.3.\\nHowever,      additionally,      it  \\n \\n  is      possible      to      use      PPO      or      reward-weighted      MLE      mentioned      in      Section      4.2.</p><p>11      Closely      Related      Directions</p><p>Lastly,      we      highlight      several      closely      related      directions      we      have      not      yet      discussed.</p><p> \\n \\n  Aligning      text-to-image      models.\\nWe      recognize      that      current      efforts      in      fine-tuning      diffusion      models  \\n \\n  primarily      revolve      around      aligning      text-to-image      models      using      human      feedback      (Lee      et      al.,      2023;  \\n \\n  Wallace      et      al.,      2023;      Wu      et      al.,      2023;      Yang      et      al.,      2023).\\nIn      this      paper,      our      goal      is      to      consider   \\n \\na      more  \\n \\n  general      and      fundamental      formulation      that      is      not      tailored      to      any      particular      task.</p><p> \\n \\n  Diffusion      models      for      RL.\\nDiffusion      models      are      well-suited      for      specific      reinforcement      learning  \\n \\n  applications      due      to      their      ability      to      model      complex      and      multimodal      distributions      as      polices      (Janner  \\n \\n  et      al.,      2022;      Ajay      et      al.,      2023;      Wang      et      al.,      2022;      Hansen-Estruch      et      al.,      2023;      Du      et      al.,      2024;      Zhu  \\n \\n  et      al.,      2023).\\nFor      example,      Wang      et      al.\\n(2022)      use      conditional      diffusion      models      as      policies      and       demonstrate      their      effectiveness      in      typical      offline      RL      benchmarks.\\nWhile      RL-based      fine-tuning      also  \\n \\n  aims      to      maximize      certain      reward      functions,      the      primary      focus      of      this      tutorial      covers      methods      that  \\n \\n  leverage      pre-trained      diffusion      models      for      this      purpose.</p><p> \\n \\n  RL      from      human      feedback      (RLHF)      for      language      models.\\nRLHF      is      widely      discussed      in      the  \\n \\n  context      of      language      models,      where      many      algorithms      share      similarities      with      those      used      in      diffusion  \\n \\n  models      (Ouyang      et      al.,      2022;      Bai      et      al.,      2022;      Casper      et      al.,      2023).\\nFor      instance,      both      domains  \\n \\n  commonly      employ      PPO      as   \\n \\na      standard      approach,      and      reward-weighted      training      (or      reward-weighted  \\n \\n  MLE)      often      serves      as   \\n \\na      basic      baseline      approach.\\nDespite      these      similarities,      nuanced      differences  \\n \\n  exist      mainly      because      diffusion      models      typically      operate      in   \\n \\na      continuous      input      space      rather      than  \\n \\n  a      discrete      one.\\nMoreover,      techniques      like      value-weighted      sampling      emerge      uniquely      within      the  \\n \\n  context      of      diffusion      models.</p><p>12      Summary</p><p> \\n \\n  In      this      article,      we      comprehensively      explain      how      fine-tuning      diffusion      models      to      maximize      down-  \\n \\n  stream      reward      functions      can      be      formalized      as   \\n \\na      reinforcement      learning      (RL)      problem      in      entropy-  \\n \\n  regularized      Markov      decision      processes      (MDPs).\\nBased      on      this      viewpoint,      we      elaborate      on      the  \\n \\n  application      of      various      RL      algorithms,      such      as      PPO      and      reward-weighted      MLE,      specifically      tailored  \\n \\n  for      fine-tuning      diffusion      models.\\nAdditionally,      we      categorize      different      scenarios      based      on      how  \\n \\n  reward      feedback      is      obtained.\\nWhile      this      categorization      is      not      always      explicitly      mentioned      in      many  \\n \\n  existing      works,      it      is      crucial      when      selecting      appropriate      algorithms.\\nFinally,      we      discuss      the      rela-  \\n \\n  tionships      with      several      related      topics,      including      classifier      guidance,      Gflownets,      path      integral      control  \\n \\n  theory,      and      sampling      from      unnormalized      distributions.</p><h4>References</h4><p>Agarwal,      A.,      N.      Jiang,      S.      M.      Kakade,      and      W.      Sun      (2019).\\nReinforcement      learning:      Theory      and       algorithms.\\nCS      Dept.,      UW      Seattle,      Seattle,      WA,      USA,      Tech.\\nRep,      10-4.</p><p>Agarwal,      V.      and      D.      R.      Kelley      (2022).\\nThe      genetic      and      biochemical      determinants      of      mrna      degradation  \\n \\n  rates      in      mammals.\\nGenome      biology      23(1),      245.</p><p> \\n \\n  Ajay,      A.,      Y.      Du,      A.      Gupta,      J.      B.      Tenenbaum,      T.      S.      Jaakkola,      and      P.      Agrawal      (2023).\\nIs      conditional  \\n \\n  generative      modeling      all      you      need      for      decision      making?\\nIn      The      Eleventh      International      Conference  \\n \\n  on      Learning      Representations.</p><p>Albergo,      M.      S.      and      E.      Vanden-Eijnden      (2022).\\nBuilding      normalizing      flows      with      stochastic      inter-  \\n \\n  polants.\\narXiv      preprint      arXiv:2209.15571.</p><p>Anderson,      B.      D.      (1982).\\nReverse-time      diffusion      equation      models.\\nStochastic      Processes      and      their  \\n \\n  Applications      12(3),      313-326.</p><p> \\n \\n  Austin,      J.,      D.      D.      Johnson,      J.      Ho,      D.      Tarlow,      and      R.      Van      Den      Berg      (2021).\\nStructured      denoising  \\n \\n  diffusion      models      in      discrete      state-spaces.\\nAdvances      in      Neural      Information      Processing      Systems      34,  \\n \\n  17981-17993.</p><p>Avdeyey,      P.,      C.      Shi,      Y.      Tan,      K.      Dudnyk,      and      J.      Zhou      (2023).\\nDirichlet      diffusion      score      model      for  \\n \\n  biological      sequence      generation.\\narXiv      preprint      arXiv:2305.\\n10699.</p><p>Bai,      Y.,      S.      Kadavath,      S.      Kundu,      A.      Askell,      J.      Kernion,      A.      Jones,      A.      Chen,      A.      Goldie,      A.      Mirhoseini, C.      McKinnon,      et      al.\\n(2022).\\nConstitutional      ai:      Harmlessness      from      ai      feedback.\\narXiv      preprint  \\n \\n  arXiv:2212.08073.</p><p> \\n \\n  Bansal,      A.,      H.-M.\\nChu,      A.      Schwarzschild,      S.      Sengupta,      M.      Goldblum,      J.      Geiping,      and      T.      Goldstein  \\n \\n  (2023).\\nUniversal      guidance      for      diffusion      models.\\nIn      Proceedings      of      the      IEEE/CVF      Conference  \\n \\n  on      Computer      Vision      and      Pattern      Recognition,      pp.\\n843-852.</p><p>Bengio,      Y.,      S.      Lahlou,      T.      Deleu,      E.      J.      Hu,      M.      Tiwari,      and      E.      Bengio      (2023).\\nGflownet      foundations.\\n \\n \\n  Journal      of      Machine      Learning      Research      24(210),      1-55.</p><p> \\n \\n  Benton,      J.,      Y.      Shi,      V.      De      Bortoli,      G.      Deligiannidis,      and      A.      Doucet      (2024).\\nFrom      denoising      diffusions  \\n \\n  to      denoising      markov      models.\\nJournal      of      the      Royal      Statistical      Society      Series      B:      Statistical  \\n \\n  Methodology      86(2),      286-301.</p><p>Bernton,      E.,      J.      Heng,      A.      Doucet,      and      P.      E.      Jacob      (2019).\\nSchr\\\\\\\\\\\\\\\\””      odinger      bridge      samplers.\\narXiv  \\n \\n  preprint      arXiv:      1912.13170.</p><p>Besag,      J.\\n(1994).\\nComments      on      “representations      of      knowledge      in      complex      systems”      by      u.      grenander  \\n \\n  and      mi      miller.\\nJ.      Roy.\\nStatist.\\nSoc.      Ser.\\n \\n \\nB      56(591-592),      4.</p><p>Black,      K.,      M.      Janner,      Y.      Du,      I.      Kostrikov,      and      S.      Levine      (2023).\\nTraining      diffusion      models      with  \\n \\n  reinforcement      learning.\\narXiv      preprint      arXiv:2305.13301.</p><p> \\n \\n  Campbell,      A.,      J.      Benton,      V.      De      Bortoli,      T.      Rainforth,      G.      Deligiannidis,      and      A.      Doucet      (2022).\\n \\n \\nA       continuous      time      framework      for      discrete      denoising      models.\\nAdvances      in      Neural      Information  \\n \\n  Processing      Systems      35,      28266-28279.</p><p> \\n \\n  Campbell,      A.,      J.      Yim,      R.      Barzilay,      T.      Rainforth,      and      T.      Jaakkola      (2024).\\nGenerative      flows      on  \\n \\n  discrete      state-spaces:      Enabling      multimodal      flows      with      applications      to      protein      co-design.\\narXiv  \\n \\n  preprint      arXiv:2402.04997.</p><p>Cao,      H.,      C.      Tan,      Z.      Gao,      Y.      Xu,      G.      Chen,      P.-A.\\nHeng,      and      S.      Z.      Li      (2024).\\n \\n \\nA      survey      on      generative  \\n \\n  diffusion      models.\\n[EEE      Transactions      on      Knowledge      and      Data      Engineering.</p><p>Casper,      S.,      X.      Davies,      C.      Shi,      T.      K.      Gilbert,      J.      Scheurer,      J.      Rando,      R.      Freedman,      T.      Korbak, D.      Lindner,   \\n \\nP P.      Freire,      et      al.\\n(2023).\\nOpen      problems      and      fundamental      limitations      of      reinforcement  \\n \\n  learning      from      human      feedback.\\narXiv      preprint      arXiv:2307.15217.</p><p>Castillo-Hair,      S.      M.      and      G.      Seelig      (2021).\\nMachine      learning      for      designing      next-generation      mrna  \\n \\n  therapeutics.\\nAccounts      of      Chemical      Research      55(1),      24-34.</p><p>Chang,      J.      D.,      M.      Uehara,      D.      Sreenivas,      R.      Kidambi,      and      W.      Sun      (2021).\\nMitigating      covariate      shift  \\n \\n  in      imitation      learning      via      offline      data      without      great      coverage.\\narXiv      preprint      arXiv:2106.03207.</p><p>Chen,      M.,      S.      Mei,      J.      Fan,      and      M.      Wang      (2024).\\nAn      overview      of      diffusion      models:      Applications,  \\n \\n  guided      generation,      statistical      rates      and      optimization.\\narXiv      preprint      arXiv:2404.07771.</p><p>Chen,      R.      T.,      Y.      Rubanova,      J.      Bettencourt,      and      D.      K.      Duvenaud      (2018).\\nNeural      ordinary      differential  \\n \\n  equations.\\nAdvances      in      neural      information      processing      systems      31.</p><p>Chung,      H.,      J.      Kim,      M.      T.      Mccann,      M.      L.      Klasky,      and      J.      C.      Ye      (2022).\\nDiffusion      posterior      sampling  \\n \\n  for      general      noisy      inverse      problems.\\narXiv      preprint      arXiv:2209.\\n14687.</p><p> \\n \\n  Chung,      H.,      B.      Sim,      D.      Ryu,      and      J.      C.      Ye      (2022).\\nImproving      diffusion      models      for      inverse      problems  \\n \\n  using      manifold      constraints.\\nAdvances      in      Neural      Information      Processing      Systems      35,      25683-—  \\n \\n  25696.</p><p>Clark,      K.,      P.      Vicol,      K.      Swersky,      and      D.      J.      Fleet      (2023).\\nDirectly      fine-tuning      diffusion      models      on  \\n \\n  differentiable      rewards.\\narXiv      preprint      arXiv:2309.\\n17400.</p><p> \\n \\n  De      Bortoli,      V.,      E.      Mathieu,      M.      Hutchinson,      J.      Thornton,      Y.      W.      Teh,      and      A.      Doucet      (2022).\\nRieman-  \\n \\n  nian      score-based      generative      modelling.\\nAdvances      in      Neural      Information      Processing      Systems      35,  \\n \\n  2406-2422.</p><p>Deleu,      T.,      P.      Nouri,      N.      Malkin,      D.      Precup,      and      Y.      Bengio      (2024).\\nDiscrete      probabilistic      inference      as  \\n \\n  control      in      multi-path      environments.\\narXiv      preprint      arXiv:2402.10309.</p><p>Dhariwal,      P.      and      A.      Nichol      (2021).\\nDiffusion      models      beat      gans      on      image      synthesis.\\nAdvances      in  \\n \\n  neural      information      processing      systems      34,      8780-8794.</p><p> \\n \\n  Du,      Y.,      S.      Yang,      B.      Dai,      H.      Dai,      O.      Nachum,      J.      Tenenbaum,      D.      Schuurmans,      and      P.      Abbeel      (2024).\\n \\n \\n  Learning      universal      policies      via      text-guided      video      generation.\\nAdvances      in      Neural      Information  \\n \\n  Processing      Systems      36.</p><p>Fan,      Y.,      O.      Watkins,      Y.      Du,      H.      Liu,      M.      Ryu,      C.      Boutilier,      P.      Abbeel,      M.      Ghavamzadeh,      K.      Lee,      and K.      Lee      (2023) 023).\\nDPOK:      Reinforcement      learning      for      fine-tuning      text-to-image      diffusion      models.\\n \\n \\n  arXiv      preprint      arXiv:2305.16381.</p><p>Finn,      C.,      S.      Levine,      and      P.      Abbeel      (2016).\\nGuided      cost      learning:      Deep      inverse      optimal      control      via  \\n \\n  policy      optimization.\\nIn      International      conference      on      machine      learning,      pp.\\n49-58.\\nPMLR.</p><p>Fox,      R.,      A.      Pakman,      and      N.      Tishby      (2015).\\nTaming      the      noise      in      reinforcement      learning      via      soft  \\n \\n  updates.\\narXiv      preprint      arXiv:      1512.08562.</p><p>Frey,      N.      C.,      D.      Berenberg,      K.      Zadorozhny,      J.      Kleinhenz,      J.      Lafrance-Vanasse,      I.      Hotzel,      Y.      Wu, S.      Ra,   \\n \\nR R.      Bonneau,      K.      Cho,      et      al.\\n(2023).\\nProtein      discovery      with      discrete      walk-jump      sampling.\\n \\n \\n  arXiv      preprint      arXiv:2306.\\n12360.</p><p>Geist,      M.,      B.      Scherrer,      and      O.      Pietquin      (2019).\\n \\n \\nA      theory      of      regularized      markov      decision      processes.\\n \\n \\n  In      International      Conference      on      Machine      Learning,      pp.\\n2160-2169.\\nPMLR.</p><p> \\n \\n  Girolami,      M.      and      B.      Calderhead      (2011).\\nRiemann      manifold      langevin      and      hamiltonian      monte  \\n \\n  carlo      methods.\\nJournal      of      the      Royal      Statistical      Society      Series      B:      Statistical      Methodology      73(2),  \\n \\n  123-214.</p><p>Gosai,      S.      J.,      R.      I.      Castro,      N.      Fuentes,      J.      C.      Butts,      S.      Kales,      R.      R.      Noche,      K.      Mouri,      P.      C.      Sabeti, S.   \\n \\nK K.      Reilly,      and      R.      Tewhey      (2023).\\nMachine-guided      design      of      synthetic      cell      type-specific  \\n \\n  cis-regulatory      elements.\\nbioRxiv.</p><p>Haarnoja,      T.,      H.      Tang,      P.      Abbeel,      and      S.      Levine      (2017).\\nReinforcement      learning      with      deep      energy-  \\n \\n  based      policies.\\nIn      International      conference      on      machine      learning,      pp.\\n1352-1361.\\nPMLR.</p><p>Hansen-Estruch,      P.,      I.      Kostrikov,      M.      Janner,      J.      G.      Kuba,      and      S.      Levine      (2023).\\nIdql:      Implicit  \\n \\n  q-learning      as      an      actor-critic      method      with      diffusion      policies.\\narXiv      preprint      arXiv:2304.10573.</p><p>Heng,      J.,      A.      N.      Bishop,      G.      Deligiannidis,      and      A.      Doucet      (2020).\\nControlled      sequential      monte      carlo.</p><p>Ho,      J.,      A.      Jain,      and      P.      Abbeel      (2020).\\nDenoising      diffusion      probabilistic      models.\\nAdvances      in      neural  \\n \\n  information      processing      systems      33,      6840-6851.</p><p>Ho,      J.      and      T.      Salimans      (2022).\\nClassifier-free      diffusion      guidance.\\narXiv      preprint      arXiv:2207.12598.</p><p>Ho,      J.,      T.      Salimans,      A.      Gritsenko,      W.      Chan,      M.      Norouzi,      and      D.      J.      Fleet      (2022).\\nVideo      diffusion  \\n \\n  models.\\nAdvances      in      Neural      Information      Processing      Systems      35,      8633-8646.</p><p> \\n \\n  Hoogeboom,      E.,      V.      G.      Satorras,      C.      Vignac,      and      M.      Welling      (2022).\\nEquivariant      diffusion      for  \\n \\n  molecule      generation      in      3d.\\nIn      International      conference      on      machine      learning,      pp.\\n8867-8887.</p><h4>      PMLR.</h4><p>Huang,      X.,      H.      Dong,      H.      Yifan,      Y.      Ma,      and      T.      Zhang      (2023).\\nReverse      diffusion      monte      carlo.\\nIn      The  \\n \\n  Twelfth      International      Conference      on      Learning      Representations.</p><p>Janner,      M.,      Y.      Du,      J.      B.      Tenenbaum,      and      S.      Levine      (2022).\\nPlanning      with      diffusion      for      flexible  \\n \\n  behavior      synthesis.\\narXiv      preprint      arXiv:2205.09991.</p><p> \\n \\n  Jo,      J.,      S.      Lee,      and      S.      J.      Hwang      (2022).\\nScore-based      generative      modeling      of      graphs      via      the      system  \\n \\n  of      stochastic      differential      equations.\\nIn      International      Conference      on      Machine      Learning,      pp.\\n \\n \\n  10362-10383.\\nPMLR.</p><p> \\n \\n  Kazim,      M.,      J.      Hong,      M.-G.\\nKim,      and      K.-K.\\nK.      Kim      (2024).\\nRecent      advances      in      path      integral      control  \\n \\n  for      trajectory      optimization:      An      overview      in      theoretical      and      algorithmic      perspectives.\\nAnnual  \\n \\n  Reviews      in      Control      57,      100931.</p><p>Krishnamoorthy,      S.,      S.      M.      Mashkaria,      and      A.      Grover      (2023).\\nDiffusion      models      for      black-box  \\n \\n  optimization.\\narXiv      preprint      arXiv:2306.07180.</p><p>Kumar,      A.,      A.      Zhou,      G.      Tucker,      and      S.      Levine      (2020).\\nConservative      q-learning      for      offline      reinforce-  \\n \\n  ment      learning.\\nAdvances      in      Neural      Information      Processing      Systems      33,      1179-1191.</p><p>Lal,      A.,      D.      Garfield,      T.      Biancalani,      and      G.      Eraslan      (2024).\\nreglm:      Designing      realistic      regulatory  \\n \\n  dna      with      autoregressive      language      models.\\nbioRxiv,      2024-02.</p><p>Lee,      K.,      H.      Liu,      M.      Ryu,      O.      Watkins,      Y.      Du,      C.      Boutilier,      P.      Abbeel,      M.      Ghavamzadeh,      and      S.      S.      Gu  \\n \\n  (2023).\\nAligning      text-to-image      models      using      human      feedback.\\narXiv      preprint      arXiv:2302.12192.</p><p>Levine,      S.      (2018).\\nReinforcement      learning      and      control      as      probabilistic      inference:      Tutorial      and       review.\\narXiv      preprint      arXiv:      1805.00909.</p><p>Li,      Z.,      Y.      Ni,      T.      A.      B.      Huygelen,      A.      Das,      G.      Xia,      G.-B.\\nStan,      and      Y.      Zhao      (2023).\\nLatent      diffusion  \\n \\n  model      for      dna      sequence      generation.\\narXiv      preprint      arXiv:2310.06150.</p><p>Lipman,      Y.,      R.      T.      Chen,      H.      Ben-Hamu,      M.      Nickel,      and      M.      Le      (2023).\\nFlow      matching      for      generative  \\n \\n  modeling.\\nJCLR      2023.</p><p>Liu,      X.,      C.      Gong,      and      Q.      Liu      (2022).\\nFlow      straight      and      fast:      Learning      to      generate      and      transfer      data  \\n \\n  with      rectified      flow.\\narXiv      preprint      arXiv:2209.03003.</p><p>Liu,      X.,      L.      Wu,      M.      Ye,      and      Q.      Liu      (2022).\\nLet      us      build      bridges:      Understanding      and      extending  \\n \\n  diffusion      generative      models.\\narXiv      preprint      arXiv:2208.14699.</p><p>Lou,      A.,      C.      Meng,      and      S.      Ermon      (2023).\\nDiscrete      diffusion      language      modeling      by      estimating      the  \\n \\n  ratios      of      the      data      distribution.\\narXiv      preprint      arXiv:2310.16834.</p><p>Malkin,      N.,      M.      Jain,      E.      Bengio,      C.      Sun,      and      Y.      Bengio      (2022).\\nTrajectory      balance:      Improved      credit  \\n \\n  assignment      in      gflownets.\\nAdvances      in      Neural      Information      Processing      Systems      35,      5955-5967.</p><p>Mohammadpour,      S.,      E.      Bengio,      E.      Frejinger,      and      P.-L.\\nBacon      (2023).\\nMaximum      entropy      gflownets  \\n \\n  with      soft      q-learning.\\narXiv      preprint      arXiv:2312.14331.</p><p>Nachum,      O.,      M.      Norouzi,      K.      Xu,      and      D.      Schuurmans      (2017).\\nBridging      the      gap      between      value      and       policy      based      reinforcement      learning.\\nAdvances      in      neural      information      processing      systems      30.</p><p>Neal,      R.      M.      et      al.\\n(2011).\\nMcmce      using      hamiltonian      dynamics.\\nHandbook      of      markov      chain      monte  \\n \\n  carlo      2(11),      2.</p><p>Neu,      G.,      A.      Jonsson,      and      V.      Gémez      (2017).\\n \\n \\nA      unified      view      of      entropy-regularized      markov      decision  \\n \\n  processes.\\narXiv      preprint      arXiv:      1705.07798.</p><p>Ouyang,      L.,      J.      Wu,      X.      Jiang,      D.      Almeida,      C.      Wainwright,      P.      Mishkin,      C.      Zhang,      S.      Agarwal, K.      Slama,   \\n \\nA A.      Ray,      et      al.\\n(2022).\\nTraining      language      models      to      follow      instructions      with      human  \\n \\n  feedback.\\nAdvances      in      Neural      Information      Processing      Systems      35,      27730-27744.</p><p>Pattanaik,      L.      and      C.      W.      Coley      (2020).\\nMolecular      representation:      going      long      on      fingerprints.\\n \\n \\n  Chem      6(6),      1204-1207.</p><p>Peng,      X.      B.,      A.      Kumar,      G.      Zhang,      and      S.      Levine      (2019).\\nAdvantage-weighted      regression:      Simple  \\n \\n  and      scalable      off-policy      reinforcement      learning.\\narXiv      preprint      arXiv:1910.00177.</p><p>Peters,      J.,      K.      Mulling,      and      Y.      Altun      (2010).\\nRelative      entropy      policy      search.\\nIn      Proceedings      of      the  \\n \\n  AAAI      Conference      on      Artificial      Intelligence,      Volume      24,      pp.\\n1607-1612.</p><p> \\n \\n  Podell,      D.,      Z.      English,      K.      Lacey,      A.      Blattmann,      T.      Dockhorn,      J.      Miiller,      J.      Penna,      and      R.      Rombach  \\n \\n  (2023).\\nSdxl:      Improving      latent      diffusion      models      for      high-resolution      image      synthesis.\\narXiv  \\n \\n  preprint      arXiv:2307.01952.</p><p>Prabhudesai,      M.,      A.      Goyal,      D.      Pathak,      and      K.      Fragkiadaki      (2023).\\nAligning      text-to-image      diffusion  \\n \\n  models      with      reward      backpropagation.\\narXiv      preprint      arXiv:2310.03739.</p><p>Ramesh,      A.,      P.      Dhariwal,      A.      Nichol,      C.      Chu,      and      M.      Chen      (2022).\\nHierarchical      text-conditional  \\n \\n  image      generation      with      clip      latents.\\narXiv      preprint      arXiv:2204.06125      1(2),      3.</p><p>Rigter,      M.,      B.      Lacerda,      and      N.      Hawes      (2022).\\nRambo-rl:      Robust      adversarial      model-based      offline  \\n \\n  reinforcement      learning.\\nAdvances      in      neural      information      processing      systems      35,      16082-16097.</p><p>Robert,      C.      (2014).\\nStatistical      modeling      and      computation.</p><p>Rogers,      L.      C.      G.      and      D.      Williams      (2000).\\nDiffusions,      Markov      processes      and      martingales:      Volume      2,  \\n \\n  It6é      calculus,      Volume      2.\\nCambridge      university      press.</p><p> \\n \\n  Rombach,      R.,      A.      Blattmann,      D.      Lorenz,      P.      Esser,      and      B.      Ommer      (2022,      June).\\nHigh-resolution  \\n \\n  image      synthesis      with      latent      diffusion      models.\\nIn      Proceedings      of      the      IEEE/CVF      Conference      on  \\n \\n  Computer      Vision      and      Pattern      Recognition      (CVPR),      pp.\\n10684-10695.</p><p>Sarkar,      A.,      Z.      Tang,      C.      Zhao,      and      P.      Koo      (2024).\\nDesigning      dna      with      tunable      regulatory      activity  \\n \\n  using      discrete      diffusion.\\nbioRxiv,      2024-05.</p><p>Schulman,      J.,      X.      Chen,      and      P.      Abbeel      (2017).\\nEquivalence      between      policy      gradients      and      soft  \\n \\n  q-learning.\\narXiv      preprint      arXiv:      1704.06440.</p><p>Schulman,      J.,      S.      Levine,      P.      Abbeel,      M.      Jordan,      and      P.      Moritz      (2015).\\nTrust      region      policy      optimization.\\n \\n \\n  In      International      conference      on      machine      learning,      pp.\\n1889-1897.\\nPMLR.</p><p>Schulman,      J.,      F.      Wolski,      P.      Dhariwal,      A.      Radford,      and      O.      Klimov      (2017).\\nProximal      policy      optimiza-  \\n \\n  tion      algorithms.\\narXiv      preprint      arXiv:      1707.06347.</p><p>Shi,      Y.,      V.      De      Bortoli,      A.      Campbell,      and      A.      Doucet      (2024).\\nDiffusion      schrédinger      bridge      matching.\\n \\n \\n  Advances      in      Neural      Information      Processing      Systems      36.</p><p> \\n \\n  Sohl-Dickstein,      J.,      E.      Weiss,      N.      Maheswaranathan,      and      S.      Ganguli      (2015).\\nDeep      unsupervised  \\n \\n  learning      using      nonequilibrium      thermodynamics.\\nIn      /nternational      conference      on      machine      learning,  \\n \\n  pp.\\n2256-2265.\\nPMLR.</p><p>Song,      J.,      C.      Meng,      and      S.      Ermon      (2020).\\nDenoising      diffusion      implicit      models.\\narXiv      preprint  \\n \\n  arXiv:2010.02502.</p><p>Song,      Y.,      C.      Durkan,      I.      Murray,      and      S.      Ermon      (2021).\\nMaximum      likelihood      training      of      score-based  \\n \\n  diffusion      models.\\nAdvances      in      neural      information      processing      systems      34,      1415-1428.</p><p>Stark,      H.,      B.      Jing,      C.      Wang,      G.      Corso,      B.      Berger,      R.      Barzilay,      and      T.      Jaakkola      (2024).\\nDirichlet  \\n \\n  flow      matching      with      applications      to      dna      sequence      design.\\narXiv      preprint      arXiv:2402.05841.</p><p>Sutton,      R.      S.      and      A.      G.      Barto      (2018).\\nReinforcement      learning:      An      introduction.\\nMIT      press.</p><p>Tang,      W.      (2024).\\nFine-tuning      of      diffusion      models      via      stochastic      control:      entropy      regularization      and       beyond.\\narXiv      preprint      arXiv:2403.06279.</p><p>Tang,      W.      and      H.      Zhao      (2024).\\nScore-based      diffusion      models      via      stochastic      differential      equations—a  \\n \\n  technical      tutorial.\\narXiv      preprint      arXiv:2402.07487.</p><p>Theodorou,      E.,      J.      Buchli,      and      S.      Schaal      (2010).\\n \\n \\nA      generalized      path      integral      control      approach      to  \\n \\n  reinforcement      learning.\\nThe      Journal      of      Machine      Learning      Research      11,      3137-3181.</p><p>Tiapkin,      D.,      N.      Morozov,      A.      Naumov,      and      D.      Vetrov      (2023).\\nGenerative      flow      networks      as      entropy-  \\n \\n  regularized      rl.\\narXiv      preprint      arXiv:2310.12934.</p><p> \\n \\n  Tong,      A.,      N.      Malkin,      G.      Huguet,      Y.      Zhang,      J.      Rector-Brooks,      K.      Fatras,      G.      Wolf,      and      Y.      Bengio  \\n \\n  (2023).\\nConditional      flow      matching:      Simulation-free      dynamic      optimal      transport.\\narXiv      preprint  \\n \\n  arXiv:2302.00482.</p><p>Uehara,      M.      and      W.      Sun      (2021).\\nPessimistic      model-based      offline      reinforcement      learning      under  \\n \\n  partial      coverage.\\narXiv      preprint      arXiv:2107.06226.</p><p>Uehara,      M.,      Y.      Zhao,      K.      Black,      E.      Hajiramezanali,      G.      Scalia,      N.      L.      Diamant,      A.      M.      Tseng, T.      Biancalani,      and      S S.      Levine      (2024).\\nFine-tuning      of      continuous-time      diffusion      models      as      entropy-  \\n \\n  regularized      control.\\narXiv      preprint      arXiv:2402.15194.</p><p> \\n \\n  Uehara,      M.,      Y.      Zhao,      K.      Black,      E.      Hajiramezanali,      G.      Scalia,      N.      L.      Diamant,      A.      M.      Tseng,      S.      Levine,  \\n \\n  and      T.      Biancalani      (2024).\\nFeedback      efficient      online      fine-tuning      of      diffusion      models.\\narXiv      preprint  \\n \\n  arXiv:2402.\\n16359.</p><p> \\n \\n  Uehara,      M.,      Y.      Zhao,      E.      Hajiramezanali,      G.      Scalia,      G.      Eraslan,      A.      Lal,      S.      Levine,      and      T.      Biancalani  \\n \\n  (2024).\\nBridging      model-based      optimization      and      generative      modeling      via      conservative      fine-tuning  \\n \\n  of      diffusion      models.\\narXiv      preprint      arXiv:2405.19673.</p><p>Vargas,      F.,      W.      Grathwohl,      and      A.      Doucet      (2023).\\nDenoising      diffusion      samplers.\\narXiv      preprint  \\n \\n  arXiv:2302.\\n13834.</p><p> \\n \\n  Wallace,      B.,      M.      Dang,      R.      Rafailov,      L.      Zhou,      A.      Lou,      S.      Purushwalkam,      S.      Ermon,      C.      Xiong,      S.      Joty,  \\n \\n  and      N.      Naik      (2023).\\nDiffusion      model      alignment      using      direct      preference      optimization.\\narXiv  \\n \\n  preprint      arXiv:2311.12908.</p><p>Wang,      Z.,      J.      J.      Hunt,      and      M.      Zhou      (2022).\\nDiffusion      policies      as      an      expressive      policy      class      for      offline  \\n \\n  reinforcement      learning.\\narXiv      preprint      arXiv:2208.06193.</p><p>Widatalla,      T.,      R.      Rafailov,      and      B.      Hie      (2024).\\nAligning      protein      generative      models      with      experimental  \\n \\n  fitness      via      direct      preference      optimization.\\nbioRxiv,      2024-05.</p><p>Williams,      G.,      A.      Aldrich,      and      E.      A.      Theodorou      (2017).\\nModel      predictive      path      integral      control:      From  \\n \\n  theory      to      parallel      computation.\\nJournal      of      Guidance,      Control,      and      Dynamics      40(2),      344-357.</p><p>Wu,      X.,      K.      Sun,      F.      Zhu,      R.      Zhao,      and      H.      Li      (2023).\\nBetter      aligning      text-to-image      models      with  \\n \\n  human      preference.\\narXiv      preprint      arXiv:2303.14420.</p><p>Wu,      Y.,      G.      Tucker,      and      O.      Nachum      (2019).\\nBehavior      regularized      offline      reinforcement      learning.\\n \\n \\n  arXiv      preprint      arXiv:      1911.11361.</p><p>Wulfmeier,      M.,      P.      Ondruska,      and      I.      Posner      (2015).\\nMaximum      entropy      deep      inverse      reinforcement  \\n \\n  learning.\\narXiv      preprint      arXiv:      1507.04888.</p><p>Xie,      T.,      C.-A.\\nCheng,      N.      Jiang,      P.      Mineiro,      and      A.      Agarwal      (2021).\\nBellman-consistent      pessimism  \\n \\n  for      offline      reinforcement      learning.\\nAdvances      in      neural      information      processing      systems      34.</p><p>Xu,      M.,      L.      Yu,      Y.      Song,      C.      Shi,      S.      Ermon,      and      J.      Tang      (2022).\\nGeodiff:   \\n \\nA      geometric      diffusion  \\n \\n  model      for      molecular      conformation      generation.\\narXiv      preprint      arXiv:2203.02923.</p><p> \\n \\n  Yang,      L.,      Z.      Zhang,      Y.      Song,      S.      Hong,      R.      Xu,      Y.      Zhao,      W.      Zhang,      B.      Cui,      and      M.-H.\\nYang      (2023).\\n \\n \\n  Diffusion      models:   \\n \\nA      comprehensive      survey      of      methods      and      applications.\\nACM      Computing  \\n \\n  Surveys      56(4),      1-39.</p><p> \\n \\n  Yu,      T.,      G.      Thomas,      L.      Yu,      S.      Ermon,      J.      Y.      Zou,      S.      Levine,      C.      Finn,      and      T.      Ma      (2020).\\nMopo:  \\n \\n  Model-based      offline      policy      optimization.\\nAdvances      in      Neural      Information      Processing      Systems      33,  \\n \\n  14129-14142.</p><p> \\n \\n  Yuan,      H.,      K.      Huang,      C.      Ni,      M.      Chen,      and      M.      Wang      (2023).\\nReward-directed      conditional      diffusion:  \\n \\n  Provable      distribution      estimation      and      reward      improvement.\\nIn      Thirty-seventh      Conference      on  \\n \\n  Neural      Information      Processing      Systems.</p><p>Zhang,      H.      and      T.      Xu      (2023).\\nTowards      controllable      diffusion      models      via      reward-guided      exploration.\\n \\n \\n  arXiv      preprint      arXiv:2304.07132.</p><p>Zhang,      Q.      and      Y.      Chen      (2021).\\nPath      integral      sampler:   \\n \\na      stochastic      control      approach      for      sampling.\\n \\n \\n  arXiv      preprint      arXiv:2111,15141.</p><p> \\n \\n  Zhao,      Y.,      M.      Uehara,      G.      Scalia,      T.      Biancalani,      S.      Levine,      and      E.      Hajiramezanali      (2024).\\n \\n \\n  Adding      conditional      control      to      diffusion      models      with      reinforcement      learning.\\narXiv      preprint  \\n \\n  arXiv:2406.\\n12120.</p><p>Zhou,      Z.,      S.      Kearnes,      L.      Li,      R.      N.      Zare,      and      P.      Riley      (2019).\\nOptimization      of      molecules      via      deep  \\n \\n  reinforcement      learning.\\nScientific      reports      9(1),      10752.</p><p>Zhu,      Z.,      H.      Zhao,      H.      He,      Y.      Zhong,      S.      Zhang,      Y.      Yu,      and      W.      Zhang      (2023).\\nDiffusion      models      for  \\n \\n  reinforcement      learning:   \\n \\nA      survey.\\narXiv      preprint      arXiv:2311.01223.</p><p>Ziebart,      B.      D.,      A.      L.      Maas,      J.      A.      Bagnell,      A.      K.      Dey,      et      al.\\n(2008).\\nMaximum      entropy      inverse  \\n \\n  reinforcement      learning.\\nIn      Aaai,      Volume      8,      pp.\\n1433-1438.\\nChicago,      IL,      USA.</p></html>')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     strategy=\"sections\"\n",
    "from langchain_community.document_loaders.llmsherpa import LLMSherpaFileLoader\n",
    "loader = LLMSherpaFileLoader(\n",
    "    file_path=\"https://arxiv.org/pdf/2402.14207.pdf\",\n",
    "    new_indent_parser=True,\n",
    "    apply_ocr=True,\n",
    "    strategy=\"sections\",\n",
    "    llmsherpa_api_url=\"http://localhost:5010/api/parseDocument?renderFormat=all\",\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 0, 'section_title': 'Abstract'}, page_content='Abstract\\n \\n \\n  We      study      how      to      apply      large      language      models  \\n \\n  to      write      grounded      and      organized      long-form      ar-  \\n \\n  ticles      from      scratch,      with      comparable      breadth  \\n \\n  and      depth      to      Wikipedia      pages.\\nThis      underex-  \\n \\n  plored      problem      poses      new      challenges      at      the  \\n \\n  pre-writing      stage,      including      how      to      research  \\n \\n  the      topic      and      prepare      an      outline      prior      to      writ-  \\n \\n  ing.\\nWe      propose      STORM,   \\n \\na      writing      system  \\n \\n  for      the      Synthesis      of      Topic      Outlines      through  \\n \\n  Retrieval      and      Multi-perspective      Question      Ask-  \\n \\n  ing.\\nSTORM      models      the      pre-writing      stage      by\\n(1)      discovering      diverse      perspectives      in      research-  \\n \\n  ing      the      given      topic,      (2)      simulating      conversa-  \\n \\n  tions      where      writers      carrying      different      perspec-  \\n \\n  tives      pose      questions      to   \\n \\na      topic      expert      grounded  \\n \\n  on      trusted      Internet      sources,      (3)      curating      the      col-  \\n \\n  lected      information      to      create      an      outline.\\n \\n \\n  For      evaluation,      we      curate      FreshWiki,   \\n \\na      dataset  \\n \\n  of      recent      high-quality      Wikipedia      articles,      and       formulate      outline      assessments      to      evaluate      the  \\n \\n  pre-writing      stage.\\nWe      further      gather      feedback  \\n \\n  from      experienced      Wikipedia      editors.\\nCom-  \\n \\n  pared      to      articles      generated      by      an      outline-  \\n \\n  driven      retrieval-augmented      baseline,      more      of  \\n \\n  STORM’;      articles      are      deemed      to      be      organized  \\n \\n  (by   \\n \\na      25%      absolute      increase)      and      broad      in      cov-  \\n \\n  erage      (by      10%).\\nThe      expert      feedback      also  \\n \\n  helps      identify      new      challenges      for      generating  \\n \\n  grounded      long      articles,      such      as      source      bias  \\n \\n  transfer      and      over-association      of      unrelated      facts.\\n1\\n \\n   Introduction  \\n \\n  Large      language      models      (LLMs)      have      demonstrated  \\n \\n  impressive      writing      capabilities      (Yang      et      al.,      2023;  \\n \\n  Pavlik,      2023;      Wenzlaff      and      Spaeth,      2022;      Fitria,  \\n \\n  2023),      but      it      is      unclear      how      we      can      use      them      to  \\n \\n  write      grounded,      long-form      articles,      like      full-length  \\n \\n  Wikipedia      pages.\\nSuch      expository      writing,      which  \\n \\n  seeks      to      inform      the      reader      on   \\n \\na      topic      in      an      or-  \\n \\n  ganized      manner      (Weaver      II      and      Kintsch,      1991;  \\n \\n  Balepur      et      al.,      2023),      requires      thorough      research  \\n \\n  and      planning      in      the      pre-writing      stage      (Rohman,  \\n \\n  Writing  \\n \\n  tify,      evaluate,      and      organize      external      sources   \\n \\n-   \\n \\na      task  \\n \\n  that      is      challenging      even      for      experienced      writers.\\n \\n \\n  Automating      this      process      can      facilitate      individuals  \\n \\n  in      initiating      in-depth      learning      about   \\n \\na      topic      and       greatly      reduce      the      expensive      expert      hours      neces-  \\n \\n  sary      for      their      expository      writing.\\n |  |       ee      Prewriting | \\n | --- | --- | ---\\n |  | =\\n \\n   Full-length  \\n \\n  5      Article |       =      Full-length       5      Article\\n |       arXiv:2402.14207v2      [cs.CL]      8      Apr      2024 | 2022      Winter      Olympics      [=      Outline   \\n \\n|       Opening      Ceremony  \\n \\n  Research      via      Question      Asking  \\n \\n  (A)      Direct      Prompting  \\n \\n  -y      Prompt:      Ask      30      questions      about      the      given      topic.\\n \\n \\n  1.\\nWhen      was      the      opening      ceremony      held?\\n \\n \\n  {22}      2.\\nWhere      was      the      opening      ceremony      held?\\n \\n \\n  LLM      3.\\nHow      many      countries      participated      in      the      opening      ceremony?\\n \\n \\n  (B)      Perspective-Guided      Question      Asking  \\n \\n  Prompt:      You      are      an      event      planner      who      focuses      on      the B® preparation of the opening ceremony.\\n \\n \\n  1.\\nCan      you      provide      any      information      about      the      transportation  \\n \\n  arrangements      for      the      opening      ceremony?\\n \\n \\n  Lim      2.\\nCan      you      provide      any      information      about      the      budget      for      the  \\n \\n  2022      Winter      Olympics      opening      ceremony?\\n \\n \\n  (C)      Conversational      Question      Asking  \\n \\n  Can      you      provide      me      with   \\n \\na      list      of      the      participating      countries  \\n \\n  tim-      in      the      2022      Winter      Olympics      opening      ceremony?\\n \\n \\n  Role1  \\n \\n  The      2022      Winter      Olympics      featured   \\n \\na      diverse      group      of  \\n \\n  countries      participating      in      the      opening      ceremony.\\nThese LLM- included Athletes from over 90 countries will enter the  \\n \\n  Role2      stadium      ina      specific      order.\\n \\n \\n  How      is      the      order      of      participating      countries      in      the      2022  \\n \\n  Winter      Olympics      opening      ceremony      determined?\\n \\n \\n  LLM-  \\n \\n  Role1  \\n \\n  Figure      1:      We      explore      writing      Wikipedia-like      articles  \\n \\n  from      scratch,      which      demands   \\n \\na      pre-writing      stage      before  \\n \\n  producing      the      article.\\nIn      this      stage,      simpler      approaches  \\n \\n  like      Direct      Prompting      have      limited      planning      capacity.\\nIn  \\n \\n  contrast,      STORM      researches      the      topic      via      perspective-  \\n \\n  guided      question      asking      in      simulated      conversations.\\n \\n \\n  1965),      even      before      the      actual      writing      process      can  \\n \\n  start.\\nHowever,      prior      work      on      generating      Wikipedia  \\n \\n  articles      (Banerjee      and      Mitra,      2015;      Minguillén  \\n \\n  et      al.,      2017;      Liu      et      al.,      2018;      Fan      and      Gardent,  \\n \\n  2022)      has      generally      bypassed      the      pre-writing      stage:  \\n \\n  for      instance,      Liu      et      al.\\n(2018)      presume      reference  \\n \\n  documents      are      provided      in      advance,      while      Fan      and       Gardent      (2022)      assume      an      article      outline      is      avail-  \\n \\n  able      and      focus      on      expanding      each      section.\\nThese  \\n \\n  assumptions      do      not      hold      in      general,      as      collecting  \\n \\n  references      and      crafting      outlines      demand      advanced  \\n \\n  information      literacy      skills      (Doyle,      1994)      to      iden- | \\n\\n \\n \\n  We      explore      these      challenges      by      focusing      on      how  \\n \\n  to      generate      Wikipedia-like      articles      from      scratch.\\n \\n \\n  We      decompose      this      problem      into      two      tasks.\\nThe  \\n \\n  first      is      to      conduct      research      to      generate      an      outline,  \\n \\n  i.e.,   \\n \\na      list      of      multi-level      sections,      and      collect   \\n \\na      set      of  \\n \\n  reference      documents.\\nThe      second      uses      the      outline  \\n \\n  and      the      references      to      produce      the      full-length      arti-  \\n \\n  cle.\\nSuch   \\n \\na      task      decomposition      mirrors      the      human  \\n \\n  writing      process      which      usually      includes      phases      of  \\n \\n  pre-writing,      drafting,      and      revising      (Rohman,      1965;  \\n \\n  Munoz-Luna,      2015).\\n \\n \\n  As      pre-trained      language      models      inherently      pos-  \\n \\n  sess   \\n \\na      wealth      of      knowledge,   \\n \\na      direct      approach      is      to  \\n \\n  rely      on      their      parametric      knowledge      for      generating  \\n \\n  outlines      or      even      entire      articles      (Direct      Gen).\\nHow-  \\n \\n  ever,      this      approach      is      limited      by   \\n \\na      lack      of      details  \\n \\n  and      hallucinations      (Xu      et      al.,      2023),      particularly      in  \\n \\n  addressing      long-tail      topics      (Kandpal      et      al.,      2023).\\n \\n \\n  This      underscores      the      importance      of      leveraging      ex-  \\n \\n  ternal      sources,      and      current      strategies      often      involve  \\n \\n  retrieval-augmented      generation      (RAG),      which      cir-  \\n \\n  cles      back      to      the      problem      of      researching      the      topic      in  \\n \\n  the      pre-writing      stage,      as      much      information      cannot  \\n \\n  be      surfaced      through      simple      topic      searches.\\n \\n \\n  Human      learning      theories      (Tawfik      et      al.,      2020;  \\n \\n  Booth      et      al.,      2003)      highlight      asking      effective  \\n \\n  questions      in      information      acquisition.\\nAlthough  \\n \\n  instruction-tuned      models      (Ouyang      et      al.,      2022)      can  \\n \\n  be      prompted      directly      to      generate      questions,      we      find  \\n \\n  that      they      typically      produce      basic      “What”,      “When”,  \\n \\n  and      “Where”      questions      (Figure   \\n \\n1      (A))      which      often  \\n \\n  only      address      surface-level      facts      about      the      topic.\\nTo  \\n \\n  endow      LLMs      with      the      capacity      to      conduct      better  \\n \\n  research,      we      propose      the      STORM      paradigm      for  \\n \\n  the      Synthesis      of      Topic      Outlines      through      Retrieval  \\n \\n  and      Multi-perspective      Question      Asking.\\n \\n \\n  The      design      of      STORM      is      based      on      two      hypothe-  \\n \\n  ses:      (1)      diverse      perspectives      lead      to      varied      ques-  \\n \\n  tions;      (2)      formulating      in-depth      questions      requires  \\n \\n  iterative      research.\\nBuilding      upon      these      hypotheses,  \\n \\n  STORM      employs   \\n \\na      novel      multi-stage      approach.\\nIt  \\n \\n  first      discovers      diverse      perspectives      by      retrieving  \\n \\n  and      analyzing      Wikipedia      articles      from      similar      top-  \\n \\n  ics      and      then      personifies      the      LLM      with      specific      per-  \\n \\n  spectives      for      question      asking      (Figure   \\n \\n1      (B)).\\nNext,  \\n \\n  to      elicit      follow-up      questions      for      iterative      research  \\n \\n  (Figure   \\n \\n1      (C)),      STORM      simulates      multi-turn      con-  \\n \\n  versations      where      the      answers      to      the      generated      ques-  \\n \\n  tions      are      grounded      on      the      Internet.\\nFinally,      based  \\n \\n  on      the      LLM’s      internal      knowledge      and      the      collected  \\n \\n  information,      STORM      creates      an      outline      that      can  \\n \\n  be      expanded      section      by      section      to      develop   \\n \\na      full-  \\n \\n  length      Wikipedia-like      article.\\n \\n \\n  We      evaluate      STORM      using      our      FreshWiki  \\n \\n  dataset      (§2.1)      which      curates      recent,      high-quality  \\n \\n  Wikipedia      articles      to      avoid      data      leakage      during      pre-  \\n \\n  training.!\\nTo      facilitate      the      study      of      the      pre-writing  \\n \\n  stage,      we      define      metrics      for      evaluating      the      outline  \\n \\n  quality      against      human-written      articles.\\n \\n \\n  We      further      invited   \\n \\na      group      of      experienced  \\n \\n  Wikipedia      editors      for      expert      evaluation.\\nThe      ed-  \\n \\n  itors      found      STORM      outperforms      an      outline-driven  \\n \\n  RAG      baseline,      especially      regarding      the      breadth      and       organization      of      the      articles.\\nThey      also      identified  \\n \\n  challenges      for      future      research,      including      address-  \\n \\n  ing      cases      where:      (1)      the      bias      on      the      Internet      affects  \\n \\n  the      generated      articles;      (2)      LLMs      fabricate      connec-  \\n \\n  tions      between      unrelated      facts.\\nThese      challenges  \\n \\n  present      new      frontiers      to      grounded      writing      systems.\\n \\n \\n  Our      main      contributions      include:\\n*\\n \\n   To      evaluate      the      capacity      of      LLM      systems      at  \\n \\n  generating      long-form      grounded      articles      from  \\n \\n  scratch,      and      the      pre-writing      challenge      in      par-  \\n \\n  ticular,      we      curate      the      FreshWiki      dataset      and       establish      evaluation      criteria      for      both      outline  \\n \\n  and      final      article      quality.\\n \\n \\n \\n¢      We      propose      STORM,   \\n \\na      novel      system      that      au-  \\n \\n  tomates      the      pre-writing      stage.\\nSTORM      re-  \\n \\n  searches      the      topic      and      creates      an      outline      by  \\n \\n  using      LLMs      to      ask      incisive      questions      and      re-  \\n \\n  trieving      trusted      information      from      the      Internet.\\n \\n \\n \\n¢      Both      automatic      and      human      evaluation      demon-  \\n \\n  strate      the      effectiveness      of      our      approach.\\nEx-  \\n \\n  pert      feedback      further      reveals      new      challenges  \\n \\n  in      generating      grounded      long-form      articles.\\n2\\n \\n   FreshWiki  \\n \\n  We      study      generating      Wikipedia-like      articles      from  \\n \\n  scratch,      placing      emphasis      on      the      pre-writing  \\n \\n  stage      (Rohman,      1965),      which      involves      the      demand-  \\n \\n  ing      sub-tasks      of      gathering      and      curating      relevant  \\n \\n  information      (“‘research’’).      This      models      the      human ‘Our      resources      and      code      are      released      at      https:      //github.\\n \\n \\n  com/stanford-oval/storm.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 1, 'section_title': 'Domain      Scope      Given      Given       P      Outline?      Refs?'}, page_content='Domain      Scope      Given      Given       P      Outline?      Refs?\\n \\n \\n  Balepur      et      al.\\n(2023)      One      One      para.\\n \\n \\n/      Yes  \\n \\n  Qian      et      al.\\n(2023)      All      One      para.\\n \\n \\n/      No  \\n \\n  Fan      and      Gardent      (2022)      One      Full      article      Yes      No  \\n \\n  Liu      et      al.\\n(2018)      All      One      para.\\n \\n \\n/      Yes  \\n \\n  Sauper      and      Barzilay      (2009)      Two      Full      article      No      No  \\n \\n  Ours      All      Full      article      No      No  \\n \\n  Table      1:      Comparison      of      different      Wikipedia      generation  \\n \\n  setups      in      existing      literature.\\nGenerating      one      paragraph  \\n \\n  does      not      need      an      article      outline.\\n \\n \\n  writing      approach      which      has      prompted      some      educa-  \\n \\n  tors      to      view      Wikipedia      article      writing      as      an      educa-  \\n \\n  tional      exercise      for      academic      training      (Tardy,      2010).\\n \\n \\n  Table   \\n \\n1      compares      our      work      against      prior      bench-  \\n \\n  marks      for      Wikipedia      generation.\\nExisting      work  \\n \\n  has      generally      focused      on      evaluating      the      generation  \\n \\n  of      shorter      snippets      (e.g.,      one      paragraph),      within   \\n \\na       narrower      scope      (e.g.,   \\n \\na      specific      domain      or      two),      or  \\n \\n  when      an      explicit      outline      or      reference      documents  \\n \\n  are      supplied.\\n \\n \\nA      notable      example      is      WikiSum      (Liu  \\n \\n  et      al.,      2018),      which      treats      generating      Wikipedia      ar-  \\n \\n  ticles      as   \\n \\na      multi-document      summarization      problem,  \\n \\n  with      respect      to      the      reference      documents.\\n \\n \\n  Our      setup      emphasizes      the      capability      of      long-  \\n \\n  form      grounded      writing      systems      to      research      and       curate      content.\\nSpecifically,      given   \\n \\na      topic      ¢,      the  \\n \\n  task      is      to      find   \\n \\na      set      of      references   \\n \\n®      and      generate a full-length article S = s1598,, where each  \\n \\n  sentence      s;      cites   \\n \\na      list      of      documents      in      R.7\\n2.1      The      FreshWiki      Dataset\\naset  \\n \\n  Creating   \\n \\na      new      Wikipedia-like      article      demands      not  \\n \\n  only      fluent      writing      but      also      good      research      skills.\\nAs  \\n \\n  modern      LLMs      are      generally      trained      on      Wikipedia  \\n \\n  text,      we      mitigate      data      leakage      by      explicitly      seeking  \\n \\n  out      recent      Wikipedia      articles      that      were      created      (or  \\n \\n  very      heavily      edited)      after      the      training      cutoff      of      the  \\n \\n  LLMs      we      test.\\nOur      process      can      be      repeated      at  \\n \\n  future      dates      when      new      LLMs      emerge.\\n \\n \\n  To      apply      our      date      criteria,      we      focus      on      the      top  \\n \\n  100      most-edited      pages,      based      on      edit      counts,      for  \\n \\n  each      month      from      February      2022      to      September  \\n \\n  2023.      To      ensure      high-quality      references,      we      filter these      articles      to      keep      only      those      having      B-class  \\n \\n  quality      or      above      assessed      by      ORES*.\\nWe      also      ex-  \\n \\n  \"In      practice,      S      also      includes      organizational      elements      such  \\n \\n  as      section      and      subsection      titles,      which      do      not      require      citations.\\n \\n \\n  3      Obtained      from      https:      //wikimedia.\\n \\n \\n  org/api/rest_v1/metrics/edited-pages/  \\n \\n  top-by-edits/en.wikipedia/all-editor-types/  \\n \\n  content/      {year      }/{month}/all-days  \\n \\n  ‘https:      //www.mediawiki.org/wiki/ORES  \\n \\n  clude      list      articles      and      articles      that      have      no      sub-  \\n \\n  sections.\\nWhile      high-quality      Wikipedia      articles  \\n \\n  usually      contain      structured      data      (e.g.,      tables)      and      are  \\n \\n  multi-modal,      we      only      consider      the      plain      text      com-  \\n \\n  ponent      in      constructing      the      dataset      to      simplify      our  \\n \\n  task.\\nMore      details      of      the      dataset      are      in      Appendix      A.\\n2.2      Outline      Creation      and      Evaluation\\ntion  \\n \\n  A      full-length      article      is      hard      to      generate      or      evalu-  \\n \\n  ate      (Xu      et      al.,      2023;      Krishna      et      al.,      2023).\\nWhen  \\n \\n  human      educators      teach      students      academic      writing,  \\n \\n  they      sometimes      supervise      students      at      the      outline  \\n \\n  stage      (Eriksson      and      Makitalo,      2015)      because      an  \\n \\n  extensive      outline      indicates   \\n \\na      comprehensive      under-  \\n \\n  standing      of      the      topic      and      provides   \\n \\na      solid      founda-  \\n \\n  tion      for      writing      the      full-length      article      (Dietz      and       Foley,      2019).\\nInspired      by      this,      we      decompose      the  \\n \\n  generation      of      S      into      two      stages.\\nIn      the      pre-writing  \\n \\n  stage,      we      require      the      system      to      create      an      outline  \\n \\n  O,      which      is      defined      as   \\n \\na      list      of      multi-level      section  \\n \\n  headings®.\\nIn      the      writing      stage,      the      system      uses  \\n \\n  the      topic      t,      the      references      R,      and      an      outline   \\n \\nO      to  \\n \\n  produce      the      full-length      article      S.\\n \\n \\n  To      evaluate      the      outline      coverage,      we      introduce  \\n \\n  two      metrics:      heading      soft      recall      and      heading      en-  \\n \\n  tity      recall.\\nThese      metrics      compare      the      multi-level  \\n \\n  section      headings      of      the      human-written      article,      con-  \\n \\n  sidered      as      ground      truth,      and      those      in      O.      Recog-  \\n \\n  nizing      that      an      exact      match      between      elements      in  \\n \\n  these      two      sets      of      headings      is      unnecessary,      we      cal-  \\n \\n  culate      the      heading      soft      recall      (Franti      and      Mariescu-  \\n \\n  Istodor,      2023)      using      cosine      similarity      derived      from  \\n \\n  Sentence-BERT      (Reimers      and      Gurevych,      2019)      em-  \\n \\n  beddings      of      the      headings      (details      in      Appendix      C.1).\\n \\n \\n  We      also      compute      the      heading      entity      recall      which  \\n \\n  is      quantified      as      the      percentage      of      named      entities      in  \\n \\n  human-written      article      headings      covered      by      O.      We  \\n \\n  extract      entities      with      FLAIR      named      entity      recogni-  \\n \\n  tion      (NER)      (Akbik      et      al.,      2019).\\n3\\n \\n   Method  \\n \\n  We      present      STORM      to      automate      the      pre-writing  \\n \\n  stage      by      researching   \\n \\na      given      topic      via      effective  \\n \\n  question      asking      (§3.1,      §3.2)      and      creating      an      out-  \\n \\n  line      (§3.3).\\nThe      outline      will      be      extended      to   \\n \\na      full-  \\n \\n  length      article      grounded      on      the      collected      references  \\n \\n  Shttps://en.wikipedia.org/wiki/Wikipedia:  \\n \\n  Stand-alone_lists  \\n \\n  ®Since      language      models      process      and      produce      sequences,  \\n \\n  we      can      linearize   \\n \\nO      by      adding      “#”      to      indicate      section      titles,  \\n \\n  “#4?”\\nto      indicate      subsection      titles,      etc.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 2, 'section_title': '2.1      The      FreshWiki      Dataset'}, page_content='2.1      The      FreshWiki      Dataset\\naset  \\n \\n  Creating   \\n \\na      new      Wikipedia-like      article      demands      not  \\n \\n  only      fluent      writing      but      also      good      research      skills.\\nAs  \\n \\n  modern      LLMs      are      generally      trained      on      Wikipedia  \\n \\n  text,      we      mitigate      data      leakage      by      explicitly      seeking  \\n \\n  out      recent      Wikipedia      articles      that      were      created      (or  \\n \\n  very      heavily      edited)      after      the      training      cutoff      of      the  \\n \\n  LLMs      we      test.\\nOur      process      can      be      repeated      at  \\n \\n  future      dates      when      new      LLMs      emerge.\\n \\n \\n  To      apply      our      date      criteria,      we      focus      on      the      top  \\n \\n  100      most-edited      pages,      based      on      edit      counts,      for  \\n \\n  each      month      from      February      2022      to      September  \\n \\n  2023.      To      ensure      high-quality      references,      we      filter these      articles      to      keep      only      those      having      B-class  \\n \\n  quality      or      above      assessed      by      ORES*.\\nWe      also      ex-  \\n \\n  \"In      practice,      S      also      includes      organizational      elements      such  \\n \\n  as      section      and      subsection      titles,      which      do      not      require      citations.\\n \\n \\n  3      Obtained      from      https:      //wikimedia.\\n \\n \\n  org/api/rest_v1/metrics/edited-pages/  \\n \\n  top-by-edits/en.wikipedia/all-editor-types/  \\n \\n  content/      {year      }/{month}/all-days  \\n \\n  ‘https:      //www.mediawiki.org/wiki/ORES  \\n \\n  clude      list      articles      and      articles      that      have      no      sub-  \\n \\n  sections.\\nWhile      high-quality      Wikipedia      articles  \\n \\n  usually      contain      structured      data      (e.g.,      tables)      and      are  \\n \\n  multi-modal,      we      only      consider      the      plain      text      com-  \\n \\n  ponent      in      constructing      the      dataset      to      simplify      our  \\n \\n  task.\\nMore      details      of      the      dataset      are      in      Appendix      A.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 3, 'section_title': '2.2      Outline      Creation      and      Evaluation'}, page_content='2.2      Outline      Creation      and      Evaluation\\ntion  \\n \\n  A      full-length      article      is      hard      to      generate      or      evalu-  \\n \\n  ate      (Xu      et      al.,      2023;      Krishna      et      al.,      2023).\\nWhen  \\n \\n  human      educators      teach      students      academic      writing,  \\n \\n  they      sometimes      supervise      students      at      the      outline  \\n \\n  stage      (Eriksson      and      Makitalo,      2015)      because      an  \\n \\n  extensive      outline      indicates   \\n \\na      comprehensive      under-  \\n \\n  standing      of      the      topic      and      provides   \\n \\na      solid      founda-  \\n \\n  tion      for      writing      the      full-length      article      (Dietz      and       Foley,      2019).\\nInspired      by      this,      we      decompose      the  \\n \\n  generation      of      S      into      two      stages.\\nIn      the      pre-writing  \\n \\n  stage,      we      require      the      system      to      create      an      outline  \\n \\n  O,      which      is      defined      as   \\n \\na      list      of      multi-level      section  \\n \\n  headings®.\\nIn      the      writing      stage,      the      system      uses  \\n \\n  the      topic      t,      the      references      R,      and      an      outline   \\n \\nO      to  \\n \\n  produce      the      full-length      article      S.\\n \\n \\n  To      evaluate      the      outline      coverage,      we      introduce  \\n \\n  two      metrics:      heading      soft      recall      and      heading      en-  \\n \\n  tity      recall.\\nThese      metrics      compare      the      multi-level  \\n \\n  section      headings      of      the      human-written      article,      con-  \\n \\n  sidered      as      ground      truth,      and      those      in      O.      Recog-  \\n \\n  nizing      that      an      exact      match      between      elements      in  \\n \\n  these      two      sets      of      headings      is      unnecessary,      we      cal-  \\n \\n  culate      the      heading      soft      recall      (Franti      and      Mariescu-  \\n \\n  Istodor,      2023)      using      cosine      similarity      derived      from  \\n \\n  Sentence-BERT      (Reimers      and      Gurevych,      2019)      em-  \\n \\n  beddings      of      the      headings      (details      in      Appendix      C.1).\\n \\n \\n  We      also      compute      the      heading      entity      recall      which  \\n \\n  is      quantified      as      the      percentage      of      named      entities      in  \\n \\n  human-written      article      headings      covered      by      O.      We  \\n \\n  extract      entities      with      FLAIR      named      entity      recogni-  \\n \\n  tion      (NER)      (Akbik      et      al.,      2019).\\n3\\n \\n   Method  \\n \\n  We      present      STORM      to      automate      the      pre-writing  \\n \\n  stage      by      researching   \\n \\na      given      topic      via      effective  \\n \\n  question      asking      (§3.1,      §3.2)      and      creating      an      out-  \\n \\n  line      (§3.3).\\nThe      outline      will      be      extended      to   \\n \\na      full-  \\n \\n  length      article      grounded      on      the      collected      references  \\n \\n  Shttps://en.wikipedia.org/wiki/Wikipedia:  \\n \\n  Stand-alone_lists  \\n \\n  ®Since      language      models      process      and      produce      sequences,  \\n \\n  we      can      linearize   \\n \\nO      by      adding      “#”      to      indicate      section      titles,  \\n \\n  “#4?”\\nto      indicate      subsection      titles,      etc.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 4, 'section_title': 'Topic      t       @      Identify       Perspectives       @      Survey       —      Writer       =o       —       Related      Articles      |'}, page_content='Topic      t       @      Identify       Perspectives       @      Survey       —      Writer       =o       —       Related      Articles      |\\n@\\n \\n   Direct      Generate Question   \\n \\nq    \\n \\n@      Split      Queries  \\n \\n  ©      Search   \\n \\n&      Sift  \\n \\n  ©      Synthesize |\\n \\n   Answer   \\n \\na      \\\\\\\\\\\\\\\\\\n |       \\\\\\\\\\\\\\\\       y      Gather       ‘\\\\\\\\\\\\\\\\      Add      Trusted | \\n | ¥       ,      Cy}\\n\\n |       Draft      Outline      Op | Conversations {Cg,       Refine |  | \\n | --- | --- | --- | ---\\n | \\\\\\\\\\\\\\\\      Sources       Ns\\n'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 5, 'section_title': 'References      R'}, page_content='References      R\\n(§3.4).\\nFigure   \\n \\n2      gives      an      overview      of      STORM      and       we      include      the      pseudo      code      in      Appendix      B.\\n3.1      Perspective-Guided      Question      Asking\\nking  \\n \\n  Rohman      (1965)      defines      pre-writing      as      the      stage  \\n \\n  of      discovery      in      the      writing      process.\\nIn      parallel  \\n \\n  with      stakeholder      theory      in      business      (Freeman      et      al.,  \\n \\n  2010),      where      diverse      stakeholders      prioritize      vary-  \\n \\n  ing      facets      of   \\n \\na      company,      individuals      with      distinct  \\n \\n  perspectives      may      concentrate      on      different      aspects  \\n \\n  when      researching      the      same      topic      and      discover      mul-  \\n \\n  tifaceted      information.\\nFurther,      the      specific      perspec-  \\n \\n  tives      can      serve      as      prior      knowledge,      guiding      individ-  \\n \\n  uals      to      ask      more      in-depth      questions.\\nFor      example,  \\n \\n  an      event      planner      might      ask      about      the      “‘transporta-  \\n \\n  tion      arrangements”      and      “budget”      for      “the      2022  \\n \\n  Winter      Olympics      opening      ceremony”,      whereas   \\n \\na       layperson      might      ask      more      general      questions      about  \\n \\n  the      event’s      basic      information      (Figure   \\n \\n1      (A)).\\n \\n \\n  Given      the      input      topic      t,      STORM      discovers      differ-  \\n \\n  ent      perspectives      by      surveying      existing      articles      from  \\n \\n  similar      topics      and      uses      these      perspectives      to      control  \\n \\n  the      question      asking      process.\\nSpecifically,      STORM  \\n \\n  prompts      an      LLM      to      generate   \\n \\na      list      of      related      top-  \\n \\n  ics      and      subsequently      extracts      the      tables      of      contents  \\n \\n  from      their      corresponding      Wikipedia      articles,      if      such  \\n \\n  articles      can      be      obtained      through      Wikipedia      API’  \\n \\n  (Figure   \\n \\n2      (1).\\nThese      tables      of      contents      are      con- catenated      to      create   \\n \\na      context      to      prompt      the      LLM to identify N perspectives P = {p1,, pn} that  \\n \\n  be      evaluated      using   \\n \\na      rule-based      filter      according      to  \\n \\n  the      Wikipedia      guideline®      to      exclude      untrustworthy  \\n \\n  sources      (Figure   \\n \\n2      (5)).\\nFinally,      the      LLM      synthe-  \\n \\n  Thttps://pypi.org/project/Wikipedia-API/  \\n \\n  can      collectively      contribute      to   \\n \\na      comprehensive      ar-  \\n \\n  ticle      on   \\n \\n¢      (Figure   \\n \\n2      (2)).\\nTo      ensure      that      the      basic  \\n \\n  information      about   \\n \\n¢      is      also      covered,      we      add      pg      as  \\n \\n  “basic      fact      writer      focusing      on      broadly      covering      the  \\n \\n  basic      facts      about      the      topic”      into      P.      Each      perspec-  \\n \\n  tive      p   \\n \\n€      P      will      be      utilized      to      guide      the      LLM      in      the  \\n \\n  process      of      question      asking      in      parallel.\\n3.2      Simulating      Conversations\\nions  \\n \\n  The      theory      of      questions      and      question      asking      (Ram,  \\n \\n  1991)      highlights      that      while      answers      to      existing  \\n \\n  questions      contribute      to   \\n \\na      more      comprehensive  \\n \\n  understanding      of   \\n \\na      topic,      they      often      simultane-  \\n \\n  ously      give      rise      to      new      questions.\\nTo      kick      off      this  \\n \\n  dynamic      process,      STORM      simulates   \\n \\na      conversa-  \\n \\n  tion      between   \\n \\na      Wikipedia      writer      and   \\n \\na      topic      ex-  \\n \\n  pert.\\nIn      the      z-th      round      of      the      conversation,      the  \\n \\n  LLM-powered      Wikipedia      writer      generates   \\n \\na      sin-  \\n \\n  gle      question      q;      based      on      the      topic      1,      its      assigned  \\n \\n  perspective      p   \\n \\n€      P,      and      the      conversation      history  \\n \\n  {q1,      41,      ---,      Gi-1,      41-1}      where      a;      denotes      the      sim-  \\n \\n  ulated      expert’s      answer.\\nThe      conversation      history  \\n \\n  enables      the      LLM      to      update      its      understanding      of      the  \\n \\n  topic      and      ask      follow-up      questions.\\nIn      practice,      we  \\n \\n  limit      the      conversation      to      at      most   \\n \\n/      rounds.\\n \\n \\n  To      ensure      that      the      conversation      history      provides  \\n \\n  factual      information,      we      use      trusted      sources      from  \\n \\n  the      Internet      to      ground      the      answer      a;      to      each      query  \\n \\n  sizes      the      trustworthy      sources      to      generate      the      answer  \\n \\n  a;,      and      these      sources      will      also      be      added      to      R      for  \\n \\n  full      article      generation      (§3.4).\\nq.      Since      q;      can      be      complicated,      we      first      prompt  \\n \\n  the      LLM      to      break      down      q;      into   \\n \\na      set      of      search  \\n \\n  queries      (Figure   \\n \\n2      (4))      and      the      searched      results      will\\n3.3      Creating      the      Article      Outline\\nline  \\n \\n  After      thoroughly      researching      the      topic      through  \\n \\n  N   \\n \\n+   \\n \\n1      simulated      conversations,      denoted      as {Co, Ci, -,; Cw }, STORM creates an outline before  \\n \\n  the      actual      writing      starts.\\nTo      fully      leverage      the      inter-  \\n \\n  nal      knowledge      of      LLMs,      we      first      prompt      the      model  \\n \\n  to      generate   \\n \\na      draft      outline      Op      given      only      the      topic  \\n \\n  t      (Figure   \\n \\n2      (7)).\\nOp      typically      provides   \\n \\na      general  \\n \\n  but      organized      framework.\\nSubsequently,      the      LLM  \\n \\n  is      prompted      with      the      topic      ¢,      the      draft      outline      Op, and the simulated conversations {Co, Cj,,Cw}  \\n \\n  to      refine      the      outline      (Figure   \\n \\n2      (8)).\\nThis      results in      an      improved      outline   \\n \\nO      which      will      be      used      for  \\n \\n  producing      the      full-length      article.\\n3.4      Writing      the      Full-Length      Article\\nicle  \\n \\n  Building      upon      the      references      R      collected      and      the  \\n \\n  outline   \\n \\nO      developed      during      the      pre-writing      stage,  \\n \\n  the      full-length      article      can      be      composed      section      by  \\n \\n  section.\\nSince      it      is      usually      impossible      to      fit      the  \\n \\n  entire   \\n \\n7      within      the      context      window      of      the      LLM,  \\n \\n  we      use      the      section      title      and      headings      of      its      all-level  \\n \\n  subsections      to      retrieve      relevant      documents      from  \\n \\n  R      based      on      semantic      similarity      calculated      from  \\n \\n  Sentence-BERT      embeddings.\\nWith      the      relevant      in-  \\n \\n  formation      at      hand,      the      LLM      is      then      prompted      to  \\n \\n  generate      the      section      with      citations.\\nOnce      all      sec-  \\n \\n  tions      are      generated,      they      are      concatenated      to      form  \\n \\n  the      full-length      article.\\nSince      the      sections      are      gen-  \\n \\n  erated      in      parallel,      we      prompt      the      LLM      with      the  \\n \\n  concatenated      article      to      delete      repeated      information  \\n \\n  to      improve      coherence.\\nFurthermore,      in      alignment  \\n \\n  with      Wikipedia’s      stylistic      norms,      the      LLM      is      also  \\n \\n  utilized      to      synthesize   \\n \\na      summary      of      the      entire      arti-  \\n \\n  cle,      forming      the      lead      section      at      the      beginning.\\n4\\n \\n   Experiments\\n4.1      Article      Selection\\ntion  \\n \\n  STORM      is      capable      of      researching      complicated      top-  \\n \\n  ics      and      writing      long      articles      from      detailed      outlines.\\n \\n \\n  However,      in      this      controlled      experiment,      we      limit  \\n \\n  the      final      output      to      at      most      4000      tokens      (roughly  \\n \\n  3000      words).\\nFor   \\n \\na      meaningful      comparison,      we  \\n \\n  Shttps://en.wikipedia.\\norg/wiki/Wikipedia:  \\n \\n  Reliable_sources  \\n \\n  randomly      select      100      samples      from      the      Fresh      Wiki  \\n \\n  dataset      (see      §2.1)      that      have      human-written      articles  \\n \\n  not      exceeding      3000      words.\\n4.2      Automatic      Metrics\\nrics  \\n \\n  As      discussed      in      §2.2,      we      evaluate      the      outline      qual-  \\n \\n  ity      to      assess      the      pre-writing      stage      by      calculating  \\n \\n  the      heading      soft      recall      and      heading      entity      recall.\\n \\n \\nA       higher      recall      score      signifies   \\n \\na      more      comprehensive  \\n \\n  outline      relative      to      the      human-written      article.\\n \\n \\n  To      assess      the      full-length      article      quality,      we      adopt  \\n \\n  ROUGE      scores      (Lin,      2004)      and      compute      the      entity  \\n \\n  recall      in      the      article      level      based      on      FLAIR      NER  \\n \\n  results.\\nMoreover,      based      on      Wikipedia      criteria’,  \\n \\n  we      evaluate      the      article      from      the      aspects      of      (1)      In-  \\n \\n  terest      Level,      (2)      Coherence      and      Organization,      (3)  \\n \\n  Relevance      and      Focus,      (4)      Coverage,      and      (5)      Verifia-  \\n \\n  bility.\\nFor      aspects      (1)-(4),      we      use      Prometheus      (Kim  \\n \\n  et      al.,      2023),   \\n \\na      13B      evaluator      LLM      to      score      the      arti-  \\n \\n  cle      based      on   \\n \\na      5-point      rubric      collaboratively      devel-  \\n \\n  oped      with      two      experienced      Wikipedia      editors      (see  \\n \\n  Appendix      C.2).\\nFor      verifiability,      we      calculate      the  \\n \\n  citation      recall      and      citation      precision      based      on      the  \\n \\n  definition      in      Gao      et      al.\\n(2023).\\nWe      use      Mistral      7B-  \\n \\n  Instruct      (Jiang      et      al.,      2023a)      to      examine      whether  \\n \\n  the      cited      passages      entail      the      generated      sentence.\\n4.3      Baselines\\nines  \\n \\n  As      prior      works      use      different      setups      and      do      not      use  \\n \\n  LLMs,      they      are      hard      to      compare      directly.\\nInstead,  \\n \\n  we      use      the      following      three      LLM-based      baselines.\\n1. Direct      Gen,   \\n \\na      baseline      that      directly      prompts  \\n \\n  the      LLM      to      generate      an      outline,      which      is      then  \\n \\n  used      to      generate      the      full-length      article.\\n2. RAG,   \\n \\na      retrieval-augmented      generation      base-  \\n \\n  line      that      searches      with      the      topic      and      uses      the  \\n \\n  searched      results      together      with      the      topic   \\n \\n¢      to  \\n \\n  generate      an      outline      or      the      entire      article.\\n3. Outline-driven      RAG      (ORAG),      which      is      iden-  \\n \\n  tical      to      RAG      in      outline      creation,      but      further  \\n \\n  searches      additional      information      with      section  \\n \\n  titles      to      generate      the      article      section      by      section.\\n4.4      STORM      Implementation\\ntion  \\n \\n  We      build      STORM      with      zero-shot      prompting      us-  \\n \\n  ing      the      DSPy      framework      (Khattab      et      al.,      2023).\\n \\n \\n  Appendix   \\n \\nB      includes      the      pseudo      code      and      corre-  \\n \\n  sponding      prompts.\\nThe      hyperparameters      N      and      MZ °https://en.wikipedia.org/wiki/Wikipedia:  \\n \\n  Good_article_criteria'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 6, 'section_title': '3.1      Perspective-Guided      Question      Asking'}, page_content='3.1      Perspective-Guided      Question      Asking\\nking  \\n \\n  Rohman      (1965)      defines      pre-writing      as      the      stage  \\n \\n  of      discovery      in      the      writing      process.\\nIn      parallel  \\n \\n  with      stakeholder      theory      in      business      (Freeman      et      al.,  \\n \\n  2010),      where      diverse      stakeholders      prioritize      vary-  \\n \\n  ing      facets      of   \\n \\na      company,      individuals      with      distinct  \\n \\n  perspectives      may      concentrate      on      different      aspects  \\n \\n  when      researching      the      same      topic      and      discover      mul-  \\n \\n  tifaceted      information.\\nFurther,      the      specific      perspec-  \\n \\n  tives      can      serve      as      prior      knowledge,      guiding      individ-  \\n \\n  uals      to      ask      more      in-depth      questions.\\nFor      example,  \\n \\n  an      event      planner      might      ask      about      the      “‘transporta-  \\n \\n  tion      arrangements”      and      “budget”      for      “the      2022  \\n \\n  Winter      Olympics      opening      ceremony”,      whereas   \\n \\na       layperson      might      ask      more      general      questions      about  \\n \\n  the      event’s      basic      information      (Figure   \\n \\n1      (A)).\\n \\n \\n  Given      the      input      topic      t,      STORM      discovers      differ-  \\n \\n  ent      perspectives      by      surveying      existing      articles      from  \\n \\n  similar      topics      and      uses      these      perspectives      to      control  \\n \\n  the      question      asking      process.\\nSpecifically,      STORM  \\n \\n  prompts      an      LLM      to      generate   \\n \\na      list      of      related      top-  \\n \\n  ics      and      subsequently      extracts      the      tables      of      contents  \\n \\n  from      their      corresponding      Wikipedia      articles,      if      such  \\n \\n  articles      can      be      obtained      through      Wikipedia      API’  \\n \\n  (Figure   \\n \\n2      (1).\\nThese      tables      of      contents      are      con- catenated      to      create   \\n \\na      context      to      prompt      the      LLM to identify N perspectives P = {p1,, pn} that  \\n \\n  be      evaluated      using   \\n \\na      rule-based      filter      according      to  \\n \\n  the      Wikipedia      guideline®      to      exclude      untrustworthy  \\n \\n  sources      (Figure   \\n \\n2      (5)).\\nFinally,      the      LLM      synthe-  \\n \\n  Thttps://pypi.org/project/Wikipedia-API/  \\n \\n  can      collectively      contribute      to   \\n \\na      comprehensive      ar-  \\n \\n  ticle      on   \\n \\n¢      (Figure   \\n \\n2      (2)).\\nTo      ensure      that      the      basic  \\n \\n  information      about   \\n \\n¢      is      also      covered,      we      add      pg      as  \\n \\n  “basic      fact      writer      focusing      on      broadly      covering      the  \\n \\n  basic      facts      about      the      topic”      into      P.      Each      perspec-  \\n \\n  tive      p   \\n \\n€      P      will      be      utilized      to      guide      the      LLM      in      the  \\n \\n  process      of      question      asking      in      parallel.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 7, 'section_title': '3.2      Simulating      Conversations'}, page_content='3.2      Simulating      Conversations\\nions  \\n \\n  The      theory      of      questions      and      question      asking      (Ram,  \\n \\n  1991)      highlights      that      while      answers      to      existing  \\n \\n  questions      contribute      to   \\n \\na      more      comprehensive  \\n \\n  understanding      of   \\n \\na      topic,      they      often      simultane-  \\n \\n  ously      give      rise      to      new      questions.\\nTo      kick      off      this  \\n \\n  dynamic      process,      STORM      simulates   \\n \\na      conversa-  \\n \\n  tion      between   \\n \\na      Wikipedia      writer      and   \\n \\na      topic      ex-  \\n \\n  pert.\\nIn      the      z-th      round      of      the      conversation,      the  \\n \\n  LLM-powered      Wikipedia      writer      generates   \\n \\na      sin-  \\n \\n  gle      question      q;      based      on      the      topic      1,      its      assigned  \\n \\n  perspective      p   \\n \\n€      P,      and      the      conversation      history  \\n \\n  {q1,      41,      ---,      Gi-1,      41-1}      where      a;      denotes      the      sim-  \\n \\n  ulated      expert’s      answer.\\nThe      conversation      history  \\n \\n  enables      the      LLM      to      update      its      understanding      of      the  \\n \\n  topic      and      ask      follow-up      questions.\\nIn      practice,      we  \\n \\n  limit      the      conversation      to      at      most   \\n \\n/      rounds.\\n \\n \\n  To      ensure      that      the      conversation      history      provides  \\n \\n  factual      information,      we      use      trusted      sources      from  \\n \\n  the      Internet      to      ground      the      answer      a;      to      each      query  \\n \\n  sizes      the      trustworthy      sources      to      generate      the      answer  \\n \\n  a;,      and      these      sources      will      also      be      added      to      R      for  \\n \\n  full      article      generation      (§3.4).\\nq.      Since      q;      can      be      complicated,      we      first      prompt  \\n \\n  the      LLM      to      break      down      q;      into   \\n \\na      set      of      search  \\n \\n  queries      (Figure   \\n \\n2      (4))      and      the      searched      results      will'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 8, 'section_title': '3.3      Creating      the      Article      Outline'}, page_content='3.3      Creating      the      Article      Outline\\nline  \\n \\n  After      thoroughly      researching      the      topic      through  \\n \\n  N   \\n \\n+   \\n \\n1      simulated      conversations,      denoted      as {Co, Ci, -,; Cw }, STORM creates an outline before  \\n \\n  the      actual      writing      starts.\\nTo      fully      leverage      the      inter-  \\n \\n  nal      knowledge      of      LLMs,      we      first      prompt      the      model  \\n \\n  to      generate   \\n \\na      draft      outline      Op      given      only      the      topic  \\n \\n  t      (Figure   \\n \\n2      (7)).\\nOp      typically      provides   \\n \\na      general  \\n \\n  but      organized      framework.\\nSubsequently,      the      LLM  \\n \\n  is      prompted      with      the      topic      ¢,      the      draft      outline      Op, and the simulated conversations {Co, Cj,,Cw}  \\n \\n  to      refine      the      outline      (Figure   \\n \\n2      (8)).\\nThis      results in      an      improved      outline   \\n \\nO      which      will      be      used      for  \\n \\n  producing      the      full-length      article.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 9, 'section_title': '3.4      Writing      the      Full-Length      Article'}, page_content='3.4      Writing      the      Full-Length      Article\\nicle  \\n \\n  Building      upon      the      references      R      collected      and      the  \\n \\n  outline   \\n \\nO      developed      during      the      pre-writing      stage,  \\n \\n  the      full-length      article      can      be      composed      section      by  \\n \\n  section.\\nSince      it      is      usually      impossible      to      fit      the  \\n \\n  entire   \\n \\n7      within      the      context      window      of      the      LLM,  \\n \\n  we      use      the      section      title      and      headings      of      its      all-level  \\n \\n  subsections      to      retrieve      relevant      documents      from  \\n \\n  R      based      on      semantic      similarity      calculated      from  \\n \\n  Sentence-BERT      embeddings.\\nWith      the      relevant      in-  \\n \\n  formation      at      hand,      the      LLM      is      then      prompted      to  \\n \\n  generate      the      section      with      citations.\\nOnce      all      sec-  \\n \\n  tions      are      generated,      they      are      concatenated      to      form  \\n \\n  the      full-length      article.\\nSince      the      sections      are      gen-  \\n \\n  erated      in      parallel,      we      prompt      the      LLM      with      the  \\n \\n  concatenated      article      to      delete      repeated      information  \\n \\n  to      improve      coherence.\\nFurthermore,      in      alignment  \\n \\n  with      Wikipedia’s      stylistic      norms,      the      LLM      is      also  \\n \\n  utilized      to      synthesize   \\n \\na      summary      of      the      entire      arti-  \\n \\n  cle,      forming      the      lead      section      at      the      beginning.\\n4\\n \\n   Experiments\\n4.1      Article      Selection\\ntion  \\n \\n  STORM      is      capable      of      researching      complicated      top-  \\n \\n  ics      and      writing      long      articles      from      detailed      outlines.\\n \\n \\n  However,      in      this      controlled      experiment,      we      limit  \\n \\n  the      final      output      to      at      most      4000      tokens      (roughly  \\n \\n  3000      words).\\nFor   \\n \\na      meaningful      comparison,      we  \\n \\n  Shttps://en.wikipedia.\\norg/wiki/Wikipedia:  \\n \\n  Reliable_sources  \\n \\n  randomly      select      100      samples      from      the      Fresh      Wiki  \\n \\n  dataset      (see      §2.1)      that      have      human-written      articles  \\n \\n  not      exceeding      3000      words.\\n4.2      Automatic      Metrics\\nrics  \\n \\n  As      discussed      in      §2.2,      we      evaluate      the      outline      qual-  \\n \\n  ity      to      assess      the      pre-writing      stage      by      calculating  \\n \\n  the      heading      soft      recall      and      heading      entity      recall.\\n \\n \\nA       higher      recall      score      signifies   \\n \\na      more      comprehensive  \\n \\n  outline      relative      to      the      human-written      article.\\n \\n \\n  To      assess      the      full-length      article      quality,      we      adopt  \\n \\n  ROUGE      scores      (Lin,      2004)      and      compute      the      entity  \\n \\n  recall      in      the      article      level      based      on      FLAIR      NER  \\n \\n  results.\\nMoreover,      based      on      Wikipedia      criteria’,  \\n \\n  we      evaluate      the      article      from      the      aspects      of      (1)      In-  \\n \\n  terest      Level,      (2)      Coherence      and      Organization,      (3)  \\n \\n  Relevance      and      Focus,      (4)      Coverage,      and      (5)      Verifia-  \\n \\n  bility.\\nFor      aspects      (1)-(4),      we      use      Prometheus      (Kim  \\n \\n  et      al.,      2023),   \\n \\na      13B      evaluator      LLM      to      score      the      arti-  \\n \\n  cle      based      on   \\n \\na      5-point      rubric      collaboratively      devel-  \\n \\n  oped      with      two      experienced      Wikipedia      editors      (see  \\n \\n  Appendix      C.2).\\nFor      verifiability,      we      calculate      the  \\n \\n  citation      recall      and      citation      precision      based      on      the  \\n \\n  definition      in      Gao      et      al.\\n(2023).\\nWe      use      Mistral      7B-  \\n \\n  Instruct      (Jiang      et      al.,      2023a)      to      examine      whether  \\n \\n  the      cited      passages      entail      the      generated      sentence.\\n4.3      Baselines\\nines  \\n \\n  As      prior      works      use      different      setups      and      do      not      use  \\n \\n  LLMs,      they      are      hard      to      compare      directly.\\nInstead,  \\n \\n  we      use      the      following      three      LLM-based      baselines.\\n1. Direct      Gen,   \\n \\na      baseline      that      directly      prompts  \\n \\n  the      LLM      to      generate      an      outline,      which      is      then  \\n \\n  used      to      generate      the      full-length      article.\\n2. RAG,   \\n \\na      retrieval-augmented      generation      base-  \\n \\n  line      that      searches      with      the      topic      and      uses      the  \\n \\n  searched      results      together      with      the      topic   \\n \\n¢      to  \\n \\n  generate      an      outline      or      the      entire      article.\\n3. Outline-driven      RAG      (ORAG),      which      is      iden-  \\n \\n  tical      to      RAG      in      outline      creation,      but      further  \\n \\n  searches      additional      information      with      section  \\n \\n  titles      to      generate      the      article      section      by      section.\\n4.4      STORM      Implementation\\ntion  \\n \\n  We      build      STORM      with      zero-shot      prompting      us-  \\n \\n  ing      the      DSPy      framework      (Khattab      et      al.,      2023).\\n \\n \\n  Appendix   \\n \\nB      includes      the      pseudo      code      and      corre-  \\n \\n  sponding      prompts.\\nThe      hyperparameters      N      and      MZ °https://en.wikipedia.org/wiki/Wikipedia:  \\n \\n  Good_article_criteria'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 10, 'section_title': '4.1      Article      Selection'}, page_content='4.1      Article      Selection\\ntion  \\n \\n  STORM      is      capable      of      researching      complicated      top-  \\n \\n  ics      and      writing      long      articles      from      detailed      outlines.\\n \\n \\n  However,      in      this      controlled      experiment,      we      limit  \\n \\n  the      final      output      to      at      most      4000      tokens      (roughly  \\n \\n  3000      words).\\nFor   \\n \\na      meaningful      comparison,      we  \\n \\n  Shttps://en.wikipedia.\\norg/wiki/Wikipedia:  \\n \\n  Reliable_sources  \\n \\n  randomly      select      100      samples      from      the      Fresh      Wiki  \\n \\n  dataset      (see      §2.1)      that      have      human-written      articles  \\n \\n  not      exceeding      3000      words.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 11, 'section_title': '4.2      Automatic      Metrics'}, page_content='4.2      Automatic      Metrics\\nrics  \\n \\n  As      discussed      in      §2.2,      we      evaluate      the      outline      qual-  \\n \\n  ity      to      assess      the      pre-writing      stage      by      calculating  \\n \\n  the      heading      soft      recall      and      heading      entity      recall.\\n \\n \\nA       higher      recall      score      signifies   \\n \\na      more      comprehensive  \\n \\n  outline      relative      to      the      human-written      article.\\n \\n \\n  To      assess      the      full-length      article      quality,      we      adopt  \\n \\n  ROUGE      scores      (Lin,      2004)      and      compute      the      entity  \\n \\n  recall      in      the      article      level      based      on      FLAIR      NER  \\n \\n  results.\\nMoreover,      based      on      Wikipedia      criteria’,  \\n \\n  we      evaluate      the      article      from      the      aspects      of      (1)      In-  \\n \\n  terest      Level,      (2)      Coherence      and      Organization,      (3)  \\n \\n  Relevance      and      Focus,      (4)      Coverage,      and      (5)      Verifia-  \\n \\n  bility.\\nFor      aspects      (1)-(4),      we      use      Prometheus      (Kim  \\n \\n  et      al.,      2023),   \\n \\na      13B      evaluator      LLM      to      score      the      arti-  \\n \\n  cle      based      on   \\n \\na      5-point      rubric      collaboratively      devel-  \\n \\n  oped      with      two      experienced      Wikipedia      editors      (see  \\n \\n  Appendix      C.2).\\nFor      verifiability,      we      calculate      the  \\n \\n  citation      recall      and      citation      precision      based      on      the  \\n \\n  definition      in      Gao      et      al.\\n(2023).\\nWe      use      Mistral      7B-  \\n \\n  Instruct      (Jiang      et      al.,      2023a)      to      examine      whether  \\n \\n  the      cited      passages      entail      the      generated      sentence.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 12, 'section_title': '4.3      Baselines'}, page_content='4.3      Baselines\\nines  \\n \\n  As      prior      works      use      different      setups      and      do      not      use  \\n \\n  LLMs,      they      are      hard      to      compare      directly.\\nInstead,  \\n \\n  we      use      the      following      three      LLM-based      baselines.\\n1. Direct      Gen,   \\n \\na      baseline      that      directly      prompts  \\n \\n  the      LLM      to      generate      an      outline,      which      is      then  \\n \\n  used      to      generate      the      full-length      article.\\n2. RAG,   \\n \\na      retrieval-augmented      generation      base-  \\n \\n  line      that      searches      with      the      topic      and      uses      the  \\n \\n  searched      results      together      with      the      topic   \\n \\n¢      to  \\n \\n  generate      an      outline      or      the      entire      article.\\n3. Outline-driven      RAG      (ORAG),      which      is      iden-  \\n \\n  tical      to      RAG      in      outline      creation,      but      further  \\n \\n  searches      additional      information      with      section  \\n \\n  titles      to      generate      the      article      section      by      section.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 13, 'section_title': '4.4      STORM      Implementation'}, page_content='4.4      STORM      Implementation\\ntion  \\n \\n  We      build      STORM      with      zero-shot      prompting      us-  \\n \\n  ing      the      DSPy      framework      (Khattab      et      al.,      2023).\\n \\n \\n  Appendix   \\n \\nB      includes      the      pseudo      code      and      corre-  \\n \\n  sponding      prompts.\\nThe      hyperparameters      N      and      MZ °https://en.wikipedia.org/wiki/Wikipedia:  \\n \\n  Good_article_criteria'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 14, 'section_title': 'Comparsion      with      Human-written      Articles      Rubric      Grading'}, page_content='Comparsion      with      Human-written      Articles      Rubric      Grading'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 15, 'section_title': 'ROUGE-1      ROUGE-L      Entity      Recall      Interest      Level      Organization      Relevance      Coverage'}, page_content='ROUGE-1      ROUGE-L      Entity      Recall      Interest      Level      Organization      Relevance      Coverage\\n \\n \\n  Direct      Gen      25.62      12.63      5.08      2.87      4.60      3.10      4.16  \\n \\n  RAG      28.52      13.18      7.57      3.14      4.22      3.05      4.08  \\n \\n  oRAG      44.26      16.51      12.57      3.90      4.79      4.09      4.70 STORM      45.82      16.70      14.107      3.997      4.82      4.457      4.887  \\n \\n  w/o      Outline      Stage      26.77      12.77      7.39      3.33      4.87      3.35      4.37'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 16, 'section_title': 'Heading      Heading       Soft      Recall      Entity      Recall'}, page_content='Heading      Heading       Soft      Recall      Entity      Recall\\n \\n \\n  Direct      Gen      80.23      32.39  \\n \\n  RAG/oRAG      73.59      33.85  \\n \\n  GPT-3.5   \\n \\n=      RAG-expand      74.40      33.85  \\n \\n  STORM      86.267      40.527  \\n \\n  w/o      Perspective      84.49      40.12  \\n \\n  w/o      Conversation      77.97      31.98  \\n \\n  Direct      Gen      87.66      34.78  \\n \\n  RAG/oRAG      89.55      42.38  \\n \\n  GPT-4      RAG-expand      91.36      43.53  \\n \\n  STORM      92.737      45.91  \\n \\n  w/o      Perspective      92.39      42.70  \\n \\n  w/o      Conversation      88.75      39.30  \\n \\n  Table      3:      Results      of      outline      quality      evaluation      (%).\\n \\n \\n+      de-  \\n \\n  notes      significant      differences      (p   \\n \\n<      0.05)      from   \\n \\na      paired  \\n \\n  t-test      between      STORM      and      baselines.\\n \\n \\n  in      STORM      are      both      set      as      5.      We      use      the      chat  \\n \\n  model      gpt-3.5-turbo      for      question      asking      and       use      gpt-3.5-turbo-instruct      for      other      parts      of  \\n \\n  STORM.\\nWe      also      experiment      with      using      gpt-4      for  \\n \\n  drafting      and      refining      the      outline      (Figure   \\n \\n2      ()8)).\\nFor      reported      results,      the      simulated      topic      expert      in  \\n \\n  STORM      is      grounded      on      the      You.com      search      API!°,  \\n \\n  although      the      proposed      pipeline      is      compatible      with  \\n \\n  other      search      engines.\\nThe      ground      truth      Wikipedia  \\n \\n  article      is      excluded      from      the      search      results.\\n \\n \\n  For      final      article      generation,      we      only      report      the  \\n \\n  results      using      gpt-4      as      gpt-3.5      is      not      faithful      to  \\n \\n  sources      when      generating      text      with      citations      (Gao  \\n \\n  et      al.,      2023).\\nWe      set      temperature      as      1.0      and      top_p  \\n \\n  as      0.9      for      all      experiments.\\n5\\n \\n   Results      and      Analysis\\n5.1      Main      Results\\nults  \\n \\n  We      use      outline      coverage      as   \\n \\na      proxy      to      assess      the      pre-  \\n \\n  writing      stage      (see      §2.2).\\nTable   \\n \\n3      shows      the      heading  \\n \\n  soft      recall      and      entity      recall.\\nOutlines      directly      gen-  \\n \\n  erated      by      LLMs      (Direct      Gen)      already      demonstrate  \\n \\n  https:      //documentation.\\nyou.\\ncom/api-reference/  \\n \\n  search  \\n \\n  high      heading      soft      recall,      indicating      LLMs’      ability  \\n \\n  to      grasp      high-level      aspects      of   \\n \\na      topic      through      their  \\n \\n  rich      parametric      knowledge.\\nHowever,      STORM,      by  \\n \\n  asking      effective      questions      to      research      the      topic,      can  \\n \\n  create      higher      recall      outlines      that      cover      more      topic-  \\n \\n  specific      aspects.\\nNotably,      although      RAG      leverages  \\n \\n  additional      information,      presenting      unorganized      in-  \\n \\n  formation      in      the      context      window      makes      outline  \\n \\n  generation      more      challenging      for      the      weaker      model,  \\n \\n  i.e.,      GPT-3.5,      leading      to      worse      performance.\\nTo      test  \\n \\n  the      limit      of      the      RAG      baseline,      we      further      expand  \\n \\n  the      retrieved      sources      by      starting      with      the      outline  \\n \\n  produced      by      RAG,      using      its      section      titles      as      search  \\n \\n  queries      to      collect      more      sources,      and      inputting      the  \\n \\n  newly      collected      sources      together      with      the      initial  \\n \\n  outline      to      LLM      to      generate   \\n \\na      polished      outline.\\nThis  \\n \\n  modified      approach      is      referred      to      as      “RAG-expand”  \\n \\n  in      Table      3.\\nThe      experiment      results      indicate      that  \\n \\n  even      though      having      an      additional      round      of      search  \\n \\n  and      refinement      can      improve      the      outline      produced  \\n \\n  by      RAG,      our      proposed      STORM      still      surpasses      its  \\n \\n  performance.\\n \\n \\n  We      further      evaluate      the      full-length      article      quality.\\n \\n \\n  As      shown      in      Table      2,      oRAG      significantly      outper-  \\n \\n  forms      RAG,      highlighting      the      effectiveness      of      using  \\n \\n  outlines      for      structuring      full-length      article      genera-  \\n \\n  tion.\\nDespite      this      method’s      advantages      in      leverag-  \\n \\n  ing      retrieval      and      outlining,      our      approach      still      out-  \\n \\n  performs      it.\\nThe      effective      question      asking      mecha-  \\n \\n  nism      enhances      the      articles      with      greater      entity      recall.\\n \\n \\n  The      evaluator      LLM      also      rates      these      articles      with      sig-  \\n \\n  nificantly      higher      scores      in      the      aspects      of      “Interest  \\n \\n  Level’,      “Relevance      and      Focus’,      and      “Coverage”.\\n \\n \\n  Nonetheless,      we      acknowledge      the      possibility      of  \\n \\n  the      evaluator      LLM      overrating      machine-generated  \\n \\n  text.\\nOur      careful      human      evaluation      (§6)      reveals  \\n \\n  that      STORM      still      has      much      room      for      improvement.\\n \\n \\n  Although      this      work      primarily      focuses      on      the      pre-  \\n \\n  writing      stage      and      does      not      optimize      generating      text  \\n \\n  with      citations,      we      still      examine      the      citation      quality  \\n \\n  of      articles      produced      by      our      approach.\\nAs      reported Citation      Recall Citation      Precision oRAG      STORM      value  \\n \\n  Avg.      >4Rates      Av.g.\\n \\n \\n>   \\n \\n4      Rates      peval Table      4:      Citation      quality      judged      by      Mistral      7B-Instruct.\\n84.83 85.18  \\n \\n  STORM\\n |  |       STORM      _      w/o      Perspective       w/o      Conversation | \\n | --- | --- | ---\\n |       IR|      99.83      54.36 |       39.56 |       Interest      Level      3.63      57.5%      4.03      70.0%      0.077       Organization      3.25      45.0%      4.00      70.0%      0.005       Relevance      3.93      62.5%      4.15      65.0%      0.347       Coverage      3.58      57.5%      4.00      67.5%      0.084       Verifiability      3.85      67.5%      3.80      67.5%      0.843       #Preferred      14      26\\n | Table      5:      Average      number      of      unique      references      (|R|)       collected      using      different      methods.\\n |       in      Table      4,      Mistral      7B-Instruct      judges      84.83%      of       the      sentences      are      supported      by      their      citations.      Ap-       pendix      C.3      investigates      the      unsupported      sentences       and      reveals      that      the      primary      issues      stem      from      draw-       ing      improper      inferences      and      inaccurate      paraphras-       ing,      rather      than      hallucinating      non-existent      contents.\\n | 5.2      Ablation      Studies\\n | 5.2      Ablation      Studies\\n |       As      introduced      in      §3,      STORM      prompts      LLMs      to       ask      effective      questions      by      discovering      specific       perspectives      and      simulating      multi-turn      conversa-       tions.      We      conduct      the      ablation      study      on      outline       creation      by      comparing      STORM      with      two      variants:\\n | (1)      “STORM      w/o      Perspective”,      which      omits      per-       spective      in      the      question      generation      prompt;      (2)       “STORM      w/o      Conversation”,      which      prompts      LLMs       to      generate      a      set      number      of      questions      altogether.      To       ensure      a      fair      comparison,      we      control      an      equal      total       number      of      generated      questions      across      all      variants.       Table      3      shows      the      ablation      results      and      full      STORM       pipeline      produces      outlines      with      the      highest      recall.       Also,      “STORM      w/o      Conversation”      gives      much       worse      results,      indicating      reading      relevant      informa-       tion      is      crucial      to      generating      effective      questions.      We       further      examine      how      many      unique      sources      are      col-       lected      in      ?      via      different      variants.      As      shown      in      Ta-       ble      5,      the      full      pipeline      discovers      more      different       sources      and      the      trend      is      in      accord      with      the      auto-       matic      metrics      for      outline      quality.       We      also      verify      whether      having      an      outline      stage       is      necessary      with      STORM.      In      Table      2,      “STORM       w/o      Outline      Stage”      denotes      the      results      of      generat-       ing      the      entire      article      given      the      topic      and      the      sim-       ulated      conversations.      Removing      the      outline      stage       significantly      deteriorates      the      performance      across       all      metrics.\\n | 6      Human      Evaluation\\n |       To      better      understand      the      strengths      and      weaknesses       of      STORM,      we      conduct      human      evaluation      by      col-       laborating      with      10      experienced      Wikipedia      editors       Table      6:      Human      evaluation      results      on      20      pairs      of      articles       generated      by      STORM      and      oRAG.      Each      pair      of      articles       is      evaluated      by      two      Wikipedia      editors.      The      ratings      are       given      on      a      scale      between      |      and      7,      with      values      >      4       indicating      good      quality      (see      Table      10).      We      conduct       paired      t-test      and      report      the      p-value.\\n |       who      have      made      at      least      500      edits      on      Wikipedia      and       have      more      than      |      year      of      experience.      We      randomly       sample      20      topics      from      our      dataset      and      evaluate      the       articles      generated      by      our      method      and      oRAG,      the       best      baseline      according      to      the      automatic      evaluation.\\n |       Each      pair      of      articles      is      assigned      to      2      editors.\\n |       We      request      editors      to      judge      each      article      from      the       same      five      aspects      defined      in      $4.2,      but      using      a      |      to       7      scale      for      more      fine-grained      evaluation.      While       our      automatic      evaluation      uses      citation      quality      as       a      proxy      to      evaluate      Verifiability,      we      stick      to      the       Wikipedia      standard      of      “verifiable      with      no      original       research”      in      human      evaluation.      Besides      rating      the       articles,      editors      are      asked      to      provide      open-ended       feedback      and      pairwise      preference.      After      the      evalua-       tion      finishes,      they      are      further      requested      to      compare       an      article      produced      by      our      method,      which      they      have       just      reviewed,      with      its      human-written      counterpart,       and      report      their      perceived      usefulness      of      STORM       using      a      1-5      Likert      scale.      More      human      evaluation      de-       tails      are      included      in      Appendix      D.      Table      6      presents       the      rating      and      pairwise      comparison      results.!!\\n |       Articles      produced      by      STORM      exhibit      greater       breadth      and      depth      than      oRAG      outputs.      In      ac-       cord      with      the      finding      in      §5.1,      editors      judge      articles       produced      by      STORM      as      more      interesting,      orga-       nized,      and      having      broader      coverage      compared      to       oRAG      outputs.      Specifically,      25%      more      articles      pro-       duced      by      STORM      are      considered      organized      (Orga-       nization      rating      >      4),      and      10%      more      are      deemed      to       have      good      coverage      (Coverage      rating      >      4).      Even       in      comparison      with      human-written      articles,      one       editor      praises      our      result      as      providing      “a      bit      more\\n |       \"For      the      1-7      scale      rating      results      on      each      criterion,      we      cal-       culate      the      Krippendorff’s      Alpha      to      measure      the      inter      annotator       agreement      (IAA),      and      the      results      are      as      follows:      Interest      Level       (0.349),      Organization      (0.221),      Relevance      (0.256),      Coverage       (0.346),      Verifiability      (0.388).\\n'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 17, 'section_title': '5.1      Main      Results'}, page_content='5.1      Main      Results\\nults  \\n \\n  We      use      outline      coverage      as   \\n \\na      proxy      to      assess      the      pre-  \\n \\n  writing      stage      (see      §2.2).\\nTable   \\n \\n3      shows      the      heading  \\n \\n  soft      recall      and      entity      recall.\\nOutlines      directly      gen-  \\n \\n  erated      by      LLMs      (Direct      Gen)      already      demonstrate  \\n \\n  https:      //documentation.\\nyou.\\ncom/api-reference/  \\n \\n  search  \\n \\n  high      heading      soft      recall,      indicating      LLMs’      ability  \\n \\n  to      grasp      high-level      aspects      of   \\n \\na      topic      through      their  \\n \\n  rich      parametric      knowledge.\\nHowever,      STORM,      by  \\n \\n  asking      effective      questions      to      research      the      topic,      can  \\n \\n  create      higher      recall      outlines      that      cover      more      topic-  \\n \\n  specific      aspects.\\nNotably,      although      RAG      leverages  \\n \\n  additional      information,      presenting      unorganized      in-  \\n \\n  formation      in      the      context      window      makes      outline  \\n \\n  generation      more      challenging      for      the      weaker      model,  \\n \\n  i.e.,      GPT-3.5,      leading      to      worse      performance.\\nTo      test  \\n \\n  the      limit      of      the      RAG      baseline,      we      further      expand  \\n \\n  the      retrieved      sources      by      starting      with      the      outline  \\n \\n  produced      by      RAG,      using      its      section      titles      as      search  \\n \\n  queries      to      collect      more      sources,      and      inputting      the  \\n \\n  newly      collected      sources      together      with      the      initial  \\n \\n  outline      to      LLM      to      generate   \\n \\na      polished      outline.\\nThis  \\n \\n  modified      approach      is      referred      to      as      “RAG-expand”  \\n \\n  in      Table      3.\\nThe      experiment      results      indicate      that  \\n \\n  even      though      having      an      additional      round      of      search  \\n \\n  and      refinement      can      improve      the      outline      produced  \\n \\n  by      RAG,      our      proposed      STORM      still      surpasses      its  \\n \\n  performance.\\n \\n \\n  We      further      evaluate      the      full-length      article      quality.\\n \\n \\n  As      shown      in      Table      2,      oRAG      significantly      outper-  \\n \\n  forms      RAG,      highlighting      the      effectiveness      of      using  \\n \\n  outlines      for      structuring      full-length      article      genera-  \\n \\n  tion.\\nDespite      this      method’s      advantages      in      leverag-  \\n \\n  ing      retrieval      and      outlining,      our      approach      still      out-  \\n \\n  performs      it.\\nThe      effective      question      asking      mecha-  \\n \\n  nism      enhances      the      articles      with      greater      entity      recall.\\n \\n \\n  The      evaluator      LLM      also      rates      these      articles      with      sig-  \\n \\n  nificantly      higher      scores      in      the      aspects      of      “Interest  \\n \\n  Level’,      “Relevance      and      Focus’,      and      “Coverage”.\\n \\n \\n  Nonetheless,      we      acknowledge      the      possibility      of  \\n \\n  the      evaluator      LLM      overrating      machine-generated  \\n \\n  text.\\nOur      careful      human      evaluation      (§6)      reveals  \\n \\n  that      STORM      still      has      much      room      for      improvement.\\n \\n \\n  Although      this      work      primarily      focuses      on      the      pre-  \\n \\n  writing      stage      and      does      not      optimize      generating      text  \\n \\n  with      citations,      we      still      examine      the      citation      quality  \\n \\n  of      articles      produced      by      our      approach.\\nAs      reported Citation      Recall Citation      Precision oRAG      STORM      value  \\n \\n  Avg.      >4Rates      Av.g.\\n \\n \\n>   \\n \\n4      Rates      peval Table      4:      Citation      quality      judged      by      Mistral      7B-Instruct.\\n84.83 85.18  \\n \\n  STORM\\n |  |       STORM      _      w/o      Perspective       w/o      Conversation | \\n | --- | --- | ---\\n |       IR|      99.83      54.36 |       39.56 |       Interest      Level      3.63      57.5%      4.03      70.0%      0.077       Organization      3.25      45.0%      4.00      70.0%      0.005       Relevance      3.93      62.5%      4.15      65.0%      0.347       Coverage      3.58      57.5%      4.00      67.5%      0.084       Verifiability      3.85      67.5%      3.80      67.5%      0.843       #Preferred      14      26\\n | Table      5:      Average      number      of      unique      references      (|R|)       collected      using      different      methods.\\n |       in      Table      4,      Mistral      7B-Instruct      judges      84.83%      of       the      sentences      are      supported      by      their      citations.      Ap-       pendix      C.3      investigates      the      unsupported      sentences       and      reveals      that      the      primary      issues      stem      from      draw-       ing      improper      inferences      and      inaccurate      paraphras-       ing,      rather      than      hallucinating      non-existent      contents.\\n | 5.2      Ablation      Studies\\n | 5.2      Ablation      Studies\\n |       As      introduced      in      §3,      STORM      prompts      LLMs      to       ask      effective      questions      by      discovering      specific       perspectives      and      simulating      multi-turn      conversa-       tions.      We      conduct      the      ablation      study      on      outline       creation      by      comparing      STORM      with      two      variants:\\n | (1)      “STORM      w/o      Perspective”,      which      omits      per-       spective      in      the      question      generation      prompt;      (2)       “STORM      w/o      Conversation”,      which      prompts      LLMs       to      generate      a      set      number      of      questions      altogether.      To       ensure      a      fair      comparison,      we      control      an      equal      total       number      of      generated      questions      across      all      variants.       Table      3      shows      the      ablation      results      and      full      STORM       pipeline      produces      outlines      with      the      highest      recall.       Also,      “STORM      w/o      Conversation”      gives      much       worse      results,      indicating      reading      relevant      informa-       tion      is      crucial      to      generating      effective      questions.      We       further      examine      how      many      unique      sources      are      col-       lected      in      ?      via      different      variants.      As      shown      in      Ta-       ble      5,      the      full      pipeline      discovers      more      different       sources      and      the      trend      is      in      accord      with      the      auto-       matic      metrics      for      outline      quality.       We      also      verify      whether      having      an      outline      stage       is      necessary      with      STORM.      In      Table      2,      “STORM       w/o      Outline      Stage”      denotes      the      results      of      generat-       ing      the      entire      article      given      the      topic      and      the      sim-       ulated      conversations.      Removing      the      outline      stage       significantly      deteriorates      the      performance      across       all      metrics.\\n | 6      Human      Evaluation\\n |       To      better      understand      the      strengths      and      weaknesses       of      STORM,      we      conduct      human      evaluation      by      col-       laborating      with      10      experienced      Wikipedia      editors       Table      6:      Human      evaluation      results      on      20      pairs      of      articles       generated      by      STORM      and      oRAG.      Each      pair      of      articles       is      evaluated      by      two      Wikipedia      editors.      The      ratings      are       given      on      a      scale      between      |      and      7,      with      values      >      4       indicating      good      quality      (see      Table      10).      We      conduct       paired      t-test      and      report      the      p-value.\\n |       who      have      made      at      least      500      edits      on      Wikipedia      and       have      more      than      |      year      of      experience.      We      randomly       sample      20      topics      from      our      dataset      and      evaluate      the       articles      generated      by      our      method      and      oRAG,      the       best      baseline      according      to      the      automatic      evaluation.\\n |       Each      pair      of      articles      is      assigned      to      2      editors.\\n |       We      request      editors      to      judge      each      article      from      the       same      five      aspects      defined      in      $4.2,      but      using      a      |      to       7      scale      for      more      fine-grained      evaluation.      While       our      automatic      evaluation      uses      citation      quality      as       a      proxy      to      evaluate      Verifiability,      we      stick      to      the       Wikipedia      standard      of      “verifiable      with      no      original       research”      in      human      evaluation.      Besides      rating      the       articles,      editors      are      asked      to      provide      open-ended       feedback      and      pairwise      preference.      After      the      evalua-       tion      finishes,      they      are      further      requested      to      compare       an      article      produced      by      our      method,      which      they      have       just      reviewed,      with      its      human-written      counterpart,       and      report      their      perceived      usefulness      of      STORM       using      a      1-5      Likert      scale.      More      human      evaluation      de-       tails      are      included      in      Appendix      D.      Table      6      presents       the      rating      and      pairwise      comparison      results.!!\\n |       Articles      produced      by      STORM      exhibit      greater       breadth      and      depth      than      oRAG      outputs.      In      ac-       cord      with      the      finding      in      §5.1,      editors      judge      articles       produced      by      STORM      as      more      interesting,      orga-       nized,      and      having      broader      coverage      compared      to       oRAG      outputs.      Specifically,      25%      more      articles      pro-       duced      by      STORM      are      considered      organized      (Orga-       nization      rating      >      4),      and      10%      more      are      deemed      to       have      good      coverage      (Coverage      rating      >      4).      Even       in      comparison      with      human-written      articles,      one       editor      praises      our      result      as      providing      “a      bit      more\\n |       \"For      the      1-7      scale      rating      results      on      each      criterion,      we      cal-       culate      the      Krippendorff’s      Alpha      to      measure      the      inter      annotator       agreement      (IAA),      and      the      results      are      as      follows:      Interest      Level       (0.349),      Organization      (0.221),      Relevance      (0.256),      Coverage       (0.346),      Verifiability      (0.388).\\n'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 18, 'section_title': 'Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree'}, page_content='Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree\\n \\n \\n \\nI      think      it      can      be      specifically      helpful  \\n \\n  wae      70%      30%  \\n \\n  for      my      pre-writing      stage.\\nI\\n \\n   think      it      will      help      me      edit   \\n \\na      Wikipedia      anes   \\n \\n3      oars      30%  \\n \\n  article      for   \\n \\na      new      topic.\\n \\n \\n= I      think      it      can      be   \\n \\na      potentially      useful      10%      20%   \\n \\n:      60%      10%  \\n \\n  tool      for      the      Wikipedia      community.\\nFigure      3:      Survey      results      of      the      perceived      usefulness      of  \\n \\n  STORM      (n   \\n \\n=      10).\\n \\n \\n  background      information”      and      another      notes      that      “I  \\n \\n  found      that      the      AI      articles      had      more      depth      compared  \\n \\n  to      the      Wikipedia      articles”.\\nSTORM      also      outper-  \\n \\n  forms      the      best      baseline      in      pairwise      comparison.\\n \\n \\n  More      information      in      |R|      poses      challenges      be-  \\n \\n  yond      factual      hallucination.\\nWe      examine      14      pair-  \\n \\n  wise      comparison      responses      where      editors      prefer  \\n \\n  oORAG      outputs      over      STORM.\\nExcluding   \\n \\n3      cases  \\n \\n  where      pairwise      preferences      do      not      align      with      their  \\n \\n  ratings,      editors      assign      lower      Verifiability      scores      to  \\n \\n  articles      from      our      approach      in      over      50%      of      the      cases.\\n \\n \\n  Through      analyzing      the      articles      and      editors’      free-  \\n \\n  form      feedback,      we      discover      that      low      Verifiability  \\n \\n  scores      stem      from      red      herring      fallacy      or      overspec-  \\n \\n  ulation      issues.\\nThese      arise      when      the      generated  \\n \\n  articles      introduce      unverifiable      connections      between  \\n \\n  different      pieces      of      information      in      |7?|      or      between  \\n \\n  the      information      and      the      topic      (examples      included  \\n \\n  in      Table      11).\\nCompared      to      the      widely      discussed  \\n \\n  factual      hallucination      (Shuster      et      al.,      2021;      Huang  \\n \\n  et      al.,      2023),      addressing      such      verifiability      issues      is  \\n \\n  more      nuanced,      surpassing      basic      fact-checking      (Min  \\n \\n  et      al.,      2023).\\n \\n \\n  Generated      articles      trail      behind      well-revised      hu-  \\n \\n  man      works.\\nWhile      STORM      outperforms      the  \\n \\n  oRAG      baseline,      editors      comment      that      the      generated  \\n \\n  articles      are      less      informative      than      actual      Wikipedia  \\n \\n  pages.\\nAnother      major      issue      identified      is      the      trans-  \\n \\n  fer      of      bias      and      tone      from      Internet      sources      to      the  \\n \\n  generated      article,      with   \\n \\n7      out      of      10      editors      men-  \\n \\n  tioning      that      the      STORM-generated      articles      sound  \\n \\n  “emotional”      or      “unneutral”.\\nMore      analysis      is      dis-  \\n \\n  cussed      in      Appendix      E.      This      feedback      suggests      that  \\n \\n  reducing      the      retrieval      bias      in      the      pre-writing      stage  \\n \\n  is   \\n \\na      worthwhile      direction      for      future      work.\\n \\n \\n  Generated      articles      are   \\n \\na      good      starting      point.\\nAs  \\n \\n  shown      in      Figure      3,      editors      are      unanimous      in      agree-  \\n \\n  ing      that      STORM      can      aid      them      in      their      pre-writing  \\n \\n  stage.\\nIt      is      gratifying      to      know      that      the      tool      is      help-  \\n \\n  ful      to      experienced      editors.\\n80%      of      the      editors      think  \\n \\n  that      STORM      can      help      them      edit   \\n \\na      Wikipedia      article  \\n \\n  for   \\n \\na      new      topic.\\nMore      reservation      is      expressed      to  \\n \\n  the      usefulness      of      STORM      for      the      Wikipedia      com-  \\n \\n  munity      at      large;      nonetheless,      70%      of      the      editors  \\n \\n  think      it      is      useful,      with      only      10%      disagreeing.\\n7\\n \\n   Related      Works  \\n \\n  Retrieval-Augmented      Generation      (RAG)      Aug-  \\n \\n  menting      language      models      (LMs)      with      retrieval      at  \\n \\n  inference      time      is   \\n \\na      typical      way      to      leverage      exter-  \\n \\n  nal      knowledge      stores      (Ram      et      al.,      2023;      Izacard  \\n \\n  et      al.,      2023).\\nWhile      some      works      use      retrieval  \\n \\n  to      construct      demonstrations      for      in-context      learn-  \\n \\n  ing      (Li      et      al.,      2023;      Liu      et      al.,      2022;      Agrawal      et      al.,  \\n \\n  2023;      Poesia      et      al.,      2022;      Shi      et      al.,      2022;      Khattab  \\n \\n  et      al.,      2022),      another      line      of      works      uses      retrieval      to  \\n \\n  provide      additional      information      for      LMs      to      ground  \\n \\n  on.\\nLewis      et      al.\\n(2020)      study      RAG      on      knowledge-  \\n \\n  intensive      NLP      tasks      and      find      it      improves      diver-  \\n \\n  sity      and      factuality.\\nSemnani      et      al.\\n(2023)      de-  \\n \\n  signs   \\n \\na      RAG-based      chatbot      grounded      on      English  \\n \\n  Wikipedia      to      stop      LLM-based      chatbots      from      hal-  \\n \\n  lucination.\\nBesides,      RAG      can      be      used      to      generate  \\n \\n  text      with      citations      (Menick      et      al.,      2022;      Gao      et      al.,  \\n \\n  2023)      and      build      attributed      question      answering      sys-  \\n \\n  tems      (Bohnet      et      al.,      2023).\\nWhile      RAG      is      widely  \\n \\n  studied      in      question      answering,      how      to      use      it      for  \\n \\n  long-form      article      generation      is      less      investigated.\\n \\n \\n  As   \\n \\na      general      framework,      RAG      is      flexible      in      both  \\n \\n  the      retrieval      source      and      time.\\nThe      retrieval      sources  \\n \\n  can      vary      from      domain      databases      (Zakka      et      al.,  \\n \\n  2023),      code      documentation      (Zhou      et      al.,      2023),  \\n \\n  to      the      whole      Internet      (Nakano      et      al.,      2022;      Komeili  \\n \\n  et      al.,      2022).\\nRegarding      the      time,      besides   \\n \\na      one-  \\n \\n  time      retrieval      before      generation,      the      system      can      be  \\n \\n  designed      to      self-decide      when      to      retrieve      across      the  \\n \\n  course      of      the      generation      (Jiang      et      al.,      2023b;      Parisi  \\n \\n  et      al.,      2022;      Shuster      et      al.,      2022;      Yao      et      al.,      2023).\\n \\n \\n  Automatic      Expository      Writing      Different      from  \\n \\n  other      types      of      long-form      generation      (Yang      et      al.,  \\n \\n  2022;      Feng      et      al.,      2018),      automatic      expository      writ-  \\n \\n  ing      requires      grounding      on      external      documents      and       leveraging      the      interplay      between      reading      and      writ-  \\n \\n  ing.\\nBalepur      et      al.\\n(2023)      propose      the      Imitate-  \\n \\n  Retrieve-Paraphrase      framework      for      expository      writ-  \\n \\n  ing      at      the      paragraph      level      to      address      the      challenges  \\n \\n  in      synthesizing      information      from      multiple      sources.\\n \\n \\n  Beyond      summarizing      sources,      Shen      et      al.\\n(2023)  \\n \\n  highlight      that      expository      writing      requires      the      au-  \\n \\n  thor’s      sensemaking      process      over      source      documents  \\n \\n  and      good      outline      planning.\\nWe      tackle      these      chal-  \\n \\n  lenges      by      focusing      on      the      pre-writing      stage.\\n \\n \\n  Question      Asking      in      NLP      Question      asking      capa-  \\n \\n  bilities      in      NLP      systems      have      expanded      across      sev-  \\n \\n  eral      fronts,      including      generating      clarification      ques-  \\n \\n  tions      to      understand      user      intents      (Aliannejadi      et      al.,  \\n \\n  2019;      Rahmani      et      al.,      2023),      and      breaking      large  \\n \\n  questions      into      smaller      ones      to      improve      composi-  \\n \\n  tional      reasoning      (Press      et      al.,      2023).\\nWhile      humans  \\n \\n  usually      ask      questions      to      learn      new      knowledge      (Taw-  \\n \\n  fik      et      al.,      2020;      Booth      et      al.,      2003),      how      to      opti-  \\n \\n  mize      question      informativeness      and      specificity      in  \\n \\n  information-seeking      conversations      remains      less      ex-  \\n \\n  plored.\\nThe      closest      work      is      Qi      et      al.\\n(2020)      which  \\n \\n  defines      the      question      informativeness      using      the      un-  \\n \\n  igram      precision      function      and      uses      reinforcement  \\n \\n  learning      to      increase      the      question      informativeness.\\n8\\n \\n   Conclusion  \\n \\n  We      propose      STORM,      an      LLM-based      writing      sys-  \\n \\n  tem      that      automates      the      pre-writing      stage      for      creat-  \\n \\n  ing      Wikipedia-like      articles      from      scratch.\\nWe      cu-  \\n \\n  rate      the      FreshWiki      dataset      and      establish      evaluation  \\n \\n  criteria      to      study      the      generation      of      grounded      long-  \\n \\n  form      articles.\\nExperimental      results      demonstrate  \\n \\n  that      the      question      asking      mechanism      in      STORM  \\n \\n  improves      both      the      outline      and      article      quality.\\nWith  \\n \\n  the      improved      breadth      and      depth,      STORM      helps  \\n \\n  surface      new      challenges      for      grounded      writing      sys-  \\n \\n  tems      through      expert      evaluation.\\nThe      experienced  \\n \\n  Wikipedia      editors      in      our      study      unanimously      agree  \\n \\n  that      STORM      is      helpful      for      their      pre-writing      stage.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 19, 'section_title': 'Limitations'}, page_content='Limitations\\n \\n \\n  In      this      work,      we      explore      generating      Wikipedia-  \\n \\n  like      articles      from      scratch      as   \\n \\na      way      to      push      the  \\n \\n  frontier      of      automatic      expository      writing      and      long-  \\n \\n  form      article      generation.\\nWhile      our      approach      sig-  \\n \\n  nificantly      outperforms      baseline      methods      in      both  \\n \\n  automatic      and      human      evaluations,      the      quality      of  \\n \\n  machine-written      articles      still      lags      behind      well-  \\n \\n  revised      human-authored      articles,      specifically      in  \\n \\n  aspects      of      neutrality      and      verifiability.\\nAlthough  \\n \\n  STORM      discovers      different      perspectives      in      re-  \\n \\n  searching      the      given      topic,      the      collected      information  \\n \\n  may      still      be      biased      towards      dominant      sources      on  \\n \\n  the      Internet      and      may      contain      promotional      content.\\n \\n \\n  Moreover,      the      verifiability      issues      identified      in      this  \\n \\n  work      go      beyond      factual      hallucination,      which      high-  \\n \\n  lights      new      challenges      to      grounded      writing      systems.\\n \\n \\n  Another      limitation      of      this      work      is      that      although  \\n \\n  we      focus      on      the      task      of      generating      Wikipedia-like  \\n \\n  articles      from      scratch,      our      task      setup      is      still      simpli-  \\n \\n  fied      to      only      consider      the      generation      of      free-form  \\n \\n  text.\\nHuman-authored      high-quality      Wikipedia      ar-  \\n \\n  ticles      usually      contain      structured      data      and      multi-  \\n \\n  modal      information.\\nWe      leave      the      exploration      of  \\n \\n  generating      multi-modal      grounded      articles      for      fu-  \\n \\n  ture      work.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 20, 'section_title': 'Acknowledgements'}, page_content='Acknowledgements\\n \\n \\n  We      thank      You.com      for      generously      providing      the  \\n \\n  search      API      that      supported      our      experiments.\\nWe  \\n \\n  also      thank      Sina      J.      Semnani,      Shicheng      Liu,      Eric      Ze-  \\n \\n  likman      for      providing      helpful      feedback      and      the      ACL  \\n \\n  ARR      reviewers      for      their      valuable      comments.\\nThis  \\n \\n  work      is      supported      in      part      by      the      Verdant      Founda-  \\n \\n  tion      and      Microsoft      Azure      AI      credits.\\nYijia      Shao  \\n \\n  is      supported      by   \\n \\na      Stanford      School      of      Engineering  \\n \\n  Fellowship.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 21, 'section_title': 'Ethics      Statement'}, page_content='Ethics      Statement\\n \\n \\n  Different      from      the      creative      generation,      grounded      ar-  \\n \\n  ticle      generation      may      impact      how      people      learn      about  \\n \\n  topics      or      consume      source      information.\\nAll      the      stud-  \\n \\n  ies      and      the      evaluation      in      this      work      are      designed  \\n \\n  to      prevent      the      dissemination      of      misinformation      by  \\n \\n  not      publishing      generated      content      online      and      im-  \\n \\n  plementing      strict      accuracy      checks.\\nWe      avoid      any  \\n \\n  disruption      to      Wikipedia      or      related      communities,      as  \\n \\n  our      system      does      not      interact      with      live      pages.\\nAlso,  \\n \\n  although      we      try      to      generate      grounded      articles,      we  \\n \\n  believe      there      is      no      privacy      issue      related      to      this      work  \\n \\n  as      we      only      use      information      publicly      available      on  \\n \\n  the      Internet.\\n \\n \\n  The      primary      risk      of      our      work      is      that      the  \\n \\n  Wikipedia      articles      written      by      our      system      are  \\n \\n  grounded      on      information      on      the      Internet      which  \\n \\n  contains      some      biased      or      discriminative      content      on  \\n \\n  its      own.\\nCurrently,      our      system      relies      on      the      search  \\n \\n  engine      to      retrieve      information      but      does      not      include  \\n \\n  any      post-processing      module.\\nWe      believe      improv-  \\n \\n  ing      the      retrieval      module      to      have      good      coverage      of  \\n \\n  different      viewpoints      and      adding   \\n \\na      content      sifting  \\n \\n  module      to      the      current      system      will      be   \\n \\na      critical      next  \\n \\n  step      to      achieve      better      neutrality      and      balance      in      the  \\n \\n  generated      articles.\\n \\n \\n  Another      limitation      we      see      from      an      ethical      point  \\n \\n  of      view      is      that      we      only      consider      writing      English  \\n \\n  Wikipedia      articles      in      this      work.\\nExtending      the      cur-  \\n \\n  rent      system      to   \\n \\na      multilingual      setup      is   \\n \\na      meaningful  \\n \\n  direction      for      future      work      as      more      topics      do      not      have  \\n \\n  Wikipedia      pages      in      non-English      languages.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 22, 'section_title': 'References'}, page_content='References\\n \\n \\n  Sweta      Agrawal,      Chunting      Zhou,      Mike      Lewis,      Luke  \\n \\n  Zettlemoyer,      and      Marjan      Ghazvininejad.\\n2023.\\nIn-  \\n \\n  context      examples      selection      for      machine      translation.\\n \\n \\n  In      Findings      of      the      Association      for      Computational  \\n \\n  Linguistics:      ACL      2023,      pages      8857-8873,      Toronto,  \\n \\n  Canada.\\nAssociation      for      Computational      Linguistics.\\n \\n \\n  Alan      Akbik,      Tanja      Bergmann,      Duncan      Blythe,      Kashif  \\n \\n  Rasul,      Stefan      Schweter,      and      Roland      Vollgraf.\\n2019.  \\n \\n  FLAIR:      An      easy-to-use      framework      for      state-of-the-  \\n \\n  art      NLP.\\nIn      Proceedings      of      the      2019      Conference      of  \\n \\n  the      North      American      Chapter      of      the      Association      for  \\n \\n  Computational      Linguistics      (Demonstrations),      pages  \\n \\n  54-59,      Minneapolis,      Minnesota.\\nAssociation      for  \\n \\n  Computational      Linguistics.\\n \\n \\n  Mohammad      Aliannejadi,      Hamed      Zamani,      Fabio  \\n \\n  Crestani,      and   \\n \\nW      Bruce      Croft.\\n2019.\\nAsking      clari-  \\n \\n  fying      questions      in      open-domain      information-seeking  \\n \\n  conversations.\\nIn      Proceedings      of      the      42nd      interna-  \\n \\n  tional      acm      sigir      conference      on      research      and      develop-  \\n \\n  ment      in      information      retrieval,      pages      475-484.\\n \\n \\n  Nishant      Balepur,      Jie      Huang,      and      Kevin      Chang.\\n2023.  \\n \\n  Expository      text      generation:      Imitate,      retrieve,      para-  \\n \\n  phrase.\\nIn      Proceedings      of      the      2023      Conference      on  \\n \\n  Empirical      Methods      in      Natural      Language      Process-  \\n \\n  ing,      pages      11896-11919,      Singapore.\\nAssociation      for  \\n \\n  Computational      Linguistics.\\n \\n \\n  Siddhartha      Banerjee      and      Prasenjit      Mitra.\\n2015.  \\n \\n  WikiKreator:      Improving      Wikipedia      stubs      automat-  \\n \\n  ically.\\nIn      Proceedings      of      the      53rd      Annual      Meet-  \\n \\n  ing      of      the      Association      for      Computational      Linguis-  \\n \\n  tics      and      the      7th      International      Joint      Conference      on  \\n \\n  Natural      Language      Processing      (Volume      1:      Long      Pa-  \\n \\n  pers),      pages      867-877,      Beijing,      China.\\nAssociation  \\n \\n  for      Computational      Linguistics.\\n \\n \\n  Bernd      Bohnet,      Vinh      Q.      Tran,      Pat      Verga,      Roee      Aha-  \\n \\n  roni,      Daniel      Andor,      Livio      Baldini      Soares,      Massimil-  \\n \\n  iano      Ciaramita,      Jacob      Eisenstein,      Kuzman      Ganchev,  \\n \\n  Jonathan      Herzig,      Kai      Hui,      Tom      Kwiatkowski,      Ji      Ma,  \\n \\n  Jianmo      Ni,      Lierni      Sestorain      Saralegui,      Tal      Schus-  \\n \\n  ter,      William      W.      Cohen,      Michael      Collins,      Dipanjan  \\n \\n  Das,      Donald      Metzler,      Slav      Petrov,      and      Kellie      Webster.\\n \\n \\n  2023.\\nAttributed      question      answering:      Evaluation      and       modeling      for      attributed      large      language      models.\\n \\n \\n  Wayne      C      Booth,      Gregory      G      Colomb,      and      Joseph      M       Williams.\\n2003.\\nThe      craft      of      research.\\nUniversity      of  \\n \\n  Chicago      press.\\n \\n \\n  Laura      Dietz      and      John      Foley.\\n2019.\\nTrec      car      y3:      Com-  \\n \\n  plex      answer      retrieval      overview.\\nIn      Proceedings      of  \\n \\n  Text      REtrieval      Conference      (TREC).\\n \\n \\n  Christina      S      Doyle.\\n1994.\\nInformation      literacy      in      an  \\n \\n  information      society:   \\n \\nA      concept      for      the      information  \\n \\n  age.\\nDiane      Publishing.\\n \\n \\n  Ann-Marie      Eriksson      and      Asa      Mikitalo.\\n2015.\\nSupervi-  \\n \\n  sion      at      the      outline      stage:      Introducing      and      encounter-  \\n \\n  ing      issues      of      sustainable      development      through      aca-  \\n \\n  demic      writing      assignments.\\nText   \\n \\n&      Talk,      35(2):123-  \\n \\n  153.\\n \\n \\n  Angela      Fan      and      Claire      Gardent.\\n2022.\\nGenerating      bi-  \\n \\n  ographies      on      Wikipedia:      The      impact      of      gender      bias  \\n \\n  on      the      retrieval-based      generation      of      women      biogra-  \\n \\n  phies.\\nIn      Proceedings      of      the      60th      Annual      Meeting      of  \\n \\n  the      Association      for      Computational      Linguistics      (Vol-  \\n \\n  ume      I:      Long      Papers),      pages      8561-8576,      Dublin,  \\n \\n  Ireland.\\nAssociation      for      Computational      Linguistics.\\n \\n \\n  Xiaocheng      Feng,      Ming      Liu,      Jiahao      Liu,      Bing      Qin,      Yibo  \\n \\n  Sun,      and      Ting      Liu.\\n2018.\\nTopic-to-essay      generation  \\n \\n  with      neural      networks.\\nIn      JJCAI,      pages      4078-4084.\\n \\n \\n  Tira      Nur      Fitria.\\n2023.\\nArtificial      intelligence      (ai)      tech-  \\n \\n  nology      in      openai      chatgpt      application:   \\n \\nA      review      of  \\n \\n  chatgpt      in      writing      english      essay.\\nIn      ELT      Forum:      Jour-  \\n \\n  nal      of      English      Language      Teaching,      volume      12,      pages  \\n \\n  44-58.\\n \\n \\n  Pasi      Franti      and      Radu      Mariescu-Istodor.\\n2023.\\nSoft      preci-  \\n \\n  sion      and      recall.\\nPattern      Recognition      Letters,      167:115—  \\n \\n  121.\\n \\n \\n  R      Edward      Freeman,      Jeffrey      S      Harrison,      Andrew      C       Wicks,      Bidhan   \\n \\nL      Parmar,      and      Simone      De      Colle.\\n2010.\\n \\n \\n  Stakeholder      theory:      The      state      of      the      art.\\n \\n \\n  Tianyu      Gao,      Howard      Yen,      Jiatong      Yu,      and      Danqi      Chen.\\n \\n \\n  2023.\\nEnabling      large      language      models      to      generate  \\n \\n  text      with      citations.\\nIn      Proceedings      of      the      2023      Con-  \\n \\n  ference      on      Empirical      Methods      in      Natural      Language  \\n \\n  Processing,      pages      6465-6488,      Singapore.\\nAssocia-  \\n \\n  tion      for      Computational      Linguistics.\\n \\n \\n  Lei      Huang,      Weijiang      Yu,      Weitao      Ma,      Weihong      Zhong,  \\n \\n  Zhangyin      Feng,      Haotian      Wang,      Qianglong      Chen,  \\n \\n  Weihua      Peng,      Xiaocheng      Feng,      Bing      Qin,      and      Ting  \\n \\n  Liu.\\n2023.   \\n \\nA      survey      on      hallucination      in      large      lan-  \\n \\n  guage      models:      Principles,      taxonomy,      challenges,      and       open      questions.\\n \\n \\n  Gautier      Izacard,      Patrick      Lewis,      Maria      Lomeli,      Lucas  \\n \\n  Hosseini,      Fabio      Petroni,      Timo      Schick,      Jane      Dwivedi-  \\n \\n  Yu,      Armand      Joulin,      Sebastian      Riedel,      and      Edouard  \\n \\n  Grave.\\n2023.\\nAtlas:      Few-shot      learning      with      retrieval  \\n \\n  augmented      language      models.\\nJournal      of      Machine  \\n \\n  Learning      Research,      24(251):1-43.\\n \\n \\n  Albert   \\n \\nQ      Jiang,      Alexandre      Sablayrolles,      Arthur      Men-  \\n \\n  sch,      Chris      Bamford,      Devendra      Singh      Chaplot,      Diego  \\n \\n  de      las      Casas,      Florian      Bressand,      Gianna      Lengyel,      Guil-  \\n \\n  laume      Lample,      Lucile      Saulnier,      et      al.\\n2023a.\\nMistral  \\n \\n  7b.\\narXiv      preprint      arXiv:2310.06825.\\n \\n \\n  Zhengbao      Jiang,      Frank      Xu,      Luyu      Gao,      Zhiqing      Sun,  \\n \\n  Qian      Liu,      Jane      Dwivedi-Yu,      Yiming      Yang,      Jamie  \\n \\n  Callan,      and      Graham      Neubig.\\n2023b.\\nActive      retrieval  \\n \\n  augmented      generation.\\nIn      Proceedings      of      the      2023  \\n \\n  Conference      on      Empirical      Methods      in      Natural      Lan-  \\n \\n  guage      Processing,      pages      7969-7992,      Singapore.\\nAs-  \\n \\n  sociation      for      Computational      Linguistics.\\n \\n \\n  Nikhil      Kandpal,      Haikang      Deng,      Adam      Roberts,      Eric  \\n \\n  Wallace,      and      Colin      Raffel.\\n2023.\\nLarge      language  \\n \\n  models      struggle      to      learn      long-tail      knowledge.\\nIn      In-  \\n \\n  ternational      Conference      on      Machine      Learning,      pages  \\n \\n  15696-15707.\\nPMLR.\\n \\n \\n  Omar      Khattab,      Keshav      Santhanam,      Xiang      Lisa  \\n \\n  Li,      David      Hall,      Percy      Liang,      Christopher      Potts,  \\n \\n  and      Matei      Zaharia.\\n2022.\\nDemonstrate-search-  \\n \\n  predict:      Composing      retrieval      and      language      mod-  \\n \\n  els      for      knowledge-intensive      NLP.\\narXiv      preprint  \\n \\n  arXiv:2212.14024.\\n \\n \\n  Omar      Khattab,      Arnav      Singhvi,      Paridhi      Maheshwari,  \\n \\n  Zhiyuan      Zhang,      Keshav      Santhanam,      Sri      Vard-  \\n \\n  hamanan,      Saiful      Haq,      Ashutosh      Sharma,      Thomas      T.  \\n \\n  Joshi,      Hanna      Moazam,      Heather      Miller,      Matei      Za-  \\n \\n  haria,      and      Christopher      Potts.\\n2023.\\nDspy:      Compiling  \\n \\n  declarative      language      model      calls      into      self-improving  \\n \\n  pipelines.\\narXiv      preprint      arXiv:2310.03714.\\n \\n \\n  Seungone      Kim,      Jamin      Shin,      Yejin      Cho,      Joel      Jang,  \\n \\n  Shayne      Longpre,      Hwaran      Lee,      Sangdoo      Yun,  \\n \\n  Seongjin      Shin,      Sungdong      Kim,      James      Thorne,      et      al.\\n \\n \\n  2023.\\nPrometheus:      Inducing      fine-grained      evalua-  \\n \\n  tion      capability      in      language      models.\\narXiv      preprint  \\n \\n  arXiv:2310.08491.\\n \\n \\n  Mojtaba      Komeili,      Kurt      Shuster,      and      Jason      Weston.\\n2022.  \\n \\n  Internet-augmented      dialogue      generation.\\nIn      Proceed-  \\n \\n  ings      of      the      60th      Annual      Meeting      of      the      Association  \\n \\n  for      Computational      Linguistics      (Volume      1:      Long      Pa-  \\n \\n  pers),      pages      8460-8478,      Dublin,      Ireland.\\nAssociation  \\n \\n  for      Computational      Linguistics.\\n \\n \\n  Kalpesh      Krishna,      Erin      Bransom,      Bailey      Kuehl,      Mohit  \\n \\n  Iyyer,      Pradeep      Dasigi,      Arman      Cohan,      and      Kyle      Lo.\\n \\n \\n  2023.\\nLongEval:      Guidelines      for      human      evaluation      of  \\n \\n  faithfulness      in      long-form      summarization.\\nIn      Proceed-  \\n \\n  ings      of      the      17th      Conference      of      the      European      Chap-  \\n \\n  ter      of      the      Association      for      Computational      Linguistics,  \\n \\n  pages      1650-1669,      Dubrovnik,      Croatia.\\nAssociation  \\n \\n  for      Computational      Linguistics.\\n \\n \\n  Patrick      Lewis,      Ethan      Perez,      Aleksandra      Piktus,      Fabio  \\n \\n  Petroni,      Vladimir      Karpukhin,      Naman      Goyal,      Hein-  \\n \\n  rich      Kiittler,      Mike      Lewis,      Wen-tau      Yih,      Tim      Rock-  \\n \\n  taschel,      et      al.\\n2020.\\nRetrieval-augmented      generation  \\n \\n  for      knowledge-intensive      nlp      tasks.\\nAdvances      in      Neu-  \\n \\n  ral      Information      Processing      Systems,      33:9459-9474.\\n \\n \\n  Xiaonan      Li,      Kai      Lv,      Hang      Yan,      Tianyang      Lin,      Wei      Zhu,  \\n \\n  Yuan      Ni,      Guotong      Xie,      Xiaoling      Wang,      and      Xipeng  \\n \\n  Qiu.\\n2023.\\nUnified      demonstration      retriever      for      in-  \\n \\n  context      learning.\\nIn      Proceedings      of      the      61st      Annual  \\n \\n  Meeting      of      the      Association      for      Computational      Lin-  \\n \\n  guistics      (Volume      1:      Long      Papers),      pages      4644-4668,  \\n \\n  Toronto,      Canada.\\nAssociation      for      Computational      Lin-  \\n \\n  guistics.\\n \\n \\n  Chin-Yew      Lin.\\n2004.\\nROUGE:   \\n \\nA      package      for      auto-  \\n \\n  matic      evaluation      of      summaries.\\nIn      Text      Summariza-  \\n \\n  tion      Branches      Out,      pages      74-81,      Barcelona,      Spain.\\n \\n \\n  Association      for      Computational      Linguistics.\\n \\n \\n  Jiachang      Liu,      Dinghan      Shen,      Yizhe      Zhang,      Bill      Dolan,  \\n \\n  Lawrence      Carin,      and      Weizhu      Chen.\\n2022.\\nWhat  \\n \\n  makes      good      in-context      examples      for      GPT-3?\\nIn  \\n \\n  Proceedings      of      Deep      Learning      Inside      Out      (DeeLIO  \\n \\n  2022):      The      3rd      Workshop      on      Knowledge      Extrac-  \\n \\n  tion      and      Integration      for      Deep      Learning      Architectures,  \\n \\n  pages      100-114,      Dublin,      Ireland      and      Online.\\nAssocia-  \\n \\n  tion      for      Computational      Linguistics.\\n \\n \\n  Peter      J.      Liu,      Mohammad      Saleh,      Etienne      Pot,      Ben  \\n \\n  Goodrich,      Ryan      Sepassi,      Lukasz      Kaiser,      and      Noam  \\n \\n  Shazeer.\\n2018.\\nGenerating      wikipedia      by      summariz-  \\n \\n  ing      long      sequences.\\nIn      International      Conference      on  \\n \\n  Learning      Representations.\\n \\n \\n  Jacob      Menick,      Maja      Trebacz,      Vladimir      Mikulik,  \\n \\n  John      Aslanides,      Francis      Song,      Martin      Chadwick,  \\n \\n  Mia      Glaese,      Susannah      Young,      Lucy      Campbell-  \\n \\n  Gillingham,      Geoffrey      Irving,      and      Nat      McAleese.\\n \\n \\n  2022.\\nTeaching      language      models      to      support      answers  \\n \\n  with      verified      quotes.\\n \\n \\n  Sewon      Min,      Kalpesh      Krishna,      Xinxi      Lyu,      Mike      Lewis,  \\n \\n  Wen-tau      Yih,      Pang      Koh,      Mohit      Iyyer,      Luke      Zettle-  \\n \\n  moyer,      and      Hannaneh      Hajishirzi.\\n2023.\\nFActScore:\\n \\n \\n  Fine-grained      atomic      evaluation      of      factual      precision  \\n \\n  in      long      form      text      generation.\\nIn      Proceedings      of      the  \\n \\n  2023      Conference      on      Empirical      Methods      in      Natural  \\n \\n  Language      Processing,      pages      12076-12100,      Singa-  \\n \\n  pore.\\nAssociation      for      Computational      Linguistics.\\n \\n \\n  Julia      Minguill6n,      Maura      Lerga,      Eduard      Aibar,      Josep  \\n \\n  Lladés-Masllorens,      and      Antoni      Meseguer-Artola.\\n \\n \\n  2017.\\nSemi-automatic      generation      of   \\n \\na      corpus      of  \\n \\n  wikipedia      articles      on      science      and      technology.\\nProfe-  \\n \\n  sional      de      la      Informacion,      26(5):995—1005.\\n \\n \\n  Rosa      Munoz-Luna.\\n2015.\\nMain      ingredients      for      suc-  \\n \\n  cess      in      12      academic      writing:      Outlining,      drafting      and       proofreading.\\nPloS      one,      10(6):e0128309.\\n \\n \\n  Reiichiro      Nakano,      Jacob      Hilton,      Suchir      Balaji,      Jeff      Wu,  \\n \\n  Long      Ouyang,      Christina      Kim,      Christopher      Hesse,  \\n \\n  Shantanu      Jain,      Vineet      Kosaraju,      William      Saunders,  \\n \\n  Xu      Jiang,      Karl      Cobbe,      Tyna      Eloundou,      Gretchen  \\n \\n  Krueger,      Kevin      Button,      Matthew      Knight,      Benjamin  \\n \\n  Chess,      and      John      Schulman.\\n2022.\\nWebgpt:      Browser-  \\n \\n  assisted      question-answering      with      human      feedback.\\n \\n \\n  Long      Ouyang,      Jeffrey      Wu,      Xu      Jiang,      Diogo      Almeida,  \\n \\n  Carroll      Wainwright,      Pamela      Mishkin,      Chong      Zhang,  \\n \\n  Sandhini      Agarwal,      Katarina      Slama,      Alex      Ray,      et      al.\\n \\n \\n  2022.\\nTraining      language      models      to      follow      instruc-  \\n \\n  tions      with      human      feedback.\\nAdvances      in      Neural  \\n \\n  Information      Processing      Systems,      35:27730—27744.\\nAaron      Parisi,      Yao      Zhao,      and      Noah      Fiedel.\\n2022.\\nTalm:  \\n \\n  Tool      augmented      language      models.\\n \\n \\n  John      V      Pavlik.\\n2023.\\nCollaborating      with      chatgpt:      Con-  \\n \\n  sidering      the      implications      of      generative      artificial      intel-  \\n \\n  ligence      for      journalism      and      media      education.\\nJournal-  \\n \\n  ism   \\n \\n&      Mass      Communication      Educator,      78(1):84—93.\\n \\n \\n  Gabriel      Poesia,      Alex      Polozov,      Vu      Le,      Ashish      Tiwari,  \\n \\n  Gustavo      Soares,      Christopher      Meek,      and      Sumit      Gul-  \\n \\n  wani.\\n2022.\\nSynchromesh:      Reliable      code      generation  \\n \\n  from      pre-trained      language      models.\\nIn      International  \\n \\n  Conference      on      Learning      Representations.\\n \\n \\n  Ofir      Press,      Muru      Zhang,      Sewon      Min,      Ludwig      Schmidt,  \\n \\n  Noah      Smith,      and      Mike      Lewis.\\n2023.\\nMeasuring      and       narrowing      the      compositionality      gap      in      language      mod-  \\n \\n  els.\\nIn      Findings      of      the      Association      for      Computational  \\n \\n  Linguistics:      EMNLP      2023,      pages      5687-5711,      Singa-  \\n \\n  pore.\\nAssociation      for      Computational      Linguistics.\\n \\n \\n  Peng      Qi,      Yuhao      Zhang,      and      Christopher      D.      Manning.\\n \\n \\n  2020.      Stay      hungry,      stay      focused:      Generating      infor-  \\n \\n  mative      and      specific      questions      in      information-seeking  \\n \\n  conversations.\\nIn      Findings      of      the      Association      for  \\n \\n  Computational      Linguistics:      EMNLP      2020,      pages      25—  \\n \\n  40,      Online.\\nAssociation      for      Computational      Linguis-  \\n \\n  tics.\\n \\n \\n  Hongjing      Qian,      Yutao      Zhu,      Zhicheng      Dou,      Haoqi      Gu,  \\n \\n  Xinyu      Zhang,      Zheng      Liu,      Ruofei      Lai,      Zhao      Cao,  \\n \\n  Jian-Yun      Nie,      and      Ji-Rong      Wen.\\n2023.\\nWebbrain:\\n \\n \\n  Learning      to      generate      factually      correct      articles      for  \\n \\n  queries      by      grounding      on      large      web      corpus.\\n \\n \\n  Hossein      A.      Rahmani,      Xi      Wang,      Yue      Feng,      Qiang      Zhang,  \\n \\n  Emine      Yilmaz,      and      Aldo      Lipani.\\n2023.   \\n \\nA      survey      on  \\n \\n  asking      clarification      questions      datasets      in      conversa-  \\n \\n  tional      systems.\\nIn      Proceedings      of      the      61st      Annual  \\n \\n  Meeting      of      the      Association      for      Computational      Lin-  \\n \\n  guistics      (Volume      1:      Long      Papers),      pages      2698-2716,  \\n \\n  Toronto,      Canada.\\nAssociation      for      Computational      Lin-  \\n \\n  guistics.\\n \\n \\n  Ashwin      Ram.\\n1991.   \\n \\nA      theory      of      questions      and      question  \\n \\n  asking.\\nJournal      of      the      Learning      Sciences,      1(3-4):273-  \\n \\n  318.\\n \\n \\n  Ori      Ram,      Yoav      Levine,      Itay      Dalmedigos,      Dor      Muhlgay,  \\n \\n  Amnon      Shashua,      Kevin      Leyton-Brown,      and      Yoav  \\n \\n  Shoham.\\n2023.\\nIn-context      retrieval-augmented      lan-  \\n \\n  guage      models.\\nTransactions      of      the      Association      for  \\n \\n  Computational      Linguistics.\\n \\n \\n  Nils      Reimers      and      Iryna      Gurevych.\\n2019.\\nSentence-  \\n \\n  BERT:      Sentence      embeddings      using      Siamese      BERT-  \\n \\n  networks.\\nIn      Proceedings      of      the      2019      Conference      on  \\n \\n  Empirical      Methods      in      Natural      Language      Processing  \\n \\n  and      the      9th      International      Joint      Conference      on      Natu-  \\n \\n  ral      Language      Processing      (EMNLP-IJCNLP),      pages  \\n \\n  3982-3992,      Hong      Kong,      China.\\nAssociation      for      Com-  \\n \\n  putational      Linguistics.\\n \\n \\n \\nD      Gordon      Rohman.\\n1965.\\nPre-writing      the      stage      of      dis-  \\n \\n  covery      in      the      writing      process.\\nCollege      composition  \\n \\n  and      communication,      16(2):106—112.\\n \\n \\n  Christina      Sauper      and      Regina      Barzilay.\\n2009.\\nAuto-  \\n \\n  matically      generating      Wikipedia      articles:   \\n \\nA      structure-  \\n \\n  aware      approach.\\nIn      Proceedings      of      the      Joint      Con-  \\n \\n  ference      of      the      47th      Annual      Meeting      of      the      ACL      and       the      4th      International      Joint      Conference      on      Natural  \\n \\n  Language      Processing      of      the      AFNLP,      pages      208-216,  \\n \\n  Suntec,      Singapore.\\nAssociation      for      Computational  \\n \\n  Linguistics.\\n \\n \\n  Lam.\\n2023.\\nWikiChat:      Stopping      the      hallucination      of  \\n \\n  large      language      model      chatbots      by      few-shot      ground-  \\n \\n  ing      on      Wikipedia.\\nIn      Findings      of      the      Association  \\n \\n  for      Computational      Linguistics:      EMNLP      2023,      pages  \\n \\n  2387-2413,      Singapore.\\nAssociation      for      Computa-  \\n \\n  tional      Linguistics.\\n \\n \\n  Jonathan      Bragg,      Jeff      Hammerbacher,      Doug      Downey,  \\n \\n  Joseph      Chee      Chang,      and      David      Sontag.\\n2023.\\nBe-  \\n \\n  yond      summarization:      Designing      ai      support      for      real-  \\n \\n  world      expository      writing      tasks.\\n \\n \\n  Luke      Zettlemoyer.\\n2022.\\nNearest      neighbor      zero-shot  \\n \\n  inference.\\nIn      Proceedings      of      the      2022      Conference      on  \\n \\n  Empirical      Methods      in      Natural      Language      Processing,  \\n \\n  pages      3254-3265,      Abu      Dhabi,      United      Arab      Emirates.\\n \\n \\n  Association      for      Computational      Linguistics.\\n \\n \\n  Stephen      Roller,      Arthur      Szlam,      and      Jason      Weston.\\n \\n \\n  2022.\\nLanguage      models      that      seek      for      knowledge:  \\n \\n  Modular      search   \\n \\n&      generation      for      dialogue      and       prompt      completion.\\nIn      Findings      of      the      Association  \\n \\n  for      Computational      Linguistics:      EMNLP      2022,      pages  \\n \\n  373-393,      Abu      Dhabi,      United      Arab      Emirates.\\nAssoci-  \\n \\n  ation      for      Computational      Linguistics.\\n \\n \\n  and      Jason      Weston.\\n2021.\\nRetrieval      augmentation  \\n \\n  reduces      hallucination      in      conversation.\\nIn      Findings  \\n \\n  of      the      Association      for      Computational      Linguistics:  \\n \\n  EMNLP      2021,      pages      3784-3803,      Punta      Cana,      Do-  \\n \\n  minican      Republic.\\nAssociation      for      Computational  \\n \\n  Linguistics.\\nWikipedia      as      an      introduction      to      academic      writing.\\nIn  \\n \\n  English      teaching      forum,      volume      48,      page      12.\\nERIC.\\n \\n \\n  and      Jaclyn      Gishbaugher.\\n2020.\\nRole      of      questions      in  \\n \\n  inquiry-based      instruction:      towards   \\n \\na      design      taxon-  \\n \\n  omy      for      question-asking      and      implications      for      design.\\n \\n \\n  Educational      Technology      Research      and      Development,  \\n \\n  68:653-678.\\nitory      text.\\n \\n \\n  than      humans?\\nvalidating      how      openai’s      chatgpt      model  \\n \\n  explains      crowdfunding,      alternative      finance      and      com-  \\n \\n  munity      finance.\\nValidating      how      OpenAlI’s      ChatGPT  \\n \\n  model      explains      Crowdfunding,      Alternative      Finance  \\n \\n  and      Community      Finance.(December      22,      2022).\\n \\n \\n  Choi.\\n2023.   \\n \\nA      critical      evaluation      of      evaluations      for  \\n \\n  long-form      question      answering.\\nIn      Proceedings      of      the  \\n \\n  61st      Annual      Meeting      of      the      Association      for      Compu-  \\n \\n  tational      Linguistics      (Volume      1:      Long      Papers),      pages  \\n \\n  3225-3245,      Toronto,      Canada.\\nAssociation      for      Com-  \\n \\n  putational      Linguistics.\\n \\n \\n  Kevin      Yang,      Dan      Klein,      Nanyun      Peng,      and      Yuandong  \\n \\n  Tian.\\n2023.      DOC:      Improving      long      story      coherence  \\n \\n  with      detailed      outline      control.\\nIn      Proceedings      of      the  \\n \\n  61st      Annual      Meeting      of      the      Association      for      Compu-  \\n \\n  tational      Linguistics      (Volume      I:      Long      Papers),      pages  \\n \\n  3378-3465,      Toronto,      Canada.\\nAssociation      for      Com-  \\n \\n  putational      Linguistics.\\n \\n \\n  Kevin      Yang,      Yuandong      Tian,      Nanyun      Peng,      and      Dan  \\n \\n  Klein.\\n2022.\\nRe3:      Generating      longer      stories      with  \\n \\n  recursive      reprompting      and      revision.\\nIn      Proceedings  \\n \\n  of      the      2022      Conference      on      Empirical      Methods      in      Nat-  \\n \\n  ural      Language      Processing,      pages      4393-4479,      Abu  \\n \\n  Dhabi,      United      Arab      Emirates.\\nAssociation      for      Com-  \\n \\n  putational      Linguistics.\\n \\n \\n  Shunyu      Yao,      Jeffrey      Zhao,      Dian      Yu,      Nan      Du,      Izhak  \\n \\n  Shafran,      Karthik      R      Narasimhan,      and      Yuan      Cao.\\n2023.  \\n \\n  React:      Synergizing      reasoning      and      acting      in      language  \\n \\n  models.\\nIn      The      Eleventh      International      Conference  \\n \\n  on      Learning      Representations.\\n \\n \\n  Cyril      Zakka,      Akash      Chaurasia,      Rohan      Shad,      Alex      R       Dalal,      Jennifer   \\n \\nL      Kim,      Michael      Moor,      Kevin      Alexan-  \\n \\n  der,      Euan      Ashley,      Jack      Boyd,      Kathleen      Boyd,      et      al.\\n \\n \\n  2023.\\nAlmanac:      Retrieval-augmented      language      mod-  \\n \\n  els      for      clinical      medicine.\\nResearch      Square.\\n \\n \\n  Shuyan      Zhou,      Uri      Alon,      Frank      F.      Xu,      Zhengbao      Jiang,  \\n \\n  and      Graham      Neubig.\\n2023.\\nDocprompting:      Gener-  \\n \\n  ating      code      by      retrieving      the      docs.\\nIn      The      Eleventh  \\n \\n  International      Conference      on      Learning      Representa-  \\n \\n  tions.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 23, 'section_title': 'Average      Numer      of      Sections      8.4'}, page_content='Average      Numer      of      Sections      8.4'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 24, 'section_title': 'Average      Number      of      All-level      Headings      15.8 Average      Length      of      a      Section      327.8       Average      Length      of      Total      Article      2159.1'}, page_content='Average      Number      of      All-level      Headings      15.8 Average      Length      of      a      Section      327.8       Average      Length      of      Total      Article      2159.1'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 25, 'section_title': 'Average      Number      of      References      90.1'}, page_content='Average      Number      of      References      90.1\\nTable      7:      Statistics      of      the      dataset      used      in      our      experiments.\\n—\\n \\n   Average Number      of      references  \\n \\n  a      BR  \\n \\n  N      Py      oa      feo}      Oo      N       Oo      Oo      oO      Oo      oO      Oo  \\n \\n  fo)  \\n \\n  1 0\\n \\n   20      40      60      80      100  \\n \\n  Edit      progress      (%      of      total      edits) Figure      4:      Evolution      of      reference      count      in      the      Wikipedia  \\n \\n  article      editing      process.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 26, 'section_title': 'A_      Dataset      Details'}, page_content='A_      Dataset      Details\\n \\n \\n  As      discussed      in      §2.1,      we      curate      the      FreshWiki  \\n \\n  dataset      by      collecting      recent      and      high-quality      En-  \\n \\n  glish      Wikipedia      articles.\\nWe      select      the      most-edited  \\n \\n  pages      over   \\n \\na      specific      period      rather      than      using      cre-  \\n \\n  ation      dates      as   \\n \\na      cutoff      because      most      of      Wikipedia  \\n \\n  articles      are      “stubs”      or      are      of      low      quality      when      they  \\n \\n  were      created.\\nFor      quality,      we      consider      articles      pre-  \\n \\n  dicted      to      be      of      B-class      quality      or      above.\\nAccording  \\n \\n  to      Wikipedia      statistics!\\n*,      only      around      3%      of      ex-  \\n \\n  isting      Wikipedia      pages      meet      this      quality      standard.\\n \\n \\n  As      LLMs      can      generate      reasonably      good      outputs,  \\n \\n  we      think      it      is      important      to      use      high-quality      human-  \\n \\n  written      articles      as      references      for      further      research.\\n \\n \\n  For      experiments      in      this      work,      we      randomly      se-  \\n \\n  lect      100      samples      with      human-written      articles      un-  \\n \\n  der      3000      words      to      have   \\n \\na      meaningful      comparison.\\n \\n \\n  Table   \\n \\n7      gives      the      data      statistics.\\nNotably,      human-  \\n \\n  authored      articles      have   \\n \\na      large      number      of      references  \\n \\n  but      they      require      numerous      edits      to      achieve      this.\\nFig-  \\n \\n  ure   \\n \\n4      illustrates      the      evolution      of      the      reference      count  \\n \\n  in      the      article      edit      process      and      Figure   \\n \\n5      gives      the      dis-  \\n \\n  tribution      of      edit      counts      for      human-authored      articles  \\n \\n  used      in      our      experiments.\\ncount      (A;)   \\n \\n=       where      embed(-)      in      Equation      (1)      is      parameterized  \\n \\n  by      paraphrase-MiniLM-L6-v2      provided      in      the  \\n \\n  Sentence-Transformers      library!*.\\nThe      cardinality  \\n \\n  https://en.wikipedia.\\norg/wiki/Wikipedia:  \\n \\n  Content_assessment  \\n \\n  100) Bhttps://huggingface.co/sentence-transformers/  \\n \\n  paraphrase-MiniLM-L6-v2 Percentage      of      articles      (n\\n |       T      T      T      T       0      500      1000      1500 |       y      1      7      1      7      7      7       2000      2500      3000      3500      4000      4500      5000       Number      of      edits\\n | Figure      5:      Distribution      of      edit      counts      for      Wikipedia      arti-       cles      in      our      experiments      (n      =      100).\\n'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 27, 'section_title': 'B_      Pseudo      Code      of      STORM'}, page_content='B_      Pseudo      Code      of      STORM\\n \\n \\n  In      §3,      we      introduce      STORM,   \\n \\na      framework      that      au-  \\n \\n  tomates      the      pre-writing      stage      by      discovering      differ-  \\n \\n  ent      perspectives,      simulating      information-seeking  \\n \\n  conversations,      and      creating   \\n \\na      comprehensive      out-  \\n \\n  line.\\nAlgorithm   \\n \\n1      displays      the      skeleton      of      STORM.\\n \\n \\n  We      implement      STORM      with      zero-shot      prompt-  \\n \\n  ing      using      the      DSPy      framework      (Khattab      et      al.,  \\n \\n  2023).\\nListing   \\n \\n1      and   \\n \\n2      show      the      prompts      used  \\n \\n  in      our      implementation.\\nWe      highlight      that      STORM  \\n \\n  offers   \\n \\na      general      framework      designed      to      assist      the  \\n \\n  creation      of      grounded,      long-form      articles,      without  \\n \\n  depending      extensively      on      prompt      engineering      for   \\n \\na       single      domain.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 28, 'section_title': 'C      Automatic      Evaluation      Details'}, page_content='C      Automatic      Evaluation      Details'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 29, 'section_title': 'C.1      Soft      Heading      Recall'}, page_content='C.1      Soft      Heading      Recall\\n \\n \\n  We      calculate      the      soft      heading      recall      between      the  \\n \\n  multi-level      headings      in      the      generated      outline,      con-  \\n \\n  sidered      as      the      prediction      P,      and      those      in      the      human-  \\n \\n  written      article,      considered      as      the      ground      truth      G.  \\n \\n  The      calculation      is      based      on      the      soft      recall      defini-  \\n \\n  tion      in      Franti      and      Mariescu-Istodor      (2023).\\nGiven  \\n \\n  aset   \\n \\nA   \\n \\n=      {Ai}*.,,      soft      count      of      an      item      is      defined as      the      inverse      of      the      sum      of      its      similarity      to      other  \\n \\n  items      in      the      set:\\n1\\n \\n \\n  Dj      Sim      (Aj,      Aj)      (1) Sim      (A;,      A;)   \\n \\n=      cos      (embed(A;),      embed(A;))   \\n \\n,       28  \\n \\n  29  \\n \\n  class      GenRelatedTopicsPrompt      (dspy.      Signature):\\n \\n \\n  I\\\\\\\\\\\\\\'m      writing   \\n \\na      Wikipedia      page      for   \\n \\na      topic      mentioned      below.\\nPlease      identify      and       recommend      some      Wikipedia      pages      on      closely      related      subjects.\\nI\\\\\\\\\\\\\\'m      looking      for  \\n \\n  examples      that      provide      insights      into      interesting      aspects      commonly      associated  \\n \\n  with      this      topic,      or      examples      that      help      me      understand      the      typical      content      and       structure      included      in      Wikipedia      pages      for      similar      topics.\\n \\n \\n  Please      list      the      urls      in      separate      lines.\\n \\n \\n  non topic   \\n \\n=      dspy.InputField(prefix=\"Topic      of      interest:”,      format=str)  \\n \\n  related_topics   \\n \\n=      dspy.OutputField() class      GenPerspectivesPrompt      (dspy.Signature):\\n \\n \\n  You      need      to      select   \\n \\na      group      of      Wikipedia      editors      who      will      work      together      to      create  \\n \\n  a      comprehensive      article      on      the      topic.\\nEach      of      them      represents   \\n \\na      different  \\n \\n  perspective,      role,      or      affiliation      related      to      this      topic.\\nYou      can      use      other  \\n \\n  Wikipedia      pages      of      related      topics      for      inspiration.\\nFor      each      editor,      add       description      of      what      they      will      focus      on.\\n \\n \\n  Give      your      answer      in      the      following      format:      1.      short      summary      of      editor      1:\\ndescription\\\\\\\\2.\\nshort summary of editor 2: description\\\\\\\\  \\n \\n  non  \\n \\n  topic   \\n \\n=      dspy.InputField(prefix=\\\\\\\\\\\\\\'Topic      of      interest:\\\\\\\\\\\\\\',      format=str)  \\n \\n  examples   \\n \\n=      dspy.InputField(prefix=\\\\\\\\\\\\\\'Wiki      page      outlines      of      related      topics      for  \\n \\n  inspiration:\\\\\\\\\\\\\\\\\\\\\\',      format=str)  \\n \\n  perspectives   \\n \\n=      dspy.OutputField() class      GenQnPrompt(dspy.      Signature):\\n \\n \\n  You      are      an      experienced      Wikipedia      writer      and      want      to      edit   \\n \\na      specific      page.\\n \\n \\n  Besides      your      identity      as   \\n \\na      Wikipedia      writer,      you      have   \\n \\na      specific      focus      when  \\n \\n  researching      the      topic.\\n \\n \\n  Now,      you      are      chatting      with      an      expert      to      get      information.\\nAsk      good      questions      to  \\n \\n  get      more      useful      information.\\n \\n \\n  When      you      have      no      more      question      to      ask,      say      \"Thank      you      so      much      for      your      help!”\\nto  \\n \\n  end      the      conversation.\\n \\n \\n  Please      only      ask      one      question      at   \\n \\na      time      and      don\\\\\\\\\\\\\\'t      ask      what      you      have      asked      before.\\n \\n \\n  Your      questions      should      be      related      to      the      topic      you      want      to      write.\\n \\n \\n  non  \\n \\n  topic   \\n \\n=      dspy.InputField(prefix=\\\\\\\\\\\\\\'Topic      you      want      to      write:      \\\\\\\\\\\\\\',      format=str)  \\n \\n  persona   \\n \\n=      dspy.InputField(prefix=\\\\\\\\\\\\\\'Your      specific      perspective:      \\\\\\\\\\\\\\',      format=str)  \\n \\n  conv   \\n \\n=      dspy.InputField(prefix=\\\\\\\\\\\\\\'Conversation      history:\\\\\\\\\\\\\\\\\\\\\\',      format=str)  \\n \\n  question   \\n \\n=      dspy.OutputField()  \\n \\n  class      GenQueriesPrompt      (dspy.      Signature):  \\n \\n  nnn  \\n \\n  You      want      to      answer      the      question      using      Google      search.\\nWhat      do      you      type      in      the  \\n \\n  search      box?\\nWrite the queries you will use in the following format:- query 1\\\\\\\\- query 2\\\\\\\\\\n \\n \\n  topic   \\n \\n=      dspy.InputField(prefix=\\\\\\\\\\\\\\'Topic      you      are      discussing      about:      \\\\\\\\\\\\\\',      format=str)  \\n \\n  question   \\n \\n=      dspy.InputField(prefix=\\\\\\\\\\\\\\'Question      you      want      to      answer:      \\\\\\\\\\\\\\',      format=str)  \\n \\n  queries   \\n \\n=      dspy.OutputField()\\nListing      1:      Prompts      used      in      STORM,      corresponding      to      Line      4,      11,      19,      22      in      Algorithm      1.\\nwow      Ne\\n \\n \\n  20  \\n \\n  21  \\n \\n  22  \\n \\n  23  \\n \\n  24  \\n \\n  25  \\n \\n  26  \\n \\n  27  \\n \\n  28\\n29  \\n \\n  30 class      GenAnswerPrompt(dspy.      Signature):\\n \\n \\n  You      are      an      expert      who      can      use      information      effectively.\\nYou      are      chatting      with   \\n \\na       Wikipedia      writer      who      wants      to      write   \\n \\na      Wikipedia      page      on      topic      you      know.\\nYou  \\n \\n  have      gathered      the      related      information      and      will      now      use      the      information      to  \\n \\n  form   \\n \\na      response.\\n \\n \\n  Make      your      response      as      informative      as      possible      and      make      sure      every      sentence      is  \\n \\n  supported      by      the      gathered      information.\\n \\n \\n  non  \\n \\n  topic   \\n \\n=      dspy.InputField(prefix=\\\\\\\\\\\\\\'Topic      you      are      discussing      about:\\\\\\\\\\\\\\',      format=str)  \\n \\n  conv      dspy.InputField(prefix=\\\\\\\\\\\\\\'Question:\\\\\\\\\\\\\\\\\\\\\\',      format=str)  \\n \\n  info   \\n \\n=      dspy.InputField(      prefix=\\\\\\\\\\\\\\'Gathered      information:\\\\\\\\\\\\\\\\\\\\\\',      format=str)  \\n \\n  answer   \\n \\n=      dspy.OutputField(prefix=\\\\\\\\\\\\\\'Now      give      your      response:\\\\\\\\\\\\\\\\\\\\\\')\\nclass      DirectGenOutlinePrompt      (dspy.      Signature):\\nWrite      an      outline      for   \\n \\na      Wikipedia      page.\\n \\n \\n  Here      is      the      format      of      your      writing:\\n2.      Do      not      include      other      information.\\n \\n \\n  non\\n1. Use      \"#\"      Title”      to      indicate      section      title,      \"##\"      Title”      to      indicate  \\n \\n  subsection      title,      \"###\"”      Title”      to      indicate      subsubsection      title,      and      so  \\n \\n  on.\\ntopic   \\n \\n=      dspy.InputField(prefix=\"Topic      you      want      to      write:      ”\",      format=str)  \\n \\n  outline   \\n \\n=      dspy.OutputField(prefix=\"Write      the      Wikipedia      page      outline:\\\\\\\\\"”)”\\nclass      RefineOutlinePrompt(dspy.      Signature):\\n \\n \\n  Improve      an      outline      for   \\n \\na      Wikipedia      page.\\nYou      already      have   \\n \\na      draft      outline      that  \\n \\n  covers      the      general      information.\\nNow      you      want      to      improve      it      based      on      the  \\n \\n  information      learned      from      an      information-seeking      conversation      to      make      it      more  \\n \\n  comprehensive.\\n \\n \\n  Here      is      the      format      of      your      writing:\\n2.      Do      not      include      other      information.\\n \\n \\n  non\\n1. Use      \"#\"      Title”      to      indicate      section      title,      \"##\"      Title”      to      indicate  \\n \\n  subsection      title,      \"###\"      Title”      to      indicate      subsubsection      title,      and      so  \\n \\n  on.\\n \\n \\n  topic   \\n \\n=      dspy.InputField(prefix=\"Topic      you      want      to      write:      \",      format=str)  \\n \\n  conv   \\n \\n=      dspy.InputField(prefix=\"Conversation      history:\\\\\\\\\",      format=str)  \\n \\n  old_outline   \\n \\n=      dspy.OutputField(prefix=\"Current      outline:\\\\\\\\”,      format=str)  \\n \\n  outline   \\n \\n=      dspy.OutputField(      prefix=\\\\\\\\\\\\\\'Write      the      Wikipedia      page      outline:\\\\\\\\\\\\\\\\\\\\\\')”\\nListing      2:      Prompts      used      in      STORM      (continue),      corresponding      to      Line      24,      31,      32      in      Algorithm      1.\\nyay      aA      uu      &}      WwW      YY      —\\n \\n \\n  11  \\n \\n  12  \\n \\n  13  \\n \\n  14  \\n \\n  15  \\n \\n  16  \\n \\n  17  \\n \\n  18  \\n \\n  19  \\n \\n  20  \\n \\n  21  \\n \\n  22  \\n \\n  23\\n \\n \\n  24  \\n \\n  25  \\n \\n  26  \\n \\n  27  \\n \\n  28  \\n \\n  29  \\n \\n  30  \\n \\n  31  \\n \\n  32  \\n \\n  33  \\n \\n  Input      :Topic      t,      maximum      perspective      N,  \\n \\n  maximum      conversation      round      MJ  \\n \\n  Output   \\n \\n:      Outline      O,      references   \\n \\nR PO = \"basic fact writer \" // Constant.\\n \\n \\n  R-[]  \\n \\n  //      Discover      perspectives      P.  \\n \\n  related_topics   \\n \\n+      gen_related_topics(t)  \\n \\n  tocs   \\n \\n+   \\n \\n|   \\n \\n|       foreach      related_t      in      related_topics      do  \\n \\n  article   \\n \\n<      get_wiki_article(related_t)  \\n \\n  if      article      then  \\n \\n  |      tocs.append(extract_toc(article))  \\n \\n  end  \\n \\n  end  \\n \\n  P   \\n \\n<      gen_perspectives(t,      tocs)  \\n \\n  P<      [PO]   \\n \\n+      P[:N]  \\n \\n  //      Simulate      conversations.\\n \\n \\n  convos   \\n \\n<      [|  \\n \\n  foreach      p      in      P      do  \\n \\n  convo_history   \\n \\n<   \\n \\n|   \\n \\n]       for:      =1to      M      do  \\n \\n  //      Question      asking.\\n \\n \\n  q+      gen_qn(t,      p,      dlg_history)  \\n \\n  convo_history.append(q)  \\n \\n  //      Question      answering.\\n \\n \\n  queries   \\n \\n<      gen_queries(t,      q)  \\n \\n  sources      <—  \\n \\n  search_and_sift(queries)  \\n \\n  a   \\n \\n+      gen_ans(t,      q,      sources)  \\n \\n  convo_history.append(a)  \\n \\n  R.append(sources)  \\n \\n  end  \\n \\n  convos.append(convo_history)  \\n \\n  end  \\n \\n  //      Create      the      outline.\\n \\n \\n  Op   \\n \\n<      direct_gen_outline(t)  \\n \\n  O      «<      refine_outline(t,      Op,      convos)  \\n \\n  return      O,      R       of   \\n \\nA      is      the      sum      of      the      counts      of      its      individual      items:'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 30, 'section_title': 'K       card(A)      =      S-      count      (A;)      (2)'}, page_content='K       card(A)      =      S-      count      (A;)      (2)\\ni=1 The      soft      heading      recall      is      calculated      as card(Gn      P)  \\n \\n  card(G)   \\n \\n”      @) soft      heading      recall      =\\nwhere      the      cardinality      of      intersection      is      defined      via  \\n \\n  the      union      as      follows:\\ncard(Gn      P)   \\n \\n=       card(G)   \\n \\n+      card(P)   \\n \\n—      card(G   \\n \\nU      P).\\n®'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 31, 'section_title': 'C.2.      LLM      Evaluator'}, page_content='C.2.      LLM      Evaluator\\nWe      use      Prometheus!\\n*      (Kim      et      al.,      2023),   \\n \\na      13B  \\n \\n  open-source      evaluator      LLM      that      can      assess      long-  \\n \\n  form      text      based      on      customized      1-5      scale      rubric,      to  \\n \\n  grade      the      article      from      the      aspects      of      Interest      level,  \\n \\n  Coherence      and      Organization,      Relevance      and      Fo-  \\n \\n  cus,      and      Coverage.\\nTable   \\n \\n8      gives      our      grading      rubric.\\n \\n \\n  While      Prometheus      is      best      used      with   \\n \\na      score   \\n \\n5      ref-  \\n \\n  erence      answer,      we      find      adding      the      reference      will  \\n \\n  exceed      the      context      length      limit      of      the      model.\\nSince  \\n \\n  Kim      et      al.\\n(2023)      show      Prometheus      ratings      without  \\n \\n  reference      also      correlate      well      with      human      prefer-  \\n \\n  ences,      we      omit      the      reference      and      trim      the      input  \\n \\n  article      to      be      within      2000      words      by      iteratively      re-  \\n \\n  moving      contents      from      the      shortest      section      to      ensure  \\n \\n  the      input      can      fit      into      the      model’s      context      window.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 32, 'section_title': 'C.3      More      Discussion      of      the      Citation      Quality'}, page_content='C.3      More      Discussion      of      the      Citation      Quality\\n \\n \\n  Irrelevant  \\n \\n  Source  \\n \\n  Inaccurate  \\n \\n  Othe      Paraphrasing  \\n \\n  1%      4%  \\n \\n  7%  \\n \\n  Improper  \\n \\n  Inferential      Linking  \\n \\n  Lack      Citation      14%  \\n \\n  47%  \\n \\n  Incorrectly      Split  \\n \\n  12%'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 33, 'section_title': 'False      Negative       15%'}, page_content='False      Negative       15%\\nFigure      6:      Error      analysis      of      unsupported      sentences      in      10  \\n \\n  sampled      articles.\\n \\n \\n  https:      //huggingface.co/kaist-ai/  \\n \\n  Criteria      Description  \\n \\n  Score   \\n \\n|      Description  \\n \\n  Score   \\n \\n2      Description  \\n \\n  Score   \\n \\n3      Description  \\n \\n  Score   \\n \\n4      Description  \\n \\n  Score   \\n \\n5      Description\\n \\n \\n  Interest      Level:      How      engaging      and      thought-provoking      is      the      article?\\n \\n \\n  Not      engaging      at      all;      no      attempt      to      capture      the      reader’s      attention.\\n \\n \\n  Fairly      engaging      with   \\n \\na      basic      narrative      but      lacking      depth.\\n \\n \\n  Moderately      engaging      with      several      interesting      points.\\n \\n \\n  Quite      engaging      with   \\n \\na      well-structured      narrative      and      noteworthy      points      that      frequently      capture      and      retain      attention.\\n \\n \\n  Exceptionally      engaging      throughout,      with   \\n \\na      compelling      narrative      that      consistently      stimulates      interest.\\n \\n \\n  Criteria      Description  \\n \\n  Score   \\n \\n|      Description  \\n \\n  Score   \\n \\n2      Description  \\n \\n  Score   \\n \\n3      Description  \\n \\n  Score   \\n \\n4      Description  \\n \\n  Score   \\n \\n5      Description  \\n \\n  Coherence      and      Organization:      Is      the      article      well-organized      and      logically      structured?\\n \\n \\n  Disorganized;      lacks      logical      structure      and      coherence.\\n \\n \\n  Fairly      organized;   \\n \\na      basic      structure      is      present      but      not      consistently      followed.\\n \\n \\n  Organized;   \\n \\na      clear      structure      is      mostly      followed      with      some      lapses      in      coherence.\\n \\n \\n  Good      organization;   \\n \\na      clear      structure      with      minor      lapses      in      coherence.\\n \\n \\n  Excellently      organized;      the      article      is      logically      structured      with      seamless      transitions      and   \\n \\na      clear      argument.\\n \\n \\n  Criteria      Description  \\n \\n  Score   \\n \\n|      Description  \\n \\n  Score   \\n \\n2      Description  \\n \\n  Score   \\n \\n3      Description  \\n \\n  Score   \\n \\n4      Description  \\n \\n  Score   \\n \\n5      Description  \\n \\n  Relevance      and      Focus:      Does      the      article      stay      on      topic      and      maintain   \\n \\na      clear      focus?\\n \\n \\n  Off-topic;      the      content      does      not      align      with      the      headline      or      core      subject.\\n \\n \\n  Somewhat      on      topic      but      with      several      digressions;      the      core      subject      is      evident      but      not      consistently      adhered      to.\\n \\n \\n  Generally      on      topic,      despite   \\n \\na      few      unrelated      details.\\n \\n \\n  Mostly      on      topic      and      focused;      the      narrative      has   \\n \\na      consistent      relevance      to      the      core      subject      with      infrequent      digressions.\\n \\n \\n  Exceptionally      focused      and      entirely      on      topic;      the      article      is      tightly      centered      on      the      subject,      with      every      piece      of      information      contributing  \\n \\n  to   \\n \\na      comprehensive      understanding      of      the      topic.\\n \\n \\n  Criteria      Description  \\n \\n  Score   \\n \\n|      Description  \\n \\n  Score   \\n \\n2      Description  \\n \\n  Score   \\n \\n3      Description  \\n \\n  Score   \\n \\n4      Description  \\n \\n  Score   \\n \\n5      Description  \\n \\n  Broad      Coverage:      Does      the      article      provide      an      in-depth      exploration      of      the      topic      and      have      good      coverage?\\n \\n \\n  Severely      lacking;      offers      little      to      no      coverage      of      the      topic’s      primary      aspects,      resulting      in   \\n \\na      very      narrow      perspective.\\n \\n \\n  Partial      coverage;      includes      some      of      the      topic’s      main      aspects      but      misses      others,      resulting      in      an      incomplete      portrayal.\\n \\n \\n  Acceptable      breadth;      covers      most      main      aspects,      though      it      may      stray      into      minor      unnecessary      details      or      overlook      some      relevant      points.\\n \\n \\n  Good      coverage;      achieves      broad      coverage      of      the      topic,      hitting      on      all      major      points      with      minimal      extraneous      information.\\n \\n \\n  Exemplary      in      breadth;      delivers      outstanding      coverage,      thoroughly      detailing      all      crucial      aspects      of      the      topic      without      including      irrelevant  \\n \\n  information.\\nTable      8:      Scoring      rubrics      on   \\n \\na      1-5      scale      for      the      evaluator      LLM.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 34, 'section_title': 'Error      Type       Topic      Unsupported      Sentence      Source'}, page_content='Error      Type       Topic      Unsupported      Sentence      Source\\n\\n |       Throughout      its      history,      religion      has      remained      the       paramount      aspect      of      Hawaiian      life      in      Lahaina      ,       permeating      every      daily      activity      and      significant      event[5]. |       [5]      “Religion,      Beliefs      &      Spirituality”       (The      source      discusses      religion      as      part      of      Hawaiian      life       but      does      not      mention      Lahania      .)\\n |       Lahaina,      Hawaii\\n |       [2]      “Crimean      Bridge      -      Wikipedia”       (The      source      says      “The      first      scheduled      passenger      train       crossed      the      bridge      on      25      December      2019,      while      the       bridge      was      opened      for      freight      trains      on      30      June      2020      ”.)       Completed      in      June      2020      ,      the      bridge      serves      as      a       major      supply      route      for      Russian      forces      in      the      region       and      is      significant      to      Russia’s      claim      over      the      disputed       territory[2][11].\\n | 2022      Crimean       Bridge      explosion\\n |       For      example,      comparisons      have      been      drawn      between       the      performance      of      LK-9      and      the      dynamic      resolution       capabilities      of      video      games      such      as      Battlefield      2042[22]. |       [22]      “Battlefield      2042      PC      performance      guide:      The      best       settings      for      a      high      frame      rate”       (      The      source      is      irrelevant      to      LK-99.      )\\n |       LK-99\\n\\nTable      9:      Examples      of      different      error      types      of      unsupported      sentences.\\nWe      use      Mistral      7B-Instruct!>      (Jiang      et      al.,      2023a)\\n \\n \\n  to      examine      whether      the      cited      passages      entail      the  \\n \\n  generated      sentence.\\nTable   \\n \\n4      reports      the      citation  \\n \\n  quality      of      articles      produced      by      our      approach,      show-  \\n \\n  ing      that      around      15%      sentences      in      generated      articles  \\n \\n  are      unsupported      by      citations.\\nWe      further      investi-  \\n \\n  gate      the      failure      cases      by      randomly      sampling      10  \\n \\n  articles      and      an      author      manually      examines      all      the  \\n \\n  unsupported      sentences      in      these      articles.\\nBesides  \\n \\n  sentences      that      are      incorrectly      split!®,      lack      citations,  \\n \\n  or      are      deemed      supported      by      the      author’s      judgment,  \\n \\n  our      analysis      identifies      three      main      error      categories  \\n \\n  (examples      are      given      in      Table      9):      improper      inferen-  \\n \\n  tial      linking,      inaccurate      paraphrasing,      and      citing  \\n \\n  irrelevant      sources.\\n \\n \\n  We      show      the      error      distribution      in      Figure      6.      No-  \\n \\n  tably,      the      most      common      errors      stem      from      the      ten-  \\n \\n  dency      of      LLMs      to      form      improper      inferential      links  \\n \\n  between      different      pieces      of      information      presented  \\n \\n  in      the      context      window.\\nOur      analysis      of      citation  \\n \\n  quality      suggests      that,      in      addition      to      avoiding      hallu-  \\n \\n  cinations,      future      research      in      grounded      text      gener-  \\n \\n  ation      should      also      focus      on      preventing      LLMs      from  \\n \\n  making      overly      inferential      leaps      based      on      the      pro-  \\n \\n  vided      information.\\nD      Human      Evaluation      Details\\n \\n \\n  We      recruited      10      experienced      Wikipedia      editors  \\n \\n  to      participate      in      our      study      by      creating   \\n \\na      research  \\n \\n  page      on      Meta-Wiki!”\\nand      reaching      out      to      active editors      who      have      recently      approved      articles      for  \\n \\n  Wikipedia.\\\\\\\\\\\\\\'®      Our      participation      group      includes   \\n \\n3       editors      with      1-5      years      of      experience,   \\n \\n4      with      6-10  \\n \\n  years,      and   \\n \\n3      with      over      15      years      of      contribution.\\n \\n \\n  The      study      was      approved      by      the      Institutional      Re-  \\n \\n  view      Board      of      our      institution      and      the      participants  \\n \\n  signed      the      consent      form      through      Qualtrics      ques-  \\n \\n  tionnaires      before      the      study      started.\\n \\n \\n  To      streamline      the      evaluation      of      grounded      articles,  \\n \\n  we      developed   \\n \\na      web      application,      which      features   \\n \\na       side-by-side      display      of      the      article      and      its      citation  \\n \\n  snippets,      to      gather      ratings      and      open-ended      feedback  \\n \\n  Shttps      ://huggingface.co/mistralai/  \\n \\n  Mistral-7B-Instruct-vQ.1  \\n \\n  \\\\\\\\\\\\\\'6Rollowing      Gao      et      al.\\n(2023),      we      check      citation      quality      in  \\n \\n  the      sentence      level      and      split      articles      into      sentences      using      NLTK  \\n \\n  sent_tokenize.\\nsent_tokenize      sometimes      fails      to      split      sen-  \\n \\n  tences      correctly      when      the      article      contains      special      words      like  \\n \\n  “No.12847”,      “Bhatia      et      al.\\n”,      etc.\\n \\n \\n  \"https      ://meta.wikimedia.org  \\n \\n  \\\\\\\\\\\\\\'8Since      evaluating      Wikipedia-like      articles      is      time-  \\n \\n  consuming      and      requires      expertise,      we      paid      each      participant  \\n \\n  50$      for      our      study.\\n \\n \\n  for      each      article.\\nFigure   \\n \\n7      shows      the      screenshot      of  \\n \\n  our      web      application      and      the      full      article      produced  \\n \\n  by      STORM      is      included      in      Table      12.\\nFor      human  \\n \\n  evaluation,      we      use   \\n \\na   \\n \\n|      to   \\n \\n7      scale      for      more      fine-  \\n \\n  grained      evaluation.\\nThe      grading      rubric      is      included  \\n \\n  in      Table      10.\\n \\n \\n  We      collected      the      pairwise      preferences      and      the  \\n \\n  perceived      usefulness      of      STORM      via      an      online      ques-  \\n \\n  tionnaire.\\nSpecifically,      for      the      perceived      usefulness,  \\n \\n  we      request      editors      to      rate      their      agreement      with      state-  \\n \\n  ments      “I      think      it      can      be      specifically      helpful      for      my  \\n \\n  pre-writing      stage      (e.g.,      collecting      relevant      sources,  \\n \\n  outlining,      drafting).\\n”,      “I      think      it      will      help      me      edit  \\n \\n  a      Wikipedia      article      for   \\n \\na      new      topic”,      “I      think      it  \\n \\n  can      be   \\n \\na      potentially      useful      tool      for      the      Wikipedia  \\n \\n  community”      on   \\n \\na      Likert      scale      of      1-5,      correspond-  \\n \\n  ing      to      Strongly      disagree,      Somewhat      disagree,      Nei-  \\n \\n  ther      agree      nor      disagree,      Somewhat      agree,      Strongly agree.\\nE_      Error      Analysis\\n \\n \\n  While      articles      produced      by      STORM      are      preferred  \\n \\n  by      both      automatic      metrics      and      human      evaluation,  \\n \\n  experienced      editors      still      identified      multiple      prob-  \\n \\n  lems      with      the      machine-generated      articles.\\nWe      an-  \\n \\n  alyze      the      free-form      comments      and      summarize      the  \\n \\n  major      issues      in      Table      11.\\n \\n \\n  The      primary      issue      raised      is      that      the      generated  \\n \\n  articles      often      contain      emotional      language      and      lack  \\n \\n  neutrality,      primarily      due      to      the      source      material.\\n \\n \\n  STORM      currently      retrieves      grounding      sources  \\n \\n  from      the      Internet      which      is      not      neutral      and      con-  \\n \\n  tains      considerable      promotional      content      on      its      own.\\n \\n \\n  Addressing      this      bias      in      the      pre-writing      stage      repre-  \\n \\n  sents   \\n \\na      valuable      direction      for      future      research.\\nAn-  \\n \\n  other      major      issue      is      the      red      herring      fallacy      or      the  \\n \\n  over-association      of      unrelated      facts.\\nAddressing      this  \\n \\n  challenge      calls      for      high-level      sensemaking      rather  \\n \\n  than      mere      fact-level      verification.\\n \\n \\n  Interest      Level  \\n \\n  Not      engaging      at      all;      no      attempt      to      capture      the      reader’s      attention.\\n \\n \\n  Slightly      engaging      with      rare      moments      that      capture      attention.\\n \\n \\n  Fairly      engaging      with   \\n \\na      basic      narrative      but      lacking      depth.\\n \\n \\n  Moderately      engaging      with      several      interesting      points.\\n \\n \\n  Quite      engaging      with   \\n \\na      well-structured      narrative      and      noteworthy      points      that      frequently      capture      and      retain      attention.\\n \\n \\n  Very      engaging      with   \\n \\na      compelling      narrative      that      captures      and      mostly      retains      attention.\\n \\n \\n  Exceptionally      engaging      throughout,      with   \\n \\na      compelling      narrative      that      consistently      stimulates      interest.\\nMOawWPYWNr\\n \\n \\n  Coherence      and      Organization  \\n \\n  Disorganized;      lacks      logical      structure      and      coherence.\\n \\n \\n  Poor      organization;      some      structure      is      evident      but      very      weak.\\n \\n \\n  Fairly      organized;   \\n \\na      basic      structure      is      present      but      not      consistently      followed.\\n \\n \\n  Organized;   \\n \\na      clear      structure      is      mostly      followed      with      some      lapses      in      coherence.\\n \\n \\n  Good      organization;   \\n \\na      clear      structure      with      minor      lapses      in      coherence.\\n \\n \\n  Very      well-organized;   \\n \\na      logical      structure      with      transitions      that      effectively      guide      the      reader.\\n \\n \\n  Excellently      organized;      the      article      is      logically      structured      with      seamless      transitions      and   \\n \\na      clear      argument.\\naw:\\n \\n \\n  Relevance      and      Focus  \\n \\n  1:      Off-topic;      the      content      does      not      align      with      the      headline      or      core      subject.\\n \\n \\n  2:      Mostly      off-topic      with      some      relevant      points.\\n \\n \\n  3:      Somewhat      on      topic      but      with      several      digressions;      the      core      subject      is      evident      but      not      consistently      adhered      to.\\n \\n \\n  4:      Generally      on      topic,      despite   \\n \\na      few      unrelated      details.\\n \\n \\n  5:      Mostly      on      topic      and      focused;      the      narrative      has   \\n \\na      consistent      relevance      to      the      core      subject      with      infrequent      digressions.\\n \\n \\n  6:      Highly      relevant      with   \\n \\na      focused      narrative      and      purpose.\\n \\n \\n  7:      Exceptionally      focused      and      entirely      on      topic;      the      article      is      tightly      centered      on      the      subject,      with      every      piece      of      information      contributing      to   \\n \\na       comprehensive      understanding      of      the      topic.\\n \\n \\n  Broad      Coverage  \\n \\n  Severely      lacking;      offers      little      to      no      coverage      of      the      topic’s      primary      aspects,      resulting      in   \\n \\na      very      narrow      perspective.\\n \\n \\n  Minimal      coverage;      addresses      only   \\n \\na      small      selection      of      the      topic’s      main      aspects,      with      significant      omissions.\\n \\n \\n  Partial      coverage;      includes      some      of      the      topic’s      main      aspects      but      misses      others,      resulting      in      an      incomplete      portrayal.\\n \\n \\n  Acceptable      breadth;      covers      most      main      aspects,      though      it      may      stray      into      minor      unnecessary      details      or      overlook      some      relevant      points.\\n \\n \\n  Good      coverage;      achieves      broad      coverage      of      the      topic,      hitting      on      all      major      points      with      minimal      extraneous      information.\\n \\n \\n  Comprehensive;      provides      thorough      coverage      of      all      significant      aspects      of      the      topic,      with   \\n \\na      well-balanced      focus.\\n \\n \\n  Exemplary      in      breadth;      delivers      outstanding      coverage,      thoroughly      detailing      all      crucial      aspects      of      the      topic      without      including      irrelevant      information.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 35, 'section_title': 'We      use      Mistral      7B-Instruct!>      (Jiang      et      al.,      2023a)'}, page_content='We      use      Mistral      7B-Instruct!>      (Jiang      et      al.,      2023a)\\n \\n \\n  to      examine      whether      the      cited      passages      entail      the  \\n \\n  generated      sentence.\\nTable   \\n \\n4      reports      the      citation  \\n \\n  quality      of      articles      produced      by      our      approach,      show-  \\n \\n  ing      that      around      15%      sentences      in      generated      articles  \\n \\n  are      unsupported      by      citations.\\nWe      further      investi-  \\n \\n  gate      the      failure      cases      by      randomly      sampling      10  \\n \\n  articles      and      an      author      manually      examines      all      the  \\n \\n  unsupported      sentences      in      these      articles.\\nBesides  \\n \\n  sentences      that      are      incorrectly      split!®,      lack      citations,  \\n \\n  or      are      deemed      supported      by      the      author’s      judgment,  \\n \\n  our      analysis      identifies      three      main      error      categories  \\n \\n  (examples      are      given      in      Table      9):      improper      inferen-  \\n \\n  tial      linking,      inaccurate      paraphrasing,      and      citing  \\n \\n  irrelevant      sources.\\n \\n \\n  We      show      the      error      distribution      in      Figure      6.      No-  \\n \\n  tably,      the      most      common      errors      stem      from      the      ten-  \\n \\n  dency      of      LLMs      to      form      improper      inferential      links  \\n \\n  between      different      pieces      of      information      presented  \\n \\n  in      the      context      window.\\nOur      analysis      of      citation  \\n \\n  quality      suggests      that,      in      addition      to      avoiding      hallu-  \\n \\n  cinations,      future      research      in      grounded      text      gener-  \\n \\n  ation      should      also      focus      on      preventing      LLMs      from  \\n \\n  making      overly      inferential      leaps      based      on      the      pro-  \\n \\n  vided      information.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 36, 'section_title': 'D      Human      Evaluation      Details'}, page_content='D      Human      Evaluation      Details\\n \\n \\n  We      recruited      10      experienced      Wikipedia      editors  \\n \\n  to      participate      in      our      study      by      creating   \\n \\na      research  \\n \\n  page      on      Meta-Wiki!”\\nand      reaching      out      to      active editors      who      have      recently      approved      articles      for  \\n \\n  Wikipedia.\\\\\\\\\\\\\\'®      Our      participation      group      includes   \\n \\n3       editors      with      1-5      years      of      experience,   \\n \\n4      with      6-10  \\n \\n  years,      and   \\n \\n3      with      over      15      years      of      contribution.\\n \\n \\n  The      study      was      approved      by      the      Institutional      Re-  \\n \\n  view      Board      of      our      institution      and      the      participants  \\n \\n  signed      the      consent      form      through      Qualtrics      ques-  \\n \\n  tionnaires      before      the      study      started.\\n \\n \\n  To      streamline      the      evaluation      of      grounded      articles,  \\n \\n  we      developed   \\n \\na      web      application,      which      features   \\n \\na       side-by-side      display      of      the      article      and      its      citation  \\n \\n  snippets,      to      gather      ratings      and      open-ended      feedback  \\n \\n  Shttps      ://huggingface.co/mistralai/  \\n \\n  Mistral-7B-Instruct-vQ.1  \\n \\n  \\\\\\\\\\\\\\'6Rollowing      Gao      et      al.\\n(2023),      we      check      citation      quality      in  \\n \\n  the      sentence      level      and      split      articles      into      sentences      using      NLTK  \\n \\n  sent_tokenize.\\nsent_tokenize      sometimes      fails      to      split      sen-  \\n \\n  tences      correctly      when      the      article      contains      special      words      like  \\n \\n  “No.12847”,      “Bhatia      et      al.\\n”,      etc.\\n \\n \\n  \"https      ://meta.wikimedia.org  \\n \\n  \\\\\\\\\\\\\\'8Since      evaluating      Wikipedia-like      articles      is      time-  \\n \\n  consuming      and      requires      expertise,      we      paid      each      participant  \\n \\n  50$      for      our      study.\\n \\n \\n  for      each      article.\\nFigure   \\n \\n7      shows      the      screenshot      of  \\n \\n  our      web      application      and      the      full      article      produced  \\n \\n  by      STORM      is      included      in      Table      12.\\nFor      human  \\n \\n  evaluation,      we      use   \\n \\na   \\n \\n|      to   \\n \\n7      scale      for      more      fine-  \\n \\n  grained      evaluation.\\nThe      grading      rubric      is      included  \\n \\n  in      Table      10.\\n \\n \\n  We      collected      the      pairwise      preferences      and      the  \\n \\n  perceived      usefulness      of      STORM      via      an      online      ques-  \\n \\n  tionnaire.\\nSpecifically,      for      the      perceived      usefulness,  \\n \\n  we      request      editors      to      rate      their      agreement      with      state-  \\n \\n  ments      “I      think      it      can      be      specifically      helpful      for      my  \\n \\n  pre-writing      stage      (e.g.,      collecting      relevant      sources,  \\n \\n  outlining,      drafting).\\n”,      “I      think      it      will      help      me      edit  \\n \\n  a      Wikipedia      article      for   \\n \\na      new      topic”,      “I      think      it  \\n \\n  can      be   \\n \\na      potentially      useful      tool      for      the      Wikipedia  \\n \\n  community”      on   \\n \\na      Likert      scale      of      1-5,      correspond-  \\n \\n  ing      to      Strongly      disagree,      Somewhat      disagree,      Nei-  \\n \\n  ther      agree      nor      disagree,      Somewhat      agree,      Strongly agree.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 37, 'section_title': 'E_      Error      Analysis'}, page_content='E_      Error      Analysis\\n \\n \\n  While      articles      produced      by      STORM      are      preferred  \\n \\n  by      both      automatic      metrics      and      human      evaluation,  \\n \\n  experienced      editors      still      identified      multiple      prob-  \\n \\n  lems      with      the      machine-generated      articles.\\nWe      an-  \\n \\n  alyze      the      free-form      comments      and      summarize      the  \\n \\n  major      issues      in      Table      11.\\n \\n \\n  The      primary      issue      raised      is      that      the      generated  \\n \\n  articles      often      contain      emotional      language      and      lack  \\n \\n  neutrality,      primarily      due      to      the      source      material.\\n \\n \\n  STORM      currently      retrieves      grounding      sources  \\n \\n  from      the      Internet      which      is      not      neutral      and      con-  \\n \\n  tains      considerable      promotional      content      on      its      own.\\n \\n \\n  Addressing      this      bias      in      the      pre-writing      stage      repre-  \\n \\n  sents   \\n \\na      valuable      direction      for      future      research.\\nAn-  \\n \\n  other      major      issue      is      the      red      herring      fallacy      or      the  \\n \\n  over-association      of      unrelated      facts.\\nAddressing      this  \\n \\n  challenge      calls      for      high-level      sensemaking      rather  \\n \\n  than      mere      fact-level      verification.\\n \\n \\n  Interest      Level  \\n \\n  Not      engaging      at      all;      no      attempt      to      capture      the      reader’s      attention.\\n \\n \\n  Slightly      engaging      with      rare      moments      that      capture      attention.\\n \\n \\n  Fairly      engaging      with   \\n \\na      basic      narrative      but      lacking      depth.\\n \\n \\n  Moderately      engaging      with      several      interesting      points.\\n \\n \\n  Quite      engaging      with   \\n \\na      well-structured      narrative      and      noteworthy      points      that      frequently      capture      and      retain      attention.\\n \\n \\n  Very      engaging      with   \\n \\na      compelling      narrative      that      captures      and      mostly      retains      attention.\\n \\n \\n  Exceptionally      engaging      throughout,      with   \\n \\na      compelling      narrative      that      consistently      stimulates      interest.\\nMOawWPYWNr\\n \\n \\n  Coherence      and      Organization  \\n \\n  Disorganized;      lacks      logical      structure      and      coherence.\\n \\n \\n  Poor      organization;      some      structure      is      evident      but      very      weak.\\n \\n \\n  Fairly      organized;   \\n \\na      basic      structure      is      present      but      not      consistently      followed.\\n \\n \\n  Organized;   \\n \\na      clear      structure      is      mostly      followed      with      some      lapses      in      coherence.\\n \\n \\n  Good      organization;   \\n \\na      clear      structure      with      minor      lapses      in      coherence.\\n \\n \\n  Very      well-organized;   \\n \\na      logical      structure      with      transitions      that      effectively      guide      the      reader.\\n \\n \\n  Excellently      organized;      the      article      is      logically      structured      with      seamless      transitions      and   \\n \\na      clear      argument.\\naw:\\n \\n \\n  Relevance      and      Focus  \\n \\n  1:      Off-topic;      the      content      does      not      align      with      the      headline      or      core      subject.\\n \\n \\n  2:      Mostly      off-topic      with      some      relevant      points.\\n \\n \\n  3:      Somewhat      on      topic      but      with      several      digressions;      the      core      subject      is      evident      but      not      consistently      adhered      to.\\n \\n \\n  4:      Generally      on      topic,      despite   \\n \\na      few      unrelated      details.\\n \\n \\n  5:      Mostly      on      topic      and      focused;      the      narrative      has   \\n \\na      consistent      relevance      to      the      core      subject      with      infrequent      digressions.\\n \\n \\n  6:      Highly      relevant      with   \\n \\na      focused      narrative      and      purpose.\\n \\n \\n  7:      Exceptionally      focused      and      entirely      on      topic;      the      article      is      tightly      centered      on      the      subject,      with      every      piece      of      information      contributing      to   \\n \\na       comprehensive      understanding      of      the      topic.\\n \\n \\n  Broad      Coverage  \\n \\n  Severely      lacking;      offers      little      to      no      coverage      of      the      topic’s      primary      aspects,      resulting      in   \\n \\na      very      narrow      perspective.\\n \\n \\n  Minimal      coverage;      addresses      only   \\n \\na      small      selection      of      the      topic’s      main      aspects,      with      significant      omissions.\\n \\n \\n  Partial      coverage;      includes      some      of      the      topic’s      main      aspects      but      misses      others,      resulting      in      an      incomplete      portrayal.\\n \\n \\n  Acceptable      breadth;      covers      most      main      aspects,      though      it      may      stray      into      minor      unnecessary      details      or      overlook      some      relevant      points.\\n \\n \\n  Good      coverage;      achieves      broad      coverage      of      the      topic,      hitting      on      all      major      points      with      minimal      extraneous      information.\\n \\n \\n  Comprehensive;      provides      thorough      coverage      of      all      significant      aspects      of      the      topic,      with   \\n \\na      well-balanced      focus.\\n \\n \\n  Exemplary      in      breadth;      delivers      outstanding      coverage,      thoroughly      detailing      all      crucial      aspects      of      the      topic      without      including      irrelevant      information.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 38, 'section_title': 'TMAWP      YN'}, page_content='TMAWP      YN\\nVerifiability\\n \\n \\n  1:      No      supporting      evidence;      claims      are      unsubstantiated.\\n \\n \\n  2:      Rarely      supported      with      evidence;      many      claims      are      unsubstantiated.\\n \\n \\n  3:      Inconsistently      verified;      some      claims      are      supported;      evidence      is      occasionally      provided.\\n \\n \\n  4:      Generally      verified;      claims      are      usually      supported      with      evidence;      however,      there      might      be   \\n \\na      few      instances      where      verification      is      lacking  \\n \\n  5:      Well-supported;      claims      are      very      well      supported      with      credible      evidence,      and      instances      of      unsupported      claims      are      rare.\\n \\n \\n  6:      Very      well-supported;      almost      every      claim      is      substantiated      with      credible      evidence,      showing   \\n \\na      high      level      of      thorough      verification.\\n \\n \\n  7:      Exemplary      verification;      each      claim      is      supported      by      robust,      credible      evidence      from      authoritative      sources,      reflecting      strict      adherence      to      the      no  \\n \\n  original      research      policy.\\nTable      10:      Scoring      rubrics      on   \\n \\na      1-7      scale      for      human      evaluation.\\nIssue       Mentioned      Time       Example      Comments\\n \\n \\n  The      word      “significant”      is      used      17      times      in      this      article.\\nVague      and      unsupported      claims      are  \\n \\n  made      about      broader      political      importance      and      “pivotal      role[s]”,      and      is      unencyclopedic.\\nUse      of      emotional      words,  \\n \\n  (comment      on      article      Lahaina,      Hawaii) [] but they still have not fixed the issue of neutral point of view.\\nIt is also evident in this  \\n \\n  article      that      the      writer’s      standpoint      is      biased      towards      Taylor      Swift.\\nOther      than      that,      it      did  \\n \\n  a      good      job      at      summarizing      key      points      and      putting      depth      into      this.\\n \\n \\n  unneutral      12      (comment      on      article      Speak      Now      (Taylor’s      Version))  \\n \\n  “The      film      was      also      featured      in      an      art      and      film      festival      hosted      by      The      California      Endowment,  \\n \\n  highlighting      the      power      of      stories      in      reshaping      narratives      about      communities.”\\nYes,      technically  \\n \\n  the      source      says      that,      but      it’s   \\n \\na      stretch      to      say      in      Wikipedia      voice      and      just      sounds      like  \\n \\n  non-neutral,      promotional      prose.\\n(comment      on      article      Gehraiyaan)  \\n \\n  Polling      from      America      shouldn’t      be      included      and      links      to      climate      change      shouldn’t      be  \\n \\n  made      unless      explicitly      connected      by      the      source.\\n(comment      on      article      Typhoon      Hinnamnor)  \\n \\n  Red      herring      fallacy,   \\n \\nu      Sourcing      seems      mostly      fine,      though      some      aren’t      directly      related      (Ex.      39,40).\\n \\n \\n  associating      unrelated      sources      (comment      on      article      Gehraiyaan)  \\n \\n  Here      is   \\n \\na      lengthy      digression      about      KISS,      not      necessary      because      the      article      on      the      band  \\n \\n  should      be      linked      to.\\n(comment      on      article      2022      AFL      Grand      Final)  \\n \\n  “One      study,      conducted      by      Sinéad      Griffin,   \\n \\na      physicist      at      the      Lawrence      Berkeley      National  \\n \\n  Laboratory,      provided      some      analysis      of      LK-99’s      abilities      using      supercomputer      simulations[20].”\\n \\n \\n  This      is      not      enough      information      about      the      analysis,      which      would      have      been      very      useful      in      the  \\n \\n  rr  \\n \\n.    \\n.\\n     article.\\n(comment      on      article      LK-99)  \\n \\n  Missing      important      information   \\n \\n6       Although      the      earthquake’s      immediate      aftermath      and      response      are      adequately      covered,      there  \\n \\n  could      be      more      about      the      long-term      socioeconomic      impact      and      recovery      processes.\\n \\n \\n  (comment      on      article      2022      West      Java      earthquake)  \\n \\n  Words      like      “now”      should      be      avoided      in      Wikipedia      articles      to      prevent      them      from      becoming  \\n \\n  dated      and      phrases      such      as,      “as      of      December      2023”      should      be      used      instead.\\n \\n \\n  Improper      handling      of   \\n \\n5      (comment      on      article      Cyclone      Batsirai)  \\n \\n  time-sensitive      information      “as      of      December      13”      doesn’t      specify   \\n \\na      year,      and      is      old      information  \\n \\n  (comment      on      article      2022      West      Java      earthquake) too      many      subsections      in      the      “Recovery      and      Rehabilitation”      section  \\n \\n  (comment      on      article      2022      West      Java      earthquake)\\n      (comment      on      article      2022      Crimean      Bridge      explosion)\\n \\n \\n  Section      organization      problem   \\n \\n5   \\n \\nI      do      not      like      how      the      article      is      organized,      with      too      many      headers      cluttering      the      article,  \\n \\n  making      it      not      as      readable.\\nOther      than      that,      the      AI      did      great      work      on      the      piece.\\n |       Select      a      key       8      v       Selected      Key:      8       Title:      Taylor      Hawkins      -      marshall.com       Snippets:       In      1995,      off      the      back      of      her      hit      album       ‘Jagged      Little      Pill’,      Canadian      American       superstar      Alanis      Morissette      recruited      him       to      be      her      touring      drummer      for      her      18       month      album      tour,      along      with      him       featuring      in      the      music      videos      for      “You       Oughta      Know’,      “All|      Really      Want”      and       “You      Learn’.      It      was      during      this      tour      that      he       met      his      musical      soul      mate,      Dave      Grohl.       The      tour      with      Alanis      ended      and      he\\\\\\\\\\\\\\'d      heard       that      Dave      and      Foo      Fighters      were      looking       for      anew      drummer,      so      Taylor      enquired.       Dave      initially      thought      that      Taylor      wouldn’t       be      interested      in      joining      as      Alanis       Morissette      was      much      bigger      than      the      Foo       Fighters      at      that      time,      but      Taylor      jumped      at       the      chance      due      to      his      overwhelming       desire      to      be      in      a      rock      band.      It      was      from       that      point      that      the      bromance      between       them      began      and      on      March      18th      1997       Taylor      was      announced      as      their      new       drummer.       Url:      https://marshall.com/live-for-       music/drum-icons/taylor-hawkins | Logout  \\n \\n  Select      an      option:  \\n \\n  (11,      ‘Taylor      Hawkins      [Quite      Richard}’)   \\n \\n¥       You      selected:      Taylor      Hawkins      [Quite      Richard]  \\n \\n  Taylor      Hawkins  \\n \\n  Oliver      Taylor      Hawkins      (1972-2022)      was      an      American      musician,      best      known      as      the      drummer      for      the      rock      band      Foo      Fighters[1].\\nBorn      in      Fort      Walton,      Texas,      Hawkins’      love      for      music      was      ignited      at   \\n \\na      young  \\n \\n  age,      particularly      after      watching   \\n \\na      Queen      concert      in      1982[2][3][5].\\nHe      kick-started      his      professional      career      as      the      drummer      for      Alanis      Morissette’s      band      during      her      world      tour      for      the      hit      album      ‘Jagged      Little  \\n \\n  Pill\\\\\\\\\\\\\\'[8][9].\\nHis      talents      were      recognized      by      Dave      Grohl,      who      invited      him      to      join      the      Foo      Fighters      in      1997,      marking      the      beginning      of      his      impactful      tenure      with      the      band{7][8].\\n \\n \\n  Hawkins      was      celebrated      for      his      versatile      drumming      style,      drawing      influence      from      renowned      drummers      like      Roger      Taylor,      Neil      Peart,      Phil      Collins,      Alex      Van      Halen,      and      Stewart      Copeland[14).\\nHis  \\n \\n  performances,      marked      by   \\n \\na      unique      energy      and      aggressive      style      of      play,      earned      him      recognition      as      one      of      the      top      rock      drummers      of      his      era[15].\\nApart      from      his      role      in      the      Foo      Fighters,      Hawkins\\\\\\\\\\\\\\'      passion  \\n \\n  for      music      saw      him      involved      in      numerous      side      projects      and      collaborations,      cementing      his      place      in      the      world      of      rock      music(10].\\n \\n \\n  Outside      of      his      professional      life,      Hawkins      was      known      for      his      resilience      and      dedication      to      his      family.\\nDespite      personal      struggles,      including   \\n \\na      near-fatal      drug      overdose      in      2001,      Hawkins      remained      committed  \\n \\n  to      his      musical      career{4][9].\\nHis      legacy      continues      to      inspire      musicians      and      fans      alike,      as      his      contributions      to      rock      music,      coupled      with      his   \\n \\ni       industry(13}.\\n \\n \\n  spirit,      made      him      an      icon      in      the      music  \\n \\n  Hawkins’      sudden      death      in      2022      while      on      tour      in      Bogotd,      Colombia,      sent      shockwaves      through      the      music      world[34].\\nTributes      poured      in      from      around      the      globe,      reflecting      the      respect      and      admiration  \\n \\n  Hawkins      had      garnered      during      his      lifetime[21][31].\\nHis      life      and      career      were      honored      at   \\n \\na      star-studded      tribute      concert      in      Los      Angeles,      attesting      to      the      enduring      impact      of      his      music[22].\\n | Early      Life      and      Background\\n |       Oliver      Taylor      Hawkins,      known      as      Taylor      Hawkins,      was      born      and      raised      in      Fort      Walton,      Texas[3].      His      family      moved      to      Laguna      Beach,      California      when      he      was      four      years      old[3].      He      has      two      younger      siblings,       a      brother      named      Jason,      and      a      sister      named      Heather[3].      Asa      child,      Hawkins      wa:      influenced      by      his      paternal      Josie      Hawkins,      who      had      grown      up      during      the      Great      Depression      and       lived      in      Jackson,      Mississippi(2].\\n |       During      his      high      school      days      at      Laguna      Beach      High      School,      from      where      he      graduated      in      1990,      he      became      friends      with      Jon      Davison,      who      later      became      the      lead      vocalist      of      the      band      Yes(2][3].      His      interest      in       music      was      nurtured      from      an      early      age,      particularly      after      watching      a      Queen      concert      in      1982      which      inspired      him      to      learn      to      play      the      drums(2][5].      He      noted      that      music      was      a      constant      presence      in      his      family       Taylor      Hawkins\\n\\n \\n \\n  Oliver      Taylor      Hawkins      (1972-2022)      was      an      American      musician,      best      known      as      the      drummer      for      the      rock  \\n \\n  band      Foo      Fighters[1].\\nBorn      in      Fort      Walton,      Texas,      Hawkins’      love      for      music      was      ignited      at   \\n \\na      young      age,  \\n \\n  particularly      after      watching   \\n \\na      Queen      concert      in      1982[2][3][5].\\nHe      kick-started      his      professional      career  \\n \\n  as      the      drummer      for      Alanis      Morissette’s      band      during      her      world      tour      for      the      hit      album      ’Jagged      Little  \\n \\n  Pill’[8]L[9].\\nHis      talents      were      recognized      by      Dave      Grohl,      who      invited      him      to      join      the      Foo      Fighters      in  \\n \\n  1997,      marking      the      beginning      of      his      impactful      tenure      with      the      band[7][8].\\n \\n \\n  Hawkins      was      celebrated      for      his      versatile      drumming      style,      drawing      influence      from      renowned      drummers      like  \\n \\n  Roger      Taylor,      Neil      Peart,      Phil      Collins,      Alex      Van      Halen,      and      Stewart      Copeland[14].\\nHis      performances,  \\n \\n  marked      by   \\n \\na      unique      energy      and      aggressive      style      of      play,      earned      him      recognition      as      one      of      the      top      rock  \\n \\n  drummers      of      his      era[15].\\nApart      from      his      role      in      the      Foo      Fighters,      Hawkins’      passion      for      music      saw  \\n \\n  him      involved      in      numerous      side      projects      and      collaborations,      cementing      his      place      in      the      world      of      rock  \\n \\n  music[10].\\n \\n \\n  Outside      of      his      professional      life,      Hawkins      was      known      for      his      resilience      and      dedication      to      his      family.\\n \\n \\n  Despite      personal      struggles,      including   \\n \\na      near-fatal      drug      overdose      in      2001,      Hawkins      remained      committed      to  \\n \\n  his      musical      career[4][9].\\nHis      legacy      continues      to      inspire      musicians      and      fans      alike,      as      his      contributions  \\n \\n  to      rock      music,      coupled      with      his      indomitable      spirit,      made      him      an      unforgettable      icon      in      the      music  \\n \\n  industry[13].\\n \\n \\n  Hawkins’      sudden      death      in      2022      while      on      tour      in      Bogota,      Colombia,      sent      shockwaves      through      the      music  \\n \\n  world[34].\\nTributes      poured      in      from      around      the      globe,      reflecting      the      respect      and      admiration      Hawkins  \\n \\n  had      garnered      during      his      lifetime[21][31].\\nHis      life      and      career      were      honored      at   \\n \\na      star-studded      tribute  \\n \\n  concert      in      Los      Angeles,      attesting      to      the      enduring      impact      of      his      music[22].\\n#\\n \\n   Early      Life      and      Background  \\n \\n  Oliver      Taylor      Hawkins,      known      as      Taylor      Hawkins,      was      born      and      raised      in      Fort      Walton,      Texas[3].\\nHis  \\n \\n  family      moved      to      Laguna      Beach,      California      when      he      was      four      years      old[3].\\nHe      has      two      younger      siblings,  \\n \\n  a      brother      named      Jason,      and   \\n \\na      sister      named      Heather[3].\\nAs   \\n \\na      child,      Hawkins      was      particularly      influenced  \\n \\n  by      his      paternal      grandmother,      Josie      Hawkins,      who      had      grown      up      during      the      Great      Depression      and      lived      in  \\n \\n  Jackson,      Mississippi[1].\\n \\n \\n  During      his      high      school      days      at      Laguna      Beach      High      School,      from      where      he      graduated      in      1990,      he      became  \\n \\n  friends      with      Jon      Davison,      who      later      became      the      lead      vocalist      of      the      band      Yes[2][3].\\nHis      interest      in  \\n \\n  music      was      nurtured      from      an      early      age,      particularly      after      watching   \\n \\na      Queen      concert      in      1982      which      inspired  \\n \\n  him      to      learn      to      play      the      drums[2][5].\\nHe      noted      that      music      was   \\n \\na      constant      presence      in      his      family      home[5].\\n \\n \\n  Despite      facing      certain      hardships      during      his      upbringing,      including      his      mother’s      struggles      with      \"demons\",  \\n \\n  Hawkins      pursued      his      musical      ambitions[4].\\nHe      credits      his      older      sister      Heather      for      taking      care      of      the  \\n \\n  family      during      difficult      times[4].\\n \\n \\n  His      first      major      musical      experience      came      from      playing      drums      for      Alanis      Morissette’s      album,      Jagged      Little  \\n \\n  Pill,      and      accompanying      her      on      the      subsequent      tour[3].\\nThis      marked      the      beginning      of      his      professional  \\n \\n  career      in      the      music      industry.\\n#\\n \\n   Career  \\n \\n  Taylor      Hawkins      began      his      professional      music      career      playing      in      Alanis      Morissette’s      band      during      her  \\n \\n  18-month      world      tour      in      support      of      the      hit      album      ’Jagged      Little      Pill’      from      1995      to      1997[8][9].\\nHis  \\n \\n  performances      not      only      in      the      tour      but      also      in      the      music      videos      for      “You      Oughta      Know”,      “All   \\n \\nI      Really      Want”  \\n \\n  and      “You      Learn”      introduced      him      to      the      world      of      rock      music      and      ultimately      led      to      his      meeting      with      Dave  \\n \\n  Grohl[8].\\nThroughout      this      time,      Hawkins      contributed      significantly      to      the      band’s      sound      and      performance,  \\n \\n  transforming      the      songs      from      their      original      drum      loop      format      to   \\n \\na      rock-band      vibe      that      resonated      with  \\n \\n  audiences[1][7].\\n \\n \\n  In      1997,      Hawkins      was      asked      by      Grohl      to      join      the      Foo      Fighters,      an      invitation      that      he      readily      accepted[7][8].\\n \\n \\n  At      the      time,      Grohl      thought      it      was   \\n \\na      long      shot      to      recruit      Hawkins      given      that      Morissette      was      at      the      height  \\n \\n  of      her      career,      but      Hawkins’      desire      to      be   \\n \\na      part      of   \\n \\na      rock      band      compelled      him      to      make      the      move[7].\\nThis  \\n \\n  marked      the      beginning      of      Hawkins’      tenure      as      the      drummer      of      the      Foo      Fighters,   \\n \\na      role      that      he      would      play  \\n \\n  until      his      passing[6][9].\\n \\n \\n  Apart      from      his      work      with      Morissette      and      the      Foo      Fighters,      Hawkins      had      an      array      of      other      musical  \\n \\n  experiences[10].\\nHe      drummed      for      Sass      Jordan      before      joining      Morissette’s      touring      band[10].\\nHe      was      part  \\n \\n  of      an      ad      hoc      drum      supergroup      called      SOS      Allstars      and      filled      the      void      for      Coheed      and      Cambria’s      2007  \\n \\n  album      after      their      drummer      Josh      Eppard      left      the      group[10].\\nIn      addition,      Hawkins      formed      his      own      side  \\n \\n  project,      the      Coattail      Riders,      in      2005,      through      which      he      recorded      his      own      music      and      took      the      project      on  \\n \\n  the      road,      performing      in      small      clubs      despite      the      Foo      Fighters’      arena-status[7].\\nHis      son,      Shane      Hawkins,  \\n \\n  has      since      taken      on      his      father’s      legacy,      joining      the      Foo      Fighters      for   \\n \\na      performance      during      the      Boston  \\n \\n  Calling      Music      Festival      in      2023[6].\\n#\\n \\n   Musical      Style      and      Influences  \\n \\n  Taylor      Hawkins      was   \\n \\na      profound      drummer,      with      his      musical      style      and      influences      spreading      across   \\n \\na      wide  \\n \\n  array      of      rock      genres[11].\\nKnown      for      his      passionate      fandom      of      groups      that      came      before      him,      Hawkins  \\n \\n  regularly      expressed      his      admiration      for      bands      like      Rush,      Genesis,      and      the      Police,      all      of      which      featured  \\n \\n  some      of      the      greatest      drummers      in      rock      history      like      Neil      Peart,      Phil      Collins,      and      Stewart      Copeland[11].\\nHe      was      heavily      influenced      by      his      love      for      classic      rock,      as      evidenced      by      his      performances,      where      he  \\n \\n  covered      songs      from      bands      like      Van      Halen[11].\\n \\n \\n  Hawkins      drew      influences      from   \\n \\na      variety      of      drumming      styles,      developing   \\n \\na      signature      style      inspired      by  \\n \\n  greats      like      Roger      Taylor,      Neil      Peart,      Phil      Collins,      Alex      Van      Halen,      and      Stewart      Copeland[14].\\nThis  \\n \\n  distinctive      style      and      influence      extended      to      his      drum      kit,      which      incorporated      elements      like      rototoms  \\n \\n  and      concert      toms[14].\\n \\n \\n  Beyond      his      influences,      Hawkins      had   \\n \\na      unique      energy      that      made      him      stand      out      as   \\n \\na      drummer.\\nHis      performances  \\n \\n  were      recognized      for      their      power,      and      he      was      known      for      his      enthusiastic      and      aggressive      style      of      play[15].\\n \\n \\n  This      earned      him      recognition      as      one      of      the      top      rock      drummers      of      his      time,      with      his      passion      for      music  \\n \\n  living      on      through      his      performances[14].\\n \\n \\n  Through      his      career,      Hawkins      left      an      indelible      mark      on      rock      music,      through      his      distinct      style,      passion,  \\n \\n  and      contributions      to      the      music      industry[13].\\nHis      love      for      music      and      dedication      to      his      craft      made      him  \\n \\n  an      unforgettable      icon      in      the      world      of      rock      music[13].\\n#\\n \\n   Personal      Life  \\n \\n  Taylor      Hawkins      married      Alison      Hawkins,      an      American      celebrity      and      entrepreneur,      in      2005[18].\\nThe      couple  \\n \\n  had      three      children,      Oliver,      Annabelle,      and      Everleigh[19].\\nHawkins’      commitment      to      his      family      was      evident;\\n \\n \\n  in      fact,      he      even      wrote   \\n \\na      song      for      his      middle      child,      Annabelle[9].\\n \\n \\n  In      his      personal      life,      Hawkins      had      also      struggled      with      drug      use,      which      nearly      claimed      his      life      in   \\n \\na      2001  \\n \\n  overdose[9][7][4].\\nHowever,      he      managed      to      overcome      this      challenge,      and      later      expressed      gratitude      for  \\n \\n  the      experience      as   \\n \\na      lesson      that      allowed      him      to      realize      the      destructive      path      he      was      on[7].\\n \\n \\n  Outside      of      his      main      role      in      the      Foo      Fighters,      Hawkins      also      pursued      various      side      projects      including      the  \\n \\n  Birds      of      Satan,      NHC,      and      Chevy      Metal.\\nHis      motivation      for      such      ventures      was   \\n \\na      constant      drive      to      create  \\n \\n  and      his      love      for      music[7].\\nHawkins      was      also      known      for      his      unabashed      fanboy      nature,      often      vocalizing  \\n \\n  his      admiration      for      fellow      musicians      and      his      heroes[7].\\n#\\n \\n   Legacy      and      Impact Taylor      Hawkins      was      known      for      his      raw      and      authentic      drumming      style,      described      as      \"courageous,      damaged  \\n \\n  and      unflinchingly      authentic”[20].\\nHis      work      with      the      Foo      Fighters,      as      well      as      his      various      collaborations n’      roll[10].\\n \\n \\n  ‘ and      side      projects,      made      him   \\n \\na      celebrated      figure      in      rock  \\n \\n  Hawkins’      death      in      2022      was      met      with      heartfelt      tributes      from      colleagues      and      fans      around      the      world.\\n \\n \\n  Notable      tributes      came      from      rock      legends      like      Roger      Taylor      of      Queen,      who      considered      Hawkins      as   \\n \\na      kind,  \\n \\n  brilliant      man      and      an      inspirational      mentor,      likening      his      death      to      \"losing   \\n \\na      younger      favourite      brother”[21].\\n \\n \\n  Similarly,      Led      Zeppelin’s      Jimmy      Page      admired      his      technique,      energy      and      spirited      enthusiasm[21].\\n \\n \\n  An      LA      tribute      concert      held      in      his      honor      included      guest      drummers      like      Lars      Ulrich      of      Metallica,      Travis  \\n \\n  Barker      of      blink-182,      and      Brad      Wilk      of      Rage      Against      the      Machine.\\nSingers      like      Miley      Cyrus      and      Alanis  \\n \\n  Morissette      also      performed      at      the      concert[22].\\n \\n \\n  Apart      from      his      music,      Taylor      Hawkins      also      contributed      to      charities      Music      Support      and      MusiCares,      both      of  \\n \\n  which      were      chosen      by      the      Hawkins      family[23].\\nHe      had      received      numerous      accolades      throughout      his      career,  \\n \\n  including      27      Grammy      nominations,      of      which      he      won      14[2].\\nIn      2021,      the      Foo      Fighters      were      inducted      into  \\n \\n  the      Rock      and      Roll      Hall      of      Fame[9].\\n#\\n \\n   Discography  \\n \\n  Taylor      Hawkins      also      led   \\n \\na      notable      music      career      through      his      own      side      projects      and      collaborations[10].\\n \\n \\n  Aside      from      his      work      with      the      Foo      Fighters,      Hawkins      formed      and      fronted      the      band      Taylor      Hawkins   \\n \\n&      The  \\n \\n  Coattail      Riders,   \\n \\na      project      which      originated      from      jamming      sessions      with      his      friend      Drew      Hester[10].\\n###      Taylor      Hawkins   \\n \\n&      The      Coattail      Riders  \\n \\n  Taylor      Hawkins   \\n \\n&      The      Coattail      Riders,   \\n \\na      band      formed      in      2004,      have      released      three      albums      and      their  \\n \\n  music      spans      genres      including      Hard      Rock,      Art      Rock,      and      Alternative      Rock[24][25][26].\\nThe      band      grew      from  \\n \\n  an      initial      casual      jamming      session,      gradually      evolving      into   \\n \\na      more      formal      arrangement      that      led      to      the  \\n \\n  production      of      record      albums.\\nNotably,      these      albums      featured      guest      appearances      by      renowned      musicians  \\n \\n  such      as      Dave      Grohl,      Queen’s      Brian      May      and      Roger      Taylor,      The      Cars’      Elliot      Easton,      Perry      Farrell,      and       Jon      Davison,      who      is   \\n \\na      school      friend      of      Hawkins’[10].\\n###      Red      Light      Fever  \\n \\n  Red      Light      Fever,      released      on      April      19,      2010,      was      the      band’s      first      album[29][30].\\nPrior      to      its      release,  \\n \\n  Hawkins      revealed      in      an      interview      that      the      album      had      completed      the      recording      and      production      stages,      but  \\n \\n  its      title      and      release      date      were      yet      to      be      determined[29].\\nRed      Light      Fever      was      recorded      at      the      Foo  \\n \\n  Fighters’      Studio      606      in      California      and      featured      guest      musicians      such      as      Brian      May      and      Roger      Taylor      of  \\n \\n  Queen,      Dave      Grohl      of      Foo      Fighters,      and      Elliot      Easton      of      The      Cars[29][30].\\n##      Get      the      Money  \\n \\n  Get      the      Money,      the      third      album      from      Taylor      Hawkins   \\n \\n&      The      Coattail      Riders,      was      released      on      November      8,  \\n \\n  2019[29].\\nThe      album’s      first      single,      \"Crossed      the      Line”,      released      on      October      15,      2019,      featured      Dave  \\n \\n  Grohl      and      Jon      Davison,      the      frontman      of      Yes[29].\\nThe      music      video      for      the      single      \"I      Really      Blew      It”      also  \\n \\n  featured      appearances      from      Grohl      and      Perry      Farrel1[29].\\n#\\n \\n   Collaborations      and      Guest      Appearances  \\n \\n  Throughout      his      career,      Taylor      Hawkins      collaborated      with      various      prominent      artists      and      bands.\\nThe  \\n \\n  Coattail      Riders’      albums      notably      featured      appearances      from      luminaries      such      as      Brian      May      and      Roger      Taylor  \\n \\n  of      Queen,      Chrissie      Hynde,      Nancy      Wilson      of      Heart,      Sex      Pistol      Steve      Jones      and      James      Gang’s      Joe      Walsh[28].\\n \\n \\n  Hawkins      also      fronted      another      group,      The      Birds      of      Satan,      which      evolved      from      his      heavy      rock      covers      band,  \\n \\n  Chevy      Metal[28].\\nDespite      his      diverse      musical      engagements,      Hawkins      always      maintained   \\n \\na      close      allegiance      with      the      Foo  \\n \\n  Fighters,      which      remained      the      center      of      his      music      life[7][28].\\n#\\n \\n   Tragic      Passing  \\n \\n  Taylor      Hawkins,      the      esteemed      drummer      of      the      alt-rock      band      Foo      Fighters,      passed      away      suddenly      on      March  \\n \\n  25,      2022,      while      on      tour      with      his      band      in      Bogota,      Colombia[34].\\nThe      official      cause      of      death      was      cardiac  \\n \\n  arrest,      though      inquiries      were      raised      concerning      the      presence      of      drugs      in      his      system      and      their      potential  \\n \\n  contribution      to      his      death[33][34].\\nOn      the      night      of      his      passing,      paramedics      were      called      to      the      Four  \\n \\n  Seasons      hotel      in      Bogota      due      to      reports      of      chest      pain      from      an      unnamed      guest,      later      revealed      to      be  \\n \\n  Hawkins[34].\\nUnfortunately,      resuscitation      efforts      were      unsuccessful,      and      Hawkins      was      declared      dead      at  \\n \\n  the      scene[34].\\n \\n \\n  The      news      of      Hawkins’      sudden      demise      was      announced      on      the      morning      of      March      25th,      2022,      which      left      the      music  \\n \\n  world      in      shock[32].\\nThe      band      confirmed      the      news      with   \\n \\na      short      statement,      expressing      their      devastation  \\n \\n  at      the      loss      of      Hawkins,      whose      \"musical      spirit      and      infectious      laughter”      would      live      on      forever[32].\\n \\n \\n  As   \\n \\na      result      of      Hawkins’      untimely      passing,      the      band      canceled      their      ongoing      South      American      tour[33].\\nThe  \\n \\n  festival      stage      at      the      Estéreo      Picnic      Festival,      where      the      Foo      Fighters      were      scheduled      to      perform      that  \\n \\n  night,      was      transformed      into   \\n \\na      candlelight      vigil      in      memory      of      Hawkins[33].\\n##      Tributes      and      Remembrances  \\n \\n  In      the      wake      of      Hawkins’      death,      tributes      from      fans      and      colleagues      alike      poured      in      from      around      the  \\n \\n  world[21][31].\\nAmong      the      many      paying      their      respects      were      legendary      rock      and      roll      musicians      like      Roger  \\n \\n  Taylor,      the      drummer      of      Queen,      who      Hawkins      credited      with      inspiring      his      own      career      behind      the      drum      set[21].\\n \\n \\n  In      heartfelt      social      media      posts,      Taylor      described      Hawkins      as      an      \"inspirational      mentor”      and   \\n \\na      \"kind  \\n \\n  brilliant      man\"[21],      while      Led      Zeppelin’s      Jimmy      Page      reminisced      about      sharing      the      stage      with      Hawkins  \\n \\n  and      praised      his      \"technique,      energy      and      spirited      enthusiasm\"[21].\\n \\n \\n  There      were      also      numerous      onstage      tributes      to      Hawkins.\\nNotably,      Miley      Cyrus      expressed      her      grief      and      sent  \\n \\n  peaceful      wishes      to      the      Foo      Fighters      and      the      Hawkins      family      during   \\n \\na      performance      at      Lollapalooza[31].\\n \\n \\n  Similarly,      Liam      Gallagher      of      Oasis      dedicated      one      of      the      band’s      biggest      hits      to      Hawkins      during   \\n \\na      concert  \\n \\n  at      the      Royal      Albert      Hall      in      London[31].\\nFans      gathered      outside      the      hotel      where      Hawkins      died,      lighting      candles,      leaving      flowers,      and      singing      the  \\n \\n  band’s      songs      in      his      honor[31].\\n \\n \\n  Hawkins’      life      and      career      were      celebrated      in   \\n \\na      star-studded      tribute      concert      in      Los      Angeles,      which      saw  \\n \\n  performances      from      over      5@      musicians,      including      his      former      bands      and      colleagues      from      Def      Leppard,      Queen,  \\n \\n  and      Foo      Fighters[22].\\nTable      12:      STORM’s      generated      article      for      “Taylor      Hawkins”.\\n“#’,      “##”      indicate      the      section      title      and      subsection      title  \\n \\n  respectively.\\nNumbers      in      brackets      indicate      the      cited      references.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 39, 'section_title': 'Verifiability'}, page_content='Verifiability\\n \\n \\n  1:      No      supporting      evidence;      claims      are      unsubstantiated.\\n \\n \\n  2:      Rarely      supported      with      evidence;      many      claims      are      unsubstantiated.\\n \\n \\n  3:      Inconsistently      verified;      some      claims      are      supported;      evidence      is      occasionally      provided.\\n \\n \\n  4:      Generally      verified;      claims      are      usually      supported      with      evidence;      however,      there      might      be   \\n \\na      few      instances      where      verification      is      lacking  \\n \\n  5:      Well-supported;      claims      are      very      well      supported      with      credible      evidence,      and      instances      of      unsupported      claims      are      rare.\\n \\n \\n  6:      Very      well-supported;      almost      every      claim      is      substantiated      with      credible      evidence,      showing   \\n \\na      high      level      of      thorough      verification.\\n \\n \\n  7:      Exemplary      verification;      each      claim      is      supported      by      robust,      credible      evidence      from      authoritative      sources,      reflecting      strict      adherence      to      the      no  \\n \\n  original      research      policy.\\nTable      10:      Scoring      rubrics      on   \\n \\na      1-7      scale      for      human      evaluation.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 40, 'section_title': 'Issue       Mentioned      Time       Example      Comments'}, page_content='Issue       Mentioned      Time       Example      Comments\\n \\n \\n  The      word      “significant”      is      used      17      times      in      this      article.\\nVague      and      unsupported      claims      are  \\n \\n  made      about      broader      political      importance      and      “pivotal      role[s]”,      and      is      unencyclopedic.\\nUse      of      emotional      words,  \\n \\n  (comment      on      article      Lahaina,      Hawaii) [] but they still have not fixed the issue of neutral point of view.\\nIt is also evident in this  \\n \\n  article      that      the      writer’s      standpoint      is      biased      towards      Taylor      Swift.\\nOther      than      that,      it      did  \\n \\n  a      good      job      at      summarizing      key      points      and      putting      depth      into      this.\\n \\n \\n  unneutral      12      (comment      on      article      Speak      Now      (Taylor’s      Version))  \\n \\n  “The      film      was      also      featured      in      an      art      and      film      festival      hosted      by      The      California      Endowment,  \\n \\n  highlighting      the      power      of      stories      in      reshaping      narratives      about      communities.”\\nYes,      technically  \\n \\n  the      source      says      that,      but      it’s   \\n \\na      stretch      to      say      in      Wikipedia      voice      and      just      sounds      like  \\n \\n  non-neutral,      promotional      prose.\\n(comment      on      article      Gehraiyaan)  \\n \\n  Polling      from      America      shouldn’t      be      included      and      links      to      climate      change      shouldn’t      be  \\n \\n  made      unless      explicitly      connected      by      the      source.\\n(comment      on      article      Typhoon      Hinnamnor)  \\n \\n  Red      herring      fallacy,   \\n \\nu      Sourcing      seems      mostly      fine,      though      some      aren’t      directly      related      (Ex.      39,40).\\n \\n \\n  associating      unrelated      sources      (comment      on      article      Gehraiyaan)  \\n \\n  Here      is   \\n \\na      lengthy      digression      about      KISS,      not      necessary      because      the      article      on      the      band  \\n \\n  should      be      linked      to.\\n(comment      on      article      2022      AFL      Grand      Final)  \\n \\n  “One      study,      conducted      by      Sinéad      Griffin,   \\n \\na      physicist      at      the      Lawrence      Berkeley      National  \\n \\n  Laboratory,      provided      some      analysis      of      LK-99’s      abilities      using      supercomputer      simulations[20].”\\n \\n \\n  This      is      not      enough      information      about      the      analysis,      which      would      have      been      very      useful      in      the  \\n \\n  rr  \\n \\n.    \\n.\\n     article.\\n(comment      on      article      LK-99)  \\n \\n  Missing      important      information   \\n \\n6       Although      the      earthquake’s      immediate      aftermath      and      response      are      adequately      covered,      there  \\n \\n  could      be      more      about      the      long-term      socioeconomic      impact      and      recovery      processes.\\n \\n \\n  (comment      on      article      2022      West      Java      earthquake)  \\n \\n  Words      like      “now”      should      be      avoided      in      Wikipedia      articles      to      prevent      them      from      becoming  \\n \\n  dated      and      phrases      such      as,      “as      of      December      2023”      should      be      used      instead.\\n \\n \\n  Improper      handling      of   \\n \\n5      (comment      on      article      Cyclone      Batsirai)  \\n \\n  time-sensitive      information      “as      of      December      13”      doesn’t      specify   \\n \\na      year,      and      is      old      information  \\n \\n  (comment      on      article      2022      West      Java      earthquake) too      many      subsections      in      the      “Recovery      and      Rehabilitation”      section  \\n \\n  (comment      on      article      2022      West      Java      earthquake)\\n      (comment      on      article      2022      Crimean      Bridge      explosion)\\n \\n \\n  Section      organization      problem   \\n \\n5   \\n \\nI      do      not      like      how      the      article      is      organized,      with      too      many      headers      cluttering      the      article,  \\n \\n  making      it      not      as      readable.\\nOther      than      that,      the      AI      did      great      work      on      the      piece.\\n |       Select      a      key       8      v       Selected      Key:      8       Title:      Taylor      Hawkins      -      marshall.com       Snippets:       In      1995,      off      the      back      of      her      hit      album       ‘Jagged      Little      Pill’,      Canadian      American       superstar      Alanis      Morissette      recruited      him       to      be      her      touring      drummer      for      her      18       month      album      tour,      along      with      him       featuring      in      the      music      videos      for      “You       Oughta      Know’,      “All|      Really      Want”      and       “You      Learn’.      It      was      during      this      tour      that      he       met      his      musical      soul      mate,      Dave      Grohl.       The      tour      with      Alanis      ended      and      he\\\\\\\\\\\\\\'d      heard       that      Dave      and      Foo      Fighters      were      looking       for      anew      drummer,      so      Taylor      enquired.       Dave      initially      thought      that      Taylor      wouldn’t       be      interested      in      joining      as      Alanis       Morissette      was      much      bigger      than      the      Foo       Fighters      at      that      time,      but      Taylor      jumped      at       the      chance      due      to      his      overwhelming       desire      to      be      in      a      rock      band.      It      was      from       that      point      that      the      bromance      between       them      began      and      on      March      18th      1997       Taylor      was      announced      as      their      new       drummer.       Url:      https://marshall.com/live-for-       music/drum-icons/taylor-hawkins | Logout  \\n \\n  Select      an      option:  \\n \\n  (11,      ‘Taylor      Hawkins      [Quite      Richard}’)   \\n \\n¥       You      selected:      Taylor      Hawkins      [Quite      Richard]  \\n \\n  Taylor      Hawkins  \\n \\n  Oliver      Taylor      Hawkins      (1972-2022)      was      an      American      musician,      best      known      as      the      drummer      for      the      rock      band      Foo      Fighters[1].\\nBorn      in      Fort      Walton,      Texas,      Hawkins’      love      for      music      was      ignited      at   \\n \\na      young  \\n \\n  age,      particularly      after      watching   \\n \\na      Queen      concert      in      1982[2][3][5].\\nHe      kick-started      his      professional      career      as      the      drummer      for      Alanis      Morissette’s      band      during      her      world      tour      for      the      hit      album      ‘Jagged      Little  \\n \\n  Pill\\\\\\\\\\\\\\'[8][9].\\nHis      talents      were      recognized      by      Dave      Grohl,      who      invited      him      to      join      the      Foo      Fighters      in      1997,      marking      the      beginning      of      his      impactful      tenure      with      the      band{7][8].\\n \\n \\n  Hawkins      was      celebrated      for      his      versatile      drumming      style,      drawing      influence      from      renowned      drummers      like      Roger      Taylor,      Neil      Peart,      Phil      Collins,      Alex      Van      Halen,      and      Stewart      Copeland[14).\\nHis  \\n \\n  performances,      marked      by   \\n \\na      unique      energy      and      aggressive      style      of      play,      earned      him      recognition      as      one      of      the      top      rock      drummers      of      his      era[15].\\nApart      from      his      role      in      the      Foo      Fighters,      Hawkins\\\\\\\\\\\\\\'      passion  \\n \\n  for      music      saw      him      involved      in      numerous      side      projects      and      collaborations,      cementing      his      place      in      the      world      of      rock      music(10].\\n \\n \\n  Outside      of      his      professional      life,      Hawkins      was      known      for      his      resilience      and      dedication      to      his      family.\\nDespite      personal      struggles,      including   \\n \\na      near-fatal      drug      overdose      in      2001,      Hawkins      remained      committed  \\n \\n  to      his      musical      career{4][9].\\nHis      legacy      continues      to      inspire      musicians      and      fans      alike,      as      his      contributions      to      rock      music,      coupled      with      his   \\n \\ni       industry(13}.\\n \\n \\n  spirit,      made      him      an      icon      in      the      music  \\n \\n  Hawkins’      sudden      death      in      2022      while      on      tour      in      Bogotd,      Colombia,      sent      shockwaves      through      the      music      world[34].\\nTributes      poured      in      from      around      the      globe,      reflecting      the      respect      and      admiration  \\n \\n  Hawkins      had      garnered      during      his      lifetime[21][31].\\nHis      life      and      career      were      honored      at   \\n \\na      star-studded      tribute      concert      in      Los      Angeles,      attesting      to      the      enduring      impact      of      his      music[22].\\n | Early      Life      and      Background\\n |       Oliver      Taylor      Hawkins,      known      as      Taylor      Hawkins,      was      born      and      raised      in      Fort      Walton,      Texas[3].      His      family      moved      to      Laguna      Beach,      California      when      he      was      four      years      old[3].      He      has      two      younger      siblings,       a      brother      named      Jason,      and      a      sister      named      Heather[3].      Asa      child,      Hawkins      wa:      influenced      by      his      paternal      Josie      Hawkins,      who      had      grown      up      during      the      Great      Depression      and       lived      in      Jackson,      Mississippi(2].\\n |       During      his      high      school      days      at      Laguna      Beach      High      School,      from      where      he      graduated      in      1990,      he      became      friends      with      Jon      Davison,      who      later      became      the      lead      vocalist      of      the      band      Yes(2][3].      His      interest      in       music      was      nurtured      from      an      early      age,      particularly      after      watching      a      Queen      concert      in      1982      which      inspired      him      to      learn      to      play      the      drums(2][5].      He      noted      that      music      was      a      constant      presence      in      his      family       Taylor      Hawkins\\n\\n \\n \\n  Oliver      Taylor      Hawkins      (1972-2022)      was      an      American      musician,      best      known      as      the      drummer      for      the      rock  \\n \\n  band      Foo      Fighters[1].\\nBorn      in      Fort      Walton,      Texas,      Hawkins’      love      for      music      was      ignited      at   \\n \\na      young      age,  \\n \\n  particularly      after      watching   \\n \\na      Queen      concert      in      1982[2][3][5].\\nHe      kick-started      his      professional      career  \\n \\n  as      the      drummer      for      Alanis      Morissette’s      band      during      her      world      tour      for      the      hit      album      ’Jagged      Little  \\n \\n  Pill’[8]L[9].\\nHis      talents      were      recognized      by      Dave      Grohl,      who      invited      him      to      join      the      Foo      Fighters      in  \\n \\n  1997,      marking      the      beginning      of      his      impactful      tenure      with      the      band[7][8].\\n \\n \\n  Hawkins      was      celebrated      for      his      versatile      drumming      style,      drawing      influence      from      renowned      drummers      like  \\n \\n  Roger      Taylor,      Neil      Peart,      Phil      Collins,      Alex      Van      Halen,      and      Stewart      Copeland[14].\\nHis      performances,  \\n \\n  marked      by   \\n \\na      unique      energy      and      aggressive      style      of      play,      earned      him      recognition      as      one      of      the      top      rock  \\n \\n  drummers      of      his      era[15].\\nApart      from      his      role      in      the      Foo      Fighters,      Hawkins’      passion      for      music      saw  \\n \\n  him      involved      in      numerous      side      projects      and      collaborations,      cementing      his      place      in      the      world      of      rock  \\n \\n  music[10].\\n \\n \\n  Outside      of      his      professional      life,      Hawkins      was      known      for      his      resilience      and      dedication      to      his      family.\\n \\n \\n  Despite      personal      struggles,      including   \\n \\na      near-fatal      drug      overdose      in      2001,      Hawkins      remained      committed      to  \\n \\n  his      musical      career[4][9].\\nHis      legacy      continues      to      inspire      musicians      and      fans      alike,      as      his      contributions  \\n \\n  to      rock      music,      coupled      with      his      indomitable      spirit,      made      him      an      unforgettable      icon      in      the      music  \\n \\n  industry[13].\\n \\n \\n  Hawkins’      sudden      death      in      2022      while      on      tour      in      Bogota,      Colombia,      sent      shockwaves      through      the      music  \\n \\n  world[34].\\nTributes      poured      in      from      around      the      globe,      reflecting      the      respect      and      admiration      Hawkins  \\n \\n  had      garnered      during      his      lifetime[21][31].\\nHis      life      and      career      were      honored      at   \\n \\na      star-studded      tribute  \\n \\n  concert      in      Los      Angeles,      attesting      to      the      enduring      impact      of      his      music[22].\\n#\\n \\n   Early      Life      and      Background  \\n \\n  Oliver      Taylor      Hawkins,      known      as      Taylor      Hawkins,      was      born      and      raised      in      Fort      Walton,      Texas[3].\\nHis  \\n \\n  family      moved      to      Laguna      Beach,      California      when      he      was      four      years      old[3].\\nHe      has      two      younger      siblings,  \\n \\n  a      brother      named      Jason,      and   \\n \\na      sister      named      Heather[3].\\nAs   \\n \\na      child,      Hawkins      was      particularly      influenced  \\n \\n  by      his      paternal      grandmother,      Josie      Hawkins,      who      had      grown      up      during      the      Great      Depression      and      lived      in  \\n \\n  Jackson,      Mississippi[1].\\n \\n \\n  During      his      high      school      days      at      Laguna      Beach      High      School,      from      where      he      graduated      in      1990,      he      became  \\n \\n  friends      with      Jon      Davison,      who      later      became      the      lead      vocalist      of      the      band      Yes[2][3].\\nHis      interest      in  \\n \\n  music      was      nurtured      from      an      early      age,      particularly      after      watching   \\n \\na      Queen      concert      in      1982      which      inspired  \\n \\n  him      to      learn      to      play      the      drums[2][5].\\nHe      noted      that      music      was   \\n \\na      constant      presence      in      his      family      home[5].\\n \\n \\n  Despite      facing      certain      hardships      during      his      upbringing,      including      his      mother’s      struggles      with      \"demons\",  \\n \\n  Hawkins      pursued      his      musical      ambitions[4].\\nHe      credits      his      older      sister      Heather      for      taking      care      of      the  \\n \\n  family      during      difficult      times[4].\\n \\n \\n  His      first      major      musical      experience      came      from      playing      drums      for      Alanis      Morissette’s      album,      Jagged      Little  \\n \\n  Pill,      and      accompanying      her      on      the      subsequent      tour[3].\\nThis      marked      the      beginning      of      his      professional  \\n \\n  career      in      the      music      industry.\\n#\\n \\n   Career  \\n \\n  Taylor      Hawkins      began      his      professional      music      career      playing      in      Alanis      Morissette’s      band      during      her  \\n \\n  18-month      world      tour      in      support      of      the      hit      album      ’Jagged      Little      Pill’      from      1995      to      1997[8][9].\\nHis  \\n \\n  performances      not      only      in      the      tour      but      also      in      the      music      videos      for      “You      Oughta      Know”,      “All   \\n \\nI      Really      Want”  \\n \\n  and      “You      Learn”      introduced      him      to      the      world      of      rock      music      and      ultimately      led      to      his      meeting      with      Dave  \\n \\n  Grohl[8].\\nThroughout      this      time,      Hawkins      contributed      significantly      to      the      band’s      sound      and      performance,  \\n \\n  transforming      the      songs      from      their      original      drum      loop      format      to   \\n \\na      rock-band      vibe      that      resonated      with  \\n \\n  audiences[1][7].\\n \\n \\n  In      1997,      Hawkins      was      asked      by      Grohl      to      join      the      Foo      Fighters,      an      invitation      that      he      readily      accepted[7][8].\\n \\n \\n  At      the      time,      Grohl      thought      it      was   \\n \\na      long      shot      to      recruit      Hawkins      given      that      Morissette      was      at      the      height  \\n \\n  of      her      career,      but      Hawkins’      desire      to      be   \\n \\na      part      of   \\n \\na      rock      band      compelled      him      to      make      the      move[7].\\nThis  \\n \\n  marked      the      beginning      of      Hawkins’      tenure      as      the      drummer      of      the      Foo      Fighters,   \\n \\na      role      that      he      would      play  \\n \\n  until      his      passing[6][9].\\n \\n \\n  Apart      from      his      work      with      Morissette      and      the      Foo      Fighters,      Hawkins      had      an      array      of      other      musical  \\n \\n  experiences[10].\\nHe      drummed      for      Sass      Jordan      before      joining      Morissette’s      touring      band[10].\\nHe      was      part  \\n \\n  of      an      ad      hoc      drum      supergroup      called      SOS      Allstars      and      filled      the      void      for      Coheed      and      Cambria’s      2007  \\n \\n  album      after      their      drummer      Josh      Eppard      left      the      group[10].\\nIn      addition,      Hawkins      formed      his      own      side  \\n \\n  project,      the      Coattail      Riders,      in      2005,      through      which      he      recorded      his      own      music      and      took      the      project      on  \\n \\n  the      road,      performing      in      small      clubs      despite      the      Foo      Fighters’      arena-status[7].\\nHis      son,      Shane      Hawkins,  \\n \\n  has      since      taken      on      his      father’s      legacy,      joining      the      Foo      Fighters      for   \\n \\na      performance      during      the      Boston  \\n \\n  Calling      Music      Festival      in      2023[6].\\n#\\n \\n   Musical      Style      and      Influences  \\n \\n  Taylor      Hawkins      was   \\n \\na      profound      drummer,      with      his      musical      style      and      influences      spreading      across   \\n \\na      wide  \\n \\n  array      of      rock      genres[11].\\nKnown      for      his      passionate      fandom      of      groups      that      came      before      him,      Hawkins  \\n \\n  regularly      expressed      his      admiration      for      bands      like      Rush,      Genesis,      and      the      Police,      all      of      which      featured  \\n \\n  some      of      the      greatest      drummers      in      rock      history      like      Neil      Peart,      Phil      Collins,      and      Stewart      Copeland[11].\\nHe      was      heavily      influenced      by      his      love      for      classic      rock,      as      evidenced      by      his      performances,      where      he  \\n \\n  covered      songs      from      bands      like      Van      Halen[11].\\n \\n \\n  Hawkins      drew      influences      from   \\n \\na      variety      of      drumming      styles,      developing   \\n \\na      signature      style      inspired      by  \\n \\n  greats      like      Roger      Taylor,      Neil      Peart,      Phil      Collins,      Alex      Van      Halen,      and      Stewart      Copeland[14].\\nThis  \\n \\n  distinctive      style      and      influence      extended      to      his      drum      kit,      which      incorporated      elements      like      rototoms  \\n \\n  and      concert      toms[14].\\n \\n \\n  Beyond      his      influences,      Hawkins      had   \\n \\na      unique      energy      that      made      him      stand      out      as   \\n \\na      drummer.\\nHis      performances  \\n \\n  were      recognized      for      their      power,      and      he      was      known      for      his      enthusiastic      and      aggressive      style      of      play[15].\\n \\n \\n  This      earned      him      recognition      as      one      of      the      top      rock      drummers      of      his      time,      with      his      passion      for      music  \\n \\n  living      on      through      his      performances[14].\\n \\n \\n  Through      his      career,      Hawkins      left      an      indelible      mark      on      rock      music,      through      his      distinct      style,      passion,  \\n \\n  and      contributions      to      the      music      industry[13].\\nHis      love      for      music      and      dedication      to      his      craft      made      him  \\n \\n  an      unforgettable      icon      in      the      world      of      rock      music[13].\\n#\\n \\n   Personal      Life  \\n \\n  Taylor      Hawkins      married      Alison      Hawkins,      an      American      celebrity      and      entrepreneur,      in      2005[18].\\nThe      couple  \\n \\n  had      three      children,      Oliver,      Annabelle,      and      Everleigh[19].\\nHawkins’      commitment      to      his      family      was      evident;\\n \\n \\n  in      fact,      he      even      wrote   \\n \\na      song      for      his      middle      child,      Annabelle[9].\\n \\n \\n  In      his      personal      life,      Hawkins      had      also      struggled      with      drug      use,      which      nearly      claimed      his      life      in   \\n \\na      2001  \\n \\n  overdose[9][7][4].\\nHowever,      he      managed      to      overcome      this      challenge,      and      later      expressed      gratitude      for  \\n \\n  the      experience      as   \\n \\na      lesson      that      allowed      him      to      realize      the      destructive      path      he      was      on[7].\\n \\n \\n  Outside      of      his      main      role      in      the      Foo      Fighters,      Hawkins      also      pursued      various      side      projects      including      the  \\n \\n  Birds      of      Satan,      NHC,      and      Chevy      Metal.\\nHis      motivation      for      such      ventures      was   \\n \\na      constant      drive      to      create  \\n \\n  and      his      love      for      music[7].\\nHawkins      was      also      known      for      his      unabashed      fanboy      nature,      often      vocalizing  \\n \\n  his      admiration      for      fellow      musicians      and      his      heroes[7].\\n#\\n \\n   Legacy      and      Impact Taylor      Hawkins      was      known      for      his      raw      and      authentic      drumming      style,      described      as      \"courageous,      damaged  \\n \\n  and      unflinchingly      authentic”[20].\\nHis      work      with      the      Foo      Fighters,      as      well      as      his      various      collaborations n’      roll[10].\\n \\n \\n  ‘ and      side      projects,      made      him   \\n \\na      celebrated      figure      in      rock  \\n \\n  Hawkins’      death      in      2022      was      met      with      heartfelt      tributes      from      colleagues      and      fans      around      the      world.\\n \\n \\n  Notable      tributes      came      from      rock      legends      like      Roger      Taylor      of      Queen,      who      considered      Hawkins      as   \\n \\na      kind,  \\n \\n  brilliant      man      and      an      inspirational      mentor,      likening      his      death      to      \"losing   \\n \\na      younger      favourite      brother”[21].\\n \\n \\n  Similarly,      Led      Zeppelin’s      Jimmy      Page      admired      his      technique,      energy      and      spirited      enthusiasm[21].\\n \\n \\n  An      LA      tribute      concert      held      in      his      honor      included      guest      drummers      like      Lars      Ulrich      of      Metallica,      Travis  \\n \\n  Barker      of      blink-182,      and      Brad      Wilk      of      Rage      Against      the      Machine.\\nSingers      like      Miley      Cyrus      and      Alanis  \\n \\n  Morissette      also      performed      at      the      concert[22].\\n \\n \\n  Apart      from      his      music,      Taylor      Hawkins      also      contributed      to      charities      Music      Support      and      MusiCares,      both      of  \\n \\n  which      were      chosen      by      the      Hawkins      family[23].\\nHe      had      received      numerous      accolades      throughout      his      career,  \\n \\n  including      27      Grammy      nominations,      of      which      he      won      14[2].\\nIn      2021,      the      Foo      Fighters      were      inducted      into  \\n \\n  the      Rock      and      Roll      Hall      of      Fame[9].\\n#\\n \\n   Discography  \\n \\n  Taylor      Hawkins      also      led   \\n \\na      notable      music      career      through      his      own      side      projects      and      collaborations[10].\\n \\n \\n  Aside      from      his      work      with      the      Foo      Fighters,      Hawkins      formed      and      fronted      the      band      Taylor      Hawkins   \\n \\n&      The  \\n \\n  Coattail      Riders,   \\n \\na      project      which      originated      from      jamming      sessions      with      his      friend      Drew      Hester[10].\\n###      Taylor      Hawkins   \\n \\n&      The      Coattail      Riders  \\n \\n  Taylor      Hawkins   \\n \\n&      The      Coattail      Riders,   \\n \\na      band      formed      in      2004,      have      released      three      albums      and      their  \\n \\n  music      spans      genres      including      Hard      Rock,      Art      Rock,      and      Alternative      Rock[24][25][26].\\nThe      band      grew      from  \\n \\n  an      initial      casual      jamming      session,      gradually      evolving      into   \\n \\na      more      formal      arrangement      that      led      to      the  \\n \\n  production      of      record      albums.\\nNotably,      these      albums      featured      guest      appearances      by      renowned      musicians  \\n \\n  such      as      Dave      Grohl,      Queen’s      Brian      May      and      Roger      Taylor,      The      Cars’      Elliot      Easton,      Perry      Farrell,      and       Jon      Davison,      who      is   \\n \\na      school      friend      of      Hawkins’[10].\\n###      Red      Light      Fever  \\n \\n  Red      Light      Fever,      released      on      April      19,      2010,      was      the      band’s      first      album[29][30].\\nPrior      to      its      release,  \\n \\n  Hawkins      revealed      in      an      interview      that      the      album      had      completed      the      recording      and      production      stages,      but  \\n \\n  its      title      and      release      date      were      yet      to      be      determined[29].\\nRed      Light      Fever      was      recorded      at      the      Foo  \\n \\n  Fighters’      Studio      606      in      California      and      featured      guest      musicians      such      as      Brian      May      and      Roger      Taylor      of  \\n \\n  Queen,      Dave      Grohl      of      Foo      Fighters,      and      Elliot      Easton      of      The      Cars[29][30].\\n##      Get      the      Money  \\n \\n  Get      the      Money,      the      third      album      from      Taylor      Hawkins   \\n \\n&      The      Coattail      Riders,      was      released      on      November      8,  \\n \\n  2019[29].\\nThe      album’s      first      single,      \"Crossed      the      Line”,      released      on      October      15,      2019,      featured      Dave  \\n \\n  Grohl      and      Jon      Davison,      the      frontman      of      Yes[29].\\nThe      music      video      for      the      single      \"I      Really      Blew      It”      also  \\n \\n  featured      appearances      from      Grohl      and      Perry      Farrel1[29].\\n#\\n \\n   Collaborations      and      Guest      Appearances  \\n \\n  Throughout      his      career,      Taylor      Hawkins      collaborated      with      various      prominent      artists      and      bands.\\nThe  \\n \\n  Coattail      Riders’      albums      notably      featured      appearances      from      luminaries      such      as      Brian      May      and      Roger      Taylor  \\n \\n  of      Queen,      Chrissie      Hynde,      Nancy      Wilson      of      Heart,      Sex      Pistol      Steve      Jones      and      James      Gang’s      Joe      Walsh[28].\\n \\n \\n  Hawkins      also      fronted      another      group,      The      Birds      of      Satan,      which      evolved      from      his      heavy      rock      covers      band,  \\n \\n  Chevy      Metal[28].\\nDespite      his      diverse      musical      engagements,      Hawkins      always      maintained   \\n \\na      close      allegiance      with      the      Foo  \\n \\n  Fighters,      which      remained      the      center      of      his      music      life[7][28].\\n#\\n \\n   Tragic      Passing  \\n \\n  Taylor      Hawkins,      the      esteemed      drummer      of      the      alt-rock      band      Foo      Fighters,      passed      away      suddenly      on      March  \\n \\n  25,      2022,      while      on      tour      with      his      band      in      Bogota,      Colombia[34].\\nThe      official      cause      of      death      was      cardiac  \\n \\n  arrest,      though      inquiries      were      raised      concerning      the      presence      of      drugs      in      his      system      and      their      potential  \\n \\n  contribution      to      his      death[33][34].\\nOn      the      night      of      his      passing,      paramedics      were      called      to      the      Four  \\n \\n  Seasons      hotel      in      Bogota      due      to      reports      of      chest      pain      from      an      unnamed      guest,      later      revealed      to      be  \\n \\n  Hawkins[34].\\nUnfortunately,      resuscitation      efforts      were      unsuccessful,      and      Hawkins      was      declared      dead      at  \\n \\n  the      scene[34].\\n \\n \\n  The      news      of      Hawkins’      sudden      demise      was      announced      on      the      morning      of      March      25th,      2022,      which      left      the      music  \\n \\n  world      in      shock[32].\\nThe      band      confirmed      the      news      with   \\n \\na      short      statement,      expressing      their      devastation  \\n \\n  at      the      loss      of      Hawkins,      whose      \"musical      spirit      and      infectious      laughter”      would      live      on      forever[32].\\n \\n \\n  As   \\n \\na      result      of      Hawkins’      untimely      passing,      the      band      canceled      their      ongoing      South      American      tour[33].\\nThe  \\n \\n  festival      stage      at      the      Estéreo      Picnic      Festival,      where      the      Foo      Fighters      were      scheduled      to      perform      that  \\n \\n  night,      was      transformed      into   \\n \\na      candlelight      vigil      in      memory      of      Hawkins[33].\\n##      Tributes      and      Remembrances  \\n \\n  In      the      wake      of      Hawkins’      death,      tributes      from      fans      and      colleagues      alike      poured      in      from      around      the  \\n \\n  world[21][31].\\nAmong      the      many      paying      their      respects      were      legendary      rock      and      roll      musicians      like      Roger  \\n \\n  Taylor,      the      drummer      of      Queen,      who      Hawkins      credited      with      inspiring      his      own      career      behind      the      drum      set[21].\\n \\n \\n  In      heartfelt      social      media      posts,      Taylor      described      Hawkins      as      an      \"inspirational      mentor”      and   \\n \\na      \"kind  \\n \\n  brilliant      man\"[21],      while      Led      Zeppelin’s      Jimmy      Page      reminisced      about      sharing      the      stage      with      Hawkins  \\n \\n  and      praised      his      \"technique,      energy      and      spirited      enthusiasm\"[21].\\n \\n \\n  There      were      also      numerous      onstage      tributes      to      Hawkins.\\nNotably,      Miley      Cyrus      expressed      her      grief      and      sent  \\n \\n  peaceful      wishes      to      the      Foo      Fighters      and      the      Hawkins      family      during   \\n \\na      performance      at      Lollapalooza[31].\\n \\n \\n  Similarly,      Liam      Gallagher      of      Oasis      dedicated      one      of      the      band’s      biggest      hits      to      Hawkins      during   \\n \\na      concert  \\n \\n  at      the      Royal      Albert      Hall      in      London[31].\\nFans      gathered      outside      the      hotel      where      Hawkins      died,      lighting      candles,      leaving      flowers,      and      singing      the  \\n \\n  band’s      songs      in      his      honor[31].\\n \\n \\n  Hawkins’      life      and      career      were      celebrated      in   \\n \\na      star-studded      tribute      concert      in      Los      Angeles,      which      saw  \\n \\n  performances      from      over      5@      musicians,      including      his      former      bands      and      colleagues      from      Def      Leppard,      Queen,  \\n \\n  and      Foo      Fighters[22].\\nTable      12:      STORM’s      generated      article      for      “Taylor      Hawkins”.\\n“#’,      “##”      indicate      the      section      title      and      subsection      title  \\n \\n  respectively.\\nNumbers      in      brackets      indicate      the      cited      references.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 41, 'section_title': '      (comment      on      article      2022      Crimean      Bridge      explosion)'}, page_content='      (comment      on      article      2022      Crimean      Bridge      explosion)\\n \\n \\n  Section      organization      problem   \\n \\n5   \\n \\nI      do      not      like      how      the      article      is      organized,      with      too      many      headers      cluttering      the      article,  \\n \\n  making      it      not      as      readable.\\nOther      than      that,      the      AI      did      great      work      on      the      piece.\\n |       Select      a      key       8      v       Selected      Key:      8       Title:      Taylor      Hawkins      -      marshall.com       Snippets:       In      1995,      off      the      back      of      her      hit      album       ‘Jagged      Little      Pill’,      Canadian      American       superstar      Alanis      Morissette      recruited      him       to      be      her      touring      drummer      for      her      18       month      album      tour,      along      with      him       featuring      in      the      music      videos      for      “You       Oughta      Know’,      “All|      Really      Want”      and       “You      Learn’.      It      was      during      this      tour      that      he       met      his      musical      soul      mate,      Dave      Grohl.       The      tour      with      Alanis      ended      and      he\\\\\\\\\\\\\\'d      heard       that      Dave      and      Foo      Fighters      were      looking       for      anew      drummer,      so      Taylor      enquired.       Dave      initially      thought      that      Taylor      wouldn’t       be      interested      in      joining      as      Alanis       Morissette      was      much      bigger      than      the      Foo       Fighters      at      that      time,      but      Taylor      jumped      at       the      chance      due      to      his      overwhelming       desire      to      be      in      a      rock      band.      It      was      from       that      point      that      the      bromance      between       them      began      and      on      March      18th      1997       Taylor      was      announced      as      their      new       drummer.       Url:      https://marshall.com/live-for-       music/drum-icons/taylor-hawkins | Logout  \\n \\n  Select      an      option:  \\n \\n  (11,      ‘Taylor      Hawkins      [Quite      Richard}’)   \\n \\n¥       You      selected:      Taylor      Hawkins      [Quite      Richard]  \\n \\n  Taylor      Hawkins  \\n \\n  Oliver      Taylor      Hawkins      (1972-2022)      was      an      American      musician,      best      known      as      the      drummer      for      the      rock      band      Foo      Fighters[1].\\nBorn      in      Fort      Walton,      Texas,      Hawkins’      love      for      music      was      ignited      at   \\n \\na      young  \\n \\n  age,      particularly      after      watching   \\n \\na      Queen      concert      in      1982[2][3][5].\\nHe      kick-started      his      professional      career      as      the      drummer      for      Alanis      Morissette’s      band      during      her      world      tour      for      the      hit      album      ‘Jagged      Little  \\n \\n  Pill\\\\\\\\\\\\\\'[8][9].\\nHis      talents      were      recognized      by      Dave      Grohl,      who      invited      him      to      join      the      Foo      Fighters      in      1997,      marking      the      beginning      of      his      impactful      tenure      with      the      band{7][8].\\n \\n \\n  Hawkins      was      celebrated      for      his      versatile      drumming      style,      drawing      influence      from      renowned      drummers      like      Roger      Taylor,      Neil      Peart,      Phil      Collins,      Alex      Van      Halen,      and      Stewart      Copeland[14).\\nHis  \\n \\n  performances,      marked      by   \\n \\na      unique      energy      and      aggressive      style      of      play,      earned      him      recognition      as      one      of      the      top      rock      drummers      of      his      era[15].\\nApart      from      his      role      in      the      Foo      Fighters,      Hawkins\\\\\\\\\\\\\\'      passion  \\n \\n  for      music      saw      him      involved      in      numerous      side      projects      and      collaborations,      cementing      his      place      in      the      world      of      rock      music(10].\\n \\n \\n  Outside      of      his      professional      life,      Hawkins      was      known      for      his      resilience      and      dedication      to      his      family.\\nDespite      personal      struggles,      including   \\n \\na      near-fatal      drug      overdose      in      2001,      Hawkins      remained      committed  \\n \\n  to      his      musical      career{4][9].\\nHis      legacy      continues      to      inspire      musicians      and      fans      alike,      as      his      contributions      to      rock      music,      coupled      with      his   \\n \\ni       industry(13}.\\n \\n \\n  spirit,      made      him      an      icon      in      the      music  \\n \\n  Hawkins’      sudden      death      in      2022      while      on      tour      in      Bogotd,      Colombia,      sent      shockwaves      through      the      music      world[34].\\nTributes      poured      in      from      around      the      globe,      reflecting      the      respect      and      admiration  \\n \\n  Hawkins      had      garnered      during      his      lifetime[21][31].\\nHis      life      and      career      were      honored      at   \\n \\na      star-studded      tribute      concert      in      Los      Angeles,      attesting      to      the      enduring      impact      of      his      music[22].\\n | Early      Life      and      Background\\n |       Oliver      Taylor      Hawkins,      known      as      Taylor      Hawkins,      was      born      and      raised      in      Fort      Walton,      Texas[3].      His      family      moved      to      Laguna      Beach,      California      when      he      was      four      years      old[3].      He      has      two      younger      siblings,       a      brother      named      Jason,      and      a      sister      named      Heather[3].      Asa      child,      Hawkins      wa:      influenced      by      his      paternal      Josie      Hawkins,      who      had      grown      up      during      the      Great      Depression      and       lived      in      Jackson,      Mississippi(2].\\n |       During      his      high      school      days      at      Laguna      Beach      High      School,      from      where      he      graduated      in      1990,      he      became      friends      with      Jon      Davison,      who      later      became      the      lead      vocalist      of      the      band      Yes(2][3].      His      interest      in       music      was      nurtured      from      an      early      age,      particularly      after      watching      a      Queen      concert      in      1982      which      inspired      him      to      learn      to      play      the      drums(2][5].      He      noted      that      music      was      a      constant      presence      in      his      family       Taylor      Hawkins\\n\\n \\n \\n  Oliver      Taylor      Hawkins      (1972-2022)      was      an      American      musician,      best      known      as      the      drummer      for      the      rock  \\n \\n  band      Foo      Fighters[1].\\nBorn      in      Fort      Walton,      Texas,      Hawkins’      love      for      music      was      ignited      at   \\n \\na      young      age,  \\n \\n  particularly      after      watching   \\n \\na      Queen      concert      in      1982[2][3][5].\\nHe      kick-started      his      professional      career  \\n \\n  as      the      drummer      for      Alanis      Morissette’s      band      during      her      world      tour      for      the      hit      album      ’Jagged      Little  \\n \\n  Pill’[8]L[9].\\nHis      talents      were      recognized      by      Dave      Grohl,      who      invited      him      to      join      the      Foo      Fighters      in  \\n \\n  1997,      marking      the      beginning      of      his      impactful      tenure      with      the      band[7][8].\\n \\n \\n  Hawkins      was      celebrated      for      his      versatile      drumming      style,      drawing      influence      from      renowned      drummers      like  \\n \\n  Roger      Taylor,      Neil      Peart,      Phil      Collins,      Alex      Van      Halen,      and      Stewart      Copeland[14].\\nHis      performances,  \\n \\n  marked      by   \\n \\na      unique      energy      and      aggressive      style      of      play,      earned      him      recognition      as      one      of      the      top      rock  \\n \\n  drummers      of      his      era[15].\\nApart      from      his      role      in      the      Foo      Fighters,      Hawkins’      passion      for      music      saw  \\n \\n  him      involved      in      numerous      side      projects      and      collaborations,      cementing      his      place      in      the      world      of      rock  \\n \\n  music[10].\\n \\n \\n  Outside      of      his      professional      life,      Hawkins      was      known      for      his      resilience      and      dedication      to      his      family.\\n \\n \\n  Despite      personal      struggles,      including   \\n \\na      near-fatal      drug      overdose      in      2001,      Hawkins      remained      committed      to  \\n \\n  his      musical      career[4][9].\\nHis      legacy      continues      to      inspire      musicians      and      fans      alike,      as      his      contributions  \\n \\n  to      rock      music,      coupled      with      his      indomitable      spirit,      made      him      an      unforgettable      icon      in      the      music  \\n \\n  industry[13].\\n \\n \\n  Hawkins’      sudden      death      in      2022      while      on      tour      in      Bogota,      Colombia,      sent      shockwaves      through      the      music  \\n \\n  world[34].\\nTributes      poured      in      from      around      the      globe,      reflecting      the      respect      and      admiration      Hawkins  \\n \\n  had      garnered      during      his      lifetime[21][31].\\nHis      life      and      career      were      honored      at   \\n \\na      star-studded      tribute  \\n \\n  concert      in      Los      Angeles,      attesting      to      the      enduring      impact      of      his      music[22].\\n#\\n \\n   Early      Life      and      Background  \\n \\n  Oliver      Taylor      Hawkins,      known      as      Taylor      Hawkins,      was      born      and      raised      in      Fort      Walton,      Texas[3].\\nHis  \\n \\n  family      moved      to      Laguna      Beach,      California      when      he      was      four      years      old[3].\\nHe      has      two      younger      siblings,  \\n \\n  a      brother      named      Jason,      and   \\n \\na      sister      named      Heather[3].\\nAs   \\n \\na      child,      Hawkins      was      particularly      influenced  \\n \\n  by      his      paternal      grandmother,      Josie      Hawkins,      who      had      grown      up      during      the      Great      Depression      and      lived      in  \\n \\n  Jackson,      Mississippi[1].\\n \\n \\n  During      his      high      school      days      at      Laguna      Beach      High      School,      from      where      he      graduated      in      1990,      he      became  \\n \\n  friends      with      Jon      Davison,      who      later      became      the      lead      vocalist      of      the      band      Yes[2][3].\\nHis      interest      in  \\n \\n  music      was      nurtured      from      an      early      age,      particularly      after      watching   \\n \\na      Queen      concert      in      1982      which      inspired  \\n \\n  him      to      learn      to      play      the      drums[2][5].\\nHe      noted      that      music      was   \\n \\na      constant      presence      in      his      family      home[5].\\n \\n \\n  Despite      facing      certain      hardships      during      his      upbringing,      including      his      mother’s      struggles      with      \"demons\",  \\n \\n  Hawkins      pursued      his      musical      ambitions[4].\\nHe      credits      his      older      sister      Heather      for      taking      care      of      the  \\n \\n  family      during      difficult      times[4].\\n \\n \\n  His      first      major      musical      experience      came      from      playing      drums      for      Alanis      Morissette’s      album,      Jagged      Little  \\n \\n  Pill,      and      accompanying      her      on      the      subsequent      tour[3].\\nThis      marked      the      beginning      of      his      professional  \\n \\n  career      in      the      music      industry.\\n#\\n \\n   Career  \\n \\n  Taylor      Hawkins      began      his      professional      music      career      playing      in      Alanis      Morissette’s      band      during      her  \\n \\n  18-month      world      tour      in      support      of      the      hit      album      ’Jagged      Little      Pill’      from      1995      to      1997[8][9].\\nHis  \\n \\n  performances      not      only      in      the      tour      but      also      in      the      music      videos      for      “You      Oughta      Know”,      “All   \\n \\nI      Really      Want”  \\n \\n  and      “You      Learn”      introduced      him      to      the      world      of      rock      music      and      ultimately      led      to      his      meeting      with      Dave  \\n \\n  Grohl[8].\\nThroughout      this      time,      Hawkins      contributed      significantly      to      the      band’s      sound      and      performance,  \\n \\n  transforming      the      songs      from      their      original      drum      loop      format      to   \\n \\na      rock-band      vibe      that      resonated      with  \\n \\n  audiences[1][7].\\n \\n \\n  In      1997,      Hawkins      was      asked      by      Grohl      to      join      the      Foo      Fighters,      an      invitation      that      he      readily      accepted[7][8].\\n \\n \\n  At      the      time,      Grohl      thought      it      was   \\n \\na      long      shot      to      recruit      Hawkins      given      that      Morissette      was      at      the      height  \\n \\n  of      her      career,      but      Hawkins’      desire      to      be   \\n \\na      part      of   \\n \\na      rock      band      compelled      him      to      make      the      move[7].\\nThis  \\n \\n  marked      the      beginning      of      Hawkins’      tenure      as      the      drummer      of      the      Foo      Fighters,   \\n \\na      role      that      he      would      play  \\n \\n  until      his      passing[6][9].\\n \\n \\n  Apart      from      his      work      with      Morissette      and      the      Foo      Fighters,      Hawkins      had      an      array      of      other      musical  \\n \\n  experiences[10].\\nHe      drummed      for      Sass      Jordan      before      joining      Morissette’s      touring      band[10].\\nHe      was      part  \\n \\n  of      an      ad      hoc      drum      supergroup      called      SOS      Allstars      and      filled      the      void      for      Coheed      and      Cambria’s      2007  \\n \\n  album      after      their      drummer      Josh      Eppard      left      the      group[10].\\nIn      addition,      Hawkins      formed      his      own      side  \\n \\n  project,      the      Coattail      Riders,      in      2005,      through      which      he      recorded      his      own      music      and      took      the      project      on  \\n \\n  the      road,      performing      in      small      clubs      despite      the      Foo      Fighters’      arena-status[7].\\nHis      son,      Shane      Hawkins,  \\n \\n  has      since      taken      on      his      father’s      legacy,      joining      the      Foo      Fighters      for   \\n \\na      performance      during      the      Boston  \\n \\n  Calling      Music      Festival      in      2023[6].\\n#\\n \\n   Musical      Style      and      Influences  \\n \\n  Taylor      Hawkins      was   \\n \\na      profound      drummer,      with      his      musical      style      and      influences      spreading      across   \\n \\na      wide  \\n \\n  array      of      rock      genres[11].\\nKnown      for      his      passionate      fandom      of      groups      that      came      before      him,      Hawkins  \\n \\n  regularly      expressed      his      admiration      for      bands      like      Rush,      Genesis,      and      the      Police,      all      of      which      featured  \\n \\n  some      of      the      greatest      drummers      in      rock      history      like      Neil      Peart,      Phil      Collins,      and      Stewart      Copeland[11].\\nHe      was      heavily      influenced      by      his      love      for      classic      rock,      as      evidenced      by      his      performances,      where      he  \\n \\n  covered      songs      from      bands      like      Van      Halen[11].\\n \\n \\n  Hawkins      drew      influences      from   \\n \\na      variety      of      drumming      styles,      developing   \\n \\na      signature      style      inspired      by  \\n \\n  greats      like      Roger      Taylor,      Neil      Peart,      Phil      Collins,      Alex      Van      Halen,      and      Stewart      Copeland[14].\\nThis  \\n \\n  distinctive      style      and      influence      extended      to      his      drum      kit,      which      incorporated      elements      like      rototoms  \\n \\n  and      concert      toms[14].\\n \\n \\n  Beyond      his      influences,      Hawkins      had   \\n \\na      unique      energy      that      made      him      stand      out      as   \\n \\na      drummer.\\nHis      performances  \\n \\n  were      recognized      for      their      power,      and      he      was      known      for      his      enthusiastic      and      aggressive      style      of      play[15].\\n \\n \\n  This      earned      him      recognition      as      one      of      the      top      rock      drummers      of      his      time,      with      his      passion      for      music  \\n \\n  living      on      through      his      performances[14].\\n \\n \\n  Through      his      career,      Hawkins      left      an      indelible      mark      on      rock      music,      through      his      distinct      style,      passion,  \\n \\n  and      contributions      to      the      music      industry[13].\\nHis      love      for      music      and      dedication      to      his      craft      made      him  \\n \\n  an      unforgettable      icon      in      the      world      of      rock      music[13].\\n#\\n \\n   Personal      Life  \\n \\n  Taylor      Hawkins      married      Alison      Hawkins,      an      American      celebrity      and      entrepreneur,      in      2005[18].\\nThe      couple  \\n \\n  had      three      children,      Oliver,      Annabelle,      and      Everleigh[19].\\nHawkins’      commitment      to      his      family      was      evident;\\n \\n \\n  in      fact,      he      even      wrote   \\n \\na      song      for      his      middle      child,      Annabelle[9].\\n \\n \\n  In      his      personal      life,      Hawkins      had      also      struggled      with      drug      use,      which      nearly      claimed      his      life      in   \\n \\na      2001  \\n \\n  overdose[9][7][4].\\nHowever,      he      managed      to      overcome      this      challenge,      and      later      expressed      gratitude      for  \\n \\n  the      experience      as   \\n \\na      lesson      that      allowed      him      to      realize      the      destructive      path      he      was      on[7].\\n \\n \\n  Outside      of      his      main      role      in      the      Foo      Fighters,      Hawkins      also      pursued      various      side      projects      including      the  \\n \\n  Birds      of      Satan,      NHC,      and      Chevy      Metal.\\nHis      motivation      for      such      ventures      was   \\n \\na      constant      drive      to      create  \\n \\n  and      his      love      for      music[7].\\nHawkins      was      also      known      for      his      unabashed      fanboy      nature,      often      vocalizing  \\n \\n  his      admiration      for      fellow      musicians      and      his      heroes[7].\\n#\\n \\n   Legacy      and      Impact Taylor      Hawkins      was      known      for      his      raw      and      authentic      drumming      style,      described      as      \"courageous,      damaged  \\n \\n  and      unflinchingly      authentic”[20].\\nHis      work      with      the      Foo      Fighters,      as      well      as      his      various      collaborations n’      roll[10].\\n \\n \\n  ‘ and      side      projects,      made      him   \\n \\na      celebrated      figure      in      rock  \\n \\n  Hawkins’      death      in      2022      was      met      with      heartfelt      tributes      from      colleagues      and      fans      around      the      world.\\n \\n \\n  Notable      tributes      came      from      rock      legends      like      Roger      Taylor      of      Queen,      who      considered      Hawkins      as   \\n \\na      kind,  \\n \\n  brilliant      man      and      an      inspirational      mentor,      likening      his      death      to      \"losing   \\n \\na      younger      favourite      brother”[21].\\n \\n \\n  Similarly,      Led      Zeppelin’s      Jimmy      Page      admired      his      technique,      energy      and      spirited      enthusiasm[21].\\n \\n \\n  An      LA      tribute      concert      held      in      his      honor      included      guest      drummers      like      Lars      Ulrich      of      Metallica,      Travis  \\n \\n  Barker      of      blink-182,      and      Brad      Wilk      of      Rage      Against      the      Machine.\\nSingers      like      Miley      Cyrus      and      Alanis  \\n \\n  Morissette      also      performed      at      the      concert[22].\\n \\n \\n  Apart      from      his      music,      Taylor      Hawkins      also      contributed      to      charities      Music      Support      and      MusiCares,      both      of  \\n \\n  which      were      chosen      by      the      Hawkins      family[23].\\nHe      had      received      numerous      accolades      throughout      his      career,  \\n \\n  including      27      Grammy      nominations,      of      which      he      won      14[2].\\nIn      2021,      the      Foo      Fighters      were      inducted      into  \\n \\n  the      Rock      and      Roll      Hall      of      Fame[9].\\n#\\n \\n   Discography  \\n \\n  Taylor      Hawkins      also      led   \\n \\na      notable      music      career      through      his      own      side      projects      and      collaborations[10].\\n \\n \\n  Aside      from      his      work      with      the      Foo      Fighters,      Hawkins      formed      and      fronted      the      band      Taylor      Hawkins   \\n \\n&      The  \\n \\n  Coattail      Riders,   \\n \\na      project      which      originated      from      jamming      sessions      with      his      friend      Drew      Hester[10].\\n###      Taylor      Hawkins   \\n \\n&      The      Coattail      Riders  \\n \\n  Taylor      Hawkins   \\n \\n&      The      Coattail      Riders,   \\n \\na      band      formed      in      2004,      have      released      three      albums      and      their  \\n \\n  music      spans      genres      including      Hard      Rock,      Art      Rock,      and      Alternative      Rock[24][25][26].\\nThe      band      grew      from  \\n \\n  an      initial      casual      jamming      session,      gradually      evolving      into   \\n \\na      more      formal      arrangement      that      led      to      the  \\n \\n  production      of      record      albums.\\nNotably,      these      albums      featured      guest      appearances      by      renowned      musicians  \\n \\n  such      as      Dave      Grohl,      Queen’s      Brian      May      and      Roger      Taylor,      The      Cars’      Elliot      Easton,      Perry      Farrell,      and       Jon      Davison,      who      is   \\n \\na      school      friend      of      Hawkins’[10].\\n###      Red      Light      Fever  \\n \\n  Red      Light      Fever,      released      on      April      19,      2010,      was      the      band’s      first      album[29][30].\\nPrior      to      its      release,  \\n \\n  Hawkins      revealed      in      an      interview      that      the      album      had      completed      the      recording      and      production      stages,      but  \\n \\n  its      title      and      release      date      were      yet      to      be      determined[29].\\nRed      Light      Fever      was      recorded      at      the      Foo  \\n \\n  Fighters’      Studio      606      in      California      and      featured      guest      musicians      such      as      Brian      May      and      Roger      Taylor      of  \\n \\n  Queen,      Dave      Grohl      of      Foo      Fighters,      and      Elliot      Easton      of      The      Cars[29][30].\\n##      Get      the      Money  \\n \\n  Get      the      Money,      the      third      album      from      Taylor      Hawkins   \\n \\n&      The      Coattail      Riders,      was      released      on      November      8,  \\n \\n  2019[29].\\nThe      album’s      first      single,      \"Crossed      the      Line”,      released      on      October      15,      2019,      featured      Dave  \\n \\n  Grohl      and      Jon      Davison,      the      frontman      of      Yes[29].\\nThe      music      video      for      the      single      \"I      Really      Blew      It”      also  \\n \\n  featured      appearances      from      Grohl      and      Perry      Farrel1[29].\\n#\\n \\n   Collaborations      and      Guest      Appearances  \\n \\n  Throughout      his      career,      Taylor      Hawkins      collaborated      with      various      prominent      artists      and      bands.\\nThe  \\n \\n  Coattail      Riders’      albums      notably      featured      appearances      from      luminaries      such      as      Brian      May      and      Roger      Taylor  \\n \\n  of      Queen,      Chrissie      Hynde,      Nancy      Wilson      of      Heart,      Sex      Pistol      Steve      Jones      and      James      Gang’s      Joe      Walsh[28].\\n \\n \\n  Hawkins      also      fronted      another      group,      The      Birds      of      Satan,      which      evolved      from      his      heavy      rock      covers      band,  \\n \\n  Chevy      Metal[28].\\nDespite      his      diverse      musical      engagements,      Hawkins      always      maintained   \\n \\na      close      allegiance      with      the      Foo  \\n \\n  Fighters,      which      remained      the      center      of      his      music      life[7][28].\\n#\\n \\n   Tragic      Passing  \\n \\n  Taylor      Hawkins,      the      esteemed      drummer      of      the      alt-rock      band      Foo      Fighters,      passed      away      suddenly      on      March  \\n \\n  25,      2022,      while      on      tour      with      his      band      in      Bogota,      Colombia[34].\\nThe      official      cause      of      death      was      cardiac  \\n \\n  arrest,      though      inquiries      were      raised      concerning      the      presence      of      drugs      in      his      system      and      their      potential  \\n \\n  contribution      to      his      death[33][34].\\nOn      the      night      of      his      passing,      paramedics      were      called      to      the      Four  \\n \\n  Seasons      hotel      in      Bogota      due      to      reports      of      chest      pain      from      an      unnamed      guest,      later      revealed      to      be  \\n \\n  Hawkins[34].\\nUnfortunately,      resuscitation      efforts      were      unsuccessful,      and      Hawkins      was      declared      dead      at  \\n \\n  the      scene[34].\\n \\n \\n  The      news      of      Hawkins’      sudden      demise      was      announced      on      the      morning      of      March      25th,      2022,      which      left      the      music  \\n \\n  world      in      shock[32].\\nThe      band      confirmed      the      news      with   \\n \\na      short      statement,      expressing      their      devastation  \\n \\n  at      the      loss      of      Hawkins,      whose      \"musical      spirit      and      infectious      laughter”      would      live      on      forever[32].\\n \\n \\n  As   \\n \\na      result      of      Hawkins’      untimely      passing,      the      band      canceled      their      ongoing      South      American      tour[33].\\nThe  \\n \\n  festival      stage      at      the      Estéreo      Picnic      Festival,      where      the      Foo      Fighters      were      scheduled      to      perform      that  \\n \\n  night,      was      transformed      into   \\n \\na      candlelight      vigil      in      memory      of      Hawkins[33].\\n##      Tributes      and      Remembrances  \\n \\n  In      the      wake      of      Hawkins’      death,      tributes      from      fans      and      colleagues      alike      poured      in      from      around      the  \\n \\n  world[21][31].\\nAmong      the      many      paying      their      respects      were      legendary      rock      and      roll      musicians      like      Roger  \\n \\n  Taylor,      the      drummer      of      Queen,      who      Hawkins      credited      with      inspiring      his      own      career      behind      the      drum      set[21].\\n \\n \\n  In      heartfelt      social      media      posts,      Taylor      described      Hawkins      as      an      \"inspirational      mentor”      and   \\n \\na      \"kind  \\n \\n  brilliant      man\"[21],      while      Led      Zeppelin’s      Jimmy      Page      reminisced      about      sharing      the      stage      with      Hawkins  \\n \\n  and      praised      his      \"technique,      energy      and      spirited      enthusiasm\"[21].\\n \\n \\n  There      were      also      numerous      onstage      tributes      to      Hawkins.\\nNotably,      Miley      Cyrus      expressed      her      grief      and      sent  \\n \\n  peaceful      wishes      to      the      Foo      Fighters      and      the      Hawkins      family      during   \\n \\na      performance      at      Lollapalooza[31].\\n \\n \\n  Similarly,      Liam      Gallagher      of      Oasis      dedicated      one      of      the      band’s      biggest      hits      to      Hawkins      during   \\n \\na      concert  \\n \\n  at      the      Royal      Albert      Hall      in      London[31].\\nFans      gathered      outside      the      hotel      where      Hawkins      died,      lighting      candles,      leaving      flowers,      and      singing      the  \\n \\n  band’s      songs      in      his      honor[31].\\n \\n \\n  Hawkins’      life      and      career      were      celebrated      in   \\n \\na      star-studded      tribute      concert      in      Los      Angeles,      which      saw  \\n \\n  performances      from      over      5@      musicians,      including      his      former      bands      and      colleagues      from      Def      Leppard,      Queen,  \\n \\n  and      Foo      Fighters[22].\\nTable      12:      STORM’s      generated      article      for      “Taylor      Hawkins”.\\n“#’,      “##”      indicate      the      section      title      and      subsection      title  \\n \\n  respectively.\\nNumbers      in      brackets      indicate      the      cited      references.')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'section_number': 0, 'section_title': 'Abstract'}, page_content='Abstract\\n \\n \\n  We      study      how      to      apply      large      language      models  \\n \\n  to      write      grounded      and      organized      long-form      ar-  \\n \\n  ticles      from      scratch,      with      comparable      breadth  \\n \\n  and      depth      to      Wikipedia      pages.\\nThis      underex-  \\n \\n  plored      problem      poses      new      challenges      at      the  \\n \\n  pre-writing      stage,      including      how      to      research  \\n \\n  the      topic      and      prepare      an      outline      prior      to      writ-  \\n \\n  ing.\\nWe      propose      STORM,   \\n \\na      writing      system  \\n \\n  for      the      Synthesis      of      Topic      Outlines      through  \\n \\n  Retrieval      and      Multi-perspective      Question      Ask-  \\n \\n  ing.\\nSTORM      models      the      pre-writing      stage      by\\n(1)      discovering      diverse      perspectives      in      research-  \\n \\n  ing      the      given      topic,      (2)      simulating      conversa-  \\n \\n  tions      where      writers      carrying      different      perspec-  \\n \\n  tives      pose      questions      to   \\n \\na      topic      expert      grounded  \\n \\n  on      trusted      Internet      sources,      (3)      curating      the      col-  \\n \\n  lected      information      to      create      an      outline.\\n \\n \\n  For      evaluation,      we      curate      FreshWiki,   \\n \\na      dataset  \\n \\n  of      recent      high-quality      Wikipedia      articles,      and       formulate      outline      assessments      to      evaluate      the  \\n \\n  pre-writing      stage.\\nWe      further      gather      feedback  \\n \\n  from      experienced      Wikipedia      editors.\\nCom-  \\n \\n  pared      to      articles      generated      by      an      outline-  \\n \\n  driven      retrieval-augmented      baseline,      more      of  \\n \\n  STORM’;      articles      are      deemed      to      be      organized  \\n \\n  (by   \\n \\na      25%      absolute      increase)      and      broad      in      cov-  \\n \\n  erage      (by      10%).\\nThe      expert      feedback      also  \\n \\n  helps      identify      new      challenges      for      generating  \\n \\n  grounded      long      articles,      such      as      source      bias  \\n \\n  transfer      and      over-association      of      unrelated      facts.\\n1\\n \\n   Introduction  \\n \\n  Large      language      models      (LLMs)      have      demonstrated  \\n \\n  impressive      writing      capabilities      (Yang      et      al.,      2023;  \\n \\n  Pavlik,      2023;      Wenzlaff      and      Spaeth,      2022;      Fitria,  \\n \\n  2023),      but      it      is      unclear      how      we      can      use      them      to  \\n \\n  write      grounded,      long-form      articles,      like      full-length  \\n \\n  Wikipedia      pages.\\nSuch      expository      writing,      which  \\n \\n  seeks      to      inform      the      reader      on   \\n \\na      topic      in      an      or-  \\n \\n  ganized      manner      (Weaver      II      and      Kintsch,      1991;  \\n \\n  Balepur      et      al.,      2023),      requires      thorough      research  \\n \\n  and      planning      in      the      pre-writing      stage      (Rohman,  \\n \\n  Writing  \\n \\n  tify,      evaluate,      and      organize      external      sources   \\n \\n-   \\n \\na      task  \\n \\n  that      is      challenging      even      for      experienced      writers.\\n \\n \\n  Automating      this      process      can      facilitate      individuals  \\n \\n  in      initiating      in-depth      learning      about   \\n \\na      topic      and       greatly      reduce      the      expensive      expert      hours      neces-  \\n \\n  sary      for      their      expository      writing.\\n |  |       ee      Prewriting | \\n | --- | --- | ---\\n |  | =\\n \\n   Full-length  \\n \\n  5      Article |       =      Full-length       5      Article\\n |       arXiv:2402.14207v2      [cs.CL]      8      Apr      2024 | 2022      Winter      Olympics      [=      Outline   \\n \\n|       Opening      Ceremony  \\n \\n  Research      via      Question      Asking  \\n \\n  (A)      Direct      Prompting  \\n \\n  -y      Prompt:      Ask      30      questions      about      the      given      topic.\\n \\n \\n  1.\\nWhen      was      the      opening      ceremony      held?\\n \\n \\n  {22}      2.\\nWhere      was      the      opening      ceremony      held?\\n \\n \\n  LLM      3.\\nHow      many      countries      participated      in      the      opening      ceremony?\\n \\n \\n  (B)      Perspective-Guided      Question      Asking  \\n \\n  Prompt:      You      are      an      event      planner      who      focuses      on      the B® preparation of the opening ceremony.\\n \\n \\n  1.\\nCan      you      provide      any      information      about      the      transportation  \\n \\n  arrangements      for      the      opening      ceremony?\\n \\n \\n  Lim      2.\\nCan      you      provide      any      information      about      the      budget      for      the  \\n \\n  2022      Winter      Olympics      opening      ceremony?\\n \\n \\n  (C)      Conversational      Question      Asking  \\n \\n  Can      you      provide      me      with   \\n \\na      list      of      the      participating      countries  \\n \\n  tim-      in      the      2022      Winter      Olympics      opening      ceremony?\\n \\n \\n  Role1  \\n \\n  The      2022      Winter      Olympics      featured   \\n \\na      diverse      group      of  \\n \\n  countries      participating      in      the      opening      ceremony.\\nThese LLM- included Athletes from over 90 countries will enter the  \\n \\n  Role2      stadium      ina      specific      order.\\n \\n \\n  How      is      the      order      of      participating      countries      in      the      2022  \\n \\n  Winter      Olympics      opening      ceremony      determined?\\n \\n \\n  LLM-  \\n \\n  Role1  \\n \\n  Figure      1:      We      explore      writing      Wikipedia-like      articles  \\n \\n  from      scratch,      which      demands   \\n \\na      pre-writing      stage      before  \\n \\n  producing      the      article.\\nIn      this      stage,      simpler      approaches  \\n \\n  like      Direct      Prompting      have      limited      planning      capacity.\\nIn  \\n \\n  contrast,      STORM      researches      the      topic      via      perspective-  \\n \\n  guided      question      asking      in      simulated      conversations.\\n \\n \\n  1965),      even      before      the      actual      writing      process      can  \\n \\n  start.\\nHowever,      prior      work      on      generating      Wikipedia  \\n \\n  articles      (Banerjee      and      Mitra,      2015;      Minguillén  \\n \\n  et      al.,      2017;      Liu      et      al.,      2018;      Fan      and      Gardent,  \\n \\n  2022)      has      generally      bypassed      the      pre-writing      stage:  \\n \\n  for      instance,      Liu      et      al.\\n(2018)      presume      reference  \\n \\n  documents      are      provided      in      advance,      while      Fan      and       Gardent      (2022)      assume      an      article      outline      is      avail-  \\n \\n  able      and      focus      on      expanding      each      section.\\nThese  \\n \\n  assumptions      do      not      hold      in      general,      as      collecting  \\n \\n  references      and      crafting      outlines      demand      advanced  \\n \\n  information      literacy      skills      (Doyle,      1994)      to      iden- | \\n\\n \\n \\n  We      explore      these      challenges      by      focusing      on      how  \\n \\n  to      generate      Wikipedia-like      articles      from      scratch.\\n \\n \\n  We      decompose      this      problem      into      two      tasks.\\nThe  \\n \\n  first      is      to      conduct      research      to      generate      an      outline,  \\n \\n  i.e.,   \\n \\na      list      of      multi-level      sections,      and      collect   \\n \\na      set      of  \\n \\n  reference      documents.\\nThe      second      uses      the      outline  \\n \\n  and      the      references      to      produce      the      full-length      arti-  \\n \\n  cle.\\nSuch   \\n \\na      task      decomposition      mirrors      the      human  \\n \\n  writing      process      which      usually      includes      phases      of  \\n \\n  pre-writing,      drafting,      and      revising      (Rohman,      1965;  \\n \\n  Munoz-Luna,      2015).\\n \\n \\n  As      pre-trained      language      models      inherently      pos-  \\n \\n  sess   \\n \\na      wealth      of      knowledge,   \\n \\na      direct      approach      is      to  \\n \\n  rely      on      their      parametric      knowledge      for      generating  \\n \\n  outlines      or      even      entire      articles      (Direct      Gen).\\nHow-  \\n \\n  ever,      this      approach      is      limited      by   \\n \\na      lack      of      details  \\n \\n  and      hallucinations      (Xu      et      al.,      2023),      particularly      in  \\n \\n  addressing      long-tail      topics      (Kandpal      et      al.,      2023).\\n \\n \\n  This      underscores      the      importance      of      leveraging      ex-  \\n \\n  ternal      sources,      and      current      strategies      often      involve  \\n \\n  retrieval-augmented      generation      (RAG),      which      cir-  \\n \\n  cles      back      to      the      problem      of      researching      the      topic      in  \\n \\n  the      pre-writing      stage,      as      much      information      cannot  \\n \\n  be      surfaced      through      simple      topic      searches.\\n \\n \\n  Human      learning      theories      (Tawfik      et      al.,      2020;  \\n \\n  Booth      et      al.,      2003)      highlight      asking      effective  \\n \\n  questions      in      information      acquisition.\\nAlthough  \\n \\n  instruction-tuned      models      (Ouyang      et      al.,      2022)      can  \\n \\n  be      prompted      directly      to      generate      questions,      we      find  \\n \\n  that      they      typically      produce      basic      “What”,      “When”,  \\n \\n  and      “Where”      questions      (Figure   \\n \\n1      (A))      which      often  \\n \\n  only      address      surface-level      facts      about      the      topic.\\nTo  \\n \\n  endow      LLMs      with      the      capacity      to      conduct      better  \\n \\n  research,      we      propose      the      STORM      paradigm      for  \\n \\n  the      Synthesis      of      Topic      Outlines      through      Retrieval  \\n \\n  and      Multi-perspective      Question      Asking.\\n \\n \\n  The      design      of      STORM      is      based      on      two      hypothe-  \\n \\n  ses:      (1)      diverse      perspectives      lead      to      varied      ques-  \\n \\n  tions;      (2)      formulating      in-depth      questions      requires  \\n \\n  iterative      research.\\nBuilding      upon      these      hypotheses,  \\n \\n  STORM      employs   \\n \\na      novel      multi-stage      approach.\\nIt  \\n \\n  first      discovers      diverse      perspectives      by      retrieving  \\n \\n  and      analyzing      Wikipedia      articles      from      similar      top-  \\n \\n  ics      and      then      personifies      the      LLM      with      specific      per-  \\n \\n  spectives      for      question      asking      (Figure   \\n \\n1      (B)).\\nNext,  \\n \\n  to      elicit      follow-up      questions      for      iterative      research  \\n \\n  (Figure   \\n \\n1      (C)),      STORM      simulates      multi-turn      con-  \\n \\n  versations      where      the      answers      to      the      generated      ques-  \\n \\n  tions      are      grounded      on      the      Internet.\\nFinally,      based  \\n \\n  on      the      LLM’s      internal      knowledge      and      the      collected  \\n \\n  information,      STORM      creates      an      outline      that      can  \\n \\n  be      expanded      section      by      section      to      develop   \\n \\na      full-  \\n \\n  length      Wikipedia-like      article.\\n \\n \\n  We      evaluate      STORM      using      our      FreshWiki  \\n \\n  dataset      (§2.1)      which      curates      recent,      high-quality  \\n \\n  Wikipedia      articles      to      avoid      data      leakage      during      pre-  \\n \\n  training.!\\nTo      facilitate      the      study      of      the      pre-writing  \\n \\n  stage,      we      define      metrics      for      evaluating      the      outline  \\n \\n  quality      against      human-written      articles.\\n \\n \\n  We      further      invited   \\n \\na      group      of      experienced  \\n \\n  Wikipedia      editors      for      expert      evaluation.\\nThe      ed-  \\n \\n  itors      found      STORM      outperforms      an      outline-driven  \\n \\n  RAG      baseline,      especially      regarding      the      breadth      and       organization      of      the      articles.\\nThey      also      identified  \\n \\n  challenges      for      future      research,      including      address-  \\n \\n  ing      cases      where:      (1)      the      bias      on      the      Internet      affects  \\n \\n  the      generated      articles;      (2)      LLMs      fabricate      connec-  \\n \\n  tions      between      unrelated      facts.\\nThese      challenges  \\n \\n  present      new      frontiers      to      grounded      writing      systems.\\n \\n \\n  Our      main      contributions      include:\\n*\\n \\n   To      evaluate      the      capacity      of      LLM      systems      at  \\n \\n  generating      long-form      grounded      articles      from  \\n \\n  scratch,      and      the      pre-writing      challenge      in      par-  \\n \\n  ticular,      we      curate      the      FreshWiki      dataset      and       establish      evaluation      criteria      for      both      outline  \\n \\n  and      final      article      quality.\\n \\n \\n \\n¢      We      propose      STORM,   \\n \\na      novel      system      that      au-  \\n \\n  tomates      the      pre-writing      stage.\\nSTORM      re-  \\n \\n  searches      the      topic      and      creates      an      outline      by  \\n \\n  using      LLMs      to      ask      incisive      questions      and      re-  \\n \\n  trieving      trusted      information      from      the      Internet.\\n \\n \\n \\n¢      Both      automatic      and      human      evaluation      demon-  \\n \\n  strate      the      effectiveness      of      our      approach.\\nEx-  \\n \\n  pert      feedback      further      reveals      new      challenges  \\n \\n  in      generating      grounded      long-form      articles.\\n2\\n \\n   FreshWiki  \\n \\n  We      study      generating      Wikipedia-like      articles      from  \\n \\n  scratch,      placing      emphasis      on      the      pre-writing  \\n \\n  stage      (Rohman,      1965),      which      involves      the      demand-  \\n \\n  ing      sub-tasks      of      gathering      and      curating      relevant  \\n \\n  information      (“‘research’’).      This      models      the      human ‘Our      resources      and      code      are      released      at      https:      //github.\\n \\n \\n  com/stanford-oval/storm.')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     strategy=\"sections\"\n",
    "from langchain_community.document_loaders.llmsherpa import LLMSherpaFileLoader\n",
    "loader = LLMSherpaFileLoader(\n",
    "    file_path=\"https://arxiv.org/pdf/2402.14207.pdf\",\n",
    "    new_indent_parser=True,\n",
    "    apply_ocr=True,\n",
    "    strategy=\"chunks\",\n",
    "    llmsherpa_api_url=\"http://localhost:5010/api/parseDocument?renderFormat=all\",\n",
    ")\n",
    "chunks = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 0, 'chunk_type': 'para'}, page_content='Abstract\\n \\n \\n  We      study      how      to      apply      large      language      models  \\n \\n  to      write      grounded      and      organized      long-form      ar-  \\n \\n  ticles      from      scratch,      with      comparable      breadth  \\n \\n  and      depth      to      Wikipedia      pages.\\nThis      underex-  \\n \\n  plored      problem      poses      new      challenges      at      the  \\n \\n  pre-writing      stage,      including      how      to      research  \\n \\n  the      topic      and      prepare      an      outline      prior      to      writ-  \\n \\n  ing.\\nWe      propose      STORM,   \\n \\na      writing      system  \\n \\n  for      the      Synthesis      of      Topic      Outlines      through  \\n \\n  Retrieval      and      Multi-perspective      Question      Ask-  \\n \\n  ing.\\nSTORM      models      the      pre-writing      stage      by\\n(1)      discovering      diverse      perspectives      in      research-  \\n \\n  ing      the      given      topic,      (2)      simulating      conversa-  \\n \\n  tions      where      writers      carrying      different      perspec-  \\n \\n  tives      pose      questions      to   \\n \\na      topic      expert      grounded  \\n \\n  on      trusted      Internet      sources,      (3)      curating      the      col-  \\n \\n  lected      information      to      create      an      outline.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 1, 'chunk_type': 'para'}, page_content='Abstract\\n \\n \\n  For      evaluation,      we      curate      FreshWiki,   \\n \\na      dataset  \\n \\n  of      recent      high-quality      Wikipedia      articles,      and       formulate      outline      assessments      to      evaluate      the  \\n \\n  pre-writing      stage.\\nWe      further      gather      feedback  \\n \\n  from      experienced      Wikipedia      editors.\\nCom-  \\n \\n  pared      to      articles      generated      by      an      outline-  \\n \\n  driven      retrieval-augmented      baseline,      more      of  \\n \\n  STORM’;      articles      are      deemed      to      be      organized  \\n \\n  (by   \\n \\na      25%      absolute      increase)      and      broad      in      cov-  \\n \\n  erage      (by      10%).\\nThe      expert      feedback      also  \\n \\n  helps      identify      new      challenges      for      generating  \\n \\n  grounded      long      articles,      such      as      source      bias  \\n \\n  transfer      and      over-association      of      unrelated      facts.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 2, 'chunk_type': 'para'}, page_content='Abstract\\n1\\n \\n   Introduction  \\n \\n  Large      language      models      (LLMs)      have      demonstrated  \\n \\n  impressive      writing      capabilities      (Yang      et      al.,      2023;  \\n \\n  Pavlik,      2023;      Wenzlaff      and      Spaeth,      2022;      Fitria,  \\n \\n  2023),      but      it      is      unclear      how      we      can      use      them      to  \\n \\n  write      grounded,      long-form      articles,      like      full-length  \\n \\n  Wikipedia      pages.\\nSuch      expository      writing,      which  \\n \\n  seeks      to      inform      the      reader      on   \\n \\na      topic      in      an      or-  \\n \\n  ganized      manner      (Weaver      II      and      Kintsch,      1991;  \\n \\n  Balepur      et      al.,      2023),      requires      thorough      research  \\n \\n  and      planning      in      the      pre-writing      stage      (Rohman,  \\n \\n  Writing  \\n \\n  tify,      evaluate,      and      organize      external      sources   \\n \\n-   \\n \\na      task  \\n \\n  that      is      challenging      even      for      experienced      writers.\\n \\n \\n  Automating      this      process      can      facilitate      individuals  \\n \\n  in      initiating      in-depth      learning      about   \\n \\na      topic      and       greatly      reduce      the      expensive      expert      hours      neces-  \\n \\n  sary      for      their      expository      writing.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 3, 'chunk_type': 'table'}, page_content='Abstract\\n |  |       ee      Prewriting | \\n | --- | --- | ---\\n |  | =\\n \\n   Full-length  \\n \\n  5      Article |       =      Full-length       5      Article\\n |       arXiv:2402.14207v2      [cs.CL]      8      Apr      2024 | 2022      Winter      Olympics      [=      Outline   \\n \\n|       Opening      Ceremony  \\n \\n  Research      via      Question      Asking  \\n \\n  (A)      Direct      Prompting  \\n \\n  -y      Prompt:      Ask      30      questions      about      the      given      topic.\\n \\n \\n  1.\\nWhen      was      the      opening      ceremony      held?\\n \\n \\n  {22}      2.\\nWhere      was      the      opening      ceremony      held?\\n \\n \\n  LLM      3.\\nHow      many      countries      participated      in      the      opening      ceremony?\\n \\n \\n  (B)      Perspective-Guided      Question      Asking  \\n \\n  Prompt:      You      are      an      event      planner      who      focuses      on      the B® preparation of the opening ceremony.\\n \\n \\n  1.\\nCan      you      provide      any      information      about      the      transportation  \\n \\n  arrangements      for      the      opening      ceremony?\\n \\n \\n  Lim      2.\\nCan      you      provide      any      information      about      the      budget      for      the  \\n \\n  2022      Winter      Olympics      opening      ceremony?\\n \\n \\n  (C)      Conversational      Question      Asking  \\n \\n  Can      you      provide      me      with   \\n \\na      list      of      the      participating      countries  \\n \\n  tim-      in      the      2022      Winter      Olympics      opening      ceremony?\\n \\n \\n  Role1  \\n \\n  The      2022      Winter      Olympics      featured   \\n \\na      diverse      group      of  \\n \\n  countries      participating      in      the      opening      ceremony.\\nThese LLM- included Athletes from over 90 countries will enter the  \\n \\n  Role2      stadium      ina      specific      order.\\n \\n \\n  How      is      the      order      of      participating      countries      in      the      2022  \\n \\n  Winter      Olympics      opening      ceremony      determined?\\n \\n \\n  LLM-  \\n \\n  Role1  \\n \\n  Figure      1:      We      explore      writing      Wikipedia-like      articles  \\n \\n  from      scratch,      which      demands   \\n \\na      pre-writing      stage      before  \\n \\n  producing      the      article.\\nIn      this      stage,      simpler      approaches  \\n \\n  like      Direct      Prompting      have      limited      planning      capacity.\\nIn  \\n \\n  contrast,      STORM      researches      the      topic      via      perspective-  \\n \\n  guided      question      asking      in      simulated      conversations.\\n \\n \\n  1965),      even      before      the      actual      writing      process      can  \\n \\n  start.\\nHowever,      prior      work      on      generating      Wikipedia  \\n \\n  articles      (Banerjee      and      Mitra,      2015;      Minguillén  \\n \\n  et      al.,      2017;      Liu      et      al.,      2018;      Fan      and      Gardent,  \\n \\n  2022)      has      generally      bypassed      the      pre-writing      stage:  \\n \\n  for      instance,      Liu      et      al.\\n(2018)      presume      reference  \\n \\n  documents      are      provided      in      advance,      while      Fan      and       Gardent      (2022)      assume      an      article      outline      is      avail-  \\n \\n  able      and      focus      on      expanding      each      section.\\nThese  \\n \\n  assumptions      do      not      hold      in      general,      as      collecting  \\n \\n  references      and      crafting      outlines      demand      advanced  \\n \\n  information      literacy      skills      (Doyle,      1994)      to      iden- | \\n'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 4, 'chunk_type': 'para'}, page_content='Abstract\\n \\n \\n  We      explore      these      challenges      by      focusing      on      how  \\n \\n  to      generate      Wikipedia-like      articles      from      scratch.\\n \\n \\n  We      decompose      this      problem      into      two      tasks.\\nThe  \\n \\n  first      is      to      conduct      research      to      generate      an      outline,  \\n \\n  i.e.,   \\n \\na      list      of      multi-level      sections,      and      collect   \\n \\na      set      of  \\n \\n  reference      documents.\\nThe      second      uses      the      outline  \\n \\n  and      the      references      to      produce      the      full-length      arti-  \\n \\n  cle.\\nSuch   \\n \\na      task      decomposition      mirrors      the      human  \\n \\n  writing      process      which      usually      includes      phases      of  \\n \\n  pre-writing,      drafting,      and      revising      (Rohman,      1965;  \\n \\n  Munoz-Luna,      2015).'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 5, 'chunk_type': 'para'}, page_content='Abstract\\n \\n \\n  As      pre-trained      language      models      inherently      pos-  \\n \\n  sess   \\n \\na      wealth      of      knowledge,   \\n \\na      direct      approach      is      to  \\n \\n  rely      on      their      parametric      knowledge      for      generating  \\n \\n  outlines      or      even      entire      articles      (Direct      Gen).\\nHow-  \\n \\n  ever,      this      approach      is      limited      by   \\n \\na      lack      of      details  \\n \\n  and      hallucinations      (Xu      et      al.,      2023),      particularly      in  \\n \\n  addressing      long-tail      topics      (Kandpal      et      al.,      2023).\\n \\n \\n  This      underscores      the      importance      of      leveraging      ex-  \\n \\n  ternal      sources,      and      current      strategies      often      involve  \\n \\n  retrieval-augmented      generation      (RAG),      which      cir-  \\n \\n  cles      back      to      the      problem      of      researching      the      topic      in  \\n \\n  the      pre-writing      stage,      as      much      information      cannot  \\n \\n  be      surfaced      through      simple      topic      searches.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 6, 'chunk_type': 'para'}, page_content='Abstract\\n \\n \\n  Human      learning      theories      (Tawfik      et      al.,      2020;  \\n \\n  Booth      et      al.,      2003)      highlight      asking      effective  \\n \\n  questions      in      information      acquisition.\\nAlthough  \\n \\n  instruction-tuned      models      (Ouyang      et      al.,      2022)      can  \\n \\n  be      prompted      directly      to      generate      questions,      we      find  \\n \\n  that      they      typically      produce      basic      “What”,      “When”,  \\n \\n  and      “Where”      questions      (Figure   \\n \\n1      (A))      which      often  \\n \\n  only      address      surface-level      facts      about      the      topic.\\nTo  \\n \\n  endow      LLMs      with      the      capacity      to      conduct      better  \\n \\n  research,      we      propose      the      STORM      paradigm      for  \\n \\n  the      Synthesis      of      Topic      Outlines      through      Retrieval  \\n \\n  and      Multi-perspective      Question      Asking.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 7, 'chunk_type': 'para'}, page_content='Abstract\\n \\n \\n  The      design      of      STORM      is      based      on      two      hypothe-  \\n \\n  ses:      (1)      diverse      perspectives      lead      to      varied      ques-  \\n \\n  tions;      (2)      formulating      in-depth      questions      requires  \\n \\n  iterative      research.\\nBuilding      upon      these      hypotheses,  \\n \\n  STORM      employs   \\n \\na      novel      multi-stage      approach.\\nIt  \\n \\n  first      discovers      diverse      perspectives      by      retrieving  \\n \\n  and      analyzing      Wikipedia      articles      from      similar      top-  \\n \\n  ics      and      then      personifies      the      LLM      with      specific      per-  \\n \\n  spectives      for      question      asking      (Figure   \\n \\n1      (B)).\\nNext,  \\n \\n  to      elicit      follow-up      questions      for      iterative      research  \\n \\n  (Figure   \\n \\n1      (C)),      STORM      simulates      multi-turn      con-  \\n \\n  versations      where      the      answers      to      the      generated      ques-  \\n \\n  tions      are      grounded      on      the      Internet.\\nFinally,      based  \\n \\n  on      the      LLM’s      internal      knowledge      and      the      collected  \\n \\n  information,      STORM      creates      an      outline      that      can  \\n \\n  be      expanded      section      by      section      to      develop   \\n \\na      full-  \\n \\n  length      Wikipedia-like      article.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 8, 'chunk_type': 'para'}, page_content='Abstract\\n \\n \\n  We      evaluate      STORM      using      our      FreshWiki  \\n \\n  dataset      (§2.1)      which      curates      recent,      high-quality  \\n \\n  Wikipedia      articles      to      avoid      data      leakage      during      pre-  \\n \\n  training.!\\nTo      facilitate      the      study      of      the      pre-writing  \\n \\n  stage,      we      define      metrics      for      evaluating      the      outline  \\n \\n  quality      against      human-written      articles.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 9, 'chunk_type': 'para'}, page_content='Abstract\\n \\n \\n  We      further      invited   \\n \\na      group      of      experienced  \\n \\n  Wikipedia      editors      for      expert      evaluation.\\nThe      ed-  \\n \\n  itors      found      STORM      outperforms      an      outline-driven  \\n \\n  RAG      baseline,      especially      regarding      the      breadth      and       organization      of      the      articles.\\nThey      also      identified  \\n \\n  challenges      for      future      research,      including      address-  \\n \\n  ing      cases      where:      (1)      the      bias      on      the      Internet      affects  \\n \\n  the      generated      articles;      (2)      LLMs      fabricate      connec-  \\n \\n  tions      between      unrelated      facts.\\nThese      challenges  \\n \\n  present      new      frontiers      to      grounded      writing      systems.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 10, 'chunk_type': 'para'}, page_content='Abstract\\n \\n \\n  Our      main      contributions      include:'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 11, 'chunk_type': 'list_item'}, page_content='Abstract\\n*\\n \\n   To      evaluate      the      capacity      of      LLM      systems      at  \\n \\n  generating      long-form      grounded      articles      from  \\n \\n  scratch,      and      the      pre-writing      challenge      in      par-  \\n \\n  ticular,      we      curate      the      FreshWiki      dataset      and       establish      evaluation      criteria      for      both      outline  \\n \\n  and      final      article      quality.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 12, 'chunk_type': 'para'}, page_content='Abstract\\n \\n \\n \\n¢      We      propose      STORM,   \\n \\na      novel      system      that      au-  \\n \\n  tomates      the      pre-writing      stage.\\nSTORM      re-  \\n \\n  searches      the      topic      and      creates      an      outline      by  \\n \\n  using      LLMs      to      ask      incisive      questions      and      re-  \\n \\n  trieving      trusted      information      from      the      Internet.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 13, 'chunk_type': 'para'}, page_content='Abstract\\n \\n \\n \\n¢      Both      automatic      and      human      evaluation      demon-  \\n \\n  strate      the      effectiveness      of      our      approach.\\nEx-  \\n \\n  pert      feedback      further      reveals      new      challenges  \\n \\n  in      generating      grounded      long-form      articles.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 14, 'chunk_type': 'para'}, page_content='Abstract\\n2\\n \\n   FreshWiki  \\n \\n  We      study      generating      Wikipedia-like      articles      from  \\n \\n  scratch,      placing      emphasis      on      the      pre-writing  \\n \\n  stage      (Rohman,      1965),      which      involves      the      demand-  \\n \\n  ing      sub-tasks      of      gathering      and      curating      relevant  \\n \\n  information      (“‘research’’).      This      models      the      human ‘Our      resources      and      code      are      released      at      https:      //github.\\n \\n \\n  com/stanford-oval/storm.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 15, 'chunk_type': 'para'}, page_content='Domain      Scope      Given      Given       P      Outline?      Refs?\\n \\n \\n  Balepur      et      al.\\n(2023)      One      One      para.\\n \\n \\n/      Yes  \\n \\n  Qian      et      al.\\n(2023)      All      One      para.\\n \\n \\n/      No  \\n \\n  Fan      and      Gardent      (2022)      One      Full      article      Yes      No  \\n \\n  Liu      et      al.\\n(2018)      All      One      para.\\n \\n \\n/      Yes  \\n \\n  Sauper      and      Barzilay      (2009)      Two      Full      article      No      No  \\n \\n  Ours      All      Full      article      No      No  \\n \\n  Table      1:      Comparison      of      different      Wikipedia      generation  \\n \\n  setups      in      existing      literature.\\nGenerating      one      paragraph  \\n \\n  does      not      need      an      article      outline.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 16, 'chunk_type': 'para'}, page_content='Domain      Scope      Given      Given       P      Outline?      Refs?\\n \\n \\n  writing      approach      which      has      prompted      some      educa-  \\n \\n  tors      to      view      Wikipedia      article      writing      as      an      educa-  \\n \\n  tional      exercise      for      academic      training      (Tardy,      2010).\\n \\n \\n  Table   \\n \\n1      compares      our      work      against      prior      bench-  \\n \\n  marks      for      Wikipedia      generation.\\nExisting      work  \\n \\n  has      generally      focused      on      evaluating      the      generation  \\n \\n  of      shorter      snippets      (e.g.,      one      paragraph),      within   \\n \\na       narrower      scope      (e.g.,   \\n \\na      specific      domain      or      two),      or  \\n \\n  when      an      explicit      outline      or      reference      documents  \\n \\n  are      supplied.\\n \\n \\nA      notable      example      is      WikiSum      (Liu  \\n \\n  et      al.,      2018),      which      treats      generating      Wikipedia      ar-  \\n \\n  ticles      as   \\n \\na      multi-document      summarization      problem,  \\n \\n  with      respect      to      the      reference      documents.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 17, 'chunk_type': 'para'}, page_content='Domain      Scope      Given      Given       P      Outline?      Refs?\\n \\n \\n  Our      setup      emphasizes      the      capability      of      long-  \\n \\n  form      grounded      writing      systems      to      research      and       curate      content.\\nSpecifically,      given   \\n \\na      topic      ¢,      the  \\n \\n  task      is      to      find   \\n \\na      set      of      references   \\n \\n®      and      generate a full-length article S = s1598,, where each  \\n \\n  sentence      s;      cites   \\n \\na      list      of      documents      in      R.7'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 18, 'chunk_type': 'para'}, page_content='Domain      Scope      Given      Given       P      Outline?      Refs? > 2.1      The      FreshWiki      Dataset\\naset  \\n \\n  Creating   \\n \\na      new      Wikipedia-like      article      demands      not  \\n \\n  only      fluent      writing      but      also      good      research      skills.\\nAs  \\n \\n  modern      LLMs      are      generally      trained      on      Wikipedia  \\n \\n  text,      we      mitigate      data      leakage      by      explicitly      seeking  \\n \\n  out      recent      Wikipedia      articles      that      were      created      (or  \\n \\n  very      heavily      edited)      after      the      training      cutoff      of      the  \\n \\n  LLMs      we      test.\\nOur      process      can      be      repeated      at  \\n \\n  future      dates      when      new      LLMs      emerge.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 19, 'chunk_type': 'para'}, page_content='Domain      Scope      Given      Given       P      Outline?      Refs? > 2.1      The      FreshWiki      Dataset\\n \\n \\n  To      apply      our      date      criteria,      we      focus      on      the      top  \\n \\n  100      most-edited      pages,      based      on      edit      counts,      for  \\n \\n  each      month      from      February      2022      to      September  \\n \\n  2023.      To      ensure      high-quality      references,      we      filter these      articles      to      keep      only      those      having      B-class  \\n \\n  quality      or      above      assessed      by      ORES*.\\nWe      also      ex-  \\n \\n  \"In      practice,      S      also      includes      organizational      elements      such  \\n \\n  as      section      and      subsection      titles,      which      do      not      require      citations.\\n \\n \\n  3      Obtained      from      https:      //wikimedia.\\n \\n \\n  org/api/rest_v1/metrics/edited-pages/  \\n \\n  top-by-edits/en.wikipedia/all-editor-types/  \\n \\n  content/      {year      }/{month}/all-days  \\n \\n  ‘https:      //www.mediawiki.org/wiki/ORES  \\n \\n  clude      list      articles      and      articles      that      have      no      sub-  \\n \\n  sections.\\nWhile      high-quality      Wikipedia      articles  \\n \\n  usually      contain      structured      data      (e.g.,      tables)      and      are  \\n \\n  multi-modal,      we      only      consider      the      plain      text      com-  \\n \\n  ponent      in      constructing      the      dataset      to      simplify      our  \\n \\n  task.\\nMore      details      of      the      dataset      are      in      Appendix      A.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 20, 'chunk_type': 'para'}, page_content='Domain      Scope      Given      Given       P      Outline?      Refs? > 2.2      Outline      Creation      and      Evaluation\\ntion  \\n \\n  A      full-length      article      is      hard      to      generate      or      evalu-  \\n \\n  ate      (Xu      et      al.,      2023;      Krishna      et      al.,      2023).\\nWhen  \\n \\n  human      educators      teach      students      academic      writing,  \\n \\n  they      sometimes      supervise      students      at      the      outline  \\n \\n  stage      (Eriksson      and      Makitalo,      2015)      because      an  \\n \\n  extensive      outline      indicates   \\n \\na      comprehensive      under-  \\n \\n  standing      of      the      topic      and      provides   \\n \\na      solid      founda-  \\n \\n  tion      for      writing      the      full-length      article      (Dietz      and       Foley,      2019).\\nInspired      by      this,      we      decompose      the  \\n \\n  generation      of      S      into      two      stages.\\nIn      the      pre-writing  \\n \\n  stage,      we      require      the      system      to      create      an      outline  \\n \\n  O,      which      is      defined      as   \\n \\na      list      of      multi-level      section  \\n \\n  headings®.\\nIn      the      writing      stage,      the      system      uses  \\n \\n  the      topic      t,      the      references      R,      and      an      outline   \\n \\nO      to  \\n \\n  produce      the      full-length      article      S.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 21, 'chunk_type': 'para'}, page_content='Domain      Scope      Given      Given       P      Outline?      Refs? > 2.2      Outline      Creation      and      Evaluation\\n \\n \\n  To      evaluate      the      outline      coverage,      we      introduce  \\n \\n  two      metrics:      heading      soft      recall      and      heading      en-  \\n \\n  tity      recall.\\nThese      metrics      compare      the      multi-level  \\n \\n  section      headings      of      the      human-written      article,      con-  \\n \\n  sidered      as      ground      truth,      and      those      in      O.      Recog-  \\n \\n  nizing      that      an      exact      match      between      elements      in  \\n \\n  these      two      sets      of      headings      is      unnecessary,      we      cal-  \\n \\n  culate      the      heading      soft      recall      (Franti      and      Mariescu-  \\n \\n  Istodor,      2023)      using      cosine      similarity      derived      from  \\n \\n  Sentence-BERT      (Reimers      and      Gurevych,      2019)      em-  \\n \\n  beddings      of      the      headings      (details      in      Appendix      C.1).\\n \\n \\n  We      also      compute      the      heading      entity      recall      which  \\n \\n  is      quantified      as      the      percentage      of      named      entities      in  \\n \\n  human-written      article      headings      covered      by      O.      We  \\n \\n  extract      entities      with      FLAIR      named      entity      recogni-  \\n \\n  tion      (NER)      (Akbik      et      al.,      2019).'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 22, 'chunk_type': 'para'}, page_content='Domain      Scope      Given      Given       P      Outline?      Refs? > 2.2      Outline      Creation      and      Evaluation\\n3\\n \\n   Method  \\n \\n  We      present      STORM      to      automate      the      pre-writing  \\n \\n  stage      by      researching   \\n \\na      given      topic      via      effective  \\n \\n  question      asking      (§3.1,      §3.2)      and      creating      an      out-  \\n \\n  line      (§3.3).\\nThe      outline      will      be      extended      to   \\n \\na      full-  \\n \\n  length      article      grounded      on      the      collected      references  \\n \\n  Shttps://en.wikipedia.org/wiki/Wikipedia:  \\n \\n  Stand-alone_lists  \\n \\n  ®Since      language      models      process      and      produce      sequences,  \\n \\n  we      can      linearize   \\n \\nO      by      adding      “#”      to      indicate      section      titles,  \\n \\n  “#4?”\\nto      indicate      subsection      titles,      etc.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 23, 'chunk_type': 'para'}, page_content='Topic      t       @      Identify       Perspectives       @      Survey       —      Writer       =o       —       Related      Articles      |\\n@\\n \\n   Direct      Generate Question   \\n \\nq    \\n \\n@      Split      Queries  \\n \\n  ©      Search   \\n \\n&      Sift  \\n \\n  ©      Synthesize |\\n \\n   Answer   \\n \\na      \\\\\\\\\\\\\\\\'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 24, 'chunk_type': 'table'}, page_content='Topic      t       @      Identify       Perspectives       @      Survey       —      Writer       =o       —       Related      Articles      |\\n |       \\\\\\\\\\\\\\\\       y      Gather       ‘\\\\\\\\\\\\\\\\      Add      Trusted | \\n | ¥       ,      Cy}\\n'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 25, 'chunk_type': 'table'}, page_content='Topic      t       @      Identify       Perspectives       @      Survey       —      Writer       =o       —       Related      Articles      |\\n |       Draft      Outline      Op | Conversations {Cg,       Refine |  | \\n | --- | --- | --- | ---\\n | \\\\\\\\\\\\\\\\      Sources       Ns\\n'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 26, 'chunk_type': 'para'}, page_content='References      R\\n(§3.4).\\nFigure   \\n \\n2      gives      an      overview      of      STORM      and       we      include      the      pseudo      code      in      Appendix      B.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 27, 'chunk_type': 'para'}, page_content='References      R > 3.1      Perspective-Guided      Question      Asking\\nking  \\n \\n  Rohman      (1965)      defines      pre-writing      as      the      stage  \\n \\n  of      discovery      in      the      writing      process.\\nIn      parallel  \\n \\n  with      stakeholder      theory      in      business      (Freeman      et      al.,  \\n \\n  2010),      where      diverse      stakeholders      prioritize      vary-  \\n \\n  ing      facets      of   \\n \\na      company,      individuals      with      distinct  \\n \\n  perspectives      may      concentrate      on      different      aspects  \\n \\n  when      researching      the      same      topic      and      discover      mul-  \\n \\n  tifaceted      information.\\nFurther,      the      specific      perspec-  \\n \\n  tives      can      serve      as      prior      knowledge,      guiding      individ-  \\n \\n  uals      to      ask      more      in-depth      questions.\\nFor      example,  \\n \\n  an      event      planner      might      ask      about      the      “‘transporta-  \\n \\n  tion      arrangements”      and      “budget”      for      “the      2022  \\n \\n  Winter      Olympics      opening      ceremony”,      whereas   \\n \\na       layperson      might      ask      more      general      questions      about  \\n \\n  the      event’s      basic      information      (Figure   \\n \\n1      (A)).'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 28, 'chunk_type': 'para'}, page_content='References      R > 3.1      Perspective-Guided      Question      Asking\\n \\n \\n  Given      the      input      topic      t,      STORM      discovers      differ-  \\n \\n  ent      perspectives      by      surveying      existing      articles      from  \\n \\n  similar      topics      and      uses      these      perspectives      to      control  \\n \\n  the      question      asking      process.\\nSpecifically,      STORM  \\n \\n  prompts      an      LLM      to      generate   \\n \\na      list      of      related      top-  \\n \\n  ics      and      subsequently      extracts      the      tables      of      contents  \\n \\n  from      their      corresponding      Wikipedia      articles,      if      such  \\n \\n  articles      can      be      obtained      through      Wikipedia      API’  \\n \\n  (Figure   \\n \\n2      (1).\\nThese      tables      of      contents      are      con- catenated      to      create   \\n \\na      context      to      prompt      the      LLM to identify N perspectives P = {p1,, pn} that  \\n \\n  be      evaluated      using   \\n \\na      rule-based      filter      according      to  \\n \\n  the      Wikipedia      guideline®      to      exclude      untrustworthy  \\n \\n  sources      (Figure   \\n \\n2      (5)).\\nFinally,      the      LLM      synthe-  \\n \\n  Thttps://pypi.org/project/Wikipedia-API/  \\n \\n  can      collectively      contribute      to   \\n \\na      comprehensive      ar-  \\n \\n  ticle      on   \\n \\n¢      (Figure   \\n \\n2      (2)).\\nTo      ensure      that      the      basic  \\n \\n  information      about   \\n \\n¢      is      also      covered,      we      add      pg      as  \\n \\n  “basic      fact      writer      focusing      on      broadly      covering      the  \\n \\n  basic      facts      about      the      topic”      into      P.      Each      perspec-  \\n \\n  tive      p   \\n \\n€      P      will      be      utilized      to      guide      the      LLM      in      the  \\n \\n  process      of      question      asking      in      parallel.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 29, 'chunk_type': 'para'}, page_content='References      R > 3.2      Simulating      Conversations\\nions  \\n \\n  The      theory      of      questions      and      question      asking      (Ram,  \\n \\n  1991)      highlights      that      while      answers      to      existing  \\n \\n  questions      contribute      to   \\n \\na      more      comprehensive  \\n \\n  understanding      of   \\n \\na      topic,      they      often      simultane-  \\n \\n  ously      give      rise      to      new      questions.\\nTo      kick      off      this  \\n \\n  dynamic      process,      STORM      simulates   \\n \\na      conversa-  \\n \\n  tion      between   \\n \\na      Wikipedia      writer      and   \\n \\na      topic      ex-  \\n \\n  pert.\\nIn      the      z-th      round      of      the      conversation,      the  \\n \\n  LLM-powered      Wikipedia      writer      generates   \\n \\na      sin-  \\n \\n  gle      question      q;      based      on      the      topic      1,      its      assigned  \\n \\n  perspective      p   \\n \\n€      P,      and      the      conversation      history  \\n \\n  {q1,      41,      ---,      Gi-1,      41-1}      where      a;      denotes      the      sim-  \\n \\n  ulated      expert’s      answer.\\nThe      conversation      history  \\n \\n  enables      the      LLM      to      update      its      understanding      of      the  \\n \\n  topic      and      ask      follow-up      questions.\\nIn      practice,      we  \\n \\n  limit      the      conversation      to      at      most   \\n \\n/      rounds.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 30, 'chunk_type': 'para'}, page_content='References      R > 3.2      Simulating      Conversations\\n \\n \\n  To      ensure      that      the      conversation      history      provides  \\n \\n  factual      information,      we      use      trusted      sources      from  \\n \\n  the      Internet      to      ground      the      answer      a;      to      each      query  \\n \\n  sizes      the      trustworthy      sources      to      generate      the      answer  \\n \\n  a;,      and      these      sources      will      also      be      added      to      R      for  \\n \\n  full      article      generation      (§3.4).'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 31, 'chunk_type': 'list_item'}, page_content='References      R > 3.2      Simulating      Conversations\\nq.      Since      q;      can      be      complicated,      we      first      prompt  \\n \\n  the      LLM      to      break      down      q;      into   \\n \\na      set      of      search  \\n \\n  queries      (Figure   \\n \\n2      (4))      and      the      searched      results      will'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 32, 'chunk_type': 'para'}, page_content='References      R > 3.3      Creating      the      Article      Outline\\nline  \\n \\n  After      thoroughly      researching      the      topic      through  \\n \\n  N   \\n \\n+   \\n \\n1      simulated      conversations,      denoted      as {Co, Ci, -,; Cw }, STORM creates an outline before  \\n \\n  the      actual      writing      starts.\\nTo      fully      leverage      the      inter-  \\n \\n  nal      knowledge      of      LLMs,      we      first      prompt      the      model  \\n \\n  to      generate   \\n \\na      draft      outline      Op      given      only      the      topic  \\n \\n  t      (Figure   \\n \\n2      (7)).\\nOp      typically      provides   \\n \\na      general  \\n \\n  but      organized      framework.\\nSubsequently,      the      LLM  \\n \\n  is      prompted      with      the      topic      ¢,      the      draft      outline      Op, and the simulated conversations {Co, Cj,,Cw}  \\n \\n  to      refine      the      outline      (Figure   \\n \\n2      (8)).\\nThis      results in      an      improved      outline   \\n \\nO      which      will      be      used      for  \\n \\n  producing      the      full-length      article.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 33, 'chunk_type': 'para'}, page_content='References      R > 3.4      Writing      the      Full-Length      Article\\nicle  \\n \\n  Building      upon      the      references      R      collected      and      the  \\n \\n  outline   \\n \\nO      developed      during      the      pre-writing      stage,  \\n \\n  the      full-length      article      can      be      composed      section      by  \\n \\n  section.\\nSince      it      is      usually      impossible      to      fit      the  \\n \\n  entire   \\n \\n7      within      the      context      window      of      the      LLM,  \\n \\n  we      use      the      section      title      and      headings      of      its      all-level  \\n \\n  subsections      to      retrieve      relevant      documents      from  \\n \\n  R      based      on      semantic      similarity      calculated      from  \\n \\n  Sentence-BERT      embeddings.\\nWith      the      relevant      in-  \\n \\n  formation      at      hand,      the      LLM      is      then      prompted      to  \\n \\n  generate      the      section      with      citations.\\nOnce      all      sec-  \\n \\n  tions      are      generated,      they      are      concatenated      to      form  \\n \\n  the      full-length      article.\\nSince      the      sections      are      gen-  \\n \\n  erated      in      parallel,      we      prompt      the      LLM      with      the  \\n \\n  concatenated      article      to      delete      repeated      information  \\n \\n  to      improve      coherence.\\nFurthermore,      in      alignment  \\n \\n  with      Wikipedia’s      stylistic      norms,      the      LLM      is      also  \\n \\n  utilized      to      synthesize   \\n \\na      summary      of      the      entire      arti-  \\n \\n  cle,      forming      the      lead      section      at      the      beginning.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 34, 'chunk_type': 'para'}, page_content='References      R > 3.4      Writing      the      Full-Length      Article\\n4\\n \\n   Experiments'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 35, 'chunk_type': 'para'}, page_content='References      R > 3.4      Writing      the      Full-Length      Article > 4.1      Article      Selection\\ntion  \\n \\n  STORM      is      capable      of      researching      complicated      top-  \\n \\n  ics      and      writing      long      articles      from      detailed      outlines.\\n \\n \\n  However,      in      this      controlled      experiment,      we      limit  \\n \\n  the      final      output      to      at      most      4000      tokens      (roughly  \\n \\n  3000      words).\\nFor   \\n \\na      meaningful      comparison,      we  \\n \\n  Shttps://en.wikipedia.\\norg/wiki/Wikipedia:  \\n \\n  Reliable_sources  \\n \\n  randomly      select      100      samples      from      the      Fresh      Wiki  \\n \\n  dataset      (see      §2.1)      that      have      human-written      articles  \\n \\n  not      exceeding      3000      words.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 36, 'chunk_type': 'para'}, page_content='References      R > 3.4      Writing      the      Full-Length      Article > 4.2      Automatic      Metrics\\nrics  \\n \\n  As      discussed      in      §2.2,      we      evaluate      the      outline      qual-  \\n \\n  ity      to      assess      the      pre-writing      stage      by      calculating  \\n \\n  the      heading      soft      recall      and      heading      entity      recall.\\n \\n \\nA       higher      recall      score      signifies   \\n \\na      more      comprehensive  \\n \\n  outline      relative      to      the      human-written      article.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 37, 'chunk_type': 'para'}, page_content='References      R > 3.4      Writing      the      Full-Length      Article > 4.2      Automatic      Metrics\\n \\n \\n  To      assess      the      full-length      article      quality,      we      adopt  \\n \\n  ROUGE      scores      (Lin,      2004)      and      compute      the      entity  \\n \\n  recall      in      the      article      level      based      on      FLAIR      NER  \\n \\n  results.\\nMoreover,      based      on      Wikipedia      criteria’,  \\n \\n  we      evaluate      the      article      from      the      aspects      of      (1)      In-  \\n \\n  terest      Level,      (2)      Coherence      and      Organization,      (3)  \\n \\n  Relevance      and      Focus,      (4)      Coverage,      and      (5)      Verifia-  \\n \\n  bility.\\nFor      aspects      (1)-(4),      we      use      Prometheus      (Kim  \\n \\n  et      al.,      2023),   \\n \\na      13B      evaluator      LLM      to      score      the      arti-  \\n \\n  cle      based      on   \\n \\na      5-point      rubric      collaboratively      devel-  \\n \\n  oped      with      two      experienced      Wikipedia      editors      (see  \\n \\n  Appendix      C.2).\\nFor      verifiability,      we      calculate      the  \\n \\n  citation      recall      and      citation      precision      based      on      the  \\n \\n  definition      in      Gao      et      al.\\n(2023).\\nWe      use      Mistral      7B-  \\n \\n  Instruct      (Jiang      et      al.,      2023a)      to      examine      whether  \\n \\n  the      cited      passages      entail      the      generated      sentence.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 38, 'chunk_type': 'para'}, page_content='References      R > 3.4      Writing      the      Full-Length      Article > 4.3      Baselines\\nines  \\n \\n  As      prior      works      use      different      setups      and      do      not      use  \\n \\n  LLMs,      they      are      hard      to      compare      directly.\\nInstead,  \\n \\n  we      use      the      following      three      LLM-based      baselines.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 39, 'chunk_type': 'list_item'}, page_content='References      R > 3.4      Writing      the      Full-Length      Article > 4.3      Baselines\\n1. Direct      Gen,   \\n \\na      baseline      that      directly      prompts  \\n \\n  the      LLM      to      generate      an      outline,      which      is      then  \\n \\n  used      to      generate      the      full-length      article.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 40, 'chunk_type': 'list_item'}, page_content='References      R > 3.4      Writing      the      Full-Length      Article > 4.3      Baselines\\n2. RAG,   \\n \\na      retrieval-augmented      generation      base-  \\n \\n  line      that      searches      with      the      topic      and      uses      the  \\n \\n  searched      results      together      with      the      topic   \\n \\n¢      to  \\n \\n  generate      an      outline      or      the      entire      article.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 41, 'chunk_type': 'list_item'}, page_content='References      R > 3.4      Writing      the      Full-Length      Article > 4.3      Baselines\\n3. Outline-driven      RAG      (ORAG),      which      is      iden-  \\n \\n  tical      to      RAG      in      outline      creation,      but      further  \\n \\n  searches      additional      information      with      section  \\n \\n  titles      to      generate      the      article      section      by      section.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 42, 'chunk_type': 'para'}, page_content='References      R > 3.4      Writing      the      Full-Length      Article > 4.4      STORM      Implementation\\ntion  \\n \\n  We      build      STORM      with      zero-shot      prompting      us-  \\n \\n  ing      the      DSPy      framework      (Khattab      et      al.,      2023).'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 43, 'chunk_type': 'para'}, page_content='References      R > 3.4      Writing      the      Full-Length      Article > 4.4      STORM      Implementation\\n \\n \\n  Appendix   \\n \\nB      includes      the      pseudo      code      and      corre-  \\n \\n  sponding      prompts.\\nThe      hyperparameters      N      and      MZ °https://en.wikipedia.org/wiki/Wikipedia:  \\n \\n  Good_article_criteria'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 44, 'chunk_type': 'para'}, page_content='ROUGE-1      ROUGE-L      Entity      Recall      Interest      Level      Organization      Relevance      Coverage\\n \\n \\n  Direct      Gen      25.62      12.63      5.08      2.87      4.60      3.10      4.16  \\n \\n  RAG      28.52      13.18      7.57      3.14      4.22      3.05      4.08  \\n \\n  oRAG      44.26      16.51      12.57      3.90      4.79      4.09      4.70 STORM      45.82      16.70      14.107      3.997      4.82      4.457      4.887  \\n \\n  w/o      Outline      Stage      26.77      12.77      7.39      3.33      4.87      3.35      4.37'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 45, 'chunk_type': 'para'}, page_content='Heading      Heading       Soft      Recall      Entity      Recall\\n \\n \\n  Direct      Gen      80.23      32.39  \\n \\n  RAG/oRAG      73.59      33.85  \\n \\n  GPT-3.5   \\n \\n=      RAG-expand      74.40      33.85  \\n \\n  STORM      86.267      40.527  \\n \\n  w/o      Perspective      84.49      40.12  \\n \\n  w/o      Conversation      77.97      31.98  \\n \\n  Direct      Gen      87.66      34.78  \\n \\n  RAG/oRAG      89.55      42.38  \\n \\n  GPT-4      RAG-expand      91.36      43.53  \\n \\n  STORM      92.737      45.91  \\n \\n  w/o      Perspective      92.39      42.70  \\n \\n  w/o      Conversation      88.75      39.30  \\n \\n  Table      3:      Results      of      outline      quality      evaluation      (%).\\n \\n \\n+      de-  \\n \\n  notes      significant      differences      (p   \\n \\n<      0.05)      from   \\n \\na      paired  \\n \\n  t-test      between      STORM      and      baselines.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 46, 'chunk_type': 'para'}, page_content='Heading      Heading       Soft      Recall      Entity      Recall\\n \\n \\n  in      STORM      are      both      set      as      5.      We      use      the      chat  \\n \\n  model      gpt-3.5-turbo      for      question      asking      and       use      gpt-3.5-turbo-instruct      for      other      parts      of  \\n \\n  STORM.\\nWe      also      experiment      with      using      gpt-4      for  \\n \\n  drafting      and      refining      the      outline      (Figure   \\n \\n2      ()8)).'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 47, 'chunk_type': 'para'}, page_content='Heading      Heading       Soft      Recall      Entity      Recall\\nFor      reported      results,      the      simulated      topic      expert      in  \\n \\n  STORM      is      grounded      on      the      You.com      search      API!°,  \\n \\n  although      the      proposed      pipeline      is      compatible      with  \\n \\n  other      search      engines.\\nThe      ground      truth      Wikipedia  \\n \\n  article      is      excluded      from      the      search      results.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 48, 'chunk_type': 'para'}, page_content='Heading      Heading       Soft      Recall      Entity      Recall\\n \\n \\n  For      final      article      generation,      we      only      report      the  \\n \\n  results      using      gpt-4      as      gpt-3.5      is      not      faithful      to  \\n \\n  sources      when      generating      text      with      citations      (Gao  \\n \\n  et      al.,      2023).\\nWe      set      temperature      as      1.0      and      top_p  \\n \\n  as      0.9      for      all      experiments.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 49, 'chunk_type': 'para'}, page_content='Heading      Heading       Soft      Recall      Entity      Recall\\n5\\n \\n   Results      and      Analysis'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 50, 'chunk_type': 'para'}, page_content='Heading      Heading       Soft      Recall      Entity      Recall > 5.1      Main      Results\\nults  \\n \\n  We      use      outline      coverage      as   \\n \\na      proxy      to      assess      the      pre-  \\n \\n  writing      stage      (see      §2.2).\\nTable   \\n \\n3      shows      the      heading  \\n \\n  soft      recall      and      entity      recall.\\nOutlines      directly      gen-  \\n \\n  erated      by      LLMs      (Direct      Gen)      already      demonstrate  \\n \\n  https:      //documentation.\\nyou.\\ncom/api-reference/  \\n \\n  search  \\n \\n  high      heading      soft      recall,      indicating      LLMs’      ability  \\n \\n  to      grasp      high-level      aspects      of   \\n \\na      topic      through      their  \\n \\n  rich      parametric      knowledge.\\nHowever,      STORM,      by  \\n \\n  asking      effective      questions      to      research      the      topic,      can  \\n \\n  create      higher      recall      outlines      that      cover      more      topic-  \\n \\n  specific      aspects.\\nNotably,      although      RAG      leverages  \\n \\n  additional      information,      presenting      unorganized      in-  \\n \\n  formation      in      the      context      window      makes      outline  \\n \\n  generation      more      challenging      for      the      weaker      model,  \\n \\n  i.e.,      GPT-3.5,      leading      to      worse      performance.\\nTo      test  \\n \\n  the      limit      of      the      RAG      baseline,      we      further      expand  \\n \\n  the      retrieved      sources      by      starting      with      the      outline  \\n \\n  produced      by      RAG,      using      its      section      titles      as      search  \\n \\n  queries      to      collect      more      sources,      and      inputting      the  \\n \\n  newly      collected      sources      together      with      the      initial  \\n \\n  outline      to      LLM      to      generate   \\n \\na      polished      outline.\\nThis  \\n \\n  modified      approach      is      referred      to      as      “RAG-expand”  \\n \\n  in      Table      3.\\nThe      experiment      results      indicate      that  \\n \\n  even      though      having      an      additional      round      of      search  \\n \\n  and      refinement      can      improve      the      outline      produced  \\n \\n  by      RAG,      our      proposed      STORM      still      surpasses      its  \\n \\n  performance.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 51, 'chunk_type': 'para'}, page_content='Heading      Heading       Soft      Recall      Entity      Recall > 5.1      Main      Results\\n \\n \\n  We      further      evaluate      the      full-length      article      quality.\\n \\n \\n  As      shown      in      Table      2,      oRAG      significantly      outper-  \\n \\n  forms      RAG,      highlighting      the      effectiveness      of      using  \\n \\n  outlines      for      structuring      full-length      article      genera-  \\n \\n  tion.\\nDespite      this      method’s      advantages      in      leverag-  \\n \\n  ing      retrieval      and      outlining,      our      approach      still      out-  \\n \\n  performs      it.\\nThe      effective      question      asking      mecha-  \\n \\n  nism      enhances      the      articles      with      greater      entity      recall.\\n \\n \\n  The      evaluator      LLM      also      rates      these      articles      with      sig-  \\n \\n  nificantly      higher      scores      in      the      aspects      of      “Interest  \\n \\n  Level’,      “Relevance      and      Focus’,      and      “Coverage”.\\n \\n \\n  Nonetheless,      we      acknowledge      the      possibility      of  \\n \\n  the      evaluator      LLM      overrating      machine-generated  \\n \\n  text.\\nOur      careful      human      evaluation      (§6)      reveals  \\n \\n  that      STORM      still      has      much      room      for      improvement.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 52, 'chunk_type': 'para'}, page_content='Heading      Heading       Soft      Recall      Entity      Recall > 5.1      Main      Results\\n \\n \\n  Although      this      work      primarily      focuses      on      the      pre-  \\n \\n  writing      stage      and      does      not      optimize      generating      text  \\n \\n  with      citations,      we      still      examine      the      citation      quality  \\n \\n  of      articles      produced      by      our      approach.\\nAs      reported Citation      Recall Citation      Precision oRAG      STORM      value  \\n \\n  Avg.      >4Rates      Av.g.\\n \\n \\n>   \\n \\n4      Rates      peval Table      4:      Citation      quality      judged      by      Mistral      7B-Instruct.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 53, 'chunk_type': 'list_item'}, page_content='Heading      Heading       Soft      Recall      Entity      Recall > 5.1      Main      Results\\n84.83 85.18  \\n \\n  STORM'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 54, 'chunk_type': 'table'}, page_content='Heading      Heading       Soft      Recall      Entity      Recall > 5.1      Main      Results\\n |  |       STORM      _      w/o      Perspective       w/o      Conversation | \\n | --- | --- | ---\\n |       IR|      99.83      54.36 |       39.56 |       Interest      Level      3.63      57.5%      4.03      70.0%      0.077       Organization      3.25      45.0%      4.00      70.0%      0.005       Relevance      3.93      62.5%      4.15      65.0%      0.347       Coverage      3.58      57.5%      4.00      67.5%      0.084       Verifiability      3.85      67.5%      3.80      67.5%      0.843       #Preferred      14      26\\n | Table      5:      Average      number      of      unique      references      (|R|)       collected      using      different      methods.\\n |       in      Table      4,      Mistral      7B-Instruct      judges      84.83%      of       the      sentences      are      supported      by      their      citations.      Ap-       pendix      C.3      investigates      the      unsupported      sentences       and      reveals      that      the      primary      issues      stem      from      draw-       ing      improper      inferences      and      inaccurate      paraphras-       ing,      rather      than      hallucinating      non-existent      contents.\\n | 5.2      Ablation      Studies\\n | 5.2      Ablation      Studies\\n |       As      introduced      in      §3,      STORM      prompts      LLMs      to       ask      effective      questions      by      discovering      specific       perspectives      and      simulating      multi-turn      conversa-       tions.      We      conduct      the      ablation      study      on      outline       creation      by      comparing      STORM      with      two      variants:\\n | (1)      “STORM      w/o      Perspective”,      which      omits      per-       spective      in      the      question      generation      prompt;      (2)       “STORM      w/o      Conversation”,      which      prompts      LLMs       to      generate      a      set      number      of      questions      altogether.      To       ensure      a      fair      comparison,      we      control      an      equal      total       number      of      generated      questions      across      all      variants.       Table      3      shows      the      ablation      results      and      full      STORM       pipeline      produces      outlines      with      the      highest      recall.       Also,      “STORM      w/o      Conversation”      gives      much       worse      results,      indicating      reading      relevant      informa-       tion      is      crucial      to      generating      effective      questions.      We       further      examine      how      many      unique      sources      are      col-       lected      in      ?      via      different      variants.      As      shown      in      Ta-       ble      5,      the      full      pipeline      discovers      more      different       sources      and      the      trend      is      in      accord      with      the      auto-       matic      metrics      for      outline      quality.       We      also      verify      whether      having      an      outline      stage       is      necessary      with      STORM.      In      Table      2,      “STORM       w/o      Outline      Stage”      denotes      the      results      of      generat-       ing      the      entire      article      given      the      topic      and      the      sim-       ulated      conversations.      Removing      the      outline      stage       significantly      deteriorates      the      performance      across       all      metrics.\\n | 6      Human      Evaluation\\n |       To      better      understand      the      strengths      and      weaknesses       of      STORM,      we      conduct      human      evaluation      by      col-       laborating      with      10      experienced      Wikipedia      editors       Table      6:      Human      evaluation      results      on      20      pairs      of      articles       generated      by      STORM      and      oRAG.      Each      pair      of      articles       is      evaluated      by      two      Wikipedia      editors.      The      ratings      are       given      on      a      scale      between      |      and      7,      with      values      >      4       indicating      good      quality      (see      Table      10).      We      conduct       paired      t-test      and      report      the      p-value.\\n |       who      have      made      at      least      500      edits      on      Wikipedia      and       have      more      than      |      year      of      experience.      We      randomly       sample      20      topics      from      our      dataset      and      evaluate      the       articles      generated      by      our      method      and      oRAG,      the       best      baseline      according      to      the      automatic      evaluation.\\n |       Each      pair      of      articles      is      assigned      to      2      editors.\\n |       We      request      editors      to      judge      each      article      from      the       same      five      aspects      defined      in      $4.2,      but      using      a      |      to       7      scale      for      more      fine-grained      evaluation.      While       our      automatic      evaluation      uses      citation      quality      as       a      proxy      to      evaluate      Verifiability,      we      stick      to      the       Wikipedia      standard      of      “verifiable      with      no      original       research”      in      human      evaluation.      Besides      rating      the       articles,      editors      are      asked      to      provide      open-ended       feedback      and      pairwise      preference.      After      the      evalua-       tion      finishes,      they      are      further      requested      to      compare       an      article      produced      by      our      method,      which      they      have       just      reviewed,      with      its      human-written      counterpart,       and      report      their      perceived      usefulness      of      STORM       using      a      1-5      Likert      scale.      More      human      evaluation      de-       tails      are      included      in      Appendix      D.      Table      6      presents       the      rating      and      pairwise      comparison      results.!!\\n |       Articles      produced      by      STORM      exhibit      greater       breadth      and      depth      than      oRAG      outputs.      In      ac-       cord      with      the      finding      in      §5.1,      editors      judge      articles       produced      by      STORM      as      more      interesting,      orga-       nized,      and      having      broader      coverage      compared      to       oRAG      outputs.      Specifically,      25%      more      articles      pro-       duced      by      STORM      are      considered      organized      (Orga-       nization      rating      >      4),      and      10%      more      are      deemed      to       have      good      coverage      (Coverage      rating      >      4).      Even       in      comparison      with      human-written      articles,      one       editor      praises      our      result      as      providing      “a      bit      more\\n |       \"For      the      1-7      scale      rating      results      on      each      criterion,      we      cal-       culate      the      Krippendorff’s      Alpha      to      measure      the      inter      annotator       agreement      (IAA),      and      the      results      are      as      follows:      Interest      Level       (0.349),      Organization      (0.221),      Relevance      (0.256),      Coverage       (0.346),      Verifiability      (0.388).\\n'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 55, 'chunk_type': 'para'}, page_content='Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree\\n \\n \\n \\nI      think      it      can      be      specifically      helpful  \\n \\n  wae      70%      30%  \\n \\n  for      my      pre-writing      stage.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 56, 'chunk_type': 'para'}, page_content='Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree\\nI\\n \\n   think      it      will      help      me      edit   \\n \\na      Wikipedia      anes   \\n \\n3      oars      30%  \\n \\n  article      for   \\n \\na      new      topic.\\n \\n \\n= I      think      it      can      be   \\n \\na      potentially      useful      10%      20%   \\n \\n:      60%      10%  \\n \\n  tool      for      the      Wikipedia      community.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 57, 'chunk_type': 'para'}, page_content='Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree\\nFigure      3:      Survey      results      of      the      perceived      usefulness      of  \\n \\n  STORM      (n   \\n \\n=      10).'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 58, 'chunk_type': 'para'}, page_content='Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree\\n \\n \\n  background      information”      and      another      notes      that      “I  \\n \\n  found      that      the      AI      articles      had      more      depth      compared  \\n \\n  to      the      Wikipedia      articles”.\\nSTORM      also      outper-  \\n \\n  forms      the      best      baseline      in      pairwise      comparison.\\n \\n \\n  More      information      in      |R|      poses      challenges      be-  \\n \\n  yond      factual      hallucination.\\nWe      examine      14      pair-  \\n \\n  wise      comparison      responses      where      editors      prefer  \\n \\n  oORAG      outputs      over      STORM.\\nExcluding   \\n \\n3      cases  \\n \\n  where      pairwise      preferences      do      not      align      with      their  \\n \\n  ratings,      editors      assign      lower      Verifiability      scores      to  \\n \\n  articles      from      our      approach      in      over      50%      of      the      cases.\\n \\n \\n  Through      analyzing      the      articles      and      editors’      free-  \\n \\n  form      feedback,      we      discover      that      low      Verifiability  \\n \\n  scores      stem      from      red      herring      fallacy      or      overspec-  \\n \\n  ulation      issues.\\nThese      arise      when      the      generated  \\n \\n  articles      introduce      unverifiable      connections      between  \\n \\n  different      pieces      of      information      in      |7?|      or      between  \\n \\n  the      information      and      the      topic      (examples      included  \\n \\n  in      Table      11).\\nCompared      to      the      widely      discussed  \\n \\n  factual      hallucination      (Shuster      et      al.,      2021;      Huang  \\n \\n  et      al.,      2023),      addressing      such      verifiability      issues      is  \\n \\n  more      nuanced,      surpassing      basic      fact-checking      (Min  \\n \\n  et      al.,      2023).'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 59, 'chunk_type': 'para'}, page_content='Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree\\n \\n \\n  Generated      articles      trail      behind      well-revised      hu-  \\n \\n  man      works.\\nWhile      STORM      outperforms      the  \\n \\n  oRAG      baseline,      editors      comment      that      the      generated  \\n \\n  articles      are      less      informative      than      actual      Wikipedia  \\n \\n  pages.\\nAnother      major      issue      identified      is      the      trans-  \\n \\n  fer      of      bias      and      tone      from      Internet      sources      to      the  \\n \\n  generated      article,      with   \\n \\n7      out      of      10      editors      men-  \\n \\n  tioning      that      the      STORM-generated      articles      sound  \\n \\n  “emotional”      or      “unneutral”.\\nMore      analysis      is      dis-  \\n \\n  cussed      in      Appendix      E.      This      feedback      suggests      that  \\n \\n  reducing      the      retrieval      bias      in      the      pre-writing      stage  \\n \\n  is   \\n \\na      worthwhile      direction      for      future      work.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 60, 'chunk_type': 'para'}, page_content='Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree\\n \\n \\n  Generated      articles      are   \\n \\na      good      starting      point.\\nAs  \\n \\n  shown      in      Figure      3,      editors      are      unanimous      in      agree-  \\n \\n  ing      that      STORM      can      aid      them      in      their      pre-writing  \\n \\n  stage.\\nIt      is      gratifying      to      know      that      the      tool      is      help-  \\n \\n  ful      to      experienced      editors.\\n80%      of      the      editors      think  \\n \\n  that      STORM      can      help      them      edit   \\n \\na      Wikipedia      article  \\n \\n  for   \\n \\na      new      topic.\\nMore      reservation      is      expressed      to  \\n \\n  the      usefulness      of      STORM      for      the      Wikipedia      com-  \\n \\n  munity      at      large;      nonetheless,      70%      of      the      editors  \\n \\n  think      it      is      useful,      with      only      10%      disagreeing.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 61, 'chunk_type': 'para'}, page_content='Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree\\n7\\n \\n   Related      Works  \\n \\n  Retrieval-Augmented      Generation      (RAG)      Aug-  \\n \\n  menting      language      models      (LMs)      with      retrieval      at  \\n \\n  inference      time      is   \\n \\na      typical      way      to      leverage      exter-  \\n \\n  nal      knowledge      stores      (Ram      et      al.,      2023;      Izacard  \\n \\n  et      al.,      2023).\\nWhile      some      works      use      retrieval  \\n \\n  to      construct      demonstrations      for      in-context      learn-  \\n \\n  ing      (Li      et      al.,      2023;      Liu      et      al.,      2022;      Agrawal      et      al.,  \\n \\n  2023;      Poesia      et      al.,      2022;      Shi      et      al.,      2022;      Khattab  \\n \\n  et      al.,      2022),      another      line      of      works      uses      retrieval      to  \\n \\n  provide      additional      information      for      LMs      to      ground  \\n \\n  on.\\nLewis      et      al.\\n(2020)      study      RAG      on      knowledge-  \\n \\n  intensive      NLP      tasks      and      find      it      improves      diver-  \\n \\n  sity      and      factuality.\\nSemnani      et      al.\\n(2023)      de-  \\n \\n  signs   \\n \\na      RAG-based      chatbot      grounded      on      English  \\n \\n  Wikipedia      to      stop      LLM-based      chatbots      from      hal-  \\n \\n  lucination.\\nBesides,      RAG      can      be      used      to      generate  \\n \\n  text      with      citations      (Menick      et      al.,      2022;      Gao      et      al.,  \\n \\n  2023)      and      build      attributed      question      answering      sys-  \\n \\n  tems      (Bohnet      et      al.,      2023).\\nWhile      RAG      is      widely  \\n \\n  studied      in      question      answering,      how      to      use      it      for  \\n \\n  long-form      article      generation      is      less      investigated.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 62, 'chunk_type': 'para'}, page_content='Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree\\n \\n \\n  As   \\n \\na      general      framework,      RAG      is      flexible      in      both  \\n \\n  the      retrieval      source      and      time.\\nThe      retrieval      sources  \\n \\n  can      vary      from      domain      databases      (Zakka      et      al.,  \\n \\n  2023),      code      documentation      (Zhou      et      al.,      2023),  \\n \\n  to      the      whole      Internet      (Nakano      et      al.,      2022;      Komeili  \\n \\n  et      al.,      2022).\\nRegarding      the      time,      besides   \\n \\na      one-  \\n \\n  time      retrieval      before      generation,      the      system      can      be  \\n \\n  designed      to      self-decide      when      to      retrieve      across      the  \\n \\n  course      of      the      generation      (Jiang      et      al.,      2023b;      Parisi  \\n \\n  et      al.,      2022;      Shuster      et      al.,      2022;      Yao      et      al.,      2023).\\n \\n \\n  Automatic      Expository      Writing      Different      from  \\n \\n  other      types      of      long-form      generation      (Yang      et      al.,  \\n \\n  2022;      Feng      et      al.,      2018),      automatic      expository      writ-  \\n \\n  ing      requires      grounding      on      external      documents      and       leveraging      the      interplay      between      reading      and      writ-  \\n \\n  ing.\\nBalepur      et      al.\\n(2023)      propose      the      Imitate-  \\n \\n  Retrieve-Paraphrase      framework      for      expository      writ-  \\n \\n  ing      at      the      paragraph      level      to      address      the      challenges  \\n \\n  in      synthesizing      information      from      multiple      sources.\\n \\n \\n  Beyond      summarizing      sources,      Shen      et      al.\\n(2023)  \\n \\n  highlight      that      expository      writing      requires      the      au-  \\n \\n  thor’s      sensemaking      process      over      source      documents  \\n \\n  and      good      outline      planning.\\nWe      tackle      these      chal-  \\n \\n  lenges      by      focusing      on      the      pre-writing      stage.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 63, 'chunk_type': 'para'}, page_content='Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree\\n \\n \\n  Question      Asking      in      NLP      Question      asking      capa-  \\n \\n  bilities      in      NLP      systems      have      expanded      across      sev-  \\n \\n  eral      fronts,      including      generating      clarification      ques-  \\n \\n  tions      to      understand      user      intents      (Aliannejadi      et      al.,  \\n \\n  2019;      Rahmani      et      al.,      2023),      and      breaking      large  \\n \\n  questions      into      smaller      ones      to      improve      composi-  \\n \\n  tional      reasoning      (Press      et      al.,      2023).\\nWhile      humans  \\n \\n  usually      ask      questions      to      learn      new      knowledge      (Taw-  \\n \\n  fik      et      al.,      2020;      Booth      et      al.,      2003),      how      to      opti-  \\n \\n  mize      question      informativeness      and      specificity      in  \\n \\n  information-seeking      conversations      remains      less      ex-  \\n \\n  plored.\\nThe      closest      work      is      Qi      et      al.\\n(2020)      which  \\n \\n  defines      the      question      informativeness      using      the      un-  \\n \\n  igram      precision      function      and      uses      reinforcement  \\n \\n  learning      to      increase      the      question      informativeness.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 64, 'chunk_type': 'para'}, page_content='Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree\\n8\\n \\n   Conclusion  \\n \\n  We      propose      STORM,      an      LLM-based      writing      sys-  \\n \\n  tem      that      automates      the      pre-writing      stage      for      creat-  \\n \\n  ing      Wikipedia-like      articles      from      scratch.\\nWe      cu-  \\n \\n  rate      the      FreshWiki      dataset      and      establish      evaluation  \\n \\n  criteria      to      study      the      generation      of      grounded      long-  \\n \\n  form      articles.\\nExperimental      results      demonstrate  \\n \\n  that      the      question      asking      mechanism      in      STORM  \\n \\n  improves      both      the      outline      and      article      quality.\\nWith  \\n \\n  the      improved      breadth      and      depth,      STORM      helps  \\n \\n  surface      new      challenges      for      grounded      writing      sys-  \\n \\n  tems      through      expert      evaluation.\\nThe      experienced  \\n \\n  Wikipedia      editors      in      our      study      unanimously      agree  \\n \\n  that      STORM      is      helpful      for      their      pre-writing      stage.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 65, 'chunk_type': 'para'}, page_content='Limitations\\n \\n \\n  In      this      work,      we      explore      generating      Wikipedia-  \\n \\n  like      articles      from      scratch      as   \\n \\na      way      to      push      the  \\n \\n  frontier      of      automatic      expository      writing      and      long-  \\n \\n  form      article      generation.\\nWhile      our      approach      sig-  \\n \\n  nificantly      outperforms      baseline      methods      in      both  \\n \\n  automatic      and      human      evaluations,      the      quality      of  \\n \\n  machine-written      articles      still      lags      behind      well-  \\n \\n  revised      human-authored      articles,      specifically      in  \\n \\n  aspects      of      neutrality      and      verifiability.\\nAlthough  \\n \\n  STORM      discovers      different      perspectives      in      re-  \\n \\n  searching      the      given      topic,      the      collected      information  \\n \\n  may      still      be      biased      towards      dominant      sources      on  \\n \\n  the      Internet      and      may      contain      promotional      content.\\n \\n \\n  Moreover,      the      verifiability      issues      identified      in      this  \\n \\n  work      go      beyond      factual      hallucination,      which      high-  \\n \\n  lights      new      challenges      to      grounded      writing      systems.\\n \\n \\n  Another      limitation      of      this      work      is      that      although  \\n \\n  we      focus      on      the      task      of      generating      Wikipedia-like  \\n \\n  articles      from      scratch,      our      task      setup      is      still      simpli-  \\n \\n  fied      to      only      consider      the      generation      of      free-form  \\n \\n  text.\\nHuman-authored      high-quality      Wikipedia      ar-  \\n \\n  ticles      usually      contain      structured      data      and      multi-  \\n \\n  modal      information.\\nWe      leave      the      exploration      of  \\n \\n  generating      multi-modal      grounded      articles      for      fu-  \\n \\n  ture      work.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 66, 'chunk_type': 'para'}, page_content='Acknowledgements\\n \\n \\n  We      thank      You.com      for      generously      providing      the  \\n \\n  search      API      that      supported      our      experiments.\\nWe  \\n \\n  also      thank      Sina      J.      Semnani,      Shicheng      Liu,      Eric      Ze-  \\n \\n  likman      for      providing      helpful      feedback      and      the      ACL  \\n \\n  ARR      reviewers      for      their      valuable      comments.\\nThis  \\n \\n  work      is      supported      in      part      by      the      Verdant      Founda-  \\n \\n  tion      and      Microsoft      Azure      AI      credits.\\nYijia      Shao  \\n \\n  is      supported      by   \\n \\na      Stanford      School      of      Engineering  \\n \\n  Fellowship.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 67, 'chunk_type': 'para'}, page_content='Ethics      Statement\\n \\n \\n  Different      from      the      creative      generation,      grounded      ar-  \\n \\n  ticle      generation      may      impact      how      people      learn      about  \\n \\n  topics      or      consume      source      information.\\nAll      the      stud-  \\n \\n  ies      and      the      evaluation      in      this      work      are      designed  \\n \\n  to      prevent      the      dissemination      of      misinformation      by  \\n \\n  not      publishing      generated      content      online      and      im-  \\n \\n  plementing      strict      accuracy      checks.\\nWe      avoid      any  \\n \\n  disruption      to      Wikipedia      or      related      communities,      as  \\n \\n  our      system      does      not      interact      with      live      pages.\\nAlso,  \\n \\n  although      we      try      to      generate      grounded      articles,      we  \\n \\n  believe      there      is      no      privacy      issue      related      to      this      work  \\n \\n  as      we      only      use      information      publicly      available      on  \\n \\n  the      Internet.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 68, 'chunk_type': 'para'}, page_content='Ethics      Statement\\n \\n \\n  The      primary      risk      of      our      work      is      that      the  \\n \\n  Wikipedia      articles      written      by      our      system      are  \\n \\n  grounded      on      information      on      the      Internet      which  \\n \\n  contains      some      biased      or      discriminative      content      on  \\n \\n  its      own.\\nCurrently,      our      system      relies      on      the      search  \\n \\n  engine      to      retrieve      information      but      does      not      include  \\n \\n  any      post-processing      module.\\nWe      believe      improv-  \\n \\n  ing      the      retrieval      module      to      have      good      coverage      of  \\n \\n  different      viewpoints      and      adding   \\n \\na      content      sifting  \\n \\n  module      to      the      current      system      will      be   \\n \\na      critical      next  \\n \\n  step      to      achieve      better      neutrality      and      balance      in      the  \\n \\n  generated      articles.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 69, 'chunk_type': 'para'}, page_content='Ethics      Statement\\n \\n \\n  Another      limitation      we      see      from      an      ethical      point  \\n \\n  of      view      is      that      we      only      consider      writing      English  \\n \\n  Wikipedia      articles      in      this      work.\\nExtending      the      cur-  \\n \\n  rent      system      to   \\n \\na      multilingual      setup      is   \\n \\na      meaningful  \\n \\n  direction      for      future      work      as      more      topics      do      not      have  \\n \\n  Wikipedia      pages      in      non-English      languages.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 70, 'chunk_type': 'para'}, page_content='References\\n \\n \\n  Sweta      Agrawal,      Chunting      Zhou,      Mike      Lewis,      Luke  \\n \\n  Zettlemoyer,      and      Marjan      Ghazvininejad.\\n2023.\\nIn-  \\n \\n  context      examples      selection      for      machine      translation.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 71, 'chunk_type': 'para'}, page_content='References\\n \\n \\n  In      Findings      of      the      Association      for      Computational  \\n \\n  Linguistics:      ACL      2023,      pages      8857-8873,      Toronto,  \\n \\n  Canada.\\nAssociation      for      Computational      Linguistics.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 72, 'chunk_type': 'para'}, page_content='References\\n \\n \\n  Alan      Akbik,      Tanja      Bergmann,      Duncan      Blythe,      Kashif  \\n \\n  Rasul,      Stefan      Schweter,      and      Roland      Vollgraf.\\n2019.  \\n \\n  FLAIR:      An      easy-to-use      framework      for      state-of-the-  \\n \\n  art      NLP.\\nIn      Proceedings      of      the      2019      Conference      of  \\n \\n  the      North      American      Chapter      of      the      Association      for  \\n \\n  Computational      Linguistics      (Demonstrations),      pages  \\n \\n  54-59,      Minneapolis,      Minnesota.\\nAssociation      for  \\n \\n  Computational      Linguistics.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 73, 'chunk_type': 'para'}, page_content='References\\n \\n \\n  Mohammad      Aliannejadi,      Hamed      Zamani,      Fabio  \\n \\n  Crestani,      and   \\n \\nW      Bruce      Croft.\\n2019.\\nAsking      clari-  \\n \\n  fying      questions      in      open-domain      information-seeking  \\n \\n  conversations.\\nIn      Proceedings      of      the      42nd      interna-  \\n \\n  tional      acm      sigir      conference      on      research      and      develop-  \\n \\n  ment      in      information      retrieval,      pages      475-484.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 74, 'chunk_type': 'para'}, page_content='References\\n \\n \\n  Nishant      Balepur,      Jie      Huang,      and      Kevin      Chang.\\n2023.  \\n \\n  Expository      text      generation:      Imitate,      retrieve,      para-  \\n \\n  phrase.\\nIn      Proceedings      of      the      2023      Conference      on  \\n \\n  Empirical      Methods      in      Natural      Language      Process-  \\n \\n  ing,      pages      11896-11919,      Singapore.\\nAssociation      for  \\n \\n  Computational      Linguistics.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 75, 'chunk_type': 'para'}, page_content='References\\n \\n \\n  Siddhartha      Banerjee      and      Prasenjit      Mitra.\\n2015.  \\n \\n  WikiKreator:      Improving      Wikipedia      stubs      automat-  \\n \\n  ically.\\nIn      Proceedings      of      the      53rd      Annual      Meet-  \\n \\n  ing      of      the      Association      for      Computational      Linguis-  \\n \\n  tics      and      the      7th      International      Joint      Conference      on  \\n \\n  Natural      Language      Processing      (Volume      1:      Long      Pa-  \\n \\n  pers),      pages      867-877,      Beijing,      China.\\nAssociation  \\n \\n  for      Computational      Linguistics.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 76, 'chunk_type': 'para'}, page_content='References\\n \\n \\n  Bernd      Bohnet,      Vinh      Q.      Tran,      Pat      Verga,      Roee      Aha-  \\n \\n  roni,      Daniel      Andor,      Livio      Baldini      Soares,      Massimil-  \\n \\n  iano      Ciaramita,      Jacob      Eisenstein,      Kuzman      Ganchev,  \\n \\n  Jonathan      Herzig,      Kai      Hui,      Tom      Kwiatkowski,      Ji      Ma,  \\n \\n  Jianmo      Ni,      Lierni      Sestorain      Saralegui,      Tal      Schus-  \\n \\n  ter,      William      W.      Cohen,      Michael      Collins,      Dipanjan  \\n \\n  Das,      Donald      Metzler,      Slav      Petrov,      and      Kellie      Webster.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 77, 'chunk_type': 'para'}, page_content='References\\n \\n \\n  2023.\\nAttributed      question      answering:      Evaluation      and       modeling      for      attributed      large      language      models.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 78, 'chunk_type': 'para'}, page_content='References\\n \\n \\n  Wayne      C      Booth,      Gregory      G      Colomb,      and      Joseph      M       Williams.\\n2003.\\nThe      craft      of      research.\\nUniversity      of  \\n \\n  Chicago      press.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 79, 'chunk_type': 'para'}, page_content='References\\n \\n \\n  Laura      Dietz      and      John      Foley.\\n2019.\\nTrec      car      y3:      Com-  \\n \\n  plex      answer      retrieval      overview.\\nIn      Proceedings      of  \\n \\n  Text      REtrieval      Conference      (TREC).'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 80, 'chunk_type': 'para'}, page_content='References\\n \\n \\n  Christina      S      Doyle.\\n1994.\\nInformation      literacy      in      an  \\n \\n  information      society:   \\n \\nA      concept      for      the      information  \\n \\n  age.\\nDiane      Publishing.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 81, 'chunk_type': 'para'}, page_content='References\\n \\n \\n  Ann-Marie      Eriksson      and      Asa      Mikitalo.\\n2015.\\nSupervi-  \\n \\n  sion      at      the      outline      stage:      Introducing      and      encounter-  \\n \\n  ing      issues      of      sustainable      development      through      aca-  \\n \\n  demic      writing      assignments.\\nText   \\n \\n&      Talk,      35(2):123-  \\n \\n  153.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 82, 'chunk_type': 'para'}, page_content='References\\n \\n \\n  Angela      Fan      and      Claire      Gardent.\\n2022.\\nGenerating      bi-  \\n \\n  ographies      on      Wikipedia:      The      impact      of      gender      bias  \\n \\n  on      the      retrieval-based      generation      of      women      biogra-  \\n \\n  phies.\\nIn      Proceedings      of      the      60th      Annual      Meeting      of  \\n \\n  the      Association      for      Computational      Linguistics      (Vol-  \\n \\n  ume      I:      Long      Papers),      pages      8561-8576,      Dublin,  \\n \\n  Ireland.\\nAssociation      for      Computational      Linguistics.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 83, 'chunk_type': 'para'}, page_content='References\\n \\n \\n  Xiaocheng      Feng,      Ming      Liu,      Jiahao      Liu,      Bing      Qin,      Yibo  \\n \\n  Sun,      and      Ting      Liu.\\n2018.\\nTopic-to-essay      generation  \\n \\n  with      neural      networks.\\nIn      JJCAI,      pages      4078-4084.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 84, 'chunk_type': 'para'}, page_content='References\\n \\n \\n  Tira      Nur      Fitria.\\n2023.\\nArtificial      intelligence      (ai)      tech-  \\n \\n  nology      in      openai      chatgpt      application:   \\n \\nA      review      of  \\n \\n  chatgpt      in      writing      english      essay.\\nIn      ELT      Forum:      Jour-  \\n \\n  nal      of      English      Language      Teaching,      volume      12,      pages  \\n \\n  44-58.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 85, 'chunk_type': 'para'}, page_content='References\\n \\n \\n  Pasi      Franti      and      Radu      Mariescu-Istodor.\\n2023.\\nSoft      preci-  \\n \\n  sion      and      recall.\\nPattern      Recognition      Letters,      167:115—  \\n \\n  121.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 86, 'chunk_type': 'para'}, page_content='References\\n \\n \\n  R      Edward      Freeman,      Jeffrey      S      Harrison,      Andrew      C       Wicks,      Bidhan   \\n \\nL      Parmar,      and      Simone      De      Colle.\\n2010.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 87, 'chunk_type': 'para'}, page_content='References\\n \\n \\n  Stakeholder      theory:      The      state      of      the      art.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 88, 'chunk_type': 'para'}, page_content='References\\n \\n \\n  Tianyu      Gao,      Howard      Yen,      Jiatong      Yu,      and      Danqi      Chen.\\n \\n \\n  2023.\\nEnabling      large      language      models      to      generate  \\n \\n  text      with      citations.\\nIn      Proceedings      of      the      2023      Con-  \\n \\n  ference      on      Empirical      Methods      in      Natural      Language  \\n \\n  Processing,      pages      6465-6488,      Singapore.\\nAssocia-  \\n \\n  tion      for      Computational      Linguistics.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 89, 'chunk_type': 'para'}, page_content='References\\n \\n \\n  Lei      Huang,      Weijiang      Yu,      Weitao      Ma,      Weihong      Zhong,  \\n \\n  Zhangyin      Feng,      Haotian      Wang,      Qianglong      Chen,  \\n \\n  Weihua      Peng,      Xiaocheng      Feng,      Bing      Qin,      and      Ting  \\n \\n  Liu.\\n2023.   \\n \\nA      survey      on      hallucination      in      large      lan-  \\n \\n  guage      models:      Principles,      taxonomy,      challenges,      and       open      questions.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 90, 'chunk_type': 'para'}, page_content='References\\n \\n \\n  Gautier      Izacard,      Patrick      Lewis,      Maria      Lomeli,      Lucas  \\n \\n  Hosseini,      Fabio      Petroni,      Timo      Schick,      Jane      Dwivedi-  \\n \\n  Yu,      Armand      Joulin,      Sebastian      Riedel,      and      Edouard  \\n \\n  Grave.\\n2023.\\nAtlas:      Few-shot      learning      with      retrieval  \\n \\n  augmented      language      models.\\nJournal      of      Machine  \\n \\n  Learning      Research,      24(251):1-43.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 91, 'chunk_type': 'para'}, page_content='References\\n \\n \\n  Albert   \\n \\nQ      Jiang,      Alexandre      Sablayrolles,      Arthur      Men-  \\n \\n  sch,      Chris      Bamford,      Devendra      Singh      Chaplot,      Diego  \\n \\n  de      las      Casas,      Florian      Bressand,      Gianna      Lengyel,      Guil-  \\n \\n  laume      Lample,      Lucile      Saulnier,      et      al.\\n2023a.\\nMistral  \\n \\n  7b.\\narXiv      preprint      arXiv:2310.06825.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 92, 'chunk_type': 'para'}, page_content='References\\n \\n \\n  Zhengbao      Jiang,      Frank      Xu,      Luyu      Gao,      Zhiqing      Sun,  \\n \\n  Qian      Liu,      Jane      Dwivedi-Yu,      Yiming      Yang,      Jamie  \\n \\n  Callan,      and      Graham      Neubig.\\n2023b.\\nActive      retrieval  \\n \\n  augmented      generation.\\nIn      Proceedings      of      the      2023  \\n \\n  Conference      on      Empirical      Methods      in      Natural      Lan-  \\n \\n  guage      Processing,      pages      7969-7992,      Singapore.\\nAs-  \\n \\n  sociation      for      Computational      Linguistics.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 93, 'chunk_type': 'para'}, page_content='References\\n \\n \\n  Nikhil      Kandpal,      Haikang      Deng,      Adam      Roberts,      Eric  \\n \\n  Wallace,      and      Colin      Raffel.\\n2023.\\nLarge      language  \\n \\n  models      struggle      to      learn      long-tail      knowledge.\\nIn      In-  \\n \\n  ternational      Conference      on      Machine      Learning,      pages  \\n \\n  15696-15707.\\nPMLR.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 94, 'chunk_type': 'para'}, page_content='References\\n \\n \\n  Omar      Khattab,      Keshav      Santhanam,      Xiang      Lisa  \\n \\n  Li,      David      Hall,      Percy      Liang,      Christopher      Potts,  \\n \\n  and      Matei      Zaharia.\\n2022.\\nDemonstrate-search-  \\n \\n  predict:      Composing      retrieval      and      language      mod-  \\n \\n  els      for      knowledge-intensive      NLP.\\narXiv      preprint  \\n \\n  arXiv:2212.14024.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 95, 'chunk_type': 'para'}, page_content='References\\n \\n \\n  Omar      Khattab,      Arnav      Singhvi,      Paridhi      Maheshwari,  \\n \\n  Zhiyuan      Zhang,      Keshav      Santhanam,      Sri      Vard-  \\n \\n  hamanan,      Saiful      Haq,      Ashutosh      Sharma,      Thomas      T.  \\n \\n  Joshi,      Hanna      Moazam,      Heather      Miller,      Matei      Za-  \\n \\n  haria,      and      Christopher      Potts.\\n2023.\\nDspy:      Compiling  \\n \\n  declarative      language      model      calls      into      self-improving  \\n \\n  pipelines.\\narXiv      preprint      arXiv:2310.03714.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 96, 'chunk_type': 'para'}, page_content='References\\n \\n \\n  Seungone      Kim,      Jamin      Shin,      Yejin      Cho,      Joel      Jang,  \\n \\n  Shayne      Longpre,      Hwaran      Lee,      Sangdoo      Yun,  \\n \\n  Seongjin      Shin,      Sungdong      Kim,      James      Thorne,      et      al.\\n \\n \\n  2023.\\nPrometheus:      Inducing      fine-grained      evalua-  \\n \\n  tion      capability      in      language      models.\\narXiv      preprint  \\n \\n  arXiv:2310.08491.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 97, 'chunk_type': 'para'}, page_content='References\\n \\n \\n  Mojtaba      Komeili,      Kurt      Shuster,      and      Jason      Weston.\\n2022.  \\n \\n  Internet-augmented      dialogue      generation.\\nIn      Proceed-  \\n \\n  ings      of      the      60th      Annual      Meeting      of      the      Association  \\n \\n  for      Computational      Linguistics      (Volume      1:      Long      Pa-  \\n \\n  pers),      pages      8460-8478,      Dublin,      Ireland.\\nAssociation  \\n \\n  for      Computational      Linguistics.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 98, 'chunk_type': 'para'}, page_content='References\\n \\n \\n  Kalpesh      Krishna,      Erin      Bransom,      Bailey      Kuehl,      Mohit  \\n \\n  Iyyer,      Pradeep      Dasigi,      Arman      Cohan,      and      Kyle      Lo.\\n \\n \\n  2023.\\nLongEval:      Guidelines      for      human      evaluation      of  \\n \\n  faithfulness      in      long-form      summarization.\\nIn      Proceed-  \\n \\n  ings      of      the      17th      Conference      of      the      European      Chap-  \\n \\n  ter      of      the      Association      for      Computational      Linguistics,  \\n \\n  pages      1650-1669,      Dubrovnik,      Croatia.\\nAssociation  \\n \\n  for      Computational      Linguistics.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 99, 'chunk_type': 'para'}, page_content='References\\n \\n \\n  Patrick      Lewis,      Ethan      Perez,      Aleksandra      Piktus,      Fabio  \\n \\n  Petroni,      Vladimir      Karpukhin,      Naman      Goyal,      Hein-  \\n \\n  rich      Kiittler,      Mike      Lewis,      Wen-tau      Yih,      Tim      Rock-  \\n \\n  taschel,      et      al.\\n2020.\\nRetrieval-augmented      generation  \\n \\n  for      knowledge-intensive      nlp      tasks.\\nAdvances      in      Neu-  \\n \\n  ral      Information      Processing      Systems,      33:9459-9474.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 100, 'chunk_type': 'para'}, page_content='References\\n \\n \\n  Xiaonan      Li,      Kai      Lv,      Hang      Yan,      Tianyang      Lin,      Wei      Zhu,  \\n \\n  Yuan      Ni,      Guotong      Xie,      Xiaoling      Wang,      and      Xipeng  \\n \\n  Qiu.\\n2023.\\nUnified      demonstration      retriever      for      in-  \\n \\n  context      learning.\\nIn      Proceedings      of      the      61st      Annual  \\n \\n  Meeting      of      the      Association      for      Computational      Lin-  \\n \\n  guistics      (Volume      1:      Long      Papers),      pages      4644-4668,  \\n \\n  Toronto,      Canada.\\nAssociation      for      Computational      Lin-  \\n \\n  guistics.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 101, 'chunk_type': 'para'}, page_content='References\\n \\n \\n  Chin-Yew      Lin.\\n2004.\\nROUGE:   \\n \\nA      package      for      auto-  \\n \\n  matic      evaluation      of      summaries.\\nIn      Text      Summariza-  \\n \\n  tion      Branches      Out,      pages      74-81,      Barcelona,      Spain.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 102, 'chunk_type': 'para'}, page_content='References\\n \\n \\n  Association      for      Computational      Linguistics.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 103, 'chunk_type': 'para'}, page_content='References\\n \\n \\n  Jiachang      Liu,      Dinghan      Shen,      Yizhe      Zhang,      Bill      Dolan,  \\n \\n  Lawrence      Carin,      and      Weizhu      Chen.\\n2022.\\nWhat  \\n \\n  makes      good      in-context      examples      for      GPT-3?\\nIn  \\n \\n  Proceedings      of      Deep      Learning      Inside      Out      (DeeLIO  \\n \\n  2022):      The      3rd      Workshop      on      Knowledge      Extrac-  \\n \\n  tion      and      Integration      for      Deep      Learning      Architectures,  \\n \\n  pages      100-114,      Dublin,      Ireland      and      Online.\\nAssocia-  \\n \\n  tion      for      Computational      Linguistics.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 104, 'chunk_type': 'para'}, page_content='References\\n \\n \\n  Peter      J.      Liu,      Mohammad      Saleh,      Etienne      Pot,      Ben  \\n \\n  Goodrich,      Ryan      Sepassi,      Lukasz      Kaiser,      and      Noam  \\n \\n  Shazeer.\\n2018.\\nGenerating      wikipedia      by      summariz-  \\n \\n  ing      long      sequences.\\nIn      International      Conference      on  \\n \\n  Learning      Representations.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 105, 'chunk_type': 'para'}, page_content='References\\n \\n \\n  Jacob      Menick,      Maja      Trebacz,      Vladimir      Mikulik,  \\n \\n  John      Aslanides,      Francis      Song,      Martin      Chadwick,  \\n \\n  Mia      Glaese,      Susannah      Young,      Lucy      Campbell-  \\n \\n  Gillingham,      Geoffrey      Irving,      and      Nat      McAleese.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 106, 'chunk_type': 'para'}, page_content='References\\n \\n \\n  2022.\\nTeaching      language      models      to      support      answers  \\n \\n  with      verified      quotes.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 107, 'chunk_type': 'para'}, page_content='References\\n \\n \\n  Sewon      Min,      Kalpesh      Krishna,      Xinxi      Lyu,      Mike      Lewis,  \\n \\n  Wen-tau      Yih,      Pang      Koh,      Mohit      Iyyer,      Luke      Zettle-  \\n \\n  moyer,      and      Hannaneh      Hajishirzi.\\n2023.\\nFActScore:'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 108, 'chunk_type': 'para'}, page_content='References\\n \\n \\n  Fine-grained      atomic      evaluation      of      factual      precision  \\n \\n  in      long      form      text      generation.\\nIn      Proceedings      of      the  \\n \\n  2023      Conference      on      Empirical      Methods      in      Natural  \\n \\n  Language      Processing,      pages      12076-12100,      Singa-  \\n \\n  pore.\\nAssociation      for      Computational      Linguistics.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 109, 'chunk_type': 'para'}, page_content='References\\n \\n \\n  Julia      Minguill6n,      Maura      Lerga,      Eduard      Aibar,      Josep  \\n \\n  Lladés-Masllorens,      and      Antoni      Meseguer-Artola.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 110, 'chunk_type': 'para'}, page_content='References\\n \\n \\n  2017.\\nSemi-automatic      generation      of   \\n \\na      corpus      of  \\n \\n  wikipedia      articles      on      science      and      technology.\\nProfe-  \\n \\n  sional      de      la      Informacion,      26(5):995—1005.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 111, 'chunk_type': 'para'}, page_content='References\\n \\n \\n  Rosa      Munoz-Luna.\\n2015.\\nMain      ingredients      for      suc-  \\n \\n  cess      in      12      academic      writing:      Outlining,      drafting      and       proofreading.\\nPloS      one,      10(6):e0128309.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 112, 'chunk_type': 'para'}, page_content='References\\n \\n \\n  Reiichiro      Nakano,      Jacob      Hilton,      Suchir      Balaji,      Jeff      Wu,  \\n \\n  Long      Ouyang,      Christina      Kim,      Christopher      Hesse,  \\n \\n  Shantanu      Jain,      Vineet      Kosaraju,      William      Saunders,  \\n \\n  Xu      Jiang,      Karl      Cobbe,      Tyna      Eloundou,      Gretchen  \\n \\n  Krueger,      Kevin      Button,      Matthew      Knight,      Benjamin  \\n \\n  Chess,      and      John      Schulman.\\n2022.\\nWebgpt:      Browser-  \\n \\n  assisted      question-answering      with      human      feedback.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 113, 'chunk_type': 'para'}, page_content='References\\n \\n \\n  Long      Ouyang,      Jeffrey      Wu,      Xu      Jiang,      Diogo      Almeida,  \\n \\n  Carroll      Wainwright,      Pamela      Mishkin,      Chong      Zhang,  \\n \\n  Sandhini      Agarwal,      Katarina      Slama,      Alex      Ray,      et      al.\\n \\n \\n  2022.\\nTraining      language      models      to      follow      instruc-  \\n \\n  tions      with      human      feedback.\\nAdvances      in      Neural  \\n \\n  Information      Processing      Systems,      35:27730—27744.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 114, 'chunk_type': 'para'}, page_content='References\\nAaron      Parisi,      Yao      Zhao,      and      Noah      Fiedel.\\n2022.\\nTalm:  \\n \\n  Tool      augmented      language      models.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 115, 'chunk_type': 'para'}, page_content='References\\n \\n \\n  John      V      Pavlik.\\n2023.\\nCollaborating      with      chatgpt:      Con-  \\n \\n  sidering      the      implications      of      generative      artificial      intel-  \\n \\n  ligence      for      journalism      and      media      education.\\nJournal-  \\n \\n  ism   \\n \\n&      Mass      Communication      Educator,      78(1):84—93.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 116, 'chunk_type': 'para'}, page_content='References\\n \\n \\n  Gabriel      Poesia,      Alex      Polozov,      Vu      Le,      Ashish      Tiwari,  \\n \\n  Gustavo      Soares,      Christopher      Meek,      and      Sumit      Gul-  \\n \\n  wani.\\n2022.\\nSynchromesh:      Reliable      code      generation  \\n \\n  from      pre-trained      language      models.\\nIn      International  \\n \\n  Conference      on      Learning      Representations.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 117, 'chunk_type': 'para'}, page_content='References\\n \\n \\n  Ofir      Press,      Muru      Zhang,      Sewon      Min,      Ludwig      Schmidt,  \\n \\n  Noah      Smith,      and      Mike      Lewis.\\n2023.\\nMeasuring      and       narrowing      the      compositionality      gap      in      language      mod-  \\n \\n  els.\\nIn      Findings      of      the      Association      for      Computational  \\n \\n  Linguistics:      EMNLP      2023,      pages      5687-5711,      Singa-  \\n \\n  pore.\\nAssociation      for      Computational      Linguistics.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 118, 'chunk_type': 'para'}, page_content='References\\n \\n \\n  Peng      Qi,      Yuhao      Zhang,      and      Christopher      D.      Manning.\\n \\n \\n  2020.      Stay      hungry,      stay      focused:      Generating      infor-  \\n \\n  mative      and      specific      questions      in      information-seeking  \\n \\n  conversations.\\nIn      Findings      of      the      Association      for  \\n \\n  Computational      Linguistics:      EMNLP      2020,      pages      25—  \\n \\n  40,      Online.\\nAssociation      for      Computational      Linguis-  \\n \\n  tics.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 119, 'chunk_type': 'para'}, page_content='References\\n \\n \\n  Hongjing      Qian,      Yutao      Zhu,      Zhicheng      Dou,      Haoqi      Gu,  \\n \\n  Xinyu      Zhang,      Zheng      Liu,      Ruofei      Lai,      Zhao      Cao,  \\n \\n  Jian-Yun      Nie,      and      Ji-Rong      Wen.\\n2023.\\nWebbrain:'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 120, 'chunk_type': 'para'}, page_content='References\\n \\n \\n  Learning      to      generate      factually      correct      articles      for  \\n \\n  queries      by      grounding      on      large      web      corpus.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 121, 'chunk_type': 'para'}, page_content='References\\n \\n \\n  Hossein      A.      Rahmani,      Xi      Wang,      Yue      Feng,      Qiang      Zhang,  \\n \\n  Emine      Yilmaz,      and      Aldo      Lipani.\\n2023.   \\n \\nA      survey      on  \\n \\n  asking      clarification      questions      datasets      in      conversa-  \\n \\n  tional      systems.\\nIn      Proceedings      of      the      61st      Annual  \\n \\n  Meeting      of      the      Association      for      Computational      Lin-  \\n \\n  guistics      (Volume      1:      Long      Papers),      pages      2698-2716,  \\n \\n  Toronto,      Canada.\\nAssociation      for      Computational      Lin-  \\n \\n  guistics.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 122, 'chunk_type': 'para'}, page_content='References\\n \\n \\n  Ashwin      Ram.\\n1991.   \\n \\nA      theory      of      questions      and      question  \\n \\n  asking.\\nJournal      of      the      Learning      Sciences,      1(3-4):273-  \\n \\n  318.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 123, 'chunk_type': 'para'}, page_content='References\\n \\n \\n  Ori      Ram,      Yoav      Levine,      Itay      Dalmedigos,      Dor      Muhlgay,  \\n \\n  Amnon      Shashua,      Kevin      Leyton-Brown,      and      Yoav  \\n \\n  Shoham.\\n2023.\\nIn-context      retrieval-augmented      lan-  \\n \\n  guage      models.\\nTransactions      of      the      Association      for  \\n \\n  Computational      Linguistics.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 124, 'chunk_type': 'para'}, page_content='References\\n \\n \\n  Nils      Reimers      and      Iryna      Gurevych.\\n2019.\\nSentence-  \\n \\n  BERT:      Sentence      embeddings      using      Siamese      BERT-  \\n \\n  networks.\\nIn      Proceedings      of      the      2019      Conference      on  \\n \\n  Empirical      Methods      in      Natural      Language      Processing  \\n \\n  and      the      9th      International      Joint      Conference      on      Natu-  \\n \\n  ral      Language      Processing      (EMNLP-IJCNLP),      pages  \\n \\n  3982-3992,      Hong      Kong,      China.\\nAssociation      for      Com-  \\n \\n  putational      Linguistics.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 125, 'chunk_type': 'para'}, page_content='References\\n \\n \\n \\nD      Gordon      Rohman.\\n1965.\\nPre-writing      the      stage      of      dis-  \\n \\n  covery      in      the      writing      process.\\nCollege      composition  \\n \\n  and      communication,      16(2):106—112.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 126, 'chunk_type': 'para'}, page_content='References\\n \\n \\n  Christina      Sauper      and      Regina      Barzilay.\\n2009.\\nAuto-  \\n \\n  matically      generating      Wikipedia      articles:   \\n \\nA      structure-  \\n \\n  aware      approach.\\nIn      Proceedings      of      the      Joint      Con-  \\n \\n  ference      of      the      47th      Annual      Meeting      of      the      ACL      and       the      4th      International      Joint      Conference      on      Natural  \\n \\n  Language      Processing      of      the      AFNLP,      pages      208-216,  \\n \\n  Suntec,      Singapore.\\nAssociation      for      Computational  \\n \\n  Linguistics.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 127, 'chunk_type': 'para'}, page_content='References\\n \\n \\n  Lam.\\n2023.\\nWikiChat:      Stopping      the      hallucination      of  \\n \\n  large      language      model      chatbots      by      few-shot      ground-  \\n \\n  ing      on      Wikipedia.\\nIn      Findings      of      the      Association  \\n \\n  for      Computational      Linguistics:      EMNLP      2023,      pages  \\n \\n  2387-2413,      Singapore.\\nAssociation      for      Computa-  \\n \\n  tional      Linguistics.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 128, 'chunk_type': 'para'}, page_content='References\\n \\n \\n  Jonathan      Bragg,      Jeff      Hammerbacher,      Doug      Downey,  \\n \\n  Joseph      Chee      Chang,      and      David      Sontag.\\n2023.\\nBe-  \\n \\n  yond      summarization:      Designing      ai      support      for      real-  \\n \\n  world      expository      writing      tasks.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 129, 'chunk_type': 'para'}, page_content='References\\n \\n \\n  Luke      Zettlemoyer.\\n2022.\\nNearest      neighbor      zero-shot  \\n \\n  inference.\\nIn      Proceedings      of      the      2022      Conference      on  \\n \\n  Empirical      Methods      in      Natural      Language      Processing,  \\n \\n  pages      3254-3265,      Abu      Dhabi,      United      Arab      Emirates.\\n \\n \\n  Association      for      Computational      Linguistics.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 130, 'chunk_type': 'para'}, page_content='References\\n \\n \\n  Stephen      Roller,      Arthur      Szlam,      and      Jason      Weston.\\n \\n \\n  2022.\\nLanguage      models      that      seek      for      knowledge:  \\n \\n  Modular      search   \\n \\n&      generation      for      dialogue      and       prompt      completion.\\nIn      Findings      of      the      Association  \\n \\n  for      Computational      Linguistics:      EMNLP      2022,      pages  \\n \\n  373-393,      Abu      Dhabi,      United      Arab      Emirates.\\nAssoci-  \\n \\n  ation      for      Computational      Linguistics.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 131, 'chunk_type': 'para'}, page_content='References\\n \\n \\n  and      Jason      Weston.\\n2021.\\nRetrieval      augmentation  \\n \\n  reduces      hallucination      in      conversation.\\nIn      Findings  \\n \\n  of      the      Association      for      Computational      Linguistics:  \\n \\n  EMNLP      2021,      pages      3784-3803,      Punta      Cana,      Do-  \\n \\n  minican      Republic.\\nAssociation      for      Computational  \\n \\n  Linguistics.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 132, 'chunk_type': 'para'}, page_content='References\\nWikipedia      as      an      introduction      to      academic      writing.\\nIn  \\n \\n  English      teaching      forum,      volume      48,      page      12.\\nERIC.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 133, 'chunk_type': 'para'}, page_content='References\\n \\n \\n  and      Jaclyn      Gishbaugher.\\n2020.\\nRole      of      questions      in  \\n \\n  inquiry-based      instruction:      towards   \\n \\na      design      taxon-  \\n \\n  omy      for      question-asking      and      implications      for      design.\\n \\n \\n  Educational      Technology      Research      and      Development,  \\n \\n  68:653-678.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 134, 'chunk_type': 'para'}, page_content='References\\nitory      text.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 135, 'chunk_type': 'para'}, page_content='References\\n \\n \\n  than      humans?\\nvalidating      how      openai’s      chatgpt      model  \\n \\n  explains      crowdfunding,      alternative      finance      and      com-  \\n \\n  munity      finance.\\nValidating      how      OpenAlI’s      ChatGPT  \\n \\n  model      explains      Crowdfunding,      Alternative      Finance  \\n \\n  and      Community      Finance.(December      22,      2022).'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 136, 'chunk_type': 'para'}, page_content='References\\n \\n \\n  Choi.\\n2023.   \\n \\nA      critical      evaluation      of      evaluations      for  \\n \\n  long-form      question      answering.\\nIn      Proceedings      of      the  \\n \\n  61st      Annual      Meeting      of      the      Association      for      Compu-  \\n \\n  tational      Linguistics      (Volume      1:      Long      Papers),      pages  \\n \\n  3225-3245,      Toronto,      Canada.\\nAssociation      for      Com-  \\n \\n  putational      Linguistics.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 137, 'chunk_type': 'para'}, page_content='References\\n \\n \\n  Kevin      Yang,      Dan      Klein,      Nanyun      Peng,      and      Yuandong  \\n \\n  Tian.\\n2023.      DOC:      Improving      long      story      coherence  \\n \\n  with      detailed      outline      control.\\nIn      Proceedings      of      the  \\n \\n  61st      Annual      Meeting      of      the      Association      for      Compu-  \\n \\n  tational      Linguistics      (Volume      I:      Long      Papers),      pages  \\n \\n  3378-3465,      Toronto,      Canada.\\nAssociation      for      Com-  \\n \\n  putational      Linguistics.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 138, 'chunk_type': 'para'}, page_content='References\\n \\n \\n  Kevin      Yang,      Yuandong      Tian,      Nanyun      Peng,      and      Dan  \\n \\n  Klein.\\n2022.\\nRe3:      Generating      longer      stories      with  \\n \\n  recursive      reprompting      and      revision.\\nIn      Proceedings  \\n \\n  of      the      2022      Conference      on      Empirical      Methods      in      Nat-  \\n \\n  ural      Language      Processing,      pages      4393-4479,      Abu  \\n \\n  Dhabi,      United      Arab      Emirates.\\nAssociation      for      Com-  \\n \\n  putational      Linguistics.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 139, 'chunk_type': 'para'}, page_content='References\\n \\n \\n  Shunyu      Yao,      Jeffrey      Zhao,      Dian      Yu,      Nan      Du,      Izhak  \\n \\n  Shafran,      Karthik      R      Narasimhan,      and      Yuan      Cao.\\n2023.  \\n \\n  React:      Synergizing      reasoning      and      acting      in      language  \\n \\n  models.\\nIn      The      Eleventh      International      Conference  \\n \\n  on      Learning      Representations.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 140, 'chunk_type': 'para'}, page_content='References\\n \\n \\n  Cyril      Zakka,      Akash      Chaurasia,      Rohan      Shad,      Alex      R       Dalal,      Jennifer   \\n \\nL      Kim,      Michael      Moor,      Kevin      Alexan-  \\n \\n  der,      Euan      Ashley,      Jack      Boyd,      Kathleen      Boyd,      et      al.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 141, 'chunk_type': 'para'}, page_content='References\\n \\n \\n  2023.\\nAlmanac:      Retrieval-augmented      language      mod-  \\n \\n  els      for      clinical      medicine.\\nResearch      Square.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 142, 'chunk_type': 'para'}, page_content='References\\n \\n \\n  Shuyan      Zhou,      Uri      Alon,      Frank      F.      Xu,      Zhengbao      Jiang,  \\n \\n  and      Graham      Neubig.\\n2023.\\nDocprompting:      Gener-  \\n \\n  ating      code      by      retrieving      the      docs.\\nIn      The      Eleventh  \\n \\n  International      Conference      on      Learning      Representa-  \\n \\n  tions.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 143, 'chunk_type': 'para'}, page_content='Average      Number      of      References      90.1\\nTable      7:      Statistics      of      the      dataset      used      in      our      experiments.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 144, 'chunk_type': 'para'}, page_content='Average      Number      of      References      90.1\\n—\\n \\n   Average Number      of      references  \\n \\n  a      BR  \\n \\n  N      Py      oa      feo}      Oo      N       Oo      Oo      oO      Oo      oO      Oo  \\n \\n  fo)  \\n \\n  1 0\\n \\n   20      40      60      80      100  \\n \\n  Edit      progress      (%      of      total      edits) Figure      4:      Evolution      of      reference      count      in      the      Wikipedia  \\n \\n  article      editing      process.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 145, 'chunk_type': 'para'}, page_content='A_      Dataset      Details\\n \\n \\n  As      discussed      in      §2.1,      we      curate      the      FreshWiki  \\n \\n  dataset      by      collecting      recent      and      high-quality      En-  \\n \\n  glish      Wikipedia      articles.\\nWe      select      the      most-edited  \\n \\n  pages      over   \\n \\na      specific      period      rather      than      using      cre-  \\n \\n  ation      dates      as   \\n \\na      cutoff      because      most      of      Wikipedia  \\n \\n  articles      are      “stubs”      or      are      of      low      quality      when      they  \\n \\n  were      created.\\nFor      quality,      we      consider      articles      pre-  \\n \\n  dicted      to      be      of      B-class      quality      or      above.\\nAccording  \\n \\n  to      Wikipedia      statistics!\\n*,      only      around      3%      of      ex-  \\n \\n  isting      Wikipedia      pages      meet      this      quality      standard.\\n \\n \\n  As      LLMs      can      generate      reasonably      good      outputs,  \\n \\n  we      think      it      is      important      to      use      high-quality      human-  \\n \\n  written      articles      as      references      for      further      research.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 146, 'chunk_type': 'para'}, page_content='A_      Dataset      Details\\n \\n \\n  For      experiments      in      this      work,      we      randomly      se-  \\n \\n  lect      100      samples      with      human-written      articles      un-  \\n \\n  der      3000      words      to      have   \\n \\na      meaningful      comparison.\\n \\n \\n  Table   \\n \\n7      gives      the      data      statistics.\\nNotably,      human-  \\n \\n  authored      articles      have   \\n \\na      large      number      of      references  \\n \\n  but      they      require      numerous      edits      to      achieve      this.\\nFig-  \\n \\n  ure   \\n \\n4      illustrates      the      evolution      of      the      reference      count  \\n \\n  in      the      article      edit      process      and      Figure   \\n \\n5      gives      the      dis-  \\n \\n  tribution      of      edit      counts      for      human-authored      articles  \\n \\n  used      in      our      experiments.\\ncount      (A;)   \\n \\n=       where      embed(-)      in      Equation      (1)      is      parameterized  \\n \\n  by      paraphrase-MiniLM-L6-v2      provided      in      the  \\n \\n  Sentence-Transformers      library!*.\\nThe      cardinality  \\n \\n  https://en.wikipedia.\\norg/wiki/Wikipedia:  \\n \\n  Content_assessment  \\n \\n  100) Bhttps://huggingface.co/sentence-transformers/  \\n \\n  paraphrase-MiniLM-L6-v2 Percentage      of      articles      (n'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 147, 'chunk_type': 'table'}, page_content='A_      Dataset      Details\\n |       T      T      T      T       0      500      1000      1500 |       y      1      7      1      7      7      7       2000      2500      3000      3500      4000      4500      5000       Number      of      edits\\n | Figure      5:      Distribution      of      edit      counts      for      Wikipedia      arti-       cles      in      our      experiments      (n      =      100).\\n'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 148, 'chunk_type': 'para'}, page_content='B_      Pseudo      Code      of      STORM\\n \\n \\n  In      §3,      we      introduce      STORM,   \\n \\na      framework      that      au-  \\n \\n  tomates      the      pre-writing      stage      by      discovering      differ-  \\n \\n  ent      perspectives,      simulating      information-seeking  \\n \\n  conversations,      and      creating   \\n \\na      comprehensive      out-  \\n \\n  line.\\nAlgorithm   \\n \\n1      displays      the      skeleton      of      STORM.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 149, 'chunk_type': 'para'}, page_content='B_      Pseudo      Code      of      STORM\\n \\n \\n  We      implement      STORM      with      zero-shot      prompt-  \\n \\n  ing      using      the      DSPy      framework      (Khattab      et      al.,  \\n \\n  2023).\\nListing   \\n \\n1      and   \\n \\n2      show      the      prompts      used  \\n \\n  in      our      implementation.\\nWe      highlight      that      STORM  \\n \\n  offers   \\n \\na      general      framework      designed      to      assist      the  \\n \\n  creation      of      grounded,      long-form      articles,      without  \\n \\n  depending      extensively      on      prompt      engineering      for   \\n \\na       single      domain.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 150, 'chunk_type': 'para'}, page_content='C.1      Soft      Heading      Recall\\n \\n \\n  We      calculate      the      soft      heading      recall      between      the  \\n \\n  multi-level      headings      in      the      generated      outline,      con-  \\n \\n  sidered      as      the      prediction      P,      and      those      in      the      human-  \\n \\n  written      article,      considered      as      the      ground      truth      G.  \\n \\n  The      calculation      is      based      on      the      soft      recall      defini-  \\n \\n  tion      in      Franti      and      Mariescu-Istodor      (2023).\\nGiven  \\n \\n  aset   \\n \\nA   \\n \\n=      {Ai}*.,,      soft      count      of      an      item      is      defined as      the      inverse      of      the      sum      of      its      similarity      to      other  \\n \\n  items      in      the      set:'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 151, 'chunk_type': 'para'}, page_content='C.1      Soft      Heading      Recall\\n1\\n \\n \\n  Dj      Sim      (Aj,      Aj)      (1) Sim      (A;,      A;)   \\n \\n=      cos      (embed(A;),      embed(A;))   \\n \\n,       28  \\n \\n  29  \\n \\n  class      GenRelatedTopicsPrompt      (dspy.      Signature):'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 152, 'chunk_type': 'para'}, page_content=\"C.1      Soft      Heading      Recall\\n \\n \\n  I\\\\\\\\\\\\'m      writing   \\n \\na      Wikipedia      page      for   \\n \\na      topic      mentioned      below.\\nPlease      identify      and       recommend      some      Wikipedia      pages      on      closely      related      subjects.\\nI\\\\\\\\\\\\'m      looking      for  \\n \\n  examples      that      provide      insights      into      interesting      aspects      commonly      associated  \\n \\n  with      this      topic,      or      examples      that      help      me      understand      the      typical      content      and       structure      included      in      Wikipedia      pages      for      similar      topics.\\n \\n \\n  Please      list      the      urls      in      separate      lines.\"),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 153, 'chunk_type': 'para'}, page_content='C.1      Soft      Heading      Recall\\n \\n \\n  non topic   \\n \\n=      dspy.InputField(prefix=\"Topic      of      interest:”,      format=str)  \\n \\n  related_topics   \\n \\n=      dspy.OutputField() class      GenPerspectivesPrompt      (dspy.Signature):'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 154, 'chunk_type': 'para'}, page_content='C.1      Soft      Heading      Recall\\n \\n \\n  You      need      to      select   \\n \\na      group      of      Wikipedia      editors      who      will      work      together      to      create  \\n \\n  a      comprehensive      article      on      the      topic.\\nEach      of      them      represents   \\n \\na      different  \\n \\n  perspective,      role,      or      affiliation      related      to      this      topic.\\nYou      can      use      other  \\n \\n  Wikipedia      pages      of      related      topics      for      inspiration.\\nFor      each      editor,      add       description      of      what      they      will      focus      on.\\n \\n \\n  Give      your      answer      in      the      following      format:      1.      short      summary      of      editor      1:'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 155, 'chunk_type': 'para'}, page_content=\"C.1      Soft      Heading      Recall\\ndescription\\\\\\\\2.\\nshort summary of editor 2: description\\\\\\\\  \\n \\n  non  \\n \\n  topic   \\n \\n=      dspy.InputField(prefix=\\\\\\\\\\\\'Topic      of      interest:\\\\\\\\\\\\',      format=str)  \\n \\n  examples   \\n \\n=      dspy.InputField(prefix=\\\\\\\\\\\\'Wiki      page      outlines      of      related      topics      for  \\n \\n  inspiration:\\\\\\\\\\\\\\\\\\\\',      format=str)  \\n \\n  perspectives   \\n \\n=      dspy.OutputField() class      GenQnPrompt(dspy.      Signature):\"),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 156, 'chunk_type': 'para'}, page_content='C.1      Soft      Heading      Recall\\n \\n \\n  You      are      an      experienced      Wikipedia      writer      and      want      to      edit   \\n \\na      specific      page.\\n \\n \\n  Besides      your      identity      as   \\n \\na      Wikipedia      writer,      you      have   \\n \\na      specific      focus      when  \\n \\n  researching      the      topic.\\n \\n \\n  Now,      you      are      chatting      with      an      expert      to      get      information.\\nAsk      good      questions      to  \\n \\n  get      more      useful      information.\\n \\n \\n  When      you      have      no      more      question      to      ask,      say      \"Thank      you      so      much      for      your      help!”\\nto  \\n \\n  end      the      conversation.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 157, 'chunk_type': 'para'}, page_content=\"C.1      Soft      Heading      Recall\\n \\n \\n  Please      only      ask      one      question      at   \\n \\na      time      and      don\\\\\\\\\\\\'t      ask      what      you      have      asked      before.\\n \\n \\n  Your      questions      should      be      related      to      the      topic      you      want      to      write.\"),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 158, 'chunk_type': 'para'}, page_content=\"C.1      Soft      Heading      Recall\\n \\n \\n  non  \\n \\n  topic   \\n \\n=      dspy.InputField(prefix=\\\\\\\\\\\\'Topic      you      want      to      write:      \\\\\\\\\\\\',      format=str)  \\n \\n  persona   \\n \\n=      dspy.InputField(prefix=\\\\\\\\\\\\'Your      specific      perspective:      \\\\\\\\\\\\',      format=str)  \\n \\n  conv   \\n \\n=      dspy.InputField(prefix=\\\\\\\\\\\\'Conversation      history:\\\\\\\\\\\\\\\\\\\\',      format=str)  \\n \\n  question   \\n \\n=      dspy.OutputField()  \\n \\n  class      GenQueriesPrompt      (dspy.      Signature):  \\n \\n  nnn  \\n \\n  You      want      to      answer      the      question      using      Google      search.\\nWhat      do      you      type      in      the  \\n \\n  search      box?\\nWrite the queries you will use in the following format:- query 1\\\\\\\\- query 2\\\\\\\\\"),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 159, 'chunk_type': 'para'}, page_content=\"C.1      Soft      Heading      Recall\\n \\n \\n  topic   \\n \\n=      dspy.InputField(prefix=\\\\\\\\\\\\'Topic      you      are      discussing      about:      \\\\\\\\\\\\',      format=str)  \\n \\n  question   \\n \\n=      dspy.InputField(prefix=\\\\\\\\\\\\'Question      you      want      to      answer:      \\\\\\\\\\\\',      format=str)  \\n \\n  queries   \\n \\n=      dspy.OutputField()\"),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 160, 'chunk_type': 'para'}, page_content='C.1      Soft      Heading      Recall\\nListing      1:      Prompts      used      in      STORM,      corresponding      to      Line      4,      11,      19,      22      in      Algorithm      1.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 161, 'chunk_type': 'para'}, page_content='C.1      Soft      Heading      Recall\\nwow      Ne'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 162, 'chunk_type': 'para'}, page_content='C.1      Soft      Heading      Recall\\n \\n \\n  20  \\n \\n  21  \\n \\n  22  \\n \\n  23  \\n \\n  24  \\n \\n  25  \\n \\n  26  \\n \\n  27  \\n \\n  28'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 163, 'chunk_type': 'para'}, page_content='C.1      Soft      Heading      Recall\\n29  \\n \\n  30 class      GenAnswerPrompt(dspy.      Signature):'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 164, 'chunk_type': 'para'}, page_content='C.1      Soft      Heading      Recall\\n \\n \\n  You      are      an      expert      who      can      use      information      effectively.\\nYou      are      chatting      with   \\n \\na       Wikipedia      writer      who      wants      to      write   \\n \\na      Wikipedia      page      on      topic      you      know.\\nYou  \\n \\n  have      gathered      the      related      information      and      will      now      use      the      information      to  \\n \\n  form   \\n \\na      response.\\n \\n \\n  Make      your      response      as      informative      as      possible      and      make      sure      every      sentence      is  \\n \\n  supported      by      the      gathered      information.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 165, 'chunk_type': 'para'}, page_content=\"C.1      Soft      Heading      Recall\\n \\n \\n  non  \\n \\n  topic   \\n \\n=      dspy.InputField(prefix=\\\\\\\\\\\\'Topic      you      are      discussing      about:\\\\\\\\\\\\',      format=str)  \\n \\n  conv      dspy.InputField(prefix=\\\\\\\\\\\\'Question:\\\\\\\\\\\\\\\\\\\\',      format=str)  \\n \\n  info   \\n \\n=      dspy.InputField(      prefix=\\\\\\\\\\\\'Gathered      information:\\\\\\\\\\\\\\\\\\\\',      format=str)  \\n \\n  answer   \\n \\n=      dspy.OutputField(prefix=\\\\\\\\\\\\'Now      give      your      response:\\\\\\\\\\\\\\\\\\\\')\"),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 166, 'chunk_type': 'para'}, page_content='C.1      Soft      Heading      Recall\\nclass      DirectGenOutlinePrompt      (dspy.      Signature):'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 167, 'chunk_type': 'para'}, page_content='C.1      Soft      Heading      Recall\\nWrite      an      outline      for   \\n \\na      Wikipedia      page.\\n \\n \\n  Here      is      the      format      of      your      writing:'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 168, 'chunk_type': 'list_item'}, page_content='C.1      Soft      Heading      Recall\\n2.      Do      not      include      other      information.\\n \\n \\n  non\\n1. Use      \"#\"      Title”      to      indicate      section      title,      \"##\"      Title”      to      indicate  \\n \\n  subsection      title,      \"###\"”      Title”      to      indicate      subsubsection      title,      and      so  \\n \\n  on.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 169, 'chunk_type': 'para'}, page_content='C.1      Soft      Heading      Recall\\ntopic   \\n \\n=      dspy.InputField(prefix=\"Topic      you      want      to      write:      ”\",      format=str)  \\n \\n  outline   \\n \\n=      dspy.OutputField(prefix=\"Write      the      Wikipedia      page      outline:\\\\\\\\\"”)”'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 170, 'chunk_type': 'para'}, page_content='C.1      Soft      Heading      Recall\\nclass      RefineOutlinePrompt(dspy.      Signature):'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 171, 'chunk_type': 'para'}, page_content='C.1      Soft      Heading      Recall\\n \\n \\n  Improve      an      outline      for   \\n \\na      Wikipedia      page.\\nYou      already      have   \\n \\na      draft      outline      that  \\n \\n  covers      the      general      information.\\nNow      you      want      to      improve      it      based      on      the  \\n \\n  information      learned      from      an      information-seeking      conversation      to      make      it      more  \\n \\n  comprehensive.\\n \\n \\n  Here      is      the      format      of      your      writing:'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 172, 'chunk_type': 'list_item'}, page_content='C.1      Soft      Heading      Recall\\n2.      Do      not      include      other      information.\\n \\n \\n  non'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 173, 'chunk_type': 'list_item'}, page_content='C.1      Soft      Heading      Recall\\n1. Use      \"#\"      Title”      to      indicate      section      title,      \"##\"      Title”      to      indicate  \\n \\n  subsection      title,      \"###\"      Title”      to      indicate      subsubsection      title,      and      so  \\n \\n  on.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 174, 'chunk_type': 'para'}, page_content='C.1      Soft      Heading      Recall\\n \\n \\n  topic   \\n \\n=      dspy.InputField(prefix=\"Topic      you      want      to      write:      \",      format=str)  \\n \\n  conv   \\n \\n=      dspy.InputField(prefix=\"Conversation      history:\\\\\\\\\",      format=str)  \\n \\n  old_outline   \\n \\n=      dspy.OutputField(prefix=\"Current      outline:\\\\\\\\”,      format=str)  \\n \\n  outline   \\n \\n=      dspy.OutputField(      prefix=\\\\\\\\\\\\\\'Write      the      Wikipedia      page      outline:\\\\\\\\\\\\\\\\\\\\\\')”'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 175, 'chunk_type': 'para'}, page_content='C.1      Soft      Heading      Recall\\nListing      2:      Prompts      used      in      STORM      (continue),      corresponding      to      Line      24,      31,      32      in      Algorithm      1.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 176, 'chunk_type': 'para'}, page_content='C.1      Soft      Heading      Recall\\nyay      aA      uu      &}      WwW      YY      —'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 177, 'chunk_type': 'para'}, page_content='C.1      Soft      Heading      Recall\\n \\n \\n  11  \\n \\n  12  \\n \\n  13  \\n \\n  14  \\n \\n  15  \\n \\n  16  \\n \\n  17  \\n \\n  18  \\n \\n  19  \\n \\n  20  \\n \\n  21  \\n \\n  22  \\n \\n  23'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 178, 'chunk_type': 'para'}, page_content='C.1      Soft      Heading      Recall\\n \\n \\n  24  \\n \\n  25  \\n \\n  26  \\n \\n  27  \\n \\n  28  \\n \\n  29  \\n \\n  30  \\n \\n  31  \\n \\n  32  \\n \\n  33  \\n \\n  Input      :Topic      t,      maximum      perspective      N,  \\n \\n  maximum      conversation      round      MJ  \\n \\n  Output   \\n \\n:      Outline      O,      references   \\n \\nR PO = \"basic fact writer \" // Constant.\\n \\n \\n  R-[]  \\n \\n  //      Discover      perspectives      P.  \\n \\n  related_topics   \\n \\n+      gen_related_topics(t)  \\n \\n  tocs   \\n \\n+   \\n \\n|   \\n \\n|       foreach      related_t      in      related_topics      do  \\n \\n  article   \\n \\n<      get_wiki_article(related_t)  \\n \\n  if      article      then  \\n \\n  |      tocs.append(extract_toc(article))  \\n \\n  end  \\n \\n  end  \\n \\n  P   \\n \\n<      gen_perspectives(t,      tocs)  \\n \\n  P<      [PO]   \\n \\n+      P[:N]  \\n \\n  //      Simulate      conversations.\\n \\n \\n  convos   \\n \\n<      [|  \\n \\n  foreach      p      in      P      do  \\n \\n  convo_history   \\n \\n<   \\n \\n|   \\n \\n]       for:      =1to      M      do  \\n \\n  //      Question      asking.\\n \\n \\n  q+      gen_qn(t,      p,      dlg_history)  \\n \\n  convo_history.append(q)  \\n \\n  //      Question      answering.\\n \\n \\n  queries   \\n \\n<      gen_queries(t,      q)  \\n \\n  sources      <—  \\n \\n  search_and_sift(queries)  \\n \\n  a   \\n \\n+      gen_ans(t,      q,      sources)  \\n \\n  convo_history.append(a)  \\n \\n  R.append(sources)  \\n \\n  end  \\n \\n  convos.append(convo_history)  \\n \\n  end  \\n \\n  //      Create      the      outline.\\n \\n \\n  Op   \\n \\n<      direct_gen_outline(t)  \\n \\n  O      «<      refine_outline(t,      Op,      convos)  \\n \\n  return      O,      R       of   \\n \\nA      is      the      sum      of      the      counts      of      its      individual      items:'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 179, 'chunk_type': 'para'}, page_content='K       card(A)      =      S-      count      (A;)      (2)\\ni=1 The      soft      heading      recall      is      calculated      as card(Gn      P)  \\n \\n  card(G)   \\n \\n”      @) soft      heading      recall      ='),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 180, 'chunk_type': 'para'}, page_content='K       card(A)      =      S-      count      (A;)      (2)\\nwhere      the      cardinality      of      intersection      is      defined      via  \\n \\n  the      union      as      follows:'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 181, 'chunk_type': 'para'}, page_content='K       card(A)      =      S-      count      (A;)      (2)\\ncard(Gn      P)   \\n \\n=       card(G)   \\n \\n+      card(P)   \\n \\n—      card(G   \\n \\nU      P).\\n®'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 182, 'chunk_type': 'para'}, page_content='C.2.      LLM      Evaluator\\nWe      use      Prometheus!\\n*      (Kim      et      al.,      2023),   \\n \\na      13B  \\n \\n  open-source      evaluator      LLM      that      can      assess      long-  \\n \\n  form      text      based      on      customized      1-5      scale      rubric,      to  \\n \\n  grade      the      article      from      the      aspects      of      Interest      level,  \\n \\n  Coherence      and      Organization,      Relevance      and      Fo-  \\n \\n  cus,      and      Coverage.\\nTable   \\n \\n8      gives      our      grading      rubric.\\n \\n \\n  While      Prometheus      is      best      used      with   \\n \\na      score   \\n \\n5      ref-  \\n \\n  erence      answer,      we      find      adding      the      reference      will  \\n \\n  exceed      the      context      length      limit      of      the      model.\\nSince  \\n \\n  Kim      et      al.\\n(2023)      show      Prometheus      ratings      without  \\n \\n  reference      also      correlate      well      with      human      prefer-  \\n \\n  ences,      we      omit      the      reference      and      trim      the      input  \\n \\n  article      to      be      within      2000      words      by      iteratively      re-  \\n \\n  moving      contents      from      the      shortest      section      to      ensure  \\n \\n  the      input      can      fit      into      the      model’s      context      window.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 183, 'chunk_type': 'para'}, page_content='C.3      More      Discussion      of      the      Citation      Quality\\n \\n \\n  Irrelevant  \\n \\n  Source  \\n \\n  Inaccurate  \\n \\n  Othe      Paraphrasing  \\n \\n  1%      4%  \\n \\n  7%  \\n \\n  Improper  \\n \\n  Inferential      Linking  \\n \\n  Lack      Citation      14%  \\n \\n  47%  \\n \\n  Incorrectly      Split  \\n \\n  12%'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 184, 'chunk_type': 'para'}, page_content='False      Negative       15%\\nFigure      6:      Error      analysis      of      unsupported      sentences      in      10  \\n \\n  sampled      articles.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 185, 'chunk_type': 'para'}, page_content='False      Negative       15%\\n \\n \\n  https:      //huggingface.co/kaist-ai/  \\n \\n  Criteria      Description  \\n \\n  Score   \\n \\n|      Description  \\n \\n  Score   \\n \\n2      Description  \\n \\n  Score   \\n \\n3      Description  \\n \\n  Score   \\n \\n4      Description  \\n \\n  Score   \\n \\n5      Description'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 186, 'chunk_type': 'para'}, page_content='False      Negative       15%\\n \\n \\n  Interest      Level:      How      engaging      and      thought-provoking      is      the      article?\\n \\n \\n  Not      engaging      at      all;      no      attempt      to      capture      the      reader’s      attention.\\n \\n \\n  Fairly      engaging      with   \\n \\na      basic      narrative      but      lacking      depth.\\n \\n \\n  Moderately      engaging      with      several      interesting      points.\\n \\n \\n  Quite      engaging      with   \\n \\na      well-structured      narrative      and      noteworthy      points      that      frequently      capture      and      retain      attention.\\n \\n \\n  Exceptionally      engaging      throughout,      with   \\n \\na      compelling      narrative      that      consistently      stimulates      interest.\\n \\n \\n  Criteria      Description  \\n \\n  Score   \\n \\n|      Description  \\n \\n  Score   \\n \\n2      Description  \\n \\n  Score   \\n \\n3      Description  \\n \\n  Score   \\n \\n4      Description  \\n \\n  Score   \\n \\n5      Description  \\n \\n  Coherence      and      Organization:      Is      the      article      well-organized      and      logically      structured?\\n \\n \\n  Disorganized;      lacks      logical      structure      and      coherence.\\n \\n \\n  Fairly      organized;   \\n \\na      basic      structure      is      present      but      not      consistently      followed.\\n \\n \\n  Organized;   \\n \\na      clear      structure      is      mostly      followed      with      some      lapses      in      coherence.\\n \\n \\n  Good      organization;   \\n \\na      clear      structure      with      minor      lapses      in      coherence.\\n \\n \\n  Excellently      organized;      the      article      is      logically      structured      with      seamless      transitions      and   \\n \\na      clear      argument.\\n \\n \\n  Criteria      Description  \\n \\n  Score   \\n \\n|      Description  \\n \\n  Score   \\n \\n2      Description  \\n \\n  Score   \\n \\n3      Description  \\n \\n  Score   \\n \\n4      Description  \\n \\n  Score   \\n \\n5      Description  \\n \\n  Relevance      and      Focus:      Does      the      article      stay      on      topic      and      maintain   \\n \\na      clear      focus?\\n \\n \\n  Off-topic;      the      content      does      not      align      with      the      headline      or      core      subject.\\n \\n \\n  Somewhat      on      topic      but      with      several      digressions;      the      core      subject      is      evident      but      not      consistently      adhered      to.\\n \\n \\n  Generally      on      topic,      despite   \\n \\na      few      unrelated      details.\\n \\n \\n  Mostly      on      topic      and      focused;      the      narrative      has   \\n \\na      consistent      relevance      to      the      core      subject      with      infrequent      digressions.\\n \\n \\n  Exceptionally      focused      and      entirely      on      topic;      the      article      is      tightly      centered      on      the      subject,      with      every      piece      of      information      contributing  \\n \\n  to   \\n \\na      comprehensive      understanding      of      the      topic.\\n \\n \\n  Criteria      Description  \\n \\n  Score   \\n \\n|      Description  \\n \\n  Score   \\n \\n2      Description  \\n \\n  Score   \\n \\n3      Description  \\n \\n  Score   \\n \\n4      Description  \\n \\n  Score   \\n \\n5      Description  \\n \\n  Broad      Coverage:      Does      the      article      provide      an      in-depth      exploration      of      the      topic      and      have      good      coverage?\\n \\n \\n  Severely      lacking;      offers      little      to      no      coverage      of      the      topic’s      primary      aspects,      resulting      in   \\n \\na      very      narrow      perspective.\\n \\n \\n  Partial      coverage;      includes      some      of      the      topic’s      main      aspects      but      misses      others,      resulting      in      an      incomplete      portrayal.\\n \\n \\n  Acceptable      breadth;      covers      most      main      aspects,      though      it      may      stray      into      minor      unnecessary      details      or      overlook      some      relevant      points.\\n \\n \\n  Good      coverage;      achieves      broad      coverage      of      the      topic,      hitting      on      all      major      points      with      minimal      extraneous      information.\\n \\n \\n  Exemplary      in      breadth;      delivers      outstanding      coverage,      thoroughly      detailing      all      crucial      aspects      of      the      topic      without      including      irrelevant  \\n \\n  information.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 187, 'chunk_type': 'para'}, page_content='False      Negative       15%\\nTable      8:      Scoring      rubrics      on   \\n \\na      1-5      scale      for      the      evaluator      LLM.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 188, 'chunk_type': 'table'}, page_content='Error      Type       Topic      Unsupported      Sentence      Source\\n'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 189, 'chunk_type': 'table'}, page_content='Error      Type       Topic      Unsupported      Sentence      Source\\n |       Throughout      its      history,      religion      has      remained      the       paramount      aspect      of      Hawaiian      life      in      Lahaina      ,       permeating      every      daily      activity      and      significant      event[5]. |       [5]      “Religion,      Beliefs      &      Spirituality”       (The      source      discusses      religion      as      part      of      Hawaiian      life       but      does      not      mention      Lahania      .)\\n |       Lahaina,      Hawaii\\n |       [2]      “Crimean      Bridge      -      Wikipedia”       (The      source      says      “The      first      scheduled      passenger      train       crossed      the      bridge      on      25      December      2019,      while      the       bridge      was      opened      for      freight      trains      on      30      June      2020      ”.)       Completed      in      June      2020      ,      the      bridge      serves      as      a       major      supply      route      for      Russian      forces      in      the      region       and      is      significant      to      Russia’s      claim      over      the      disputed       territory[2][11].\\n | 2022      Crimean       Bridge      explosion\\n |       For      example,      comparisons      have      been      drawn      between       the      performance      of      LK-9      and      the      dynamic      resolution       capabilities      of      video      games      such      as      Battlefield      2042[22]. |       [22]      “Battlefield      2042      PC      performance      guide:      The      best       settings      for      a      high      frame      rate”       (      The      source      is      irrelevant      to      LK-99.      )\\n |       LK-99\\n'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 190, 'chunk_type': 'para'}, page_content='Error      Type       Topic      Unsupported      Sentence      Source\\nTable      9:      Examples      of      different      error      types      of      unsupported      sentences.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 191, 'chunk_type': 'para'}, page_content='Error      Type       Topic      Unsupported      Sentence      Source > We      use      Mistral      7B-Instruct!>      (Jiang      et      al.,      2023a)\\n \\n \\n  to      examine      whether      the      cited      passages      entail      the  \\n \\n  generated      sentence.\\nTable   \\n \\n4      reports      the      citation  \\n \\n  quality      of      articles      produced      by      our      approach,      show-  \\n \\n  ing      that      around      15%      sentences      in      generated      articles  \\n \\n  are      unsupported      by      citations.\\nWe      further      investi-  \\n \\n  gate      the      failure      cases      by      randomly      sampling      10  \\n \\n  articles      and      an      author      manually      examines      all      the  \\n \\n  unsupported      sentences      in      these      articles.\\nBesides  \\n \\n  sentences      that      are      incorrectly      split!®,      lack      citations,  \\n \\n  or      are      deemed      supported      by      the      author’s      judgment,  \\n \\n  our      analysis      identifies      three      main      error      categories  \\n \\n  (examples      are      given      in      Table      9):      improper      inferen-  \\n \\n  tial      linking,      inaccurate      paraphrasing,      and      citing  \\n \\n  irrelevant      sources.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 192, 'chunk_type': 'para'}, page_content='Error      Type       Topic      Unsupported      Sentence      Source > We      use      Mistral      7B-Instruct!>      (Jiang      et      al.,      2023a)\\n \\n \\n  We      show      the      error      distribution      in      Figure      6.      No-  \\n \\n  tably,      the      most      common      errors      stem      from      the      ten-  \\n \\n  dency      of      LLMs      to      form      improper      inferential      links  \\n \\n  between      different      pieces      of      information      presented  \\n \\n  in      the      context      window.\\nOur      analysis      of      citation  \\n \\n  quality      suggests      that,      in      addition      to      avoiding      hallu-  \\n \\n  cinations,      future      research      in      grounded      text      gener-  \\n \\n  ation      should      also      focus      on      preventing      LLMs      from  \\n \\n  making      overly      inferential      leaps      based      on      the      pro-  \\n \\n  vided      information.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 193, 'chunk_type': 'para'}, page_content=\"Error      Type       Topic      Unsupported      Sentence      Source > D      Human      Evaluation      Details\\n \\n \\n  We      recruited      10      experienced      Wikipedia      editors  \\n \\n  to      participate      in      our      study      by      creating   \\n \\na      research  \\n \\n  page      on      Meta-Wiki!”\\nand      reaching      out      to      active editors      who      have      recently      approved      articles      for  \\n \\n  Wikipedia.\\\\\\\\\\\\'®      Our      participation      group      includes   \\n \\n3       editors      with      1-5      years      of      experience,   \\n \\n4      with      6-10  \\n \\n  years,      and   \\n \\n3      with      over      15      years      of      contribution.\\n \\n \\n  The      study      was      approved      by      the      Institutional      Re-  \\n \\n  view      Board      of      our      institution      and      the      participants  \\n \\n  signed      the      consent      form      through      Qualtrics      ques-  \\n \\n  tionnaires      before      the      study      started.\"),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 194, 'chunk_type': 'para'}, page_content='Error      Type       Topic      Unsupported      Sentence      Source > D      Human      Evaluation      Details\\n \\n \\n  To      streamline      the      evaluation      of      grounded      articles,  \\n \\n  we      developed   \\n \\na      web      application,      which      features   \\n \\na       side-by-side      display      of      the      article      and      its      citation  \\n \\n  snippets,      to      gather      ratings      and      open-ended      feedback  \\n \\n  Shttps      ://huggingface.co/mistralai/  \\n \\n  Mistral-7B-Instruct-vQ.1  \\n \\n  \\\\\\\\\\\\\\'6Rollowing      Gao      et      al.\\n(2023),      we      check      citation      quality      in  \\n \\n  the      sentence      level      and      split      articles      into      sentences      using      NLTK  \\n \\n  sent_tokenize.\\nsent_tokenize      sometimes      fails      to      split      sen-  \\n \\n  tences      correctly      when      the      article      contains      special      words      like  \\n \\n  “No.12847”,      “Bhatia      et      al.\\n”,      etc.\\n \\n \\n  \"https      ://meta.wikimedia.org  \\n \\n  \\\\\\\\\\\\\\'8Since      evaluating      Wikipedia-like      articles      is      time-  \\n \\n  consuming      and      requires      expertise,      we      paid      each      participant  \\n \\n  50$      for      our      study.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 195, 'chunk_type': 'para'}, page_content='Error      Type       Topic      Unsupported      Sentence      Source > D      Human      Evaluation      Details\\n \\n \\n  for      each      article.\\nFigure   \\n \\n7      shows      the      screenshot      of  \\n \\n  our      web      application      and      the      full      article      produced  \\n \\n  by      STORM      is      included      in      Table      12.\\nFor      human  \\n \\n  evaluation,      we      use   \\n \\na   \\n \\n|      to   \\n \\n7      scale      for      more      fine-  \\n \\n  grained      evaluation.\\nThe      grading      rubric      is      included  \\n \\n  in      Table      10.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 196, 'chunk_type': 'para'}, page_content='Error      Type       Topic      Unsupported      Sentence      Source > D      Human      Evaluation      Details\\n \\n \\n  We      collected      the      pairwise      preferences      and      the  \\n \\n  perceived      usefulness      of      STORM      via      an      online      ques-  \\n \\n  tionnaire.\\nSpecifically,      for      the      perceived      usefulness,  \\n \\n  we      request      editors      to      rate      their      agreement      with      state-  \\n \\n  ments      “I      think      it      can      be      specifically      helpful      for      my  \\n \\n  pre-writing      stage      (e.g.,      collecting      relevant      sources,  \\n \\n  outlining,      drafting).\\n”,      “I      think      it      will      help      me      edit  \\n \\n  a      Wikipedia      article      for   \\n \\na      new      topic”,      “I      think      it  \\n \\n  can      be   \\n \\na      potentially      useful      tool      for      the      Wikipedia  \\n \\n  community”      on   \\n \\na      Likert      scale      of      1-5,      correspond-  \\n \\n  ing      to      Strongly      disagree,      Somewhat      disagree,      Nei-  \\n \\n  ther      agree      nor      disagree,      Somewhat      agree,      Strongly agree.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 197, 'chunk_type': 'para'}, page_content='Error      Type       Topic      Unsupported      Sentence      Source > E_      Error      Analysis\\n \\n \\n  While      articles      produced      by      STORM      are      preferred  \\n \\n  by      both      automatic      metrics      and      human      evaluation,  \\n \\n  experienced      editors      still      identified      multiple      prob-  \\n \\n  lems      with      the      machine-generated      articles.\\nWe      an-  \\n \\n  alyze      the      free-form      comments      and      summarize      the  \\n \\n  major      issues      in      Table      11.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 198, 'chunk_type': 'para'}, page_content='Error      Type       Topic      Unsupported      Sentence      Source > E_      Error      Analysis\\n \\n \\n  The      primary      issue      raised      is      that      the      generated  \\n \\n  articles      often      contain      emotional      language      and      lack  \\n \\n  neutrality,      primarily      due      to      the      source      material.\\n \\n \\n  STORM      currently      retrieves      grounding      sources  \\n \\n  from      the      Internet      which      is      not      neutral      and      con-  \\n \\n  tains      considerable      promotional      content      on      its      own.\\n \\n \\n  Addressing      this      bias      in      the      pre-writing      stage      repre-  \\n \\n  sents   \\n \\na      valuable      direction      for      future      research.\\nAn-  \\n \\n  other      major      issue      is      the      red      herring      fallacy      or      the  \\n \\n  over-association      of      unrelated      facts.\\nAddressing      this  \\n \\n  challenge      calls      for      high-level      sensemaking      rather  \\n \\n  than      mere      fact-level      verification.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 199, 'chunk_type': 'para'}, page_content='Error      Type       Topic      Unsupported      Sentence      Source > E_      Error      Analysis\\n \\n \\n  Interest      Level  \\n \\n  Not      engaging      at      all;      no      attempt      to      capture      the      reader’s      attention.\\n \\n \\n  Slightly      engaging      with      rare      moments      that      capture      attention.\\n \\n \\n  Fairly      engaging      with   \\n \\na      basic      narrative      but      lacking      depth.\\n \\n \\n  Moderately      engaging      with      several      interesting      points.\\n \\n \\n  Quite      engaging      with   \\n \\na      well-structured      narrative      and      noteworthy      points      that      frequently      capture      and      retain      attention.\\n \\n \\n  Very      engaging      with   \\n \\na      compelling      narrative      that      captures      and      mostly      retains      attention.\\n \\n \\n  Exceptionally      engaging      throughout,      with   \\n \\na      compelling      narrative      that      consistently      stimulates      interest.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 200, 'chunk_type': 'para'}, page_content='Error      Type       Topic      Unsupported      Sentence      Source > E_      Error      Analysis\\nMOawWPYWNr'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 201, 'chunk_type': 'para'}, page_content='Error      Type       Topic      Unsupported      Sentence      Source > E_      Error      Analysis\\n \\n \\n  Coherence      and      Organization  \\n \\n  Disorganized;      lacks      logical      structure      and      coherence.\\n \\n \\n  Poor      organization;      some      structure      is      evident      but      very      weak.\\n \\n \\n  Fairly      organized;   \\n \\na      basic      structure      is      present      but      not      consistently      followed.\\n \\n \\n  Organized;   \\n \\na      clear      structure      is      mostly      followed      with      some      lapses      in      coherence.\\n \\n \\n  Good      organization;   \\n \\na      clear      structure      with      minor      lapses      in      coherence.\\n \\n \\n  Very      well-organized;   \\n \\na      logical      structure      with      transitions      that      effectively      guide      the      reader.\\n \\n \\n  Excellently      organized;      the      article      is      logically      structured      with      seamless      transitions      and   \\n \\na      clear      argument.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 202, 'chunk_type': 'para'}, page_content='Error      Type       Topic      Unsupported      Sentence      Source > E_      Error      Analysis\\naw:'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 203, 'chunk_type': 'para'}, page_content='Error      Type       Topic      Unsupported      Sentence      Source > E_      Error      Analysis\\n \\n \\n  Relevance      and      Focus  \\n \\n  1:      Off-topic;      the      content      does      not      align      with      the      headline      or      core      subject.\\n \\n \\n  2:      Mostly      off-topic      with      some      relevant      points.\\n \\n \\n  3:      Somewhat      on      topic      but      with      several      digressions;      the      core      subject      is      evident      but      not      consistently      adhered      to.\\n \\n \\n  4:      Generally      on      topic,      despite   \\n \\na      few      unrelated      details.\\n \\n \\n  5:      Mostly      on      topic      and      focused;      the      narrative      has   \\n \\na      consistent      relevance      to      the      core      subject      with      infrequent      digressions.\\n \\n \\n  6:      Highly      relevant      with   \\n \\na      focused      narrative      and      purpose.\\n \\n \\n  7:      Exceptionally      focused      and      entirely      on      topic;      the      article      is      tightly      centered      on      the      subject,      with      every      piece      of      information      contributing      to   \\n \\na       comprehensive      understanding      of      the      topic.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 204, 'chunk_type': 'para'}, page_content='Error      Type       Topic      Unsupported      Sentence      Source > E_      Error      Analysis\\n \\n \\n  Broad      Coverage  \\n \\n  Severely      lacking;      offers      little      to      no      coverage      of      the      topic’s      primary      aspects,      resulting      in   \\n \\na      very      narrow      perspective.\\n \\n \\n  Minimal      coverage;      addresses      only   \\n \\na      small      selection      of      the      topic’s      main      aspects,      with      significant      omissions.\\n \\n \\n  Partial      coverage;      includes      some      of      the      topic’s      main      aspects      but      misses      others,      resulting      in      an      incomplete      portrayal.\\n \\n \\n  Acceptable      breadth;      covers      most      main      aspects,      though      it      may      stray      into      minor      unnecessary      details      or      overlook      some      relevant      points.\\n \\n \\n  Good      coverage;      achieves      broad      coverage      of      the      topic,      hitting      on      all      major      points      with      minimal      extraneous      information.\\n \\n \\n  Comprehensive;      provides      thorough      coverage      of      all      significant      aspects      of      the      topic,      with   \\n \\na      well-balanced      focus.\\n \\n \\n  Exemplary      in      breadth;      delivers      outstanding      coverage,      thoroughly      detailing      all      crucial      aspects      of      the      topic      without      including      irrelevant      information.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 205, 'chunk_type': 'para'}, page_content='TMAWP      YN > Verifiability\\n \\n \\n  1:      No      supporting      evidence;      claims      are      unsubstantiated.\\n \\n \\n  2:      Rarely      supported      with      evidence;      many      claims      are      unsubstantiated.\\n \\n \\n  3:      Inconsistently      verified;      some      claims      are      supported;      evidence      is      occasionally      provided.\\n \\n \\n  4:      Generally      verified;      claims      are      usually      supported      with      evidence;      however,      there      might      be   \\n \\na      few      instances      where      verification      is      lacking  \\n \\n  5:      Well-supported;      claims      are      very      well      supported      with      credible      evidence,      and      instances      of      unsupported      claims      are      rare.\\n \\n \\n  6:      Very      well-supported;      almost      every      claim      is      substantiated      with      credible      evidence,      showing   \\n \\na      high      level      of      thorough      verification.\\n \\n \\n  7:      Exemplary      verification;      each      claim      is      supported      by      robust,      credible      evidence      from      authoritative      sources,      reflecting      strict      adherence      to      the      no  \\n \\n  original      research      policy.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 206, 'chunk_type': 'para'}, page_content='TMAWP      YN > Verifiability\\nTable      10:      Scoring      rubrics      on   \\n \\na      1-7      scale      for      human      evaluation.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 207, 'chunk_type': 'para'}, page_content='TMAWP      YN > Issue       Mentioned      Time       Example      Comments\\n \\n \\n  The      word      “significant”      is      used      17      times      in      this      article.\\nVague      and      unsupported      claims      are  \\n \\n  made      about      broader      political      importance      and      “pivotal      role[s]”,      and      is      unencyclopedic.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 208, 'chunk_type': 'para'}, page_content='TMAWP      YN > Issue       Mentioned      Time       Example      Comments\\nUse      of      emotional      words,  \\n \\n  (comment      on      article      Lahaina,      Hawaii) [] but they still have not fixed the issue of neutral point of view.\\nIt is also evident in this  \\n \\n  article      that      the      writer’s      standpoint      is      biased      towards      Taylor      Swift.\\nOther      than      that,      it      did  \\n \\n  a      good      job      at      summarizing      key      points      and      putting      depth      into      this.\\n \\n \\n  unneutral      12      (comment      on      article      Speak      Now      (Taylor’s      Version))  \\n \\n  “The      film      was      also      featured      in      an      art      and      film      festival      hosted      by      The      California      Endowment,  \\n \\n  highlighting      the      power      of      stories      in      reshaping      narratives      about      communities.”\\nYes,      technically  \\n \\n  the      source      says      that,      but      it’s   \\n \\na      stretch      to      say      in      Wikipedia      voice      and      just      sounds      like  \\n \\n  non-neutral,      promotional      prose.\\n(comment      on      article      Gehraiyaan)  \\n \\n  Polling      from      America      shouldn’t      be      included      and      links      to      climate      change      shouldn’t      be  \\n \\n  made      unless      explicitly      connected      by      the      source.\\n(comment      on      article      Typhoon      Hinnamnor)  \\n \\n  Red      herring      fallacy,   \\n \\nu      Sourcing      seems      mostly      fine,      though      some      aren’t      directly      related      (Ex.      39,40).\\n \\n \\n  associating      unrelated      sources      (comment      on      article      Gehraiyaan)  \\n \\n  Here      is   \\n \\na      lengthy      digression      about      KISS,      not      necessary      because      the      article      on      the      band  \\n \\n  should      be      linked      to.\\n(comment      on      article      2022      AFL      Grand      Final)  \\n \\n  “One      study,      conducted      by      Sinéad      Griffin,   \\n \\na      physicist      at      the      Lawrence      Berkeley      National  \\n \\n  Laboratory,      provided      some      analysis      of      LK-99’s      abilities      using      supercomputer      simulations[20].”\\n \\n \\n  This      is      not      enough      information      about      the      analysis,      which      would      have      been      very      useful      in      the  \\n \\n  rr  \\n \\n.    \\n.\\n     article.\\n(comment      on      article      LK-99)  \\n \\n  Missing      important      information   \\n \\n6       Although      the      earthquake’s      immediate      aftermath      and      response      are      adequately      covered,      there  \\n \\n  could      be      more      about      the      long-term      socioeconomic      impact      and      recovery      processes.\\n \\n \\n  (comment      on      article      2022      West      Java      earthquake)  \\n \\n  Words      like      “now”      should      be      avoided      in      Wikipedia      articles      to      prevent      them      from      becoming  \\n \\n  dated      and      phrases      such      as,      “as      of      December      2023”      should      be      used      instead.\\n \\n \\n  Improper      handling      of   \\n \\n5      (comment      on      article      Cyclone      Batsirai)  \\n \\n  time-sensitive      information      “as      of      December      13”      doesn’t      specify   \\n \\na      year,      and      is      old      information  \\n \\n  (comment      on      article      2022      West      Java      earthquake) too      many      subsections      in      the      “Recovery      and      Rehabilitation”      section  \\n \\n  (comment      on      article      2022      West      Java      earthquake)'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 209, 'chunk_type': 'para'}, page_content='TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\n \\n \\n  Section      organization      problem   \\n \\n5   \\n \\nI      do      not      like      how      the      article      is      organized,      with      too      many      headers      cluttering      the      article,  \\n \\n  making      it      not      as      readable.\\nOther      than      that,      the      AI      did      great      work      on      the      piece.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 210, 'chunk_type': 'table'}, page_content=\"TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\n |       Select      a      key       8      v       Selected      Key:      8       Title:      Taylor      Hawkins      -      marshall.com       Snippets:       In      1995,      off      the      back      of      her      hit      album       ‘Jagged      Little      Pill’,      Canadian      American       superstar      Alanis      Morissette      recruited      him       to      be      her      touring      drummer      for      her      18       month      album      tour,      along      with      him       featuring      in      the      music      videos      for      “You       Oughta      Know’,      “All|      Really      Want”      and       “You      Learn’.      It      was      during      this      tour      that      he       met      his      musical      soul      mate,      Dave      Grohl.       The      tour      with      Alanis      ended      and      he\\\\\\\\\\\\'d      heard       that      Dave      and      Foo      Fighters      were      looking       for      anew      drummer,      so      Taylor      enquired.       Dave      initially      thought      that      Taylor      wouldn’t       be      interested      in      joining      as      Alanis       Morissette      was      much      bigger      than      the      Foo       Fighters      at      that      time,      but      Taylor      jumped      at       the      chance      due      to      his      overwhelming       desire      to      be      in      a      rock      band.      It      was      from       that      point      that      the      bromance      between       them      began      and      on      March      18th      1997       Taylor      was      announced      as      their      new       drummer.       Url:      https://marshall.com/live-for-       music/drum-icons/taylor-hawkins | Logout  \\n \\n  Select      an      option:  \\n \\n  (11,      ‘Taylor      Hawkins      [Quite      Richard}’)   \\n \\n¥       You      selected:      Taylor      Hawkins      [Quite      Richard]  \\n \\n  Taylor      Hawkins  \\n \\n  Oliver      Taylor      Hawkins      (1972-2022)      was      an      American      musician,      best      known      as      the      drummer      for      the      rock      band      Foo      Fighters[1].\\nBorn      in      Fort      Walton,      Texas,      Hawkins’      love      for      music      was      ignited      at   \\n \\na      young  \\n \\n  age,      particularly      after      watching   \\n \\na      Queen      concert      in      1982[2][3][5].\\nHe      kick-started      his      professional      career      as      the      drummer      for      Alanis      Morissette’s      band      during      her      world      tour      for      the      hit      album      ‘Jagged      Little  \\n \\n  Pill\\\\\\\\\\\\'[8][9].\\nHis      talents      were      recognized      by      Dave      Grohl,      who      invited      him      to      join      the      Foo      Fighters      in      1997,      marking      the      beginning      of      his      impactful      tenure      with      the      band{7][8].\\n \\n \\n  Hawkins      was      celebrated      for      his      versatile      drumming      style,      drawing      influence      from      renowned      drummers      like      Roger      Taylor,      Neil      Peart,      Phil      Collins,      Alex      Van      Halen,      and      Stewart      Copeland[14).\\nHis  \\n \\n  performances,      marked      by   \\n \\na      unique      energy      and      aggressive      style      of      play,      earned      him      recognition      as      one      of      the      top      rock      drummers      of      his      era[15].\\nApart      from      his      role      in      the      Foo      Fighters,      Hawkins\\\\\\\\\\\\'      passion  \\n \\n  for      music      saw      him      involved      in      numerous      side      projects      and      collaborations,      cementing      his      place      in      the      world      of      rock      music(10].\\n \\n \\n  Outside      of      his      professional      life,      Hawkins      was      known      for      his      resilience      and      dedication      to      his      family.\\nDespite      personal      struggles,      including   \\n \\na      near-fatal      drug      overdose      in      2001,      Hawkins      remained      committed  \\n \\n  to      his      musical      career{4][9].\\nHis      legacy      continues      to      inspire      musicians      and      fans      alike,      as      his      contributions      to      rock      music,      coupled      with      his   \\n \\ni       industry(13}.\\n \\n \\n  spirit,      made      him      an      icon      in      the      music  \\n \\n  Hawkins’      sudden      death      in      2022      while      on      tour      in      Bogotd,      Colombia,      sent      shockwaves      through      the      music      world[34].\\nTributes      poured      in      from      around      the      globe,      reflecting      the      respect      and      admiration  \\n \\n  Hawkins      had      garnered      during      his      lifetime[21][31].\\nHis      life      and      career      were      honored      at   \\n \\na      star-studded      tribute      concert      in      Los      Angeles,      attesting      to      the      enduring      impact      of      his      music[22].\\n | Early      Life      and      Background\\n |       Oliver      Taylor      Hawkins,      known      as      Taylor      Hawkins,      was      born      and      raised      in      Fort      Walton,      Texas[3].      His      family      moved      to      Laguna      Beach,      California      when      he      was      four      years      old[3].      He      has      two      younger      siblings,       a      brother      named      Jason,      and      a      sister      named      Heather[3].      Asa      child,      Hawkins      wa:      influenced      by      his      paternal      Josie      Hawkins,      who      had      grown      up      during      the      Great      Depression      and       lived      in      Jackson,      Mississippi(2].\\n |       During      his      high      school      days      at      Laguna      Beach      High      School,      from      where      he      graduated      in      1990,      he      became      friends      with      Jon      Davison,      who      later      became      the      lead      vocalist      of      the      band      Yes(2][3].      His      interest      in       music      was      nurtured      from      an      early      age,      particularly      after      watching      a      Queen      concert      in      1982      which      inspired      him      to      learn      to      play      the      drums(2][5].      He      noted      that      music      was      a      constant      presence      in      his      family       Taylor      Hawkins\\n\"),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 211, 'chunk_type': 'para'}, page_content='TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\n \\n \\n  Oliver      Taylor      Hawkins      (1972-2022)      was      an      American      musician,      best      known      as      the      drummer      for      the      rock  \\n \\n  band      Foo      Fighters[1].\\nBorn      in      Fort      Walton,      Texas,      Hawkins’      love      for      music      was      ignited      at   \\n \\na      young      age,  \\n \\n  particularly      after      watching   \\n \\na      Queen      concert      in      1982[2][3][5].\\nHe      kick-started      his      professional      career  \\n \\n  as      the      drummer      for      Alanis      Morissette’s      band      during      her      world      tour      for      the      hit      album      ’Jagged      Little  \\n \\n  Pill’[8]L[9].\\nHis      talents      were      recognized      by      Dave      Grohl,      who      invited      him      to      join      the      Foo      Fighters      in  \\n \\n  1997,      marking      the      beginning      of      his      impactful      tenure      with      the      band[7][8].'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 212, 'chunk_type': 'para'}, page_content='TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\n \\n \\n  Hawkins      was      celebrated      for      his      versatile      drumming      style,      drawing      influence      from      renowned      drummers      like  \\n \\n  Roger      Taylor,      Neil      Peart,      Phil      Collins,      Alex      Van      Halen,      and      Stewart      Copeland[14].\\nHis      performances,  \\n \\n  marked      by   \\n \\na      unique      energy      and      aggressive      style      of      play,      earned      him      recognition      as      one      of      the      top      rock  \\n \\n  drummers      of      his      era[15].\\nApart      from      his      role      in      the      Foo      Fighters,      Hawkins’      passion      for      music      saw  \\n \\n  him      involved      in      numerous      side      projects      and      collaborations,      cementing      his      place      in      the      world      of      rock  \\n \\n  music[10].'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 213, 'chunk_type': 'para'}, page_content='TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\n \\n \\n  Outside      of      his      professional      life,      Hawkins      was      known      for      his      resilience      and      dedication      to      his      family.\\n \\n \\n  Despite      personal      struggles,      including   \\n \\na      near-fatal      drug      overdose      in      2001,      Hawkins      remained      committed      to  \\n \\n  his      musical      career[4][9].\\nHis      legacy      continues      to      inspire      musicians      and      fans      alike,      as      his      contributions  \\n \\n  to      rock      music,      coupled      with      his      indomitable      spirit,      made      him      an      unforgettable      icon      in      the      music  \\n \\n  industry[13].'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 214, 'chunk_type': 'para'}, page_content='TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\n \\n \\n  Hawkins’      sudden      death      in      2022      while      on      tour      in      Bogota,      Colombia,      sent      shockwaves      through      the      music  \\n \\n  world[34].\\nTributes      poured      in      from      around      the      globe,      reflecting      the      respect      and      admiration      Hawkins  \\n \\n  had      garnered      during      his      lifetime[21][31].\\nHis      life      and      career      were      honored      at   \\n \\na      star-studded      tribute  \\n \\n  concert      in      Los      Angeles,      attesting      to      the      enduring      impact      of      his      music[22].'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 215, 'chunk_type': 'para'}, page_content='TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\n#\\n \\n   Early      Life      and      Background  \\n \\n  Oliver      Taylor      Hawkins,      known      as      Taylor      Hawkins,      was      born      and      raised      in      Fort      Walton,      Texas[3].\\nHis  \\n \\n  family      moved      to      Laguna      Beach,      California      when      he      was      four      years      old[3].\\nHe      has      two      younger      siblings,  \\n \\n  a      brother      named      Jason,      and   \\n \\na      sister      named      Heather[3].\\nAs   \\n \\na      child,      Hawkins      was      particularly      influenced  \\n \\n  by      his      paternal      grandmother,      Josie      Hawkins,      who      had      grown      up      during      the      Great      Depression      and      lived      in  \\n \\n  Jackson,      Mississippi[1].'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 216, 'chunk_type': 'para'}, page_content='TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\n \\n \\n  During      his      high      school      days      at      Laguna      Beach      High      School,      from      where      he      graduated      in      1990,      he      became  \\n \\n  friends      with      Jon      Davison,      who      later      became      the      lead      vocalist      of      the      band      Yes[2][3].\\nHis      interest      in  \\n \\n  music      was      nurtured      from      an      early      age,      particularly      after      watching   \\n \\na      Queen      concert      in      1982      which      inspired  \\n \\n  him      to      learn      to      play      the      drums[2][5].\\nHe      noted      that      music      was   \\n \\na      constant      presence      in      his      family      home[5].'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 217, 'chunk_type': 'para'}, page_content='TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\n \\n \\n  Despite      facing      certain      hardships      during      his      upbringing,      including      his      mother’s      struggles      with      \"demons\",  \\n \\n  Hawkins      pursued      his      musical      ambitions[4].\\nHe      credits      his      older      sister      Heather      for      taking      care      of      the  \\n \\n  family      during      difficult      times[4].'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 218, 'chunk_type': 'para'}, page_content='TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\n \\n \\n  His      first      major      musical      experience      came      from      playing      drums      for      Alanis      Morissette’s      album,      Jagged      Little  \\n \\n  Pill,      and      accompanying      her      on      the      subsequent      tour[3].\\nThis      marked      the      beginning      of      his      professional  \\n \\n  career      in      the      music      industry.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 219, 'chunk_type': 'para'}, page_content='TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\n#\\n \\n   Career  \\n \\n  Taylor      Hawkins      began      his      professional      music      career      playing      in      Alanis      Morissette’s      band      during      her  \\n \\n  18-month      world      tour      in      support      of      the      hit      album      ’Jagged      Little      Pill’      from      1995      to      1997[8][9].\\nHis  \\n \\n  performances      not      only      in      the      tour      but      also      in      the      music      videos      for      “You      Oughta      Know”,      “All   \\n \\nI      Really      Want”  \\n \\n  and      “You      Learn”      introduced      him      to      the      world      of      rock      music      and      ultimately      led      to      his      meeting      with      Dave  \\n \\n  Grohl[8].\\nThroughout      this      time,      Hawkins      contributed      significantly      to      the      band’s      sound      and      performance,  \\n \\n  transforming      the      songs      from      their      original      drum      loop      format      to   \\n \\na      rock-band      vibe      that      resonated      with  \\n \\n  audiences[1][7].'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 220, 'chunk_type': 'para'}, page_content='TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\n \\n \\n  In      1997,      Hawkins      was      asked      by      Grohl      to      join      the      Foo      Fighters,      an      invitation      that      he      readily      accepted[7][8].\\n \\n \\n  At      the      time,      Grohl      thought      it      was   \\n \\na      long      shot      to      recruit      Hawkins      given      that      Morissette      was      at      the      height  \\n \\n  of      her      career,      but      Hawkins’      desire      to      be   \\n \\na      part      of   \\n \\na      rock      band      compelled      him      to      make      the      move[7].\\nThis  \\n \\n  marked      the      beginning      of      Hawkins’      tenure      as      the      drummer      of      the      Foo      Fighters,   \\n \\na      role      that      he      would      play  \\n \\n  until      his      passing[6][9].'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 221, 'chunk_type': 'para'}, page_content='TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\n \\n \\n  Apart      from      his      work      with      Morissette      and      the      Foo      Fighters,      Hawkins      had      an      array      of      other      musical  \\n \\n  experiences[10].\\nHe      drummed      for      Sass      Jordan      before      joining      Morissette’s      touring      band[10].\\nHe      was      part  \\n \\n  of      an      ad      hoc      drum      supergroup      called      SOS      Allstars      and      filled      the      void      for      Coheed      and      Cambria’s      2007  \\n \\n  album      after      their      drummer      Josh      Eppard      left      the      group[10].\\nIn      addition,      Hawkins      formed      his      own      side  \\n \\n  project,      the      Coattail      Riders,      in      2005,      through      which      he      recorded      his      own      music      and      took      the      project      on  \\n \\n  the      road,      performing      in      small      clubs      despite      the      Foo      Fighters’      arena-status[7].\\nHis      son,      Shane      Hawkins,  \\n \\n  has      since      taken      on      his      father’s      legacy,      joining      the      Foo      Fighters      for   \\n \\na      performance      during      the      Boston  \\n \\n  Calling      Music      Festival      in      2023[6].'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 222, 'chunk_type': 'para'}, page_content='TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\n#\\n \\n   Musical      Style      and      Influences  \\n \\n  Taylor      Hawkins      was   \\n \\na      profound      drummer,      with      his      musical      style      and      influences      spreading      across   \\n \\na      wide  \\n \\n  array      of      rock      genres[11].\\nKnown      for      his      passionate      fandom      of      groups      that      came      before      him,      Hawkins  \\n \\n  regularly      expressed      his      admiration      for      bands      like      Rush,      Genesis,      and      the      Police,      all      of      which      featured  \\n \\n  some      of      the      greatest      drummers      in      rock      history      like      Neil      Peart,      Phil      Collins,      and      Stewart      Copeland[11].'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 223, 'chunk_type': 'para'}, page_content='TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\nHe      was      heavily      influenced      by      his      love      for      classic      rock,      as      evidenced      by      his      performances,      where      he  \\n \\n  covered      songs      from      bands      like      Van      Halen[11].'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 224, 'chunk_type': 'para'}, page_content='TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\n \\n \\n  Hawkins      drew      influences      from   \\n \\na      variety      of      drumming      styles,      developing   \\n \\na      signature      style      inspired      by  \\n \\n  greats      like      Roger      Taylor,      Neil      Peart,      Phil      Collins,      Alex      Van      Halen,      and      Stewart      Copeland[14].\\nThis  \\n \\n  distinctive      style      and      influence      extended      to      his      drum      kit,      which      incorporated      elements      like      rototoms  \\n \\n  and      concert      toms[14].'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 225, 'chunk_type': 'para'}, page_content='TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\n \\n \\n  Beyond      his      influences,      Hawkins      had   \\n \\na      unique      energy      that      made      him      stand      out      as   \\n \\na      drummer.\\nHis      performances  \\n \\n  were      recognized      for      their      power,      and      he      was      known      for      his      enthusiastic      and      aggressive      style      of      play[15].\\n \\n \\n  This      earned      him      recognition      as      one      of      the      top      rock      drummers      of      his      time,      with      his      passion      for      music  \\n \\n  living      on      through      his      performances[14].'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 226, 'chunk_type': 'para'}, page_content='TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\n \\n \\n  Through      his      career,      Hawkins      left      an      indelible      mark      on      rock      music,      through      his      distinct      style,      passion,  \\n \\n  and      contributions      to      the      music      industry[13].\\nHis      love      for      music      and      dedication      to      his      craft      made      him  \\n \\n  an      unforgettable      icon      in      the      world      of      rock      music[13].'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 227, 'chunk_type': 'para'}, page_content='TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\n#\\n \\n   Personal      Life  \\n \\n  Taylor      Hawkins      married      Alison      Hawkins,      an      American      celebrity      and      entrepreneur,      in      2005[18].\\nThe      couple  \\n \\n  had      three      children,      Oliver,      Annabelle,      and      Everleigh[19].\\nHawkins’      commitment      to      his      family      was      evident;'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 228, 'chunk_type': 'para'}, page_content='TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\n \\n \\n  in      fact,      he      even      wrote   \\n \\na      song      for      his      middle      child,      Annabelle[9].'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 229, 'chunk_type': 'para'}, page_content='TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\n \\n \\n  In      his      personal      life,      Hawkins      had      also      struggled      with      drug      use,      which      nearly      claimed      his      life      in   \\n \\na      2001  \\n \\n  overdose[9][7][4].\\nHowever,      he      managed      to      overcome      this      challenge,      and      later      expressed      gratitude      for  \\n \\n  the      experience      as   \\n \\na      lesson      that      allowed      him      to      realize      the      destructive      path      he      was      on[7].'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 230, 'chunk_type': 'para'}, page_content='TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\n \\n \\n  Outside      of      his      main      role      in      the      Foo      Fighters,      Hawkins      also      pursued      various      side      projects      including      the  \\n \\n  Birds      of      Satan,      NHC,      and      Chevy      Metal.\\nHis      motivation      for      such      ventures      was   \\n \\na      constant      drive      to      create  \\n \\n  and      his      love      for      music[7].\\nHawkins      was      also      known      for      his      unabashed      fanboy      nature,      often      vocalizing  \\n \\n  his      admiration      for      fellow      musicians      and      his      heroes[7].'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 231, 'chunk_type': 'para'}, page_content='TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\n#\\n \\n   Legacy      and      Impact Taylor      Hawkins      was      known      for      his      raw      and      authentic      drumming      style,      described      as      \"courageous,      damaged  \\n \\n  and      unflinchingly      authentic”[20].\\nHis      work      with      the      Foo      Fighters,      as      well      as      his      various      collaborations n’      roll[10].\\n \\n \\n  ‘ and      side      projects,      made      him   \\n \\na      celebrated      figure      in      rock  \\n \\n  Hawkins’      death      in      2022      was      met      with      heartfelt      tributes      from      colleagues      and      fans      around      the      world.\\n \\n \\n  Notable      tributes      came      from      rock      legends      like      Roger      Taylor      of      Queen,      who      considered      Hawkins      as   \\n \\na      kind,  \\n \\n  brilliant      man      and      an      inspirational      mentor,      likening      his      death      to      \"losing   \\n \\na      younger      favourite      brother”[21].\\n \\n \\n  Similarly,      Led      Zeppelin’s      Jimmy      Page      admired      his      technique,      energy      and      spirited      enthusiasm[21].'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 232, 'chunk_type': 'para'}, page_content='TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\n \\n \\n  An      LA      tribute      concert      held      in      his      honor      included      guest      drummers      like      Lars      Ulrich      of      Metallica,      Travis  \\n \\n  Barker      of      blink-182,      and      Brad      Wilk      of      Rage      Against      the      Machine.\\nSingers      like      Miley      Cyrus      and      Alanis  \\n \\n  Morissette      also      performed      at      the      concert[22].'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 233, 'chunk_type': 'para'}, page_content='TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\n \\n \\n  Apart      from      his      music,      Taylor      Hawkins      also      contributed      to      charities      Music      Support      and      MusiCares,      both      of  \\n \\n  which      were      chosen      by      the      Hawkins      family[23].\\nHe      had      received      numerous      accolades      throughout      his      career,  \\n \\n  including      27      Grammy      nominations,      of      which      he      won      14[2].\\nIn      2021,      the      Foo      Fighters      were      inducted      into  \\n \\n  the      Rock      and      Roll      Hall      of      Fame[9].'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 234, 'chunk_type': 'para'}, page_content='TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\n#\\n \\n   Discography  \\n \\n  Taylor      Hawkins      also      led   \\n \\na      notable      music      career      through      his      own      side      projects      and      collaborations[10].\\n \\n \\n  Aside      from      his      work      with      the      Foo      Fighters,      Hawkins      formed      and      fronted      the      band      Taylor      Hawkins   \\n \\n&      The  \\n \\n  Coattail      Riders,   \\n \\na      project      which      originated      from      jamming      sessions      with      his      friend      Drew      Hester[10].'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 235, 'chunk_type': 'para'}, page_content='TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\n###      Taylor      Hawkins   \\n \\n&      The      Coattail      Riders  \\n \\n  Taylor      Hawkins   \\n \\n&      The      Coattail      Riders,   \\n \\na      band      formed      in      2004,      have      released      three      albums      and      their  \\n \\n  music      spans      genres      including      Hard      Rock,      Art      Rock,      and      Alternative      Rock[24][25][26].\\nThe      band      grew      from  \\n \\n  an      initial      casual      jamming      session,      gradually      evolving      into   \\n \\na      more      formal      arrangement      that      led      to      the  \\n \\n  production      of      record      albums.\\nNotably,      these      albums      featured      guest      appearances      by      renowned      musicians  \\n \\n  such      as      Dave      Grohl,      Queen’s      Brian      May      and      Roger      Taylor,      The      Cars’      Elliot      Easton,      Perry      Farrell,      and       Jon      Davison,      who      is   \\n \\na      school      friend      of      Hawkins’[10].'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 236, 'chunk_type': 'para'}, page_content='TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\n###      Red      Light      Fever  \\n \\n  Red      Light      Fever,      released      on      April      19,      2010,      was      the      band’s      first      album[29][30].\\nPrior      to      its      release,  \\n \\n  Hawkins      revealed      in      an      interview      that      the      album      had      completed      the      recording      and      production      stages,      but  \\n \\n  its      title      and      release      date      were      yet      to      be      determined[29].\\nRed      Light      Fever      was      recorded      at      the      Foo  \\n \\n  Fighters’      Studio      606      in      California      and      featured      guest      musicians      such      as      Brian      May      and      Roger      Taylor      of  \\n \\n  Queen,      Dave      Grohl      of      Foo      Fighters,      and      Elliot      Easton      of      The      Cars[29][30].'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 237, 'chunk_type': 'para'}, page_content='TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\n##      Get      the      Money  \\n \\n  Get      the      Money,      the      third      album      from      Taylor      Hawkins   \\n \\n&      The      Coattail      Riders,      was      released      on      November      8,  \\n \\n  2019[29].\\nThe      album’s      first      single,      \"Crossed      the      Line”,      released      on      October      15,      2019,      featured      Dave  \\n \\n  Grohl      and      Jon      Davison,      the      frontman      of      Yes[29].\\nThe      music      video      for      the      single      \"I      Really      Blew      It”      also  \\n \\n  featured      appearances      from      Grohl      and      Perry      Farrel1[29].'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 238, 'chunk_type': 'para'}, page_content='TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\n#\\n \\n   Collaborations      and      Guest      Appearances  \\n \\n  Throughout      his      career,      Taylor      Hawkins      collaborated      with      various      prominent      artists      and      bands.\\nThe  \\n \\n  Coattail      Riders’      albums      notably      featured      appearances      from      luminaries      such      as      Brian      May      and      Roger      Taylor  \\n \\n  of      Queen,      Chrissie      Hynde,      Nancy      Wilson      of      Heart,      Sex      Pistol      Steve      Jones      and      James      Gang’s      Joe      Walsh[28].'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 239, 'chunk_type': 'para'}, page_content='TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\n \\n \\n  Hawkins      also      fronted      another      group,      The      Birds      of      Satan,      which      evolved      from      his      heavy      rock      covers      band,  \\n \\n  Chevy      Metal[28].'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 240, 'chunk_type': 'para'}, page_content='TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\nDespite      his      diverse      musical      engagements,      Hawkins      always      maintained   \\n \\na      close      allegiance      with      the      Foo  \\n \\n  Fighters,      which      remained      the      center      of      his      music      life[7][28].'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 241, 'chunk_type': 'para'}, page_content='TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\n#\\n \\n   Tragic      Passing  \\n \\n  Taylor      Hawkins,      the      esteemed      drummer      of      the      alt-rock      band      Foo      Fighters,      passed      away      suddenly      on      March  \\n \\n  25,      2022,      while      on      tour      with      his      band      in      Bogota,      Colombia[34].\\nThe      official      cause      of      death      was      cardiac  \\n \\n  arrest,      though      inquiries      were      raised      concerning      the      presence      of      drugs      in      his      system      and      their      potential  \\n \\n  contribution      to      his      death[33][34].\\nOn      the      night      of      his      passing,      paramedics      were      called      to      the      Four  \\n \\n  Seasons      hotel      in      Bogota      due      to      reports      of      chest      pain      from      an      unnamed      guest,      later      revealed      to      be  \\n \\n  Hawkins[34].\\nUnfortunately,      resuscitation      efforts      were      unsuccessful,      and      Hawkins      was      declared      dead      at  \\n \\n  the      scene[34].'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 242, 'chunk_type': 'para'}, page_content='TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\n \\n \\n  The      news      of      Hawkins’      sudden      demise      was      announced      on      the      morning      of      March      25th,      2022,      which      left      the      music  \\n \\n  world      in      shock[32].\\nThe      band      confirmed      the      news      with   \\n \\na      short      statement,      expressing      their      devastation  \\n \\n  at      the      loss      of      Hawkins,      whose      \"musical      spirit      and      infectious      laughter”      would      live      on      forever[32].'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 243, 'chunk_type': 'para'}, page_content='TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\n \\n \\n  As   \\n \\na      result      of      Hawkins’      untimely      passing,      the      band      canceled      their      ongoing      South      American      tour[33].\\nThe  \\n \\n  festival      stage      at      the      Estéreo      Picnic      Festival,      where      the      Foo      Fighters      were      scheduled      to      perform      that  \\n \\n  night,      was      transformed      into   \\n \\na      candlelight      vigil      in      memory      of      Hawkins[33].'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 244, 'chunk_type': 'para'}, page_content='TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\n##      Tributes      and      Remembrances  \\n \\n  In      the      wake      of      Hawkins’      death,      tributes      from      fans      and      colleagues      alike      poured      in      from      around      the  \\n \\n  world[21][31].\\nAmong      the      many      paying      their      respects      were      legendary      rock      and      roll      musicians      like      Roger  \\n \\n  Taylor,      the      drummer      of      Queen,      who      Hawkins      credited      with      inspiring      his      own      career      behind      the      drum      set[21].\\n \\n \\n  In      heartfelt      social      media      posts,      Taylor      described      Hawkins      as      an      \"inspirational      mentor”      and   \\n \\na      \"kind  \\n \\n  brilliant      man\"[21],      while      Led      Zeppelin’s      Jimmy      Page      reminisced      about      sharing      the      stage      with      Hawkins  \\n \\n  and      praised      his      \"technique,      energy      and      spirited      enthusiasm\"[21].'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 245, 'chunk_type': 'para'}, page_content='TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\n \\n \\n  There      were      also      numerous      onstage      tributes      to      Hawkins.\\nNotably,      Miley      Cyrus      expressed      her      grief      and      sent  \\n \\n  peaceful      wishes      to      the      Foo      Fighters      and      the      Hawkins      family      during   \\n \\na      performance      at      Lollapalooza[31].'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 246, 'chunk_type': 'para'}, page_content='TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\n \\n \\n  Similarly,      Liam      Gallagher      of      Oasis      dedicated      one      of      the      band’s      biggest      hits      to      Hawkins      during   \\n \\na      concert  \\n \\n  at      the      Royal      Albert      Hall      in      London[31].'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 247, 'chunk_type': 'para'}, page_content='TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\nFans      gathered      outside      the      hotel      where      Hawkins      died,      lighting      candles,      leaving      flowers,      and      singing      the  \\n \\n  band’s      songs      in      his      honor[31].'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 248, 'chunk_type': 'para'}, page_content='TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\n \\n \\n  Hawkins’      life      and      career      were      celebrated      in   \\n \\na      star-studded      tribute      concert      in      Los      Angeles,      which      saw  \\n \\n  performances      from      over      5@      musicians,      including      his      former      bands      and      colleagues      from      Def      Leppard,      Queen,  \\n \\n  and      Foo      Fighters[22].'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 249, 'chunk_type': 'para'}, page_content='TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\nTable      12:      STORM’s      generated      article      for      “Taylor      Hawkins”.\\n“#’,      “##”      indicate      the      section      title      and      subsection      title  \\n \\n  respectively.\\nNumbers      in      brackets      indicate      the      cited      references.')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 0, 'chunk_type': 'para'}, 'page_content': 'Abstract\\n \\n \\n  We      study      how      to      apply      large      language      models  \\n \\n  to      write      grounded      and      organized      long-form      ar-  \\n \\n  ticles      from      scratch,      with      comparable      breadth  \\n \\n  and      depth      to      Wikipedia      pages.\\nThis      underex-  \\n \\n  plored      problem      poses      new      challenges      at      the  \\n \\n  pre-writing      stage,      including      how      to      research  \\n \\n  the      topic      and      prepare      an      outline      prior      to      writ-  \\n \\n  ing.\\nWe      propose      STORM,   \\n \\na      writing      system  \\n \\n  for      the      Synthesis      of      Topic      Outlines      through  \\n \\n  Retrieval      and      Multi-perspective      Question      Ask-  \\n \\n  ing.\\nSTORM      models      the      pre-writing      stage      by\\n(1)      discovering      diverse      perspectives      in      research-  \\n \\n  ing      the      given      topic,      (2)      simulating      conversa-  \\n \\n  tions      where      writers      carrying      different      perspec-  \\n \\n  tives      pose      questions      to   \\n \\na      topic      expert      grounded  \\n \\n  on      trusted      Internet      sources,      (3)      curating      the      col-  \\n \\n  lected      information      to      create      an      outline.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 1, 'chunk_type': 'para'}, 'page_content': 'Abstract\\n \\n \\n  For      evaluation,      we      curate      FreshWiki,   \\n \\na      dataset  \\n \\n  of      recent      high-quality      Wikipedia      articles,      and       formulate      outline      assessments      to      evaluate      the  \\n \\n  pre-writing      stage.\\nWe      further      gather      feedback  \\n \\n  from      experienced      Wikipedia      editors.\\nCom-  \\n \\n  pared      to      articles      generated      by      an      outline-  \\n \\n  driven      retrieval-augmented      baseline,      more      of  \\n \\n  STORM’;      articles      are      deemed      to      be      organized  \\n \\n  (by   \\n \\na      25%      absolute      increase)      and      broad      in      cov-  \\n \\n  erage      (by      10%).\\nThe      expert      feedback      also  \\n \\n  helps      identify      new      challenges      for      generating  \\n \\n  grounded      long      articles,      such      as      source      bias  \\n \\n  transfer      and      over-association      of      unrelated      facts.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 2, 'chunk_type': 'para'}, 'page_content': 'Abstract\\n1\\n \\n   Introduction  \\n \\n  Large      language      models      (LLMs)      have      demonstrated  \\n \\n  impressive      writing      capabilities      (Yang      et      al.,      2023;  \\n \\n  Pavlik,      2023;      Wenzlaff      and      Spaeth,      2022;      Fitria,  \\n \\n  2023),      but      it      is      unclear      how      we      can      use      them      to  \\n \\n  write      grounded,      long-form      articles,      like      full-length  \\n \\n  Wikipedia      pages.\\nSuch      expository      writing,      which  \\n \\n  seeks      to      inform      the      reader      on   \\n \\na      topic      in      an      or-  \\n \\n  ganized      manner      (Weaver      II      and      Kintsch,      1991;  \\n \\n  Balepur      et      al.,      2023),      requires      thorough      research  \\n \\n  and      planning      in      the      pre-writing      stage      (Rohman,  \\n \\n  Writing  \\n \\n  tify,      evaluate,      and      organize      external      sources   \\n \\n-   \\n \\na      task  \\n \\n  that      is      challenging      even      for      experienced      writers.\\n \\n \\n  Automating      this      process      can      facilitate      individuals  \\n \\n  in      initiating      in-depth      learning      about   \\n \\na      topic      and       greatly      reduce      the      expensive      expert      hours      neces-  \\n \\n  sary      for      their      expository      writing.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 3, 'chunk_type': 'table'}, 'page_content': 'Abstract\\n |  |       ee      Prewriting | \\n | --- | --- | ---\\n |  | =\\n \\n   Full-length  \\n \\n  5      Article |       =      Full-length       5      Article\\n |       arXiv:2402.14207v2      [cs.CL]      8      Apr      2024 | 2022      Winter      Olympics      [=      Outline   \\n \\n|       Opening      Ceremony  \\n \\n  Research      via      Question      Asking  \\n \\n  (A)      Direct      Prompting  \\n \\n  -y      Prompt:      Ask      30      questions      about      the      given      topic.\\n \\n \\n  1.\\nWhen      was      the      opening      ceremony      held?\\n \\n \\n  {22}      2.\\nWhere      was      the      opening      ceremony      held?\\n \\n \\n  LLM      3.\\nHow      many      countries      participated      in      the      opening      ceremony?\\n \\n \\n  (B)      Perspective-Guided      Question      Asking  \\n \\n  Prompt:      You      are      an      event      planner      who      focuses      on      the B® preparation of the opening ceremony.\\n \\n \\n  1.\\nCan      you      provide      any      information      about      the      transportation  \\n \\n  arrangements      for      the      opening      ceremony?\\n \\n \\n  Lim      2.\\nCan      you      provide      any      information      about      the      budget      for      the  \\n \\n  2022      Winter      Olympics      opening      ceremony?\\n \\n \\n  (C)      Conversational      Question      Asking  \\n \\n  Can      you      provide      me      with   \\n \\na      list      of      the      participating      countries  \\n \\n  tim-      in      the      2022      Winter      Olympics      opening      ceremony?\\n \\n \\n  Role1  \\n \\n  The      2022      Winter      Olympics      featured   \\n \\na      diverse      group      of  \\n \\n  countries      participating      in      the      opening      ceremony.\\nThese LLM- included Athletes from over 90 countries will enter the  \\n \\n  Role2      stadium      ina      specific      order.\\n \\n \\n  How      is      the      order      of      participating      countries      in      the      2022  \\n \\n  Winter      Olympics      opening      ceremony      determined?\\n \\n \\n  LLM-  \\n \\n  Role1  \\n \\n  Figure      1:      We      explore      writing      Wikipedia-like      articles  \\n \\n  from      scratch,      which      demands   \\n \\na      pre-writing      stage      before  \\n \\n  producing      the      article.\\nIn      this      stage,      simpler      approaches  \\n \\n  like      Direct      Prompting      have      limited      planning      capacity.\\nIn  \\n \\n  contrast,      STORM      researches      the      topic      via      perspective-  \\n \\n  guided      question      asking      in      simulated      conversations.\\n \\n \\n  1965),      even      before      the      actual      writing      process      can  \\n \\n  start.\\nHowever,      prior      work      on      generating      Wikipedia  \\n \\n  articles      (Banerjee      and      Mitra,      2015;      Minguillén  \\n \\n  et      al.,      2017;      Liu      et      al.,      2018;      Fan      and      Gardent,  \\n \\n  2022)      has      generally      bypassed      the      pre-writing      stage:  \\n \\n  for      instance,      Liu      et      al.\\n(2018)      presume      reference  \\n \\n  documents      are      provided      in      advance,      while      Fan      and       Gardent      (2022)      assume      an      article      outline      is      avail-  \\n \\n  able      and      focus      on      expanding      each      section.\\nThese  \\n \\n  assumptions      do      not      hold      in      general,      as      collecting  \\n \\n  references      and      crafting      outlines      demand      advanced  \\n \\n  information      literacy      skills      (Doyle,      1994)      to      iden- | \\n'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 4, 'chunk_type': 'para'}, 'page_content': 'Abstract\\n \\n \\n  We      explore      these      challenges      by      focusing      on      how  \\n \\n  to      generate      Wikipedia-like      articles      from      scratch.\\n \\n \\n  We      decompose      this      problem      into      two      tasks.\\nThe  \\n \\n  first      is      to      conduct      research      to      generate      an      outline,  \\n \\n  i.e.,   \\n \\na      list      of      multi-level      sections,      and      collect   \\n \\na      set      of  \\n \\n  reference      documents.\\nThe      second      uses      the      outline  \\n \\n  and      the      references      to      produce      the      full-length      arti-  \\n \\n  cle.\\nSuch   \\n \\na      task      decomposition      mirrors      the      human  \\n \\n  writing      process      which      usually      includes      phases      of  \\n \\n  pre-writing,      drafting,      and      revising      (Rohman,      1965;  \\n \\n  Munoz-Luna,      2015).'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 5, 'chunk_type': 'para'}, 'page_content': 'Abstract\\n \\n \\n  As      pre-trained      language      models      inherently      pos-  \\n \\n  sess   \\n \\na      wealth      of      knowledge,   \\n \\na      direct      approach      is      to  \\n \\n  rely      on      their      parametric      knowledge      for      generating  \\n \\n  outlines      or      even      entire      articles      (Direct      Gen).\\nHow-  \\n \\n  ever,      this      approach      is      limited      by   \\n \\na      lack      of      details  \\n \\n  and      hallucinations      (Xu      et      al.,      2023),      particularly      in  \\n \\n  addressing      long-tail      topics      (Kandpal      et      al.,      2023).\\n \\n \\n  This      underscores      the      importance      of      leveraging      ex-  \\n \\n  ternal      sources,      and      current      strategies      often      involve  \\n \\n  retrieval-augmented      generation      (RAG),      which      cir-  \\n \\n  cles      back      to      the      problem      of      researching      the      topic      in  \\n \\n  the      pre-writing      stage,      as      much      information      cannot  \\n \\n  be      surfaced      through      simple      topic      searches.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 6, 'chunk_type': 'para'}, 'page_content': 'Abstract\\n \\n \\n  Human      learning      theories      (Tawfik      et      al.,      2020;  \\n \\n  Booth      et      al.,      2003)      highlight      asking      effective  \\n \\n  questions      in      information      acquisition.\\nAlthough  \\n \\n  instruction-tuned      models      (Ouyang      et      al.,      2022)      can  \\n \\n  be      prompted      directly      to      generate      questions,      we      find  \\n \\n  that      they      typically      produce      basic      “What”,      “When”,  \\n \\n  and      “Where”      questions      (Figure   \\n \\n1      (A))      which      often  \\n \\n  only      address      surface-level      facts      about      the      topic.\\nTo  \\n \\n  endow      LLMs      with      the      capacity      to      conduct      better  \\n \\n  research,      we      propose      the      STORM      paradigm      for  \\n \\n  the      Synthesis      of      Topic      Outlines      through      Retrieval  \\n \\n  and      Multi-perspective      Question      Asking.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 7, 'chunk_type': 'para'}, 'page_content': 'Abstract\\n \\n \\n  The      design      of      STORM      is      based      on      two      hypothe-  \\n \\n  ses:      (1)      diverse      perspectives      lead      to      varied      ques-  \\n \\n  tions;      (2)      formulating      in-depth      questions      requires  \\n \\n  iterative      research.\\nBuilding      upon      these      hypotheses,  \\n \\n  STORM      employs   \\n \\na      novel      multi-stage      approach.\\nIt  \\n \\n  first      discovers      diverse      perspectives      by      retrieving  \\n \\n  and      analyzing      Wikipedia      articles      from      similar      top-  \\n \\n  ics      and      then      personifies      the      LLM      with      specific      per-  \\n \\n  spectives      for      question      asking      (Figure   \\n \\n1      (B)).\\nNext,  \\n \\n  to      elicit      follow-up      questions      for      iterative      research  \\n \\n  (Figure   \\n \\n1      (C)),      STORM      simulates      multi-turn      con-  \\n \\n  versations      where      the      answers      to      the      generated      ques-  \\n \\n  tions      are      grounded      on      the      Internet.\\nFinally,      based  \\n \\n  on      the      LLM’s      internal      knowledge      and      the      collected  \\n \\n  information,      STORM      creates      an      outline      that      can  \\n \\n  be      expanded      section      by      section      to      develop   \\n \\na      full-  \\n \\n  length      Wikipedia-like      article.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 8, 'chunk_type': 'para'}, 'page_content': 'Abstract\\n \\n \\n  We      evaluate      STORM      using      our      FreshWiki  \\n \\n  dataset      (§2.1)      which      curates      recent,      high-quality  \\n \\n  Wikipedia      articles      to      avoid      data      leakage      during      pre-  \\n \\n  training.!\\nTo      facilitate      the      study      of      the      pre-writing  \\n \\n  stage,      we      define      metrics      for      evaluating      the      outline  \\n \\n  quality      against      human-written      articles.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 9, 'chunk_type': 'para'}, 'page_content': 'Abstract\\n \\n \\n  We      further      invited   \\n \\na      group      of      experienced  \\n \\n  Wikipedia      editors      for      expert      evaluation.\\nThe      ed-  \\n \\n  itors      found      STORM      outperforms      an      outline-driven  \\n \\n  RAG      baseline,      especially      regarding      the      breadth      and       organization      of      the      articles.\\nThey      also      identified  \\n \\n  challenges      for      future      research,      including      address-  \\n \\n  ing      cases      where:      (1)      the      bias      on      the      Internet      affects  \\n \\n  the      generated      articles;      (2)      LLMs      fabricate      connec-  \\n \\n  tions      between      unrelated      facts.\\nThese      challenges  \\n \\n  present      new      frontiers      to      grounded      writing      systems.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 10, 'chunk_type': 'para'}, 'page_content': 'Abstract\\n \\n \\n  Our      main      contributions      include:'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 11, 'chunk_type': 'list_item'}, 'page_content': 'Abstract\\n*\\n \\n   To      evaluate      the      capacity      of      LLM      systems      at  \\n \\n  generating      long-form      grounded      articles      from  \\n \\n  scratch,      and      the      pre-writing      challenge      in      par-  \\n \\n  ticular,      we      curate      the      FreshWiki      dataset      and       establish      evaluation      criteria      for      both      outline  \\n \\n  and      final      article      quality.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 12, 'chunk_type': 'para'}, 'page_content': 'Abstract\\n \\n \\n \\n¢      We      propose      STORM,   \\n \\na      novel      system      that      au-  \\n \\n  tomates      the      pre-writing      stage.\\nSTORM      re-  \\n \\n  searches      the      topic      and      creates      an      outline      by  \\n \\n  using      LLMs      to      ask      incisive      questions      and      re-  \\n \\n  trieving      trusted      information      from      the      Internet.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 13, 'chunk_type': 'para'}, 'page_content': 'Abstract\\n \\n \\n \\n¢      Both      automatic      and      human      evaluation      demon-  \\n \\n  strate      the      effectiveness      of      our      approach.\\nEx-  \\n \\n  pert      feedback      further      reveals      new      challenges  \\n \\n  in      generating      grounded      long-form      articles.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 14, 'chunk_type': 'para'}, 'page_content': 'Abstract\\n2\\n \\n   FreshWiki  \\n \\n  We      study      generating      Wikipedia-like      articles      from  \\n \\n  scratch,      placing      emphasis      on      the      pre-writing  \\n \\n  stage      (Rohman,      1965),      which      involves      the      demand-  \\n \\n  ing      sub-tasks      of      gathering      and      curating      relevant  \\n \\n  information      (“‘research’’).      This      models      the      human ‘Our      resources      and      code      are      released      at      https:      //github.\\n \\n \\n  com/stanford-oval/storm.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 15, 'chunk_type': 'para'}, 'page_content': 'Domain      Scope      Given      Given       P      Outline?      Refs?\\n \\n \\n  Balepur      et      al.\\n(2023)      One      One      para.\\n \\n \\n/      Yes  \\n \\n  Qian      et      al.\\n(2023)      All      One      para.\\n \\n \\n/      No  \\n \\n  Fan      and      Gardent      (2022)      One      Full      article      Yes      No  \\n \\n  Liu      et      al.\\n(2018)      All      One      para.\\n \\n \\n/      Yes  \\n \\n  Sauper      and      Barzilay      (2009)      Two      Full      article      No      No  \\n \\n  Ours      All      Full      article      No      No  \\n \\n  Table      1:      Comparison      of      different      Wikipedia      generation  \\n \\n  setups      in      existing      literature.\\nGenerating      one      paragraph  \\n \\n  does      not      need      an      article      outline.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 16, 'chunk_type': 'para'}, 'page_content': 'Domain      Scope      Given      Given       P      Outline?      Refs?\\n \\n \\n  writing      approach      which      has      prompted      some      educa-  \\n \\n  tors      to      view      Wikipedia      article      writing      as      an      educa-  \\n \\n  tional      exercise      for      academic      training      (Tardy,      2010).\\n \\n \\n  Table   \\n \\n1      compares      our      work      against      prior      bench-  \\n \\n  marks      for      Wikipedia      generation.\\nExisting      work  \\n \\n  has      generally      focused      on      evaluating      the      generation  \\n \\n  of      shorter      snippets      (e.g.,      one      paragraph),      within   \\n \\na       narrower      scope      (e.g.,   \\n \\na      specific      domain      or      two),      or  \\n \\n  when      an      explicit      outline      or      reference      documents  \\n \\n  are      supplied.\\n \\n \\nA      notable      example      is      WikiSum      (Liu  \\n \\n  et      al.,      2018),      which      treats      generating      Wikipedia      ar-  \\n \\n  ticles      as   \\n \\na      multi-document      summarization      problem,  \\n \\n  with      respect      to      the      reference      documents.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 17, 'chunk_type': 'para'}, 'page_content': 'Domain      Scope      Given      Given       P      Outline?      Refs?\\n \\n \\n  Our      setup      emphasizes      the      capability      of      long-  \\n \\n  form      grounded      writing      systems      to      research      and       curate      content.\\nSpecifically,      given   \\n \\na      topic      ¢,      the  \\n \\n  task      is      to      find   \\n \\na      set      of      references   \\n \\n®      and      generate a full-length article S = s1598,, where each  \\n \\n  sentence      s;      cites   \\n \\na      list      of      documents      in      R.7'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 18, 'chunk_type': 'para'}, 'page_content': 'Domain      Scope      Given      Given       P      Outline?      Refs? > 2.1      The      FreshWiki      Dataset\\naset  \\n \\n  Creating   \\n \\na      new      Wikipedia-like      article      demands      not  \\n \\n  only      fluent      writing      but      also      good      research      skills.\\nAs  \\n \\n  modern      LLMs      are      generally      trained      on      Wikipedia  \\n \\n  text,      we      mitigate      data      leakage      by      explicitly      seeking  \\n \\n  out      recent      Wikipedia      articles      that      were      created      (or  \\n \\n  very      heavily      edited)      after      the      training      cutoff      of      the  \\n \\n  LLMs      we      test.\\nOur      process      can      be      repeated      at  \\n \\n  future      dates      when      new      LLMs      emerge.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 19, 'chunk_type': 'para'}, 'page_content': 'Domain      Scope      Given      Given       P      Outline?      Refs? > 2.1      The      FreshWiki      Dataset\\n \\n \\n  To      apply      our      date      criteria,      we      focus      on      the      top  \\n \\n  100      most-edited      pages,      based      on      edit      counts,      for  \\n \\n  each      month      from      February      2022      to      September  \\n \\n  2023.      To      ensure      high-quality      references,      we      filter these      articles      to      keep      only      those      having      B-class  \\n \\n  quality      or      above      assessed      by      ORES*.\\nWe      also      ex-  \\n \\n  \"In      practice,      S      also      includes      organizational      elements      such  \\n \\n  as      section      and      subsection      titles,      which      do      not      require      citations.\\n \\n \\n  3      Obtained      from      https:      //wikimedia.\\n \\n \\n  org/api/rest_v1/metrics/edited-pages/  \\n \\n  top-by-edits/en.wikipedia/all-editor-types/  \\n \\n  content/      {year      }/{month}/all-days  \\n \\n  ‘https:      //www.mediawiki.org/wiki/ORES  \\n \\n  clude      list      articles      and      articles      that      have      no      sub-  \\n \\n  sections.\\nWhile      high-quality      Wikipedia      articles  \\n \\n  usually      contain      structured      data      (e.g.,      tables)      and      are  \\n \\n  multi-modal,      we      only      consider      the      plain      text      com-  \\n \\n  ponent      in      constructing      the      dataset      to      simplify      our  \\n \\n  task.\\nMore      details      of      the      dataset      are      in      Appendix      A.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 20, 'chunk_type': 'para'}, 'page_content': 'Domain      Scope      Given      Given       P      Outline?      Refs? > 2.2      Outline      Creation      and      Evaluation\\ntion  \\n \\n  A      full-length      article      is      hard      to      generate      or      evalu-  \\n \\n  ate      (Xu      et      al.,      2023;      Krishna      et      al.,      2023).\\nWhen  \\n \\n  human      educators      teach      students      academic      writing,  \\n \\n  they      sometimes      supervise      students      at      the      outline  \\n \\n  stage      (Eriksson      and      Makitalo,      2015)      because      an  \\n \\n  extensive      outline      indicates   \\n \\na      comprehensive      under-  \\n \\n  standing      of      the      topic      and      provides   \\n \\na      solid      founda-  \\n \\n  tion      for      writing      the      full-length      article      (Dietz      and       Foley,      2019).\\nInspired      by      this,      we      decompose      the  \\n \\n  generation      of      S      into      two      stages.\\nIn      the      pre-writing  \\n \\n  stage,      we      require      the      system      to      create      an      outline  \\n \\n  O,      which      is      defined      as   \\n \\na      list      of      multi-level      section  \\n \\n  headings®.\\nIn      the      writing      stage,      the      system      uses  \\n \\n  the      topic      t,      the      references      R,      and      an      outline   \\n \\nO      to  \\n \\n  produce      the      full-length      article      S.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 21, 'chunk_type': 'para'}, 'page_content': 'Domain      Scope      Given      Given       P      Outline?      Refs? > 2.2      Outline      Creation      and      Evaluation\\n \\n \\n  To      evaluate      the      outline      coverage,      we      introduce  \\n \\n  two      metrics:      heading      soft      recall      and      heading      en-  \\n \\n  tity      recall.\\nThese      metrics      compare      the      multi-level  \\n \\n  section      headings      of      the      human-written      article,      con-  \\n \\n  sidered      as      ground      truth,      and      those      in      O.      Recog-  \\n \\n  nizing      that      an      exact      match      between      elements      in  \\n \\n  these      two      sets      of      headings      is      unnecessary,      we      cal-  \\n \\n  culate      the      heading      soft      recall      (Franti      and      Mariescu-  \\n \\n  Istodor,      2023)      using      cosine      similarity      derived      from  \\n \\n  Sentence-BERT      (Reimers      and      Gurevych,      2019)      em-  \\n \\n  beddings      of      the      headings      (details      in      Appendix      C.1).\\n \\n \\n  We      also      compute      the      heading      entity      recall      which  \\n \\n  is      quantified      as      the      percentage      of      named      entities      in  \\n \\n  human-written      article      headings      covered      by      O.      We  \\n \\n  extract      entities      with      FLAIR      named      entity      recogni-  \\n \\n  tion      (NER)      (Akbik      et      al.,      2019).'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 22, 'chunk_type': 'para'}, 'page_content': 'Domain      Scope      Given      Given       P      Outline?      Refs? > 2.2      Outline      Creation      and      Evaluation\\n3\\n \\n   Method  \\n \\n  We      present      STORM      to      automate      the      pre-writing  \\n \\n  stage      by      researching   \\n \\na      given      topic      via      effective  \\n \\n  question      asking      (§3.1,      §3.2)      and      creating      an      out-  \\n \\n  line      (§3.3).\\nThe      outline      will      be      extended      to   \\n \\na      full-  \\n \\n  length      article      grounded      on      the      collected      references  \\n \\n  Shttps://en.wikipedia.org/wiki/Wikipedia:  \\n \\n  Stand-alone_lists  \\n \\n  ®Since      language      models      process      and      produce      sequences,  \\n \\n  we      can      linearize   \\n \\nO      by      adding      “#”      to      indicate      section      titles,  \\n \\n  “#4?”\\nto      indicate      subsection      titles,      etc.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 23, 'chunk_type': 'para'}, 'page_content': 'Topic      t       @      Identify       Perspectives       @      Survey       —      Writer       =o       —       Related      Articles      |\\n@\\n \\n   Direct      Generate Question   \\n \\nq    \\n \\n@      Split      Queries  \\n \\n  ©      Search   \\n \\n&      Sift  \\n \\n  ©      Synthesize |\\n \\n   Answer   \\n \\na      \\\\\\\\\\\\\\\\'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 24, 'chunk_type': 'table'}, 'page_content': 'Topic      t       @      Identify       Perspectives       @      Survey       —      Writer       =o       —       Related      Articles      |\\n |       \\\\\\\\\\\\\\\\       y      Gather       ‘\\\\\\\\\\\\\\\\      Add      Trusted | \\n | ¥       ,      Cy}\\n'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 25, 'chunk_type': 'table'}, 'page_content': 'Topic      t       @      Identify       Perspectives       @      Survey       —      Writer       =o       —       Related      Articles      |\\n |       Draft      Outline      Op | Conversations {Cg,       Refine |  | \\n | --- | --- | --- | ---\\n | \\\\\\\\\\\\\\\\      Sources       Ns\\n'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 26, 'chunk_type': 'para'}, 'page_content': 'References      R\\n(§3.4).\\nFigure   \\n \\n2      gives      an      overview      of      STORM      and       we      include      the      pseudo      code      in      Appendix      B.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 27, 'chunk_type': 'para'}, 'page_content': 'References      R > 3.1      Perspective-Guided      Question      Asking\\nking  \\n \\n  Rohman      (1965)      defines      pre-writing      as      the      stage  \\n \\n  of      discovery      in      the      writing      process.\\nIn      parallel  \\n \\n  with      stakeholder      theory      in      business      (Freeman      et      al.,  \\n \\n  2010),      where      diverse      stakeholders      prioritize      vary-  \\n \\n  ing      facets      of   \\n \\na      company,      individuals      with      distinct  \\n \\n  perspectives      may      concentrate      on      different      aspects  \\n \\n  when      researching      the      same      topic      and      discover      mul-  \\n \\n  tifaceted      information.\\nFurther,      the      specific      perspec-  \\n \\n  tives      can      serve      as      prior      knowledge,      guiding      individ-  \\n \\n  uals      to      ask      more      in-depth      questions.\\nFor      example,  \\n \\n  an      event      planner      might      ask      about      the      “‘transporta-  \\n \\n  tion      arrangements”      and      “budget”      for      “the      2022  \\n \\n  Winter      Olympics      opening      ceremony”,      whereas   \\n \\na       layperson      might      ask      more      general      questions      about  \\n \\n  the      event’s      basic      information      (Figure   \\n \\n1      (A)).'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 28, 'chunk_type': 'para'}, 'page_content': 'References      R > 3.1      Perspective-Guided      Question      Asking\\n \\n \\n  Given      the      input      topic      t,      STORM      discovers      differ-  \\n \\n  ent      perspectives      by      surveying      existing      articles      from  \\n \\n  similar      topics      and      uses      these      perspectives      to      control  \\n \\n  the      question      asking      process.\\nSpecifically,      STORM  \\n \\n  prompts      an      LLM      to      generate   \\n \\na      list      of      related      top-  \\n \\n  ics      and      subsequently      extracts      the      tables      of      contents  \\n \\n  from      their      corresponding      Wikipedia      articles,      if      such  \\n \\n  articles      can      be      obtained      through      Wikipedia      API’  \\n \\n  (Figure   \\n \\n2      (1).\\nThese      tables      of      contents      are      con- catenated      to      create   \\n \\na      context      to      prompt      the      LLM to identify N perspectives P = {p1,, pn} that  \\n \\n  be      evaluated      using   \\n \\na      rule-based      filter      according      to  \\n \\n  the      Wikipedia      guideline®      to      exclude      untrustworthy  \\n \\n  sources      (Figure   \\n \\n2      (5)).\\nFinally,      the      LLM      synthe-  \\n \\n  Thttps://pypi.org/project/Wikipedia-API/  \\n \\n  can      collectively      contribute      to   \\n \\na      comprehensive      ar-  \\n \\n  ticle      on   \\n \\n¢      (Figure   \\n \\n2      (2)).\\nTo      ensure      that      the      basic  \\n \\n  information      about   \\n \\n¢      is      also      covered,      we      add      pg      as  \\n \\n  “basic      fact      writer      focusing      on      broadly      covering      the  \\n \\n  basic      facts      about      the      topic”      into      P.      Each      perspec-  \\n \\n  tive      p   \\n \\n€      P      will      be      utilized      to      guide      the      LLM      in      the  \\n \\n  process      of      question      asking      in      parallel.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 29, 'chunk_type': 'para'}, 'page_content': 'References      R > 3.2      Simulating      Conversations\\nions  \\n \\n  The      theory      of      questions      and      question      asking      (Ram,  \\n \\n  1991)      highlights      that      while      answers      to      existing  \\n \\n  questions      contribute      to   \\n \\na      more      comprehensive  \\n \\n  understanding      of   \\n \\na      topic,      they      often      simultane-  \\n \\n  ously      give      rise      to      new      questions.\\nTo      kick      off      this  \\n \\n  dynamic      process,      STORM      simulates   \\n \\na      conversa-  \\n \\n  tion      between   \\n \\na      Wikipedia      writer      and   \\n \\na      topic      ex-  \\n \\n  pert.\\nIn      the      z-th      round      of      the      conversation,      the  \\n \\n  LLM-powered      Wikipedia      writer      generates   \\n \\na      sin-  \\n \\n  gle      question      q;      based      on      the      topic      1,      its      assigned  \\n \\n  perspective      p   \\n \\n€      P,      and      the      conversation      history  \\n \\n  {q1,      41,      ---,      Gi-1,      41-1}      where      a;      denotes      the      sim-  \\n \\n  ulated      expert’s      answer.\\nThe      conversation      history  \\n \\n  enables      the      LLM      to      update      its      understanding      of      the  \\n \\n  topic      and      ask      follow-up      questions.\\nIn      practice,      we  \\n \\n  limit      the      conversation      to      at      most   \\n \\n/      rounds.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 30, 'chunk_type': 'para'}, 'page_content': 'References      R > 3.2      Simulating      Conversations\\n \\n \\n  To      ensure      that      the      conversation      history      provides  \\n \\n  factual      information,      we      use      trusted      sources      from  \\n \\n  the      Internet      to      ground      the      answer      a;      to      each      query  \\n \\n  sizes      the      trustworthy      sources      to      generate      the      answer  \\n \\n  a;,      and      these      sources      will      also      be      added      to      R      for  \\n \\n  full      article      generation      (§3.4).'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 31, 'chunk_type': 'list_item'}, 'page_content': 'References      R > 3.2      Simulating      Conversations\\nq.      Since      q;      can      be      complicated,      we      first      prompt  \\n \\n  the      LLM      to      break      down      q;      into   \\n \\na      set      of      search  \\n \\n  queries      (Figure   \\n \\n2      (4))      and      the      searched      results      will'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 32, 'chunk_type': 'para'}, 'page_content': 'References      R > 3.3      Creating      the      Article      Outline\\nline  \\n \\n  After      thoroughly      researching      the      topic      through  \\n \\n  N   \\n \\n+   \\n \\n1      simulated      conversations,      denoted      as {Co, Ci, -,; Cw }, STORM creates an outline before  \\n \\n  the      actual      writing      starts.\\nTo      fully      leverage      the      inter-  \\n \\n  nal      knowledge      of      LLMs,      we      first      prompt      the      model  \\n \\n  to      generate   \\n \\na      draft      outline      Op      given      only      the      topic  \\n \\n  t      (Figure   \\n \\n2      (7)).\\nOp      typically      provides   \\n \\na      general  \\n \\n  but      organized      framework.\\nSubsequently,      the      LLM  \\n \\n  is      prompted      with      the      topic      ¢,      the      draft      outline      Op, and the simulated conversations {Co, Cj,,Cw}  \\n \\n  to      refine      the      outline      (Figure   \\n \\n2      (8)).\\nThis      results in      an      improved      outline   \\n \\nO      which      will      be      used      for  \\n \\n  producing      the      full-length      article.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 33, 'chunk_type': 'para'}, 'page_content': 'References      R > 3.4      Writing      the      Full-Length      Article\\nicle  \\n \\n  Building      upon      the      references      R      collected      and      the  \\n \\n  outline   \\n \\nO      developed      during      the      pre-writing      stage,  \\n \\n  the      full-length      article      can      be      composed      section      by  \\n \\n  section.\\nSince      it      is      usually      impossible      to      fit      the  \\n \\n  entire   \\n \\n7      within      the      context      window      of      the      LLM,  \\n \\n  we      use      the      section      title      and      headings      of      its      all-level  \\n \\n  subsections      to      retrieve      relevant      documents      from  \\n \\n  R      based      on      semantic      similarity      calculated      from  \\n \\n  Sentence-BERT      embeddings.\\nWith      the      relevant      in-  \\n \\n  formation      at      hand,      the      LLM      is      then      prompted      to  \\n \\n  generate      the      section      with      citations.\\nOnce      all      sec-  \\n \\n  tions      are      generated,      they      are      concatenated      to      form  \\n \\n  the      full-length      article.\\nSince      the      sections      are      gen-  \\n \\n  erated      in      parallel,      we      prompt      the      LLM      with      the  \\n \\n  concatenated      article      to      delete      repeated      information  \\n \\n  to      improve      coherence.\\nFurthermore,      in      alignment  \\n \\n  with      Wikipedia’s      stylistic      norms,      the      LLM      is      also  \\n \\n  utilized      to      synthesize   \\n \\na      summary      of      the      entire      arti-  \\n \\n  cle,      forming      the      lead      section      at      the      beginning.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 34, 'chunk_type': 'para'}, 'page_content': 'References      R > 3.4      Writing      the      Full-Length      Article\\n4\\n \\n   Experiments'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 35, 'chunk_type': 'para'}, 'page_content': 'References      R > 3.4      Writing      the      Full-Length      Article > 4.1      Article      Selection\\ntion  \\n \\n  STORM      is      capable      of      researching      complicated      top-  \\n \\n  ics      and      writing      long      articles      from      detailed      outlines.\\n \\n \\n  However,      in      this      controlled      experiment,      we      limit  \\n \\n  the      final      output      to      at      most      4000      tokens      (roughly  \\n \\n  3000      words).\\nFor   \\n \\na      meaningful      comparison,      we  \\n \\n  Shttps://en.wikipedia.\\norg/wiki/Wikipedia:  \\n \\n  Reliable_sources  \\n \\n  randomly      select      100      samples      from      the      Fresh      Wiki  \\n \\n  dataset      (see      §2.1)      that      have      human-written      articles  \\n \\n  not      exceeding      3000      words.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 36, 'chunk_type': 'para'}, 'page_content': 'References      R > 3.4      Writing      the      Full-Length      Article > 4.2      Automatic      Metrics\\nrics  \\n \\n  As      discussed      in      §2.2,      we      evaluate      the      outline      qual-  \\n \\n  ity      to      assess      the      pre-writing      stage      by      calculating  \\n \\n  the      heading      soft      recall      and      heading      entity      recall.\\n \\n \\nA       higher      recall      score      signifies   \\n \\na      more      comprehensive  \\n \\n  outline      relative      to      the      human-written      article.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 37, 'chunk_type': 'para'}, 'page_content': 'References      R > 3.4      Writing      the      Full-Length      Article > 4.2      Automatic      Metrics\\n \\n \\n  To      assess      the      full-length      article      quality,      we      adopt  \\n \\n  ROUGE      scores      (Lin,      2004)      and      compute      the      entity  \\n \\n  recall      in      the      article      level      based      on      FLAIR      NER  \\n \\n  results.\\nMoreover,      based      on      Wikipedia      criteria’,  \\n \\n  we      evaluate      the      article      from      the      aspects      of      (1)      In-  \\n \\n  terest      Level,      (2)      Coherence      and      Organization,      (3)  \\n \\n  Relevance      and      Focus,      (4)      Coverage,      and      (5)      Verifia-  \\n \\n  bility.\\nFor      aspects      (1)-(4),      we      use      Prometheus      (Kim  \\n \\n  et      al.,      2023),   \\n \\na      13B      evaluator      LLM      to      score      the      arti-  \\n \\n  cle      based      on   \\n \\na      5-point      rubric      collaboratively      devel-  \\n \\n  oped      with      two      experienced      Wikipedia      editors      (see  \\n \\n  Appendix      C.2).\\nFor      verifiability,      we      calculate      the  \\n \\n  citation      recall      and      citation      precision      based      on      the  \\n \\n  definition      in      Gao      et      al.\\n(2023).\\nWe      use      Mistral      7B-  \\n \\n  Instruct      (Jiang      et      al.,      2023a)      to      examine      whether  \\n \\n  the      cited      passages      entail      the      generated      sentence.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 38, 'chunk_type': 'para'}, 'page_content': 'References      R > 3.4      Writing      the      Full-Length      Article > 4.3      Baselines\\nines  \\n \\n  As      prior      works      use      different      setups      and      do      not      use  \\n \\n  LLMs,      they      are      hard      to      compare      directly.\\nInstead,  \\n \\n  we      use      the      following      three      LLM-based      baselines.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 39, 'chunk_type': 'list_item'}, 'page_content': 'References      R > 3.4      Writing      the      Full-Length      Article > 4.3      Baselines\\n1. Direct      Gen,   \\n \\na      baseline      that      directly      prompts  \\n \\n  the      LLM      to      generate      an      outline,      which      is      then  \\n \\n  used      to      generate      the      full-length      article.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 40, 'chunk_type': 'list_item'}, 'page_content': 'References      R > 3.4      Writing      the      Full-Length      Article > 4.3      Baselines\\n2. RAG,   \\n \\na      retrieval-augmented      generation      base-  \\n \\n  line      that      searches      with      the      topic      and      uses      the  \\n \\n  searched      results      together      with      the      topic   \\n \\n¢      to  \\n \\n  generate      an      outline      or      the      entire      article.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 41, 'chunk_type': 'list_item'}, 'page_content': 'References      R > 3.4      Writing      the      Full-Length      Article > 4.3      Baselines\\n3. Outline-driven      RAG      (ORAG),      which      is      iden-  \\n \\n  tical      to      RAG      in      outline      creation,      but      further  \\n \\n  searches      additional      information      with      section  \\n \\n  titles      to      generate      the      article      section      by      section.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 42, 'chunk_type': 'para'}, 'page_content': 'References      R > 3.4      Writing      the      Full-Length      Article > 4.4      STORM      Implementation\\ntion  \\n \\n  We      build      STORM      with      zero-shot      prompting      us-  \\n \\n  ing      the      DSPy      framework      (Khattab      et      al.,      2023).'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 43, 'chunk_type': 'para'}, 'page_content': 'References      R > 3.4      Writing      the      Full-Length      Article > 4.4      STORM      Implementation\\n \\n \\n  Appendix   \\n \\nB      includes      the      pseudo      code      and      corre-  \\n \\n  sponding      prompts.\\nThe      hyperparameters      N      and      MZ °https://en.wikipedia.org/wiki/Wikipedia:  \\n \\n  Good_article_criteria'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 44, 'chunk_type': 'para'}, 'page_content': 'ROUGE-1      ROUGE-L      Entity      Recall      Interest      Level      Organization      Relevance      Coverage\\n \\n \\n  Direct      Gen      25.62      12.63      5.08      2.87      4.60      3.10      4.16  \\n \\n  RAG      28.52      13.18      7.57      3.14      4.22      3.05      4.08  \\n \\n  oRAG      44.26      16.51      12.57      3.90      4.79      4.09      4.70 STORM      45.82      16.70      14.107      3.997      4.82      4.457      4.887  \\n \\n  w/o      Outline      Stage      26.77      12.77      7.39      3.33      4.87      3.35      4.37'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 45, 'chunk_type': 'para'}, 'page_content': 'Heading      Heading       Soft      Recall      Entity      Recall\\n \\n \\n  Direct      Gen      80.23      32.39  \\n \\n  RAG/oRAG      73.59      33.85  \\n \\n  GPT-3.5   \\n \\n=      RAG-expand      74.40      33.85  \\n \\n  STORM      86.267      40.527  \\n \\n  w/o      Perspective      84.49      40.12  \\n \\n  w/o      Conversation      77.97      31.98  \\n \\n  Direct      Gen      87.66      34.78  \\n \\n  RAG/oRAG      89.55      42.38  \\n \\n  GPT-4      RAG-expand      91.36      43.53  \\n \\n  STORM      92.737      45.91  \\n \\n  w/o      Perspective      92.39      42.70  \\n \\n  w/o      Conversation      88.75      39.30  \\n \\n  Table      3:      Results      of      outline      quality      evaluation      (%).\\n \\n \\n+      de-  \\n \\n  notes      significant      differences      (p   \\n \\n<      0.05)      from   \\n \\na      paired  \\n \\n  t-test      between      STORM      and      baselines.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 46, 'chunk_type': 'para'}, 'page_content': 'Heading      Heading       Soft      Recall      Entity      Recall\\n \\n \\n  in      STORM      are      both      set      as      5.      We      use      the      chat  \\n \\n  model      gpt-3.5-turbo      for      question      asking      and       use      gpt-3.5-turbo-instruct      for      other      parts      of  \\n \\n  STORM.\\nWe      also      experiment      with      using      gpt-4      for  \\n \\n  drafting      and      refining      the      outline      (Figure   \\n \\n2      ()8)).'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 47, 'chunk_type': 'para'}, 'page_content': 'Heading      Heading       Soft      Recall      Entity      Recall\\nFor      reported      results,      the      simulated      topic      expert      in  \\n \\n  STORM      is      grounded      on      the      You.com      search      API!°,  \\n \\n  although      the      proposed      pipeline      is      compatible      with  \\n \\n  other      search      engines.\\nThe      ground      truth      Wikipedia  \\n \\n  article      is      excluded      from      the      search      results.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 48, 'chunk_type': 'para'}, 'page_content': 'Heading      Heading       Soft      Recall      Entity      Recall\\n \\n \\n  For      final      article      generation,      we      only      report      the  \\n \\n  results      using      gpt-4      as      gpt-3.5      is      not      faithful      to  \\n \\n  sources      when      generating      text      with      citations      (Gao  \\n \\n  et      al.,      2023).\\nWe      set      temperature      as      1.0      and      top_p  \\n \\n  as      0.9      for      all      experiments.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 49, 'chunk_type': 'para'}, 'page_content': 'Heading      Heading       Soft      Recall      Entity      Recall\\n5\\n \\n   Results      and      Analysis'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 50, 'chunk_type': 'para'}, 'page_content': 'Heading      Heading       Soft      Recall      Entity      Recall > 5.1      Main      Results\\nults  \\n \\n  We      use      outline      coverage      as   \\n \\na      proxy      to      assess      the      pre-  \\n \\n  writing      stage      (see      §2.2).\\nTable   \\n \\n3      shows      the      heading  \\n \\n  soft      recall      and      entity      recall.\\nOutlines      directly      gen-  \\n \\n  erated      by      LLMs      (Direct      Gen)      already      demonstrate  \\n \\n  https:      //documentation.\\nyou.\\ncom/api-reference/  \\n \\n  search  \\n \\n  high      heading      soft      recall,      indicating      LLMs’      ability  \\n \\n  to      grasp      high-level      aspects      of   \\n \\na      topic      through      their  \\n \\n  rich      parametric      knowledge.\\nHowever,      STORM,      by  \\n \\n  asking      effective      questions      to      research      the      topic,      can  \\n \\n  create      higher      recall      outlines      that      cover      more      topic-  \\n \\n  specific      aspects.\\nNotably,      although      RAG      leverages  \\n \\n  additional      information,      presenting      unorganized      in-  \\n \\n  formation      in      the      context      window      makes      outline  \\n \\n  generation      more      challenging      for      the      weaker      model,  \\n \\n  i.e.,      GPT-3.5,      leading      to      worse      performance.\\nTo      test  \\n \\n  the      limit      of      the      RAG      baseline,      we      further      expand  \\n \\n  the      retrieved      sources      by      starting      with      the      outline  \\n \\n  produced      by      RAG,      using      its      section      titles      as      search  \\n \\n  queries      to      collect      more      sources,      and      inputting      the  \\n \\n  newly      collected      sources      together      with      the      initial  \\n \\n  outline      to      LLM      to      generate   \\n \\na      polished      outline.\\nThis  \\n \\n  modified      approach      is      referred      to      as      “RAG-expand”  \\n \\n  in      Table      3.\\nThe      experiment      results      indicate      that  \\n \\n  even      though      having      an      additional      round      of      search  \\n \\n  and      refinement      can      improve      the      outline      produced  \\n \\n  by      RAG,      our      proposed      STORM      still      surpasses      its  \\n \\n  performance.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 51, 'chunk_type': 'para'}, 'page_content': 'Heading      Heading       Soft      Recall      Entity      Recall > 5.1      Main      Results\\n \\n \\n  We      further      evaluate      the      full-length      article      quality.\\n \\n \\n  As      shown      in      Table      2,      oRAG      significantly      outper-  \\n \\n  forms      RAG,      highlighting      the      effectiveness      of      using  \\n \\n  outlines      for      structuring      full-length      article      genera-  \\n \\n  tion.\\nDespite      this      method’s      advantages      in      leverag-  \\n \\n  ing      retrieval      and      outlining,      our      approach      still      out-  \\n \\n  performs      it.\\nThe      effective      question      asking      mecha-  \\n \\n  nism      enhances      the      articles      with      greater      entity      recall.\\n \\n \\n  The      evaluator      LLM      also      rates      these      articles      with      sig-  \\n \\n  nificantly      higher      scores      in      the      aspects      of      “Interest  \\n \\n  Level’,      “Relevance      and      Focus’,      and      “Coverage”.\\n \\n \\n  Nonetheless,      we      acknowledge      the      possibility      of  \\n \\n  the      evaluator      LLM      overrating      machine-generated  \\n \\n  text.\\nOur      careful      human      evaluation      (§6)      reveals  \\n \\n  that      STORM      still      has      much      room      for      improvement.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 52, 'chunk_type': 'para'}, 'page_content': 'Heading      Heading       Soft      Recall      Entity      Recall > 5.1      Main      Results\\n \\n \\n  Although      this      work      primarily      focuses      on      the      pre-  \\n \\n  writing      stage      and      does      not      optimize      generating      text  \\n \\n  with      citations,      we      still      examine      the      citation      quality  \\n \\n  of      articles      produced      by      our      approach.\\nAs      reported Citation      Recall Citation      Precision oRAG      STORM      value  \\n \\n  Avg.      >4Rates      Av.g.\\n \\n \\n>   \\n \\n4      Rates      peval Table      4:      Citation      quality      judged      by      Mistral      7B-Instruct.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 53, 'chunk_type': 'list_item'}, 'page_content': 'Heading      Heading       Soft      Recall      Entity      Recall > 5.1      Main      Results\\n84.83 85.18  \\n \\n  STORM'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 54, 'chunk_type': 'table'}, 'page_content': 'Heading      Heading       Soft      Recall      Entity      Recall > 5.1      Main      Results\\n |  |       STORM      _      w/o      Perspective       w/o      Conversation | \\n | --- | --- | ---\\n |       IR|      99.83      54.36 |       39.56 |       Interest      Level      3.63      57.5%      4.03      70.0%      0.077       Organization      3.25      45.0%      4.00      70.0%      0.005       Relevance      3.93      62.5%      4.15      65.0%      0.347       Coverage      3.58      57.5%      4.00      67.5%      0.084       Verifiability      3.85      67.5%      3.80      67.5%      0.843       #Preferred      14      26\\n | Table      5:      Average      number      of      unique      references      (|R|)       collected      using      different      methods.\\n |       in      Table      4,      Mistral      7B-Instruct      judges      84.83%      of       the      sentences      are      supported      by      their      citations.      Ap-       pendix      C.3      investigates      the      unsupported      sentences       and      reveals      that      the      primary      issues      stem      from      draw-       ing      improper      inferences      and      inaccurate      paraphras-       ing,      rather      than      hallucinating      non-existent      contents.\\n | 5.2      Ablation      Studies\\n | 5.2      Ablation      Studies\\n |       As      introduced      in      §3,      STORM      prompts      LLMs      to       ask      effective      questions      by      discovering      specific       perspectives      and      simulating      multi-turn      conversa-       tions.      We      conduct      the      ablation      study      on      outline       creation      by      comparing      STORM      with      two      variants:\\n | (1)      “STORM      w/o      Perspective”,      which      omits      per-       spective      in      the      question      generation      prompt;      (2)       “STORM      w/o      Conversation”,      which      prompts      LLMs       to      generate      a      set      number      of      questions      altogether.      To       ensure      a      fair      comparison,      we      control      an      equal      total       number      of      generated      questions      across      all      variants.       Table      3      shows      the      ablation      results      and      full      STORM       pipeline      produces      outlines      with      the      highest      recall.       Also,      “STORM      w/o      Conversation”      gives      much       worse      results,      indicating      reading      relevant      informa-       tion      is      crucial      to      generating      effective      questions.      We       further      examine      how      many      unique      sources      are      col-       lected      in      ?      via      different      variants.      As      shown      in      Ta-       ble      5,      the      full      pipeline      discovers      more      different       sources      and      the      trend      is      in      accord      with      the      auto-       matic      metrics      for      outline      quality.       We      also      verify      whether      having      an      outline      stage       is      necessary      with      STORM.      In      Table      2,      “STORM       w/o      Outline      Stage”      denotes      the      results      of      generat-       ing      the      entire      article      given      the      topic      and      the      sim-       ulated      conversations.      Removing      the      outline      stage       significantly      deteriorates      the      performance      across       all      metrics.\\n | 6      Human      Evaluation\\n |       To      better      understand      the      strengths      and      weaknesses       of      STORM,      we      conduct      human      evaluation      by      col-       laborating      with      10      experienced      Wikipedia      editors       Table      6:      Human      evaluation      results      on      20      pairs      of      articles       generated      by      STORM      and      oRAG.      Each      pair      of      articles       is      evaluated      by      two      Wikipedia      editors.      The      ratings      are       given      on      a      scale      between      |      and      7,      with      values      >      4       indicating      good      quality      (see      Table      10).      We      conduct       paired      t-test      and      report      the      p-value.\\n |       who      have      made      at      least      500      edits      on      Wikipedia      and       have      more      than      |      year      of      experience.      We      randomly       sample      20      topics      from      our      dataset      and      evaluate      the       articles      generated      by      our      method      and      oRAG,      the       best      baseline      according      to      the      automatic      evaluation.\\n |       Each      pair      of      articles      is      assigned      to      2      editors.\\n |       We      request      editors      to      judge      each      article      from      the       same      five      aspects      defined      in      $4.2,      but      using      a      |      to       7      scale      for      more      fine-grained      evaluation.      While       our      automatic      evaluation      uses      citation      quality      as       a      proxy      to      evaluate      Verifiability,      we      stick      to      the       Wikipedia      standard      of      “verifiable      with      no      original       research”      in      human      evaluation.      Besides      rating      the       articles,      editors      are      asked      to      provide      open-ended       feedback      and      pairwise      preference.      After      the      evalua-       tion      finishes,      they      are      further      requested      to      compare       an      article      produced      by      our      method,      which      they      have       just      reviewed,      with      its      human-written      counterpart,       and      report      their      perceived      usefulness      of      STORM       using      a      1-5      Likert      scale.      More      human      evaluation      de-       tails      are      included      in      Appendix      D.      Table      6      presents       the      rating      and      pairwise      comparison      results.!!\\n |       Articles      produced      by      STORM      exhibit      greater       breadth      and      depth      than      oRAG      outputs.      In      ac-       cord      with      the      finding      in      §5.1,      editors      judge      articles       produced      by      STORM      as      more      interesting,      orga-       nized,      and      having      broader      coverage      compared      to       oRAG      outputs.      Specifically,      25%      more      articles      pro-       duced      by      STORM      are      considered      organized      (Orga-       nization      rating      >      4),      and      10%      more      are      deemed      to       have      good      coverage      (Coverage      rating      >      4).      Even       in      comparison      with      human-written      articles,      one       editor      praises      our      result      as      providing      “a      bit      more\\n |       \"For      the      1-7      scale      rating      results      on      each      criterion,      we      cal-       culate      the      Krippendorff’s      Alpha      to      measure      the      inter      annotator       agreement      (IAA),      and      the      results      are      as      follows:      Interest      Level       (0.349),      Organization      (0.221),      Relevance      (0.256),      Coverage       (0.346),      Verifiability      (0.388).\\n'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 55, 'chunk_type': 'para'}, 'page_content': 'Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree\\n \\n \\n \\nI      think      it      can      be      specifically      helpful  \\n \\n  wae      70%      30%  \\n \\n  for      my      pre-writing      stage.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 56, 'chunk_type': 'para'}, 'page_content': 'Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree\\nI\\n \\n   think      it      will      help      me      edit   \\n \\na      Wikipedia      anes   \\n \\n3      oars      30%  \\n \\n  article      for   \\n \\na      new      topic.\\n \\n \\n= I      think      it      can      be   \\n \\na      potentially      useful      10%      20%   \\n \\n:      60%      10%  \\n \\n  tool      for      the      Wikipedia      community.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 57, 'chunk_type': 'para'}, 'page_content': 'Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree\\nFigure      3:      Survey      results      of      the      perceived      usefulness      of  \\n \\n  STORM      (n   \\n \\n=      10).'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 58, 'chunk_type': 'para'}, 'page_content': 'Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree\\n \\n \\n  background      information”      and      another      notes      that      “I  \\n \\n  found      that      the      AI      articles      had      more      depth      compared  \\n \\n  to      the      Wikipedia      articles”.\\nSTORM      also      outper-  \\n \\n  forms      the      best      baseline      in      pairwise      comparison.\\n \\n \\n  More      information      in      |R|      poses      challenges      be-  \\n \\n  yond      factual      hallucination.\\nWe      examine      14      pair-  \\n \\n  wise      comparison      responses      where      editors      prefer  \\n \\n  oORAG      outputs      over      STORM.\\nExcluding   \\n \\n3      cases  \\n \\n  where      pairwise      preferences      do      not      align      with      their  \\n \\n  ratings,      editors      assign      lower      Verifiability      scores      to  \\n \\n  articles      from      our      approach      in      over      50%      of      the      cases.\\n \\n \\n  Through      analyzing      the      articles      and      editors’      free-  \\n \\n  form      feedback,      we      discover      that      low      Verifiability  \\n \\n  scores      stem      from      red      herring      fallacy      or      overspec-  \\n \\n  ulation      issues.\\nThese      arise      when      the      generated  \\n \\n  articles      introduce      unverifiable      connections      between  \\n \\n  different      pieces      of      information      in      |7?|      or      between  \\n \\n  the      information      and      the      topic      (examples      included  \\n \\n  in      Table      11).\\nCompared      to      the      widely      discussed  \\n \\n  factual      hallucination      (Shuster      et      al.,      2021;      Huang  \\n \\n  et      al.,      2023),      addressing      such      verifiability      issues      is  \\n \\n  more      nuanced,      surpassing      basic      fact-checking      (Min  \\n \\n  et      al.,      2023).'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 59, 'chunk_type': 'para'}, 'page_content': 'Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree\\n \\n \\n  Generated      articles      trail      behind      well-revised      hu-  \\n \\n  man      works.\\nWhile      STORM      outperforms      the  \\n \\n  oRAG      baseline,      editors      comment      that      the      generated  \\n \\n  articles      are      less      informative      than      actual      Wikipedia  \\n \\n  pages.\\nAnother      major      issue      identified      is      the      trans-  \\n \\n  fer      of      bias      and      tone      from      Internet      sources      to      the  \\n \\n  generated      article,      with   \\n \\n7      out      of      10      editors      men-  \\n \\n  tioning      that      the      STORM-generated      articles      sound  \\n \\n  “emotional”      or      “unneutral”.\\nMore      analysis      is      dis-  \\n \\n  cussed      in      Appendix      E.      This      feedback      suggests      that  \\n \\n  reducing      the      retrieval      bias      in      the      pre-writing      stage  \\n \\n  is   \\n \\na      worthwhile      direction      for      future      work.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 60, 'chunk_type': 'para'}, 'page_content': 'Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree\\n \\n \\n  Generated      articles      are   \\n \\na      good      starting      point.\\nAs  \\n \\n  shown      in      Figure      3,      editors      are      unanimous      in      agree-  \\n \\n  ing      that      STORM      can      aid      them      in      their      pre-writing  \\n \\n  stage.\\nIt      is      gratifying      to      know      that      the      tool      is      help-  \\n \\n  ful      to      experienced      editors.\\n80%      of      the      editors      think  \\n \\n  that      STORM      can      help      them      edit   \\n \\na      Wikipedia      article  \\n \\n  for   \\n \\na      new      topic.\\nMore      reservation      is      expressed      to  \\n \\n  the      usefulness      of      STORM      for      the      Wikipedia      com-  \\n \\n  munity      at      large;      nonetheless,      70%      of      the      editors  \\n \\n  think      it      is      useful,      with      only      10%      disagreeing.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 61, 'chunk_type': 'para'}, 'page_content': 'Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree\\n7\\n \\n   Related      Works  \\n \\n  Retrieval-Augmented      Generation      (RAG)      Aug-  \\n \\n  menting      language      models      (LMs)      with      retrieval      at  \\n \\n  inference      time      is   \\n \\na      typical      way      to      leverage      exter-  \\n \\n  nal      knowledge      stores      (Ram      et      al.,      2023;      Izacard  \\n \\n  et      al.,      2023).\\nWhile      some      works      use      retrieval  \\n \\n  to      construct      demonstrations      for      in-context      learn-  \\n \\n  ing      (Li      et      al.,      2023;      Liu      et      al.,      2022;      Agrawal      et      al.,  \\n \\n  2023;      Poesia      et      al.,      2022;      Shi      et      al.,      2022;      Khattab  \\n \\n  et      al.,      2022),      another      line      of      works      uses      retrieval      to  \\n \\n  provide      additional      information      for      LMs      to      ground  \\n \\n  on.\\nLewis      et      al.\\n(2020)      study      RAG      on      knowledge-  \\n \\n  intensive      NLP      tasks      and      find      it      improves      diver-  \\n \\n  sity      and      factuality.\\nSemnani      et      al.\\n(2023)      de-  \\n \\n  signs   \\n \\na      RAG-based      chatbot      grounded      on      English  \\n \\n  Wikipedia      to      stop      LLM-based      chatbots      from      hal-  \\n \\n  lucination.\\nBesides,      RAG      can      be      used      to      generate  \\n \\n  text      with      citations      (Menick      et      al.,      2022;      Gao      et      al.,  \\n \\n  2023)      and      build      attributed      question      answering      sys-  \\n \\n  tems      (Bohnet      et      al.,      2023).\\nWhile      RAG      is      widely  \\n \\n  studied      in      question      answering,      how      to      use      it      for  \\n \\n  long-form      article      generation      is      less      investigated.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 62, 'chunk_type': 'para'}, 'page_content': 'Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree\\n \\n \\n  As   \\n \\na      general      framework,      RAG      is      flexible      in      both  \\n \\n  the      retrieval      source      and      time.\\nThe      retrieval      sources  \\n \\n  can      vary      from      domain      databases      (Zakka      et      al.,  \\n \\n  2023),      code      documentation      (Zhou      et      al.,      2023),  \\n \\n  to      the      whole      Internet      (Nakano      et      al.,      2022;      Komeili  \\n \\n  et      al.,      2022).\\nRegarding      the      time,      besides   \\n \\na      one-  \\n \\n  time      retrieval      before      generation,      the      system      can      be  \\n \\n  designed      to      self-decide      when      to      retrieve      across      the  \\n \\n  course      of      the      generation      (Jiang      et      al.,      2023b;      Parisi  \\n \\n  et      al.,      2022;      Shuster      et      al.,      2022;      Yao      et      al.,      2023).\\n \\n \\n  Automatic      Expository      Writing      Different      from  \\n \\n  other      types      of      long-form      generation      (Yang      et      al.,  \\n \\n  2022;      Feng      et      al.,      2018),      automatic      expository      writ-  \\n \\n  ing      requires      grounding      on      external      documents      and       leveraging      the      interplay      between      reading      and      writ-  \\n \\n  ing.\\nBalepur      et      al.\\n(2023)      propose      the      Imitate-  \\n \\n  Retrieve-Paraphrase      framework      for      expository      writ-  \\n \\n  ing      at      the      paragraph      level      to      address      the      challenges  \\n \\n  in      synthesizing      information      from      multiple      sources.\\n \\n \\n  Beyond      summarizing      sources,      Shen      et      al.\\n(2023)  \\n \\n  highlight      that      expository      writing      requires      the      au-  \\n \\n  thor’s      sensemaking      process      over      source      documents  \\n \\n  and      good      outline      planning.\\nWe      tackle      these      chal-  \\n \\n  lenges      by      focusing      on      the      pre-writing      stage.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 63, 'chunk_type': 'para'}, 'page_content': 'Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree\\n \\n \\n  Question      Asking      in      NLP      Question      asking      capa-  \\n \\n  bilities      in      NLP      systems      have      expanded      across      sev-  \\n \\n  eral      fronts,      including      generating      clarification      ques-  \\n \\n  tions      to      understand      user      intents      (Aliannejadi      et      al.,  \\n \\n  2019;      Rahmani      et      al.,      2023),      and      breaking      large  \\n \\n  questions      into      smaller      ones      to      improve      composi-  \\n \\n  tional      reasoning      (Press      et      al.,      2023).\\nWhile      humans  \\n \\n  usually      ask      questions      to      learn      new      knowledge      (Taw-  \\n \\n  fik      et      al.,      2020;      Booth      et      al.,      2003),      how      to      opti-  \\n \\n  mize      question      informativeness      and      specificity      in  \\n \\n  information-seeking      conversations      remains      less      ex-  \\n \\n  plored.\\nThe      closest      work      is      Qi      et      al.\\n(2020)      which  \\n \\n  defines      the      question      informativeness      using      the      un-  \\n \\n  igram      precision      function      and      uses      reinforcement  \\n \\n  learning      to      increase      the      question      informativeness.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 64, 'chunk_type': 'para'}, 'page_content': 'Strongly      Disagree      Somewhat      Disagree      Neutral      “Somewhat      Agree      —      Strongly      Agree\\n8\\n \\n   Conclusion  \\n \\n  We      propose      STORM,      an      LLM-based      writing      sys-  \\n \\n  tem      that      automates      the      pre-writing      stage      for      creat-  \\n \\n  ing      Wikipedia-like      articles      from      scratch.\\nWe      cu-  \\n \\n  rate      the      FreshWiki      dataset      and      establish      evaluation  \\n \\n  criteria      to      study      the      generation      of      grounded      long-  \\n \\n  form      articles.\\nExperimental      results      demonstrate  \\n \\n  that      the      question      asking      mechanism      in      STORM  \\n \\n  improves      both      the      outline      and      article      quality.\\nWith  \\n \\n  the      improved      breadth      and      depth,      STORM      helps  \\n \\n  surface      new      challenges      for      grounded      writing      sys-  \\n \\n  tems      through      expert      evaluation.\\nThe      experienced  \\n \\n  Wikipedia      editors      in      our      study      unanimously      agree  \\n \\n  that      STORM      is      helpful      for      their      pre-writing      stage.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 65, 'chunk_type': 'para'}, 'page_content': 'Limitations\\n \\n \\n  In      this      work,      we      explore      generating      Wikipedia-  \\n \\n  like      articles      from      scratch      as   \\n \\na      way      to      push      the  \\n \\n  frontier      of      automatic      expository      writing      and      long-  \\n \\n  form      article      generation.\\nWhile      our      approach      sig-  \\n \\n  nificantly      outperforms      baseline      methods      in      both  \\n \\n  automatic      and      human      evaluations,      the      quality      of  \\n \\n  machine-written      articles      still      lags      behind      well-  \\n \\n  revised      human-authored      articles,      specifically      in  \\n \\n  aspects      of      neutrality      and      verifiability.\\nAlthough  \\n \\n  STORM      discovers      different      perspectives      in      re-  \\n \\n  searching      the      given      topic,      the      collected      information  \\n \\n  may      still      be      biased      towards      dominant      sources      on  \\n \\n  the      Internet      and      may      contain      promotional      content.\\n \\n \\n  Moreover,      the      verifiability      issues      identified      in      this  \\n \\n  work      go      beyond      factual      hallucination,      which      high-  \\n \\n  lights      new      challenges      to      grounded      writing      systems.\\n \\n \\n  Another      limitation      of      this      work      is      that      although  \\n \\n  we      focus      on      the      task      of      generating      Wikipedia-like  \\n \\n  articles      from      scratch,      our      task      setup      is      still      simpli-  \\n \\n  fied      to      only      consider      the      generation      of      free-form  \\n \\n  text.\\nHuman-authored      high-quality      Wikipedia      ar-  \\n \\n  ticles      usually      contain      structured      data      and      multi-  \\n \\n  modal      information.\\nWe      leave      the      exploration      of  \\n \\n  generating      multi-modal      grounded      articles      for      fu-  \\n \\n  ture      work.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 66, 'chunk_type': 'para'}, 'page_content': 'Acknowledgements\\n \\n \\n  We      thank      You.com      for      generously      providing      the  \\n \\n  search      API      that      supported      our      experiments.\\nWe  \\n \\n  also      thank      Sina      J.      Semnani,      Shicheng      Liu,      Eric      Ze-  \\n \\n  likman      for      providing      helpful      feedback      and      the      ACL  \\n \\n  ARR      reviewers      for      their      valuable      comments.\\nThis  \\n \\n  work      is      supported      in      part      by      the      Verdant      Founda-  \\n \\n  tion      and      Microsoft      Azure      AI      credits.\\nYijia      Shao  \\n \\n  is      supported      by   \\n \\na      Stanford      School      of      Engineering  \\n \\n  Fellowship.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 67, 'chunk_type': 'para'}, 'page_content': 'Ethics      Statement\\n \\n \\n  Different      from      the      creative      generation,      grounded      ar-  \\n \\n  ticle      generation      may      impact      how      people      learn      about  \\n \\n  topics      or      consume      source      information.\\nAll      the      stud-  \\n \\n  ies      and      the      evaluation      in      this      work      are      designed  \\n \\n  to      prevent      the      dissemination      of      misinformation      by  \\n \\n  not      publishing      generated      content      online      and      im-  \\n \\n  plementing      strict      accuracy      checks.\\nWe      avoid      any  \\n \\n  disruption      to      Wikipedia      or      related      communities,      as  \\n \\n  our      system      does      not      interact      with      live      pages.\\nAlso,  \\n \\n  although      we      try      to      generate      grounded      articles,      we  \\n \\n  believe      there      is      no      privacy      issue      related      to      this      work  \\n \\n  as      we      only      use      information      publicly      available      on  \\n \\n  the      Internet.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 68, 'chunk_type': 'para'}, 'page_content': 'Ethics      Statement\\n \\n \\n  The      primary      risk      of      our      work      is      that      the  \\n \\n  Wikipedia      articles      written      by      our      system      are  \\n \\n  grounded      on      information      on      the      Internet      which  \\n \\n  contains      some      biased      or      discriminative      content      on  \\n \\n  its      own.\\nCurrently,      our      system      relies      on      the      search  \\n \\n  engine      to      retrieve      information      but      does      not      include  \\n \\n  any      post-processing      module.\\nWe      believe      improv-  \\n \\n  ing      the      retrieval      module      to      have      good      coverage      of  \\n \\n  different      viewpoints      and      adding   \\n \\na      content      sifting  \\n \\n  module      to      the      current      system      will      be   \\n \\na      critical      next  \\n \\n  step      to      achieve      better      neutrality      and      balance      in      the  \\n \\n  generated      articles.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 69, 'chunk_type': 'para'}, 'page_content': 'Ethics      Statement\\n \\n \\n  Another      limitation      we      see      from      an      ethical      point  \\n \\n  of      view      is      that      we      only      consider      writing      English  \\n \\n  Wikipedia      articles      in      this      work.\\nExtending      the      cur-  \\n \\n  rent      system      to   \\n \\na      multilingual      setup      is   \\n \\na      meaningful  \\n \\n  direction      for      future      work      as      more      topics      do      not      have  \\n \\n  Wikipedia      pages      in      non-English      languages.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 70, 'chunk_type': 'para'}, 'page_content': 'References\\n \\n \\n  Sweta      Agrawal,      Chunting      Zhou,      Mike      Lewis,      Luke  \\n \\n  Zettlemoyer,      and      Marjan      Ghazvininejad.\\n2023.\\nIn-  \\n \\n  context      examples      selection      for      machine      translation.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 71, 'chunk_type': 'para'}, 'page_content': 'References\\n \\n \\n  In      Findings      of      the      Association      for      Computational  \\n \\n  Linguistics:      ACL      2023,      pages      8857-8873,      Toronto,  \\n \\n  Canada.\\nAssociation      for      Computational      Linguistics.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 72, 'chunk_type': 'para'}, 'page_content': 'References\\n \\n \\n  Alan      Akbik,      Tanja      Bergmann,      Duncan      Blythe,      Kashif  \\n \\n  Rasul,      Stefan      Schweter,      and      Roland      Vollgraf.\\n2019.  \\n \\n  FLAIR:      An      easy-to-use      framework      for      state-of-the-  \\n \\n  art      NLP.\\nIn      Proceedings      of      the      2019      Conference      of  \\n \\n  the      North      American      Chapter      of      the      Association      for  \\n \\n  Computational      Linguistics      (Demonstrations),      pages  \\n \\n  54-59,      Minneapolis,      Minnesota.\\nAssociation      for  \\n \\n  Computational      Linguistics.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 73, 'chunk_type': 'para'}, 'page_content': 'References\\n \\n \\n  Mohammad      Aliannejadi,      Hamed      Zamani,      Fabio  \\n \\n  Crestani,      and   \\n \\nW      Bruce      Croft.\\n2019.\\nAsking      clari-  \\n \\n  fying      questions      in      open-domain      information-seeking  \\n \\n  conversations.\\nIn      Proceedings      of      the      42nd      interna-  \\n \\n  tional      acm      sigir      conference      on      research      and      develop-  \\n \\n  ment      in      information      retrieval,      pages      475-484.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 74, 'chunk_type': 'para'}, 'page_content': 'References\\n \\n \\n  Nishant      Balepur,      Jie      Huang,      and      Kevin      Chang.\\n2023.  \\n \\n  Expository      text      generation:      Imitate,      retrieve,      para-  \\n \\n  phrase.\\nIn      Proceedings      of      the      2023      Conference      on  \\n \\n  Empirical      Methods      in      Natural      Language      Process-  \\n \\n  ing,      pages      11896-11919,      Singapore.\\nAssociation      for  \\n \\n  Computational      Linguistics.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 75, 'chunk_type': 'para'}, 'page_content': 'References\\n \\n \\n  Siddhartha      Banerjee      and      Prasenjit      Mitra.\\n2015.  \\n \\n  WikiKreator:      Improving      Wikipedia      stubs      automat-  \\n \\n  ically.\\nIn      Proceedings      of      the      53rd      Annual      Meet-  \\n \\n  ing      of      the      Association      for      Computational      Linguis-  \\n \\n  tics      and      the      7th      International      Joint      Conference      on  \\n \\n  Natural      Language      Processing      (Volume      1:      Long      Pa-  \\n \\n  pers),      pages      867-877,      Beijing,      China.\\nAssociation  \\n \\n  for      Computational      Linguistics.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 76, 'chunk_type': 'para'}, 'page_content': 'References\\n \\n \\n  Bernd      Bohnet,      Vinh      Q.      Tran,      Pat      Verga,      Roee      Aha-  \\n \\n  roni,      Daniel      Andor,      Livio      Baldini      Soares,      Massimil-  \\n \\n  iano      Ciaramita,      Jacob      Eisenstein,      Kuzman      Ganchev,  \\n \\n  Jonathan      Herzig,      Kai      Hui,      Tom      Kwiatkowski,      Ji      Ma,  \\n \\n  Jianmo      Ni,      Lierni      Sestorain      Saralegui,      Tal      Schus-  \\n \\n  ter,      William      W.      Cohen,      Michael      Collins,      Dipanjan  \\n \\n  Das,      Donald      Metzler,      Slav      Petrov,      and      Kellie      Webster.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 77, 'chunk_type': 'para'}, 'page_content': 'References\\n \\n \\n  2023.\\nAttributed      question      answering:      Evaluation      and       modeling      for      attributed      large      language      models.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 78, 'chunk_type': 'para'}, 'page_content': 'References\\n \\n \\n  Wayne      C      Booth,      Gregory      G      Colomb,      and      Joseph      M       Williams.\\n2003.\\nThe      craft      of      research.\\nUniversity      of  \\n \\n  Chicago      press.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 79, 'chunk_type': 'para'}, 'page_content': 'References\\n \\n \\n  Laura      Dietz      and      John      Foley.\\n2019.\\nTrec      car      y3:      Com-  \\n \\n  plex      answer      retrieval      overview.\\nIn      Proceedings      of  \\n \\n  Text      REtrieval      Conference      (TREC).'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 80, 'chunk_type': 'para'}, 'page_content': 'References\\n \\n \\n  Christina      S      Doyle.\\n1994.\\nInformation      literacy      in      an  \\n \\n  information      society:   \\n \\nA      concept      for      the      information  \\n \\n  age.\\nDiane      Publishing.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 81, 'chunk_type': 'para'}, 'page_content': 'References\\n \\n \\n  Ann-Marie      Eriksson      and      Asa      Mikitalo.\\n2015.\\nSupervi-  \\n \\n  sion      at      the      outline      stage:      Introducing      and      encounter-  \\n \\n  ing      issues      of      sustainable      development      through      aca-  \\n \\n  demic      writing      assignments.\\nText   \\n \\n&      Talk,      35(2):123-  \\n \\n  153.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 82, 'chunk_type': 'para'}, 'page_content': 'References\\n \\n \\n  Angela      Fan      and      Claire      Gardent.\\n2022.\\nGenerating      bi-  \\n \\n  ographies      on      Wikipedia:      The      impact      of      gender      bias  \\n \\n  on      the      retrieval-based      generation      of      women      biogra-  \\n \\n  phies.\\nIn      Proceedings      of      the      60th      Annual      Meeting      of  \\n \\n  the      Association      for      Computational      Linguistics      (Vol-  \\n \\n  ume      I:      Long      Papers),      pages      8561-8576,      Dublin,  \\n \\n  Ireland.\\nAssociation      for      Computational      Linguistics.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 83, 'chunk_type': 'para'}, 'page_content': 'References\\n \\n \\n  Xiaocheng      Feng,      Ming      Liu,      Jiahao      Liu,      Bing      Qin,      Yibo  \\n \\n  Sun,      and      Ting      Liu.\\n2018.\\nTopic-to-essay      generation  \\n \\n  with      neural      networks.\\nIn      JJCAI,      pages      4078-4084.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 84, 'chunk_type': 'para'}, 'page_content': 'References\\n \\n \\n  Tira      Nur      Fitria.\\n2023.\\nArtificial      intelligence      (ai)      tech-  \\n \\n  nology      in      openai      chatgpt      application:   \\n \\nA      review      of  \\n \\n  chatgpt      in      writing      english      essay.\\nIn      ELT      Forum:      Jour-  \\n \\n  nal      of      English      Language      Teaching,      volume      12,      pages  \\n \\n  44-58.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 85, 'chunk_type': 'para'}, 'page_content': 'References\\n \\n \\n  Pasi      Franti      and      Radu      Mariescu-Istodor.\\n2023.\\nSoft      preci-  \\n \\n  sion      and      recall.\\nPattern      Recognition      Letters,      167:115—  \\n \\n  121.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 86, 'chunk_type': 'para'}, 'page_content': 'References\\n \\n \\n  R      Edward      Freeman,      Jeffrey      S      Harrison,      Andrew      C       Wicks,      Bidhan   \\n \\nL      Parmar,      and      Simone      De      Colle.\\n2010.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 87, 'chunk_type': 'para'}, 'page_content': 'References\\n \\n \\n  Stakeholder      theory:      The      state      of      the      art.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 88, 'chunk_type': 'para'}, 'page_content': 'References\\n \\n \\n  Tianyu      Gao,      Howard      Yen,      Jiatong      Yu,      and      Danqi      Chen.\\n \\n \\n  2023.\\nEnabling      large      language      models      to      generate  \\n \\n  text      with      citations.\\nIn      Proceedings      of      the      2023      Con-  \\n \\n  ference      on      Empirical      Methods      in      Natural      Language  \\n \\n  Processing,      pages      6465-6488,      Singapore.\\nAssocia-  \\n \\n  tion      for      Computational      Linguistics.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 89, 'chunk_type': 'para'}, 'page_content': 'References\\n \\n \\n  Lei      Huang,      Weijiang      Yu,      Weitao      Ma,      Weihong      Zhong,  \\n \\n  Zhangyin      Feng,      Haotian      Wang,      Qianglong      Chen,  \\n \\n  Weihua      Peng,      Xiaocheng      Feng,      Bing      Qin,      and      Ting  \\n \\n  Liu.\\n2023.   \\n \\nA      survey      on      hallucination      in      large      lan-  \\n \\n  guage      models:      Principles,      taxonomy,      challenges,      and       open      questions.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 90, 'chunk_type': 'para'}, 'page_content': 'References\\n \\n \\n  Gautier      Izacard,      Patrick      Lewis,      Maria      Lomeli,      Lucas  \\n \\n  Hosseini,      Fabio      Petroni,      Timo      Schick,      Jane      Dwivedi-  \\n \\n  Yu,      Armand      Joulin,      Sebastian      Riedel,      and      Edouard  \\n \\n  Grave.\\n2023.\\nAtlas:      Few-shot      learning      with      retrieval  \\n \\n  augmented      language      models.\\nJournal      of      Machine  \\n \\n  Learning      Research,      24(251):1-43.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 91, 'chunk_type': 'para'}, 'page_content': 'References\\n \\n \\n  Albert   \\n \\nQ      Jiang,      Alexandre      Sablayrolles,      Arthur      Men-  \\n \\n  sch,      Chris      Bamford,      Devendra      Singh      Chaplot,      Diego  \\n \\n  de      las      Casas,      Florian      Bressand,      Gianna      Lengyel,      Guil-  \\n \\n  laume      Lample,      Lucile      Saulnier,      et      al.\\n2023a.\\nMistral  \\n \\n  7b.\\narXiv      preprint      arXiv:2310.06825.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 92, 'chunk_type': 'para'}, 'page_content': 'References\\n \\n \\n  Zhengbao      Jiang,      Frank      Xu,      Luyu      Gao,      Zhiqing      Sun,  \\n \\n  Qian      Liu,      Jane      Dwivedi-Yu,      Yiming      Yang,      Jamie  \\n \\n  Callan,      and      Graham      Neubig.\\n2023b.\\nActive      retrieval  \\n \\n  augmented      generation.\\nIn      Proceedings      of      the      2023  \\n \\n  Conference      on      Empirical      Methods      in      Natural      Lan-  \\n \\n  guage      Processing,      pages      7969-7992,      Singapore.\\nAs-  \\n \\n  sociation      for      Computational      Linguistics.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 93, 'chunk_type': 'para'}, 'page_content': 'References\\n \\n \\n  Nikhil      Kandpal,      Haikang      Deng,      Adam      Roberts,      Eric  \\n \\n  Wallace,      and      Colin      Raffel.\\n2023.\\nLarge      language  \\n \\n  models      struggle      to      learn      long-tail      knowledge.\\nIn      In-  \\n \\n  ternational      Conference      on      Machine      Learning,      pages  \\n \\n  15696-15707.\\nPMLR.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 94, 'chunk_type': 'para'}, 'page_content': 'References\\n \\n \\n  Omar      Khattab,      Keshav      Santhanam,      Xiang      Lisa  \\n \\n  Li,      David      Hall,      Percy      Liang,      Christopher      Potts,  \\n \\n  and      Matei      Zaharia.\\n2022.\\nDemonstrate-search-  \\n \\n  predict:      Composing      retrieval      and      language      mod-  \\n \\n  els      for      knowledge-intensive      NLP.\\narXiv      preprint  \\n \\n  arXiv:2212.14024.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 95, 'chunk_type': 'para'}, 'page_content': 'References\\n \\n \\n  Omar      Khattab,      Arnav      Singhvi,      Paridhi      Maheshwari,  \\n \\n  Zhiyuan      Zhang,      Keshav      Santhanam,      Sri      Vard-  \\n \\n  hamanan,      Saiful      Haq,      Ashutosh      Sharma,      Thomas      T.  \\n \\n  Joshi,      Hanna      Moazam,      Heather      Miller,      Matei      Za-  \\n \\n  haria,      and      Christopher      Potts.\\n2023.\\nDspy:      Compiling  \\n \\n  declarative      language      model      calls      into      self-improving  \\n \\n  pipelines.\\narXiv      preprint      arXiv:2310.03714.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 96, 'chunk_type': 'para'}, 'page_content': 'References\\n \\n \\n  Seungone      Kim,      Jamin      Shin,      Yejin      Cho,      Joel      Jang,  \\n \\n  Shayne      Longpre,      Hwaran      Lee,      Sangdoo      Yun,  \\n \\n  Seongjin      Shin,      Sungdong      Kim,      James      Thorne,      et      al.\\n \\n \\n  2023.\\nPrometheus:      Inducing      fine-grained      evalua-  \\n \\n  tion      capability      in      language      models.\\narXiv      preprint  \\n \\n  arXiv:2310.08491.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 97, 'chunk_type': 'para'}, 'page_content': 'References\\n \\n \\n  Mojtaba      Komeili,      Kurt      Shuster,      and      Jason      Weston.\\n2022.  \\n \\n  Internet-augmented      dialogue      generation.\\nIn      Proceed-  \\n \\n  ings      of      the      60th      Annual      Meeting      of      the      Association  \\n \\n  for      Computational      Linguistics      (Volume      1:      Long      Pa-  \\n \\n  pers),      pages      8460-8478,      Dublin,      Ireland.\\nAssociation  \\n \\n  for      Computational      Linguistics.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 98, 'chunk_type': 'para'}, 'page_content': 'References\\n \\n \\n  Kalpesh      Krishna,      Erin      Bransom,      Bailey      Kuehl,      Mohit  \\n \\n  Iyyer,      Pradeep      Dasigi,      Arman      Cohan,      and      Kyle      Lo.\\n \\n \\n  2023.\\nLongEval:      Guidelines      for      human      evaluation      of  \\n \\n  faithfulness      in      long-form      summarization.\\nIn      Proceed-  \\n \\n  ings      of      the      17th      Conference      of      the      European      Chap-  \\n \\n  ter      of      the      Association      for      Computational      Linguistics,  \\n \\n  pages      1650-1669,      Dubrovnik,      Croatia.\\nAssociation  \\n \\n  for      Computational      Linguistics.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 99, 'chunk_type': 'para'}, 'page_content': 'References\\n \\n \\n  Patrick      Lewis,      Ethan      Perez,      Aleksandra      Piktus,      Fabio  \\n \\n  Petroni,      Vladimir      Karpukhin,      Naman      Goyal,      Hein-  \\n \\n  rich      Kiittler,      Mike      Lewis,      Wen-tau      Yih,      Tim      Rock-  \\n \\n  taschel,      et      al.\\n2020.\\nRetrieval-augmented      generation  \\n \\n  for      knowledge-intensive      nlp      tasks.\\nAdvances      in      Neu-  \\n \\n  ral      Information      Processing      Systems,      33:9459-9474.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 100, 'chunk_type': 'para'}, 'page_content': 'References\\n \\n \\n  Xiaonan      Li,      Kai      Lv,      Hang      Yan,      Tianyang      Lin,      Wei      Zhu,  \\n \\n  Yuan      Ni,      Guotong      Xie,      Xiaoling      Wang,      and      Xipeng  \\n \\n  Qiu.\\n2023.\\nUnified      demonstration      retriever      for      in-  \\n \\n  context      learning.\\nIn      Proceedings      of      the      61st      Annual  \\n \\n  Meeting      of      the      Association      for      Computational      Lin-  \\n \\n  guistics      (Volume      1:      Long      Papers),      pages      4644-4668,  \\n \\n  Toronto,      Canada.\\nAssociation      for      Computational      Lin-  \\n \\n  guistics.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 101, 'chunk_type': 'para'}, 'page_content': 'References\\n \\n \\n  Chin-Yew      Lin.\\n2004.\\nROUGE:   \\n \\nA      package      for      auto-  \\n \\n  matic      evaluation      of      summaries.\\nIn      Text      Summariza-  \\n \\n  tion      Branches      Out,      pages      74-81,      Barcelona,      Spain.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 102, 'chunk_type': 'para'}, 'page_content': 'References\\n \\n \\n  Association      for      Computational      Linguistics.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 103, 'chunk_type': 'para'}, 'page_content': 'References\\n \\n \\n  Jiachang      Liu,      Dinghan      Shen,      Yizhe      Zhang,      Bill      Dolan,  \\n \\n  Lawrence      Carin,      and      Weizhu      Chen.\\n2022.\\nWhat  \\n \\n  makes      good      in-context      examples      for      GPT-3?\\nIn  \\n \\n  Proceedings      of      Deep      Learning      Inside      Out      (DeeLIO  \\n \\n  2022):      The      3rd      Workshop      on      Knowledge      Extrac-  \\n \\n  tion      and      Integration      for      Deep      Learning      Architectures,  \\n \\n  pages      100-114,      Dublin,      Ireland      and      Online.\\nAssocia-  \\n \\n  tion      for      Computational      Linguistics.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 104, 'chunk_type': 'para'}, 'page_content': 'References\\n \\n \\n  Peter      J.      Liu,      Mohammad      Saleh,      Etienne      Pot,      Ben  \\n \\n  Goodrich,      Ryan      Sepassi,      Lukasz      Kaiser,      and      Noam  \\n \\n  Shazeer.\\n2018.\\nGenerating      wikipedia      by      summariz-  \\n \\n  ing      long      sequences.\\nIn      International      Conference      on  \\n \\n  Learning      Representations.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 105, 'chunk_type': 'para'}, 'page_content': 'References\\n \\n \\n  Jacob      Menick,      Maja      Trebacz,      Vladimir      Mikulik,  \\n \\n  John      Aslanides,      Francis      Song,      Martin      Chadwick,  \\n \\n  Mia      Glaese,      Susannah      Young,      Lucy      Campbell-  \\n \\n  Gillingham,      Geoffrey      Irving,      and      Nat      McAleese.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 106, 'chunk_type': 'para'}, 'page_content': 'References\\n \\n \\n  2022.\\nTeaching      language      models      to      support      answers  \\n \\n  with      verified      quotes.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 107, 'chunk_type': 'para'}, 'page_content': 'References\\n \\n \\n  Sewon      Min,      Kalpesh      Krishna,      Xinxi      Lyu,      Mike      Lewis,  \\n \\n  Wen-tau      Yih,      Pang      Koh,      Mohit      Iyyer,      Luke      Zettle-  \\n \\n  moyer,      and      Hannaneh      Hajishirzi.\\n2023.\\nFActScore:'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 108, 'chunk_type': 'para'}, 'page_content': 'References\\n \\n \\n  Fine-grained      atomic      evaluation      of      factual      precision  \\n \\n  in      long      form      text      generation.\\nIn      Proceedings      of      the  \\n \\n  2023      Conference      on      Empirical      Methods      in      Natural  \\n \\n  Language      Processing,      pages      12076-12100,      Singa-  \\n \\n  pore.\\nAssociation      for      Computational      Linguistics.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 109, 'chunk_type': 'para'}, 'page_content': 'References\\n \\n \\n  Julia      Minguill6n,      Maura      Lerga,      Eduard      Aibar,      Josep  \\n \\n  Lladés-Masllorens,      and      Antoni      Meseguer-Artola.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 110, 'chunk_type': 'para'}, 'page_content': 'References\\n \\n \\n  2017.\\nSemi-automatic      generation      of   \\n \\na      corpus      of  \\n \\n  wikipedia      articles      on      science      and      technology.\\nProfe-  \\n \\n  sional      de      la      Informacion,      26(5):995—1005.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 111, 'chunk_type': 'para'}, 'page_content': 'References\\n \\n \\n  Rosa      Munoz-Luna.\\n2015.\\nMain      ingredients      for      suc-  \\n \\n  cess      in      12      academic      writing:      Outlining,      drafting      and       proofreading.\\nPloS      one,      10(6):e0128309.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 112, 'chunk_type': 'para'}, 'page_content': 'References\\n \\n \\n  Reiichiro      Nakano,      Jacob      Hilton,      Suchir      Balaji,      Jeff      Wu,  \\n \\n  Long      Ouyang,      Christina      Kim,      Christopher      Hesse,  \\n \\n  Shantanu      Jain,      Vineet      Kosaraju,      William      Saunders,  \\n \\n  Xu      Jiang,      Karl      Cobbe,      Tyna      Eloundou,      Gretchen  \\n \\n  Krueger,      Kevin      Button,      Matthew      Knight,      Benjamin  \\n \\n  Chess,      and      John      Schulman.\\n2022.\\nWebgpt:      Browser-  \\n \\n  assisted      question-answering      with      human      feedback.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 113, 'chunk_type': 'para'}, 'page_content': 'References\\n \\n \\n  Long      Ouyang,      Jeffrey      Wu,      Xu      Jiang,      Diogo      Almeida,  \\n \\n  Carroll      Wainwright,      Pamela      Mishkin,      Chong      Zhang,  \\n \\n  Sandhini      Agarwal,      Katarina      Slama,      Alex      Ray,      et      al.\\n \\n \\n  2022.\\nTraining      language      models      to      follow      instruc-  \\n \\n  tions      with      human      feedback.\\nAdvances      in      Neural  \\n \\n  Information      Processing      Systems,      35:27730—27744.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 114, 'chunk_type': 'para'}, 'page_content': 'References\\nAaron      Parisi,      Yao      Zhao,      and      Noah      Fiedel.\\n2022.\\nTalm:  \\n \\n  Tool      augmented      language      models.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 115, 'chunk_type': 'para'}, 'page_content': 'References\\n \\n \\n  John      V      Pavlik.\\n2023.\\nCollaborating      with      chatgpt:      Con-  \\n \\n  sidering      the      implications      of      generative      artificial      intel-  \\n \\n  ligence      for      journalism      and      media      education.\\nJournal-  \\n \\n  ism   \\n \\n&      Mass      Communication      Educator,      78(1):84—93.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 116, 'chunk_type': 'para'}, 'page_content': 'References\\n \\n \\n  Gabriel      Poesia,      Alex      Polozov,      Vu      Le,      Ashish      Tiwari,  \\n \\n  Gustavo      Soares,      Christopher      Meek,      and      Sumit      Gul-  \\n \\n  wani.\\n2022.\\nSynchromesh:      Reliable      code      generation  \\n \\n  from      pre-trained      language      models.\\nIn      International  \\n \\n  Conference      on      Learning      Representations.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 117, 'chunk_type': 'para'}, 'page_content': 'References\\n \\n \\n  Ofir      Press,      Muru      Zhang,      Sewon      Min,      Ludwig      Schmidt,  \\n \\n  Noah      Smith,      and      Mike      Lewis.\\n2023.\\nMeasuring      and       narrowing      the      compositionality      gap      in      language      mod-  \\n \\n  els.\\nIn      Findings      of      the      Association      for      Computational  \\n \\n  Linguistics:      EMNLP      2023,      pages      5687-5711,      Singa-  \\n \\n  pore.\\nAssociation      for      Computational      Linguistics.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 118, 'chunk_type': 'para'}, 'page_content': 'References\\n \\n \\n  Peng      Qi,      Yuhao      Zhang,      and      Christopher      D.      Manning.\\n \\n \\n  2020.      Stay      hungry,      stay      focused:      Generating      infor-  \\n \\n  mative      and      specific      questions      in      information-seeking  \\n \\n  conversations.\\nIn      Findings      of      the      Association      for  \\n \\n  Computational      Linguistics:      EMNLP      2020,      pages      25—  \\n \\n  40,      Online.\\nAssociation      for      Computational      Linguis-  \\n \\n  tics.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 119, 'chunk_type': 'para'}, 'page_content': 'References\\n \\n \\n  Hongjing      Qian,      Yutao      Zhu,      Zhicheng      Dou,      Haoqi      Gu,  \\n \\n  Xinyu      Zhang,      Zheng      Liu,      Ruofei      Lai,      Zhao      Cao,  \\n \\n  Jian-Yun      Nie,      and      Ji-Rong      Wen.\\n2023.\\nWebbrain:'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 120, 'chunk_type': 'para'}, 'page_content': 'References\\n \\n \\n  Learning      to      generate      factually      correct      articles      for  \\n \\n  queries      by      grounding      on      large      web      corpus.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 121, 'chunk_type': 'para'}, 'page_content': 'References\\n \\n \\n  Hossein      A.      Rahmani,      Xi      Wang,      Yue      Feng,      Qiang      Zhang,  \\n \\n  Emine      Yilmaz,      and      Aldo      Lipani.\\n2023.   \\n \\nA      survey      on  \\n \\n  asking      clarification      questions      datasets      in      conversa-  \\n \\n  tional      systems.\\nIn      Proceedings      of      the      61st      Annual  \\n \\n  Meeting      of      the      Association      for      Computational      Lin-  \\n \\n  guistics      (Volume      1:      Long      Papers),      pages      2698-2716,  \\n \\n  Toronto,      Canada.\\nAssociation      for      Computational      Lin-  \\n \\n  guistics.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 122, 'chunk_type': 'para'}, 'page_content': 'References\\n \\n \\n  Ashwin      Ram.\\n1991.   \\n \\nA      theory      of      questions      and      question  \\n \\n  asking.\\nJournal      of      the      Learning      Sciences,      1(3-4):273-  \\n \\n  318.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 123, 'chunk_type': 'para'}, 'page_content': 'References\\n \\n \\n  Ori      Ram,      Yoav      Levine,      Itay      Dalmedigos,      Dor      Muhlgay,  \\n \\n  Amnon      Shashua,      Kevin      Leyton-Brown,      and      Yoav  \\n \\n  Shoham.\\n2023.\\nIn-context      retrieval-augmented      lan-  \\n \\n  guage      models.\\nTransactions      of      the      Association      for  \\n \\n  Computational      Linguistics.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 124, 'chunk_type': 'para'}, 'page_content': 'References\\n \\n \\n  Nils      Reimers      and      Iryna      Gurevych.\\n2019.\\nSentence-  \\n \\n  BERT:      Sentence      embeddings      using      Siamese      BERT-  \\n \\n  networks.\\nIn      Proceedings      of      the      2019      Conference      on  \\n \\n  Empirical      Methods      in      Natural      Language      Processing  \\n \\n  and      the      9th      International      Joint      Conference      on      Natu-  \\n \\n  ral      Language      Processing      (EMNLP-IJCNLP),      pages  \\n \\n  3982-3992,      Hong      Kong,      China.\\nAssociation      for      Com-  \\n \\n  putational      Linguistics.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 125, 'chunk_type': 'para'}, 'page_content': 'References\\n \\n \\n \\nD      Gordon      Rohman.\\n1965.\\nPre-writing      the      stage      of      dis-  \\n \\n  covery      in      the      writing      process.\\nCollege      composition  \\n \\n  and      communication,      16(2):106—112.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 126, 'chunk_type': 'para'}, 'page_content': 'References\\n \\n \\n  Christina      Sauper      and      Regina      Barzilay.\\n2009.\\nAuto-  \\n \\n  matically      generating      Wikipedia      articles:   \\n \\nA      structure-  \\n \\n  aware      approach.\\nIn      Proceedings      of      the      Joint      Con-  \\n \\n  ference      of      the      47th      Annual      Meeting      of      the      ACL      and       the      4th      International      Joint      Conference      on      Natural  \\n \\n  Language      Processing      of      the      AFNLP,      pages      208-216,  \\n \\n  Suntec,      Singapore.\\nAssociation      for      Computational  \\n \\n  Linguistics.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 127, 'chunk_type': 'para'}, 'page_content': 'References\\n \\n \\n  Lam.\\n2023.\\nWikiChat:      Stopping      the      hallucination      of  \\n \\n  large      language      model      chatbots      by      few-shot      ground-  \\n \\n  ing      on      Wikipedia.\\nIn      Findings      of      the      Association  \\n \\n  for      Computational      Linguistics:      EMNLP      2023,      pages  \\n \\n  2387-2413,      Singapore.\\nAssociation      for      Computa-  \\n \\n  tional      Linguistics.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 128, 'chunk_type': 'para'}, 'page_content': 'References\\n \\n \\n  Jonathan      Bragg,      Jeff      Hammerbacher,      Doug      Downey,  \\n \\n  Joseph      Chee      Chang,      and      David      Sontag.\\n2023.\\nBe-  \\n \\n  yond      summarization:      Designing      ai      support      for      real-  \\n \\n  world      expository      writing      tasks.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 129, 'chunk_type': 'para'}, 'page_content': 'References\\n \\n \\n  Luke      Zettlemoyer.\\n2022.\\nNearest      neighbor      zero-shot  \\n \\n  inference.\\nIn      Proceedings      of      the      2022      Conference      on  \\n \\n  Empirical      Methods      in      Natural      Language      Processing,  \\n \\n  pages      3254-3265,      Abu      Dhabi,      United      Arab      Emirates.\\n \\n \\n  Association      for      Computational      Linguistics.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 130, 'chunk_type': 'para'}, 'page_content': 'References\\n \\n \\n  Stephen      Roller,      Arthur      Szlam,      and      Jason      Weston.\\n \\n \\n  2022.\\nLanguage      models      that      seek      for      knowledge:  \\n \\n  Modular      search   \\n \\n&      generation      for      dialogue      and       prompt      completion.\\nIn      Findings      of      the      Association  \\n \\n  for      Computational      Linguistics:      EMNLP      2022,      pages  \\n \\n  373-393,      Abu      Dhabi,      United      Arab      Emirates.\\nAssoci-  \\n \\n  ation      for      Computational      Linguistics.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 131, 'chunk_type': 'para'}, 'page_content': 'References\\n \\n \\n  and      Jason      Weston.\\n2021.\\nRetrieval      augmentation  \\n \\n  reduces      hallucination      in      conversation.\\nIn      Findings  \\n \\n  of      the      Association      for      Computational      Linguistics:  \\n \\n  EMNLP      2021,      pages      3784-3803,      Punta      Cana,      Do-  \\n \\n  minican      Republic.\\nAssociation      for      Computational  \\n \\n  Linguistics.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 132, 'chunk_type': 'para'}, 'page_content': 'References\\nWikipedia      as      an      introduction      to      academic      writing.\\nIn  \\n \\n  English      teaching      forum,      volume      48,      page      12.\\nERIC.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 133, 'chunk_type': 'para'}, 'page_content': 'References\\n \\n \\n  and      Jaclyn      Gishbaugher.\\n2020.\\nRole      of      questions      in  \\n \\n  inquiry-based      instruction:      towards   \\n \\na      design      taxon-  \\n \\n  omy      for      question-asking      and      implications      for      design.\\n \\n \\n  Educational      Technology      Research      and      Development,  \\n \\n  68:653-678.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 134, 'chunk_type': 'para'}, 'page_content': 'References\\nitory      text.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 135, 'chunk_type': 'para'}, 'page_content': 'References\\n \\n \\n  than      humans?\\nvalidating      how      openai’s      chatgpt      model  \\n \\n  explains      crowdfunding,      alternative      finance      and      com-  \\n \\n  munity      finance.\\nValidating      how      OpenAlI’s      ChatGPT  \\n \\n  model      explains      Crowdfunding,      Alternative      Finance  \\n \\n  and      Community      Finance.(December      22,      2022).'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 136, 'chunk_type': 'para'}, 'page_content': 'References\\n \\n \\n  Choi.\\n2023.   \\n \\nA      critical      evaluation      of      evaluations      for  \\n \\n  long-form      question      answering.\\nIn      Proceedings      of      the  \\n \\n  61st      Annual      Meeting      of      the      Association      for      Compu-  \\n \\n  tational      Linguistics      (Volume      1:      Long      Papers),      pages  \\n \\n  3225-3245,      Toronto,      Canada.\\nAssociation      for      Com-  \\n \\n  putational      Linguistics.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 137, 'chunk_type': 'para'}, 'page_content': 'References\\n \\n \\n  Kevin      Yang,      Dan      Klein,      Nanyun      Peng,      and      Yuandong  \\n \\n  Tian.\\n2023.      DOC:      Improving      long      story      coherence  \\n \\n  with      detailed      outline      control.\\nIn      Proceedings      of      the  \\n \\n  61st      Annual      Meeting      of      the      Association      for      Compu-  \\n \\n  tational      Linguistics      (Volume      I:      Long      Papers),      pages  \\n \\n  3378-3465,      Toronto,      Canada.\\nAssociation      for      Com-  \\n \\n  putational      Linguistics.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 138, 'chunk_type': 'para'}, 'page_content': 'References\\n \\n \\n  Kevin      Yang,      Yuandong      Tian,      Nanyun      Peng,      and      Dan  \\n \\n  Klein.\\n2022.\\nRe3:      Generating      longer      stories      with  \\n \\n  recursive      reprompting      and      revision.\\nIn      Proceedings  \\n \\n  of      the      2022      Conference      on      Empirical      Methods      in      Nat-  \\n \\n  ural      Language      Processing,      pages      4393-4479,      Abu  \\n \\n  Dhabi,      United      Arab      Emirates.\\nAssociation      for      Com-  \\n \\n  putational      Linguistics.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 139, 'chunk_type': 'para'}, 'page_content': 'References\\n \\n \\n  Shunyu      Yao,      Jeffrey      Zhao,      Dian      Yu,      Nan      Du,      Izhak  \\n \\n  Shafran,      Karthik      R      Narasimhan,      and      Yuan      Cao.\\n2023.  \\n \\n  React:      Synergizing      reasoning      and      acting      in      language  \\n \\n  models.\\nIn      The      Eleventh      International      Conference  \\n \\n  on      Learning      Representations.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 140, 'chunk_type': 'para'}, 'page_content': 'References\\n \\n \\n  Cyril      Zakka,      Akash      Chaurasia,      Rohan      Shad,      Alex      R       Dalal,      Jennifer   \\n \\nL      Kim,      Michael      Moor,      Kevin      Alexan-  \\n \\n  der,      Euan      Ashley,      Jack      Boyd,      Kathleen      Boyd,      et      al.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 141, 'chunk_type': 'para'}, 'page_content': 'References\\n \\n \\n  2023.\\nAlmanac:      Retrieval-augmented      language      mod-  \\n \\n  els      for      clinical      medicine.\\nResearch      Square.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 142, 'chunk_type': 'para'}, 'page_content': 'References\\n \\n \\n  Shuyan      Zhou,      Uri      Alon,      Frank      F.      Xu,      Zhengbao      Jiang,  \\n \\n  and      Graham      Neubig.\\n2023.\\nDocprompting:      Gener-  \\n \\n  ating      code      by      retrieving      the      docs.\\nIn      The      Eleventh  \\n \\n  International      Conference      on      Learning      Representa-  \\n \\n  tions.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 143, 'chunk_type': 'para'}, 'page_content': 'Average      Number      of      References      90.1\\nTable      7:      Statistics      of      the      dataset      used      in      our      experiments.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 144, 'chunk_type': 'para'}, 'page_content': 'Average      Number      of      References      90.1\\n—\\n \\n   Average Number      of      references  \\n \\n  a      BR  \\n \\n  N      Py      oa      feo}      Oo      N       Oo      Oo      oO      Oo      oO      Oo  \\n \\n  fo)  \\n \\n  1 0\\n \\n   20      40      60      80      100  \\n \\n  Edit      progress      (%      of      total      edits) Figure      4:      Evolution      of      reference      count      in      the      Wikipedia  \\n \\n  article      editing      process.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 145, 'chunk_type': 'para'}, 'page_content': 'A_      Dataset      Details\\n \\n \\n  As      discussed      in      §2.1,      we      curate      the      FreshWiki  \\n \\n  dataset      by      collecting      recent      and      high-quality      En-  \\n \\n  glish      Wikipedia      articles.\\nWe      select      the      most-edited  \\n \\n  pages      over   \\n \\na      specific      period      rather      than      using      cre-  \\n \\n  ation      dates      as   \\n \\na      cutoff      because      most      of      Wikipedia  \\n \\n  articles      are      “stubs”      or      are      of      low      quality      when      they  \\n \\n  were      created.\\nFor      quality,      we      consider      articles      pre-  \\n \\n  dicted      to      be      of      B-class      quality      or      above.\\nAccording  \\n \\n  to      Wikipedia      statistics!\\n*,      only      around      3%      of      ex-  \\n \\n  isting      Wikipedia      pages      meet      this      quality      standard.\\n \\n \\n  As      LLMs      can      generate      reasonably      good      outputs,  \\n \\n  we      think      it      is      important      to      use      high-quality      human-  \\n \\n  written      articles      as      references      for      further      research.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 146, 'chunk_type': 'para'}, 'page_content': 'A_      Dataset      Details\\n \\n \\n  For      experiments      in      this      work,      we      randomly      se-  \\n \\n  lect      100      samples      with      human-written      articles      un-  \\n \\n  der      3000      words      to      have   \\n \\na      meaningful      comparison.\\n \\n \\n  Table   \\n \\n7      gives      the      data      statistics.\\nNotably,      human-  \\n \\n  authored      articles      have   \\n \\na      large      number      of      references  \\n \\n  but      they      require      numerous      edits      to      achieve      this.\\nFig-  \\n \\n  ure   \\n \\n4      illustrates      the      evolution      of      the      reference      count  \\n \\n  in      the      article      edit      process      and      Figure   \\n \\n5      gives      the      dis-  \\n \\n  tribution      of      edit      counts      for      human-authored      articles  \\n \\n  used      in      our      experiments.\\ncount      (A;)   \\n \\n=       where      embed(-)      in      Equation      (1)      is      parameterized  \\n \\n  by      paraphrase-MiniLM-L6-v2      provided      in      the  \\n \\n  Sentence-Transformers      library!*.\\nThe      cardinality  \\n \\n  https://en.wikipedia.\\norg/wiki/Wikipedia:  \\n \\n  Content_assessment  \\n \\n  100) Bhttps://huggingface.co/sentence-transformers/  \\n \\n  paraphrase-MiniLM-L6-v2 Percentage      of      articles      (n'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 147, 'chunk_type': 'table'}, 'page_content': 'A_      Dataset      Details\\n |       T      T      T      T       0      500      1000      1500 |       y      1      7      1      7      7      7       2000      2500      3000      3500      4000      4500      5000       Number      of      edits\\n | Figure      5:      Distribution      of      edit      counts      for      Wikipedia      arti-       cles      in      our      experiments      (n      =      100).\\n'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 148, 'chunk_type': 'para'}, 'page_content': 'B_      Pseudo      Code      of      STORM\\n \\n \\n  In      §3,      we      introduce      STORM,   \\n \\na      framework      that      au-  \\n \\n  tomates      the      pre-writing      stage      by      discovering      differ-  \\n \\n  ent      perspectives,      simulating      information-seeking  \\n \\n  conversations,      and      creating   \\n \\na      comprehensive      out-  \\n \\n  line.\\nAlgorithm   \\n \\n1      displays      the      skeleton      of      STORM.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 149, 'chunk_type': 'para'}, 'page_content': 'B_      Pseudo      Code      of      STORM\\n \\n \\n  We      implement      STORM      with      zero-shot      prompt-  \\n \\n  ing      using      the      DSPy      framework      (Khattab      et      al.,  \\n \\n  2023).\\nListing   \\n \\n1      and   \\n \\n2      show      the      prompts      used  \\n \\n  in      our      implementation.\\nWe      highlight      that      STORM  \\n \\n  offers   \\n \\na      general      framework      designed      to      assist      the  \\n \\n  creation      of      grounded,      long-form      articles,      without  \\n \\n  depending      extensively      on      prompt      engineering      for   \\n \\na       single      domain.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 150, 'chunk_type': 'para'}, 'page_content': 'C.1      Soft      Heading      Recall\\n \\n \\n  We      calculate      the      soft      heading      recall      between      the  \\n \\n  multi-level      headings      in      the      generated      outline,      con-  \\n \\n  sidered      as      the      prediction      P,      and      those      in      the      human-  \\n \\n  written      article,      considered      as      the      ground      truth      G.  \\n \\n  The      calculation      is      based      on      the      soft      recall      defini-  \\n \\n  tion      in      Franti      and      Mariescu-Istodor      (2023).\\nGiven  \\n \\n  aset   \\n \\nA   \\n \\n=      {Ai}*.,,      soft      count      of      an      item      is      defined as      the      inverse      of      the      sum      of      its      similarity      to      other  \\n \\n  items      in      the      set:'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 151, 'chunk_type': 'para'}, 'page_content': 'C.1      Soft      Heading      Recall\\n1\\n \\n \\n  Dj      Sim      (Aj,      Aj)      (1) Sim      (A;,      A;)   \\n \\n=      cos      (embed(A;),      embed(A;))   \\n \\n,       28  \\n \\n  29  \\n \\n  class      GenRelatedTopicsPrompt      (dspy.      Signature):'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 152, 'chunk_type': 'para'}, 'page_content': \"C.1      Soft      Heading      Recall\\n \\n \\n  I\\\\\\\\\\\\'m      writing   \\n \\na      Wikipedia      page      for   \\n \\na      topic      mentioned      below.\\nPlease      identify      and       recommend      some      Wikipedia      pages      on      closely      related      subjects.\\nI\\\\\\\\\\\\'m      looking      for  \\n \\n  examples      that      provide      insights      into      interesting      aspects      commonly      associated  \\n \\n  with      this      topic,      or      examples      that      help      me      understand      the      typical      content      and       structure      included      in      Wikipedia      pages      for      similar      topics.\\n \\n \\n  Please      list      the      urls      in      separate      lines.\"}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 153, 'chunk_type': 'para'}, 'page_content': 'C.1      Soft      Heading      Recall\\n \\n \\n  non topic   \\n \\n=      dspy.InputField(prefix=\"Topic      of      interest:”,      format=str)  \\n \\n  related_topics   \\n \\n=      dspy.OutputField() class      GenPerspectivesPrompt      (dspy.Signature):'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 154, 'chunk_type': 'para'}, 'page_content': 'C.1      Soft      Heading      Recall\\n \\n \\n  You      need      to      select   \\n \\na      group      of      Wikipedia      editors      who      will      work      together      to      create  \\n \\n  a      comprehensive      article      on      the      topic.\\nEach      of      them      represents   \\n \\na      different  \\n \\n  perspective,      role,      or      affiliation      related      to      this      topic.\\nYou      can      use      other  \\n \\n  Wikipedia      pages      of      related      topics      for      inspiration.\\nFor      each      editor,      add       description      of      what      they      will      focus      on.\\n \\n \\n  Give      your      answer      in      the      following      format:      1.      short      summary      of      editor      1:'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 155, 'chunk_type': 'para'}, 'page_content': \"C.1      Soft      Heading      Recall\\ndescription\\\\\\\\2.\\nshort summary of editor 2: description\\\\\\\\  \\n \\n  non  \\n \\n  topic   \\n \\n=      dspy.InputField(prefix=\\\\\\\\\\\\'Topic      of      interest:\\\\\\\\\\\\',      format=str)  \\n \\n  examples   \\n \\n=      dspy.InputField(prefix=\\\\\\\\\\\\'Wiki      page      outlines      of      related      topics      for  \\n \\n  inspiration:\\\\\\\\\\\\\\\\\\\\',      format=str)  \\n \\n  perspectives   \\n \\n=      dspy.OutputField() class      GenQnPrompt(dspy.      Signature):\"}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 156, 'chunk_type': 'para'}, 'page_content': 'C.1      Soft      Heading      Recall\\n \\n \\n  You      are      an      experienced      Wikipedia      writer      and      want      to      edit   \\n \\na      specific      page.\\n \\n \\n  Besides      your      identity      as   \\n \\na      Wikipedia      writer,      you      have   \\n \\na      specific      focus      when  \\n \\n  researching      the      topic.\\n \\n \\n  Now,      you      are      chatting      with      an      expert      to      get      information.\\nAsk      good      questions      to  \\n \\n  get      more      useful      information.\\n \\n \\n  When      you      have      no      more      question      to      ask,      say      \"Thank      you      so      much      for      your      help!”\\nto  \\n \\n  end      the      conversation.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 157, 'chunk_type': 'para'}, 'page_content': \"C.1      Soft      Heading      Recall\\n \\n \\n  Please      only      ask      one      question      at   \\n \\na      time      and      don\\\\\\\\\\\\'t      ask      what      you      have      asked      before.\\n \\n \\n  Your      questions      should      be      related      to      the      topic      you      want      to      write.\"}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 158, 'chunk_type': 'para'}, 'page_content': \"C.1      Soft      Heading      Recall\\n \\n \\n  non  \\n \\n  topic   \\n \\n=      dspy.InputField(prefix=\\\\\\\\\\\\'Topic      you      want      to      write:      \\\\\\\\\\\\',      format=str)  \\n \\n  persona   \\n \\n=      dspy.InputField(prefix=\\\\\\\\\\\\'Your      specific      perspective:      \\\\\\\\\\\\',      format=str)  \\n \\n  conv   \\n \\n=      dspy.InputField(prefix=\\\\\\\\\\\\'Conversation      history:\\\\\\\\\\\\\\\\\\\\',      format=str)  \\n \\n  question   \\n \\n=      dspy.OutputField()  \\n \\n  class      GenQueriesPrompt      (dspy.      Signature):  \\n \\n  nnn  \\n \\n  You      want      to      answer      the      question      using      Google      search.\\nWhat      do      you      type      in      the  \\n \\n  search      box?\\nWrite the queries you will use in the following format:- query 1\\\\\\\\- query 2\\\\\\\\\"}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 159, 'chunk_type': 'para'}, 'page_content': \"C.1      Soft      Heading      Recall\\n \\n \\n  topic   \\n \\n=      dspy.InputField(prefix=\\\\\\\\\\\\'Topic      you      are      discussing      about:      \\\\\\\\\\\\',      format=str)  \\n \\n  question   \\n \\n=      dspy.InputField(prefix=\\\\\\\\\\\\'Question      you      want      to      answer:      \\\\\\\\\\\\',      format=str)  \\n \\n  queries   \\n \\n=      dspy.OutputField()\"}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 160, 'chunk_type': 'para'}, 'page_content': 'C.1      Soft      Heading      Recall\\nListing      1:      Prompts      used      in      STORM,      corresponding      to      Line      4,      11,      19,      22      in      Algorithm      1.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 161, 'chunk_type': 'para'}, 'page_content': 'C.1      Soft      Heading      Recall\\nwow      Ne'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 162, 'chunk_type': 'para'}, 'page_content': 'C.1      Soft      Heading      Recall\\n \\n \\n  20  \\n \\n  21  \\n \\n  22  \\n \\n  23  \\n \\n  24  \\n \\n  25  \\n \\n  26  \\n \\n  27  \\n \\n  28'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 163, 'chunk_type': 'para'}, 'page_content': 'C.1      Soft      Heading      Recall\\n29  \\n \\n  30 class      GenAnswerPrompt(dspy.      Signature):'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 164, 'chunk_type': 'para'}, 'page_content': 'C.1      Soft      Heading      Recall\\n \\n \\n  You      are      an      expert      who      can      use      information      effectively.\\nYou      are      chatting      with   \\n \\na       Wikipedia      writer      who      wants      to      write   \\n \\na      Wikipedia      page      on      topic      you      know.\\nYou  \\n \\n  have      gathered      the      related      information      and      will      now      use      the      information      to  \\n \\n  form   \\n \\na      response.\\n \\n \\n  Make      your      response      as      informative      as      possible      and      make      sure      every      sentence      is  \\n \\n  supported      by      the      gathered      information.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 165, 'chunk_type': 'para'}, 'page_content': \"C.1      Soft      Heading      Recall\\n \\n \\n  non  \\n \\n  topic   \\n \\n=      dspy.InputField(prefix=\\\\\\\\\\\\'Topic      you      are      discussing      about:\\\\\\\\\\\\',      format=str)  \\n \\n  conv      dspy.InputField(prefix=\\\\\\\\\\\\'Question:\\\\\\\\\\\\\\\\\\\\',      format=str)  \\n \\n  info   \\n \\n=      dspy.InputField(      prefix=\\\\\\\\\\\\'Gathered      information:\\\\\\\\\\\\\\\\\\\\',      format=str)  \\n \\n  answer   \\n \\n=      dspy.OutputField(prefix=\\\\\\\\\\\\'Now      give      your      response:\\\\\\\\\\\\\\\\\\\\')\"}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 166, 'chunk_type': 'para'}, 'page_content': 'C.1      Soft      Heading      Recall\\nclass      DirectGenOutlinePrompt      (dspy.      Signature):'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 167, 'chunk_type': 'para'}, 'page_content': 'C.1      Soft      Heading      Recall\\nWrite      an      outline      for   \\n \\na      Wikipedia      page.\\n \\n \\n  Here      is      the      format      of      your      writing:'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 168, 'chunk_type': 'list_item'}, 'page_content': 'C.1      Soft      Heading      Recall\\n2.      Do      not      include      other      information.\\n \\n \\n  non\\n1. Use      \"#\"      Title”      to      indicate      section      title,      \"##\"      Title”      to      indicate  \\n \\n  subsection      title,      \"###\"”      Title”      to      indicate      subsubsection      title,      and      so  \\n \\n  on.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 169, 'chunk_type': 'para'}, 'page_content': 'C.1      Soft      Heading      Recall\\ntopic   \\n \\n=      dspy.InputField(prefix=\"Topic      you      want      to      write:      ”\",      format=str)  \\n \\n  outline   \\n \\n=      dspy.OutputField(prefix=\"Write      the      Wikipedia      page      outline:\\\\\\\\\"”)”'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 170, 'chunk_type': 'para'}, 'page_content': 'C.1      Soft      Heading      Recall\\nclass      RefineOutlinePrompt(dspy.      Signature):'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 171, 'chunk_type': 'para'}, 'page_content': 'C.1      Soft      Heading      Recall\\n \\n \\n  Improve      an      outline      for   \\n \\na      Wikipedia      page.\\nYou      already      have   \\n \\na      draft      outline      that  \\n \\n  covers      the      general      information.\\nNow      you      want      to      improve      it      based      on      the  \\n \\n  information      learned      from      an      information-seeking      conversation      to      make      it      more  \\n \\n  comprehensive.\\n \\n \\n  Here      is      the      format      of      your      writing:'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 172, 'chunk_type': 'list_item'}, 'page_content': 'C.1      Soft      Heading      Recall\\n2.      Do      not      include      other      information.\\n \\n \\n  non'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 173, 'chunk_type': 'list_item'}, 'page_content': 'C.1      Soft      Heading      Recall\\n1. Use      \"#\"      Title”      to      indicate      section      title,      \"##\"      Title”      to      indicate  \\n \\n  subsection      title,      \"###\"      Title”      to      indicate      subsubsection      title,      and      so  \\n \\n  on.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 174, 'chunk_type': 'para'}, 'page_content': 'C.1      Soft      Heading      Recall\\n \\n \\n  topic   \\n \\n=      dspy.InputField(prefix=\"Topic      you      want      to      write:      \",      format=str)  \\n \\n  conv   \\n \\n=      dspy.InputField(prefix=\"Conversation      history:\\\\\\\\\",      format=str)  \\n \\n  old_outline   \\n \\n=      dspy.OutputField(prefix=\"Current      outline:\\\\\\\\”,      format=str)  \\n \\n  outline   \\n \\n=      dspy.OutputField(      prefix=\\\\\\\\\\\\\\'Write      the      Wikipedia      page      outline:\\\\\\\\\\\\\\\\\\\\\\')”'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 175, 'chunk_type': 'para'}, 'page_content': 'C.1      Soft      Heading      Recall\\nListing      2:      Prompts      used      in      STORM      (continue),      corresponding      to      Line      24,      31,      32      in      Algorithm      1.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 176, 'chunk_type': 'para'}, 'page_content': 'C.1      Soft      Heading      Recall\\nyay      aA      uu      &}      WwW      YY      —'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 177, 'chunk_type': 'para'}, 'page_content': 'C.1      Soft      Heading      Recall\\n \\n \\n  11  \\n \\n  12  \\n \\n  13  \\n \\n  14  \\n \\n  15  \\n \\n  16  \\n \\n  17  \\n \\n  18  \\n \\n  19  \\n \\n  20  \\n \\n  21  \\n \\n  22  \\n \\n  23'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 178, 'chunk_type': 'para'}, 'page_content': 'C.1      Soft      Heading      Recall\\n \\n \\n  24  \\n \\n  25  \\n \\n  26  \\n \\n  27  \\n \\n  28  \\n \\n  29  \\n \\n  30  \\n \\n  31  \\n \\n  32  \\n \\n  33  \\n \\n  Input      :Topic      t,      maximum      perspective      N,  \\n \\n  maximum      conversation      round      MJ  \\n \\n  Output   \\n \\n:      Outline      O,      references   \\n \\nR PO = \"basic fact writer \" // Constant.\\n \\n \\n  R-[]  \\n \\n  //      Discover      perspectives      P.  \\n \\n  related_topics   \\n \\n+      gen_related_topics(t)  \\n \\n  tocs   \\n \\n+   \\n \\n|   \\n \\n|       foreach      related_t      in      related_topics      do  \\n \\n  article   \\n \\n<      get_wiki_article(related_t)  \\n \\n  if      article      then  \\n \\n  |      tocs.append(extract_toc(article))  \\n \\n  end  \\n \\n  end  \\n \\n  P   \\n \\n<      gen_perspectives(t,      tocs)  \\n \\n  P<      [PO]   \\n \\n+      P[:N]  \\n \\n  //      Simulate      conversations.\\n \\n \\n  convos   \\n \\n<      [|  \\n \\n  foreach      p      in      P      do  \\n \\n  convo_history   \\n \\n<   \\n \\n|   \\n \\n]       for:      =1to      M      do  \\n \\n  //      Question      asking.\\n \\n \\n  q+      gen_qn(t,      p,      dlg_history)  \\n \\n  convo_history.append(q)  \\n \\n  //      Question      answering.\\n \\n \\n  queries   \\n \\n<      gen_queries(t,      q)  \\n \\n  sources      <—  \\n \\n  search_and_sift(queries)  \\n \\n  a   \\n \\n+      gen_ans(t,      q,      sources)  \\n \\n  convo_history.append(a)  \\n \\n  R.append(sources)  \\n \\n  end  \\n \\n  convos.append(convo_history)  \\n \\n  end  \\n \\n  //      Create      the      outline.\\n \\n \\n  Op   \\n \\n<      direct_gen_outline(t)  \\n \\n  O      «<      refine_outline(t,      Op,      convos)  \\n \\n  return      O,      R       of   \\n \\nA      is      the      sum      of      the      counts      of      its      individual      items:'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 179, 'chunk_type': 'para'}, 'page_content': 'K       card(A)      =      S-      count      (A;)      (2)\\ni=1 The      soft      heading      recall      is      calculated      as card(Gn      P)  \\n \\n  card(G)   \\n \\n”      @) soft      heading      recall      ='}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 180, 'chunk_type': 'para'}, 'page_content': 'K       card(A)      =      S-      count      (A;)      (2)\\nwhere      the      cardinality      of      intersection      is      defined      via  \\n \\n  the      union      as      follows:'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 181, 'chunk_type': 'para'}, 'page_content': 'K       card(A)      =      S-      count      (A;)      (2)\\ncard(Gn      P)   \\n \\n=       card(G)   \\n \\n+      card(P)   \\n \\n—      card(G   \\n \\nU      P).\\n®'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 182, 'chunk_type': 'para'}, 'page_content': 'C.2.      LLM      Evaluator\\nWe      use      Prometheus!\\n*      (Kim      et      al.,      2023),   \\n \\na      13B  \\n \\n  open-source      evaluator      LLM      that      can      assess      long-  \\n \\n  form      text      based      on      customized      1-5      scale      rubric,      to  \\n \\n  grade      the      article      from      the      aspects      of      Interest      level,  \\n \\n  Coherence      and      Organization,      Relevance      and      Fo-  \\n \\n  cus,      and      Coverage.\\nTable   \\n \\n8      gives      our      grading      rubric.\\n \\n \\n  While      Prometheus      is      best      used      with   \\n \\na      score   \\n \\n5      ref-  \\n \\n  erence      answer,      we      find      adding      the      reference      will  \\n \\n  exceed      the      context      length      limit      of      the      model.\\nSince  \\n \\n  Kim      et      al.\\n(2023)      show      Prometheus      ratings      without  \\n \\n  reference      also      correlate      well      with      human      prefer-  \\n \\n  ences,      we      omit      the      reference      and      trim      the      input  \\n \\n  article      to      be      within      2000      words      by      iteratively      re-  \\n \\n  moving      contents      from      the      shortest      section      to      ensure  \\n \\n  the      input      can      fit      into      the      model’s      context      window.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 183, 'chunk_type': 'para'}, 'page_content': 'C.3      More      Discussion      of      the      Citation      Quality\\n \\n \\n  Irrelevant  \\n \\n  Source  \\n \\n  Inaccurate  \\n \\n  Othe      Paraphrasing  \\n \\n  1%      4%  \\n \\n  7%  \\n \\n  Improper  \\n \\n  Inferential      Linking  \\n \\n  Lack      Citation      14%  \\n \\n  47%  \\n \\n  Incorrectly      Split  \\n \\n  12%'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 184, 'chunk_type': 'para'}, 'page_content': 'False      Negative       15%\\nFigure      6:      Error      analysis      of      unsupported      sentences      in      10  \\n \\n  sampled      articles.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 185, 'chunk_type': 'para'}, 'page_content': 'False      Negative       15%\\n \\n \\n  https:      //huggingface.co/kaist-ai/  \\n \\n  Criteria      Description  \\n \\n  Score   \\n \\n|      Description  \\n \\n  Score   \\n \\n2      Description  \\n \\n  Score   \\n \\n3      Description  \\n \\n  Score   \\n \\n4      Description  \\n \\n  Score   \\n \\n5      Description'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 186, 'chunk_type': 'para'}, 'page_content': 'False      Negative       15%\\n \\n \\n  Interest      Level:      How      engaging      and      thought-provoking      is      the      article?\\n \\n \\n  Not      engaging      at      all;      no      attempt      to      capture      the      reader’s      attention.\\n \\n \\n  Fairly      engaging      with   \\n \\na      basic      narrative      but      lacking      depth.\\n \\n \\n  Moderately      engaging      with      several      interesting      points.\\n \\n \\n  Quite      engaging      with   \\n \\na      well-structured      narrative      and      noteworthy      points      that      frequently      capture      and      retain      attention.\\n \\n \\n  Exceptionally      engaging      throughout,      with   \\n \\na      compelling      narrative      that      consistently      stimulates      interest.\\n \\n \\n  Criteria      Description  \\n \\n  Score   \\n \\n|      Description  \\n \\n  Score   \\n \\n2      Description  \\n \\n  Score   \\n \\n3      Description  \\n \\n  Score   \\n \\n4      Description  \\n \\n  Score   \\n \\n5      Description  \\n \\n  Coherence      and      Organization:      Is      the      article      well-organized      and      logically      structured?\\n \\n \\n  Disorganized;      lacks      logical      structure      and      coherence.\\n \\n \\n  Fairly      organized;   \\n \\na      basic      structure      is      present      but      not      consistently      followed.\\n \\n \\n  Organized;   \\n \\na      clear      structure      is      mostly      followed      with      some      lapses      in      coherence.\\n \\n \\n  Good      organization;   \\n \\na      clear      structure      with      minor      lapses      in      coherence.\\n \\n \\n  Excellently      organized;      the      article      is      logically      structured      with      seamless      transitions      and   \\n \\na      clear      argument.\\n \\n \\n  Criteria      Description  \\n \\n  Score   \\n \\n|      Description  \\n \\n  Score   \\n \\n2      Description  \\n \\n  Score   \\n \\n3      Description  \\n \\n  Score   \\n \\n4      Description  \\n \\n  Score   \\n \\n5      Description  \\n \\n  Relevance      and      Focus:      Does      the      article      stay      on      topic      and      maintain   \\n \\na      clear      focus?\\n \\n \\n  Off-topic;      the      content      does      not      align      with      the      headline      or      core      subject.\\n \\n \\n  Somewhat      on      topic      but      with      several      digressions;      the      core      subject      is      evident      but      not      consistently      adhered      to.\\n \\n \\n  Generally      on      topic,      despite   \\n \\na      few      unrelated      details.\\n \\n \\n  Mostly      on      topic      and      focused;      the      narrative      has   \\n \\na      consistent      relevance      to      the      core      subject      with      infrequent      digressions.\\n \\n \\n  Exceptionally      focused      and      entirely      on      topic;      the      article      is      tightly      centered      on      the      subject,      with      every      piece      of      information      contributing  \\n \\n  to   \\n \\na      comprehensive      understanding      of      the      topic.\\n \\n \\n  Criteria      Description  \\n \\n  Score   \\n \\n|      Description  \\n \\n  Score   \\n \\n2      Description  \\n \\n  Score   \\n \\n3      Description  \\n \\n  Score   \\n \\n4      Description  \\n \\n  Score   \\n \\n5      Description  \\n \\n  Broad      Coverage:      Does      the      article      provide      an      in-depth      exploration      of      the      topic      and      have      good      coverage?\\n \\n \\n  Severely      lacking;      offers      little      to      no      coverage      of      the      topic’s      primary      aspects,      resulting      in   \\n \\na      very      narrow      perspective.\\n \\n \\n  Partial      coverage;      includes      some      of      the      topic’s      main      aspects      but      misses      others,      resulting      in      an      incomplete      portrayal.\\n \\n \\n  Acceptable      breadth;      covers      most      main      aspects,      though      it      may      stray      into      minor      unnecessary      details      or      overlook      some      relevant      points.\\n \\n \\n  Good      coverage;      achieves      broad      coverage      of      the      topic,      hitting      on      all      major      points      with      minimal      extraneous      information.\\n \\n \\n  Exemplary      in      breadth;      delivers      outstanding      coverage,      thoroughly      detailing      all      crucial      aspects      of      the      topic      without      including      irrelevant  \\n \\n  information.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 187, 'chunk_type': 'para'}, 'page_content': 'False      Negative       15%\\nTable      8:      Scoring      rubrics      on   \\n \\na      1-5      scale      for      the      evaluator      LLM.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 188, 'chunk_type': 'table'}, 'page_content': 'Error      Type       Topic      Unsupported      Sentence      Source\\n'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 189, 'chunk_type': 'table'}, 'page_content': 'Error      Type       Topic      Unsupported      Sentence      Source\\n |       Throughout      its      history,      religion      has      remained      the       paramount      aspect      of      Hawaiian      life      in      Lahaina      ,       permeating      every      daily      activity      and      significant      event[5]. |       [5]      “Religion,      Beliefs      &      Spirituality”       (The      source      discusses      religion      as      part      of      Hawaiian      life       but      does      not      mention      Lahania      .)\\n |       Lahaina,      Hawaii\\n |       [2]      “Crimean      Bridge      -      Wikipedia”       (The      source      says      “The      first      scheduled      passenger      train       crossed      the      bridge      on      25      December      2019,      while      the       bridge      was      opened      for      freight      trains      on      30      June      2020      ”.)       Completed      in      June      2020      ,      the      bridge      serves      as      a       major      supply      route      for      Russian      forces      in      the      region       and      is      significant      to      Russia’s      claim      over      the      disputed       territory[2][11].\\n | 2022      Crimean       Bridge      explosion\\n |       For      example,      comparisons      have      been      drawn      between       the      performance      of      LK-9      and      the      dynamic      resolution       capabilities      of      video      games      such      as      Battlefield      2042[22]. |       [22]      “Battlefield      2042      PC      performance      guide:      The      best       settings      for      a      high      frame      rate”       (      The      source      is      irrelevant      to      LK-99.      )\\n |       LK-99\\n'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 190, 'chunk_type': 'para'}, 'page_content': 'Error      Type       Topic      Unsupported      Sentence      Source\\nTable      9:      Examples      of      different      error      types      of      unsupported      sentences.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 191, 'chunk_type': 'para'}, 'page_content': 'Error      Type       Topic      Unsupported      Sentence      Source > We      use      Mistral      7B-Instruct!>      (Jiang      et      al.,      2023a)\\n \\n \\n  to      examine      whether      the      cited      passages      entail      the  \\n \\n  generated      sentence.\\nTable   \\n \\n4      reports      the      citation  \\n \\n  quality      of      articles      produced      by      our      approach,      show-  \\n \\n  ing      that      around      15%      sentences      in      generated      articles  \\n \\n  are      unsupported      by      citations.\\nWe      further      investi-  \\n \\n  gate      the      failure      cases      by      randomly      sampling      10  \\n \\n  articles      and      an      author      manually      examines      all      the  \\n \\n  unsupported      sentences      in      these      articles.\\nBesides  \\n \\n  sentences      that      are      incorrectly      split!®,      lack      citations,  \\n \\n  or      are      deemed      supported      by      the      author’s      judgment,  \\n \\n  our      analysis      identifies      three      main      error      categories  \\n \\n  (examples      are      given      in      Table      9):      improper      inferen-  \\n \\n  tial      linking,      inaccurate      paraphrasing,      and      citing  \\n \\n  irrelevant      sources.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 192, 'chunk_type': 'para'}, 'page_content': 'Error      Type       Topic      Unsupported      Sentence      Source > We      use      Mistral      7B-Instruct!>      (Jiang      et      al.,      2023a)\\n \\n \\n  We      show      the      error      distribution      in      Figure      6.      No-  \\n \\n  tably,      the      most      common      errors      stem      from      the      ten-  \\n \\n  dency      of      LLMs      to      form      improper      inferential      links  \\n \\n  between      different      pieces      of      information      presented  \\n \\n  in      the      context      window.\\nOur      analysis      of      citation  \\n \\n  quality      suggests      that,      in      addition      to      avoiding      hallu-  \\n \\n  cinations,      future      research      in      grounded      text      gener-  \\n \\n  ation      should      also      focus      on      preventing      LLMs      from  \\n \\n  making      overly      inferential      leaps      based      on      the      pro-  \\n \\n  vided      information.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 193, 'chunk_type': 'para'}, 'page_content': \"Error      Type       Topic      Unsupported      Sentence      Source > D      Human      Evaluation      Details\\n \\n \\n  We      recruited      10      experienced      Wikipedia      editors  \\n \\n  to      participate      in      our      study      by      creating   \\n \\na      research  \\n \\n  page      on      Meta-Wiki!”\\nand      reaching      out      to      active editors      who      have      recently      approved      articles      for  \\n \\n  Wikipedia.\\\\\\\\\\\\'®      Our      participation      group      includes   \\n \\n3       editors      with      1-5      years      of      experience,   \\n \\n4      with      6-10  \\n \\n  years,      and   \\n \\n3      with      over      15      years      of      contribution.\\n \\n \\n  The      study      was      approved      by      the      Institutional      Re-  \\n \\n  view      Board      of      our      institution      and      the      participants  \\n \\n  signed      the      consent      form      through      Qualtrics      ques-  \\n \\n  tionnaires      before      the      study      started.\"}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 194, 'chunk_type': 'para'}, 'page_content': 'Error      Type       Topic      Unsupported      Sentence      Source > D      Human      Evaluation      Details\\n \\n \\n  To      streamline      the      evaluation      of      grounded      articles,  \\n \\n  we      developed   \\n \\na      web      application,      which      features   \\n \\na       side-by-side      display      of      the      article      and      its      citation  \\n \\n  snippets,      to      gather      ratings      and      open-ended      feedback  \\n \\n  Shttps      ://huggingface.co/mistralai/  \\n \\n  Mistral-7B-Instruct-vQ.1  \\n \\n  \\\\\\\\\\\\\\'6Rollowing      Gao      et      al.\\n(2023),      we      check      citation      quality      in  \\n \\n  the      sentence      level      and      split      articles      into      sentences      using      NLTK  \\n \\n  sent_tokenize.\\nsent_tokenize      sometimes      fails      to      split      sen-  \\n \\n  tences      correctly      when      the      article      contains      special      words      like  \\n \\n  “No.12847”,      “Bhatia      et      al.\\n”,      etc.\\n \\n \\n  \"https      ://meta.wikimedia.org  \\n \\n  \\\\\\\\\\\\\\'8Since      evaluating      Wikipedia-like      articles      is      time-  \\n \\n  consuming      and      requires      expertise,      we      paid      each      participant  \\n \\n  50$      for      our      study.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 195, 'chunk_type': 'para'}, 'page_content': 'Error      Type       Topic      Unsupported      Sentence      Source > D      Human      Evaluation      Details\\n \\n \\n  for      each      article.\\nFigure   \\n \\n7      shows      the      screenshot      of  \\n \\n  our      web      application      and      the      full      article      produced  \\n \\n  by      STORM      is      included      in      Table      12.\\nFor      human  \\n \\n  evaluation,      we      use   \\n \\na   \\n \\n|      to   \\n \\n7      scale      for      more      fine-  \\n \\n  grained      evaluation.\\nThe      grading      rubric      is      included  \\n \\n  in      Table      10.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 196, 'chunk_type': 'para'}, 'page_content': 'Error      Type       Topic      Unsupported      Sentence      Source > D      Human      Evaluation      Details\\n \\n \\n  We      collected      the      pairwise      preferences      and      the  \\n \\n  perceived      usefulness      of      STORM      via      an      online      ques-  \\n \\n  tionnaire.\\nSpecifically,      for      the      perceived      usefulness,  \\n \\n  we      request      editors      to      rate      their      agreement      with      state-  \\n \\n  ments      “I      think      it      can      be      specifically      helpful      for      my  \\n \\n  pre-writing      stage      (e.g.,      collecting      relevant      sources,  \\n \\n  outlining,      drafting).\\n”,      “I      think      it      will      help      me      edit  \\n \\n  a      Wikipedia      article      for   \\n \\na      new      topic”,      “I      think      it  \\n \\n  can      be   \\n \\na      potentially      useful      tool      for      the      Wikipedia  \\n \\n  community”      on   \\n \\na      Likert      scale      of      1-5,      correspond-  \\n \\n  ing      to      Strongly      disagree,      Somewhat      disagree,      Nei-  \\n \\n  ther      agree      nor      disagree,      Somewhat      agree,      Strongly agree.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 197, 'chunk_type': 'para'}, 'page_content': 'Error      Type       Topic      Unsupported      Sentence      Source > E_      Error      Analysis\\n \\n \\n  While      articles      produced      by      STORM      are      preferred  \\n \\n  by      both      automatic      metrics      and      human      evaluation,  \\n \\n  experienced      editors      still      identified      multiple      prob-  \\n \\n  lems      with      the      machine-generated      articles.\\nWe      an-  \\n \\n  alyze      the      free-form      comments      and      summarize      the  \\n \\n  major      issues      in      Table      11.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 198, 'chunk_type': 'para'}, 'page_content': 'Error      Type       Topic      Unsupported      Sentence      Source > E_      Error      Analysis\\n \\n \\n  The      primary      issue      raised      is      that      the      generated  \\n \\n  articles      often      contain      emotional      language      and      lack  \\n \\n  neutrality,      primarily      due      to      the      source      material.\\n \\n \\n  STORM      currently      retrieves      grounding      sources  \\n \\n  from      the      Internet      which      is      not      neutral      and      con-  \\n \\n  tains      considerable      promotional      content      on      its      own.\\n \\n \\n  Addressing      this      bias      in      the      pre-writing      stage      repre-  \\n \\n  sents   \\n \\na      valuable      direction      for      future      research.\\nAn-  \\n \\n  other      major      issue      is      the      red      herring      fallacy      or      the  \\n \\n  over-association      of      unrelated      facts.\\nAddressing      this  \\n \\n  challenge      calls      for      high-level      sensemaking      rather  \\n \\n  than      mere      fact-level      verification.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 199, 'chunk_type': 'para'}, 'page_content': 'Error      Type       Topic      Unsupported      Sentence      Source > E_      Error      Analysis\\n \\n \\n  Interest      Level  \\n \\n  Not      engaging      at      all;      no      attempt      to      capture      the      reader’s      attention.\\n \\n \\n  Slightly      engaging      with      rare      moments      that      capture      attention.\\n \\n \\n  Fairly      engaging      with   \\n \\na      basic      narrative      but      lacking      depth.\\n \\n \\n  Moderately      engaging      with      several      interesting      points.\\n \\n \\n  Quite      engaging      with   \\n \\na      well-structured      narrative      and      noteworthy      points      that      frequently      capture      and      retain      attention.\\n \\n \\n  Very      engaging      with   \\n \\na      compelling      narrative      that      captures      and      mostly      retains      attention.\\n \\n \\n  Exceptionally      engaging      throughout,      with   \\n \\na      compelling      narrative      that      consistently      stimulates      interest.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 200, 'chunk_type': 'para'}, 'page_content': 'Error      Type       Topic      Unsupported      Sentence      Source > E_      Error      Analysis\\nMOawWPYWNr'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 201, 'chunk_type': 'para'}, 'page_content': 'Error      Type       Topic      Unsupported      Sentence      Source > E_      Error      Analysis\\n \\n \\n  Coherence      and      Organization  \\n \\n  Disorganized;      lacks      logical      structure      and      coherence.\\n \\n \\n  Poor      organization;      some      structure      is      evident      but      very      weak.\\n \\n \\n  Fairly      organized;   \\n \\na      basic      structure      is      present      but      not      consistently      followed.\\n \\n \\n  Organized;   \\n \\na      clear      structure      is      mostly      followed      with      some      lapses      in      coherence.\\n \\n \\n  Good      organization;   \\n \\na      clear      structure      with      minor      lapses      in      coherence.\\n \\n \\n  Very      well-organized;   \\n \\na      logical      structure      with      transitions      that      effectively      guide      the      reader.\\n \\n \\n  Excellently      organized;      the      article      is      logically      structured      with      seamless      transitions      and   \\n \\na      clear      argument.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 202, 'chunk_type': 'para'}, 'page_content': 'Error      Type       Topic      Unsupported      Sentence      Source > E_      Error      Analysis\\naw:'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 203, 'chunk_type': 'para'}, 'page_content': 'Error      Type       Topic      Unsupported      Sentence      Source > E_      Error      Analysis\\n \\n \\n  Relevance      and      Focus  \\n \\n  1:      Off-topic;      the      content      does      not      align      with      the      headline      or      core      subject.\\n \\n \\n  2:      Mostly      off-topic      with      some      relevant      points.\\n \\n \\n  3:      Somewhat      on      topic      but      with      several      digressions;      the      core      subject      is      evident      but      not      consistently      adhered      to.\\n \\n \\n  4:      Generally      on      topic,      despite   \\n \\na      few      unrelated      details.\\n \\n \\n  5:      Mostly      on      topic      and      focused;      the      narrative      has   \\n \\na      consistent      relevance      to      the      core      subject      with      infrequent      digressions.\\n \\n \\n  6:      Highly      relevant      with   \\n \\na      focused      narrative      and      purpose.\\n \\n \\n  7:      Exceptionally      focused      and      entirely      on      topic;      the      article      is      tightly      centered      on      the      subject,      with      every      piece      of      information      contributing      to   \\n \\na       comprehensive      understanding      of      the      topic.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 204, 'chunk_type': 'para'}, 'page_content': 'Error      Type       Topic      Unsupported      Sentence      Source > E_      Error      Analysis\\n \\n \\n  Broad      Coverage  \\n \\n  Severely      lacking;      offers      little      to      no      coverage      of      the      topic’s      primary      aspects,      resulting      in   \\n \\na      very      narrow      perspective.\\n \\n \\n  Minimal      coverage;      addresses      only   \\n \\na      small      selection      of      the      topic’s      main      aspects,      with      significant      omissions.\\n \\n \\n  Partial      coverage;      includes      some      of      the      topic’s      main      aspects      but      misses      others,      resulting      in      an      incomplete      portrayal.\\n \\n \\n  Acceptable      breadth;      covers      most      main      aspects,      though      it      may      stray      into      minor      unnecessary      details      or      overlook      some      relevant      points.\\n \\n \\n  Good      coverage;      achieves      broad      coverage      of      the      topic,      hitting      on      all      major      points      with      minimal      extraneous      information.\\n \\n \\n  Comprehensive;      provides      thorough      coverage      of      all      significant      aspects      of      the      topic,      with   \\n \\na      well-balanced      focus.\\n \\n \\n  Exemplary      in      breadth;      delivers      outstanding      coverage,      thoroughly      detailing      all      crucial      aspects      of      the      topic      without      including      irrelevant      information.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 205, 'chunk_type': 'para'}, 'page_content': 'TMAWP      YN > Verifiability\\n \\n \\n  1:      No      supporting      evidence;      claims      are      unsubstantiated.\\n \\n \\n  2:      Rarely      supported      with      evidence;      many      claims      are      unsubstantiated.\\n \\n \\n  3:      Inconsistently      verified;      some      claims      are      supported;      evidence      is      occasionally      provided.\\n \\n \\n  4:      Generally      verified;      claims      are      usually      supported      with      evidence;      however,      there      might      be   \\n \\na      few      instances      where      verification      is      lacking  \\n \\n  5:      Well-supported;      claims      are      very      well      supported      with      credible      evidence,      and      instances      of      unsupported      claims      are      rare.\\n \\n \\n  6:      Very      well-supported;      almost      every      claim      is      substantiated      with      credible      evidence,      showing   \\n \\na      high      level      of      thorough      verification.\\n \\n \\n  7:      Exemplary      verification;      each      claim      is      supported      by      robust,      credible      evidence      from      authoritative      sources,      reflecting      strict      adherence      to      the      no  \\n \\n  original      research      policy.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 206, 'chunk_type': 'para'}, 'page_content': 'TMAWP      YN > Verifiability\\nTable      10:      Scoring      rubrics      on   \\n \\na      1-7      scale      for      human      evaluation.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 207, 'chunk_type': 'para'}, 'page_content': 'TMAWP      YN > Issue       Mentioned      Time       Example      Comments\\n \\n \\n  The      word      “significant”      is      used      17      times      in      this      article.\\nVague      and      unsupported      claims      are  \\n \\n  made      about      broader      political      importance      and      “pivotal      role[s]”,      and      is      unencyclopedic.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 208, 'chunk_type': 'para'}, 'page_content': 'TMAWP      YN > Issue       Mentioned      Time       Example      Comments\\nUse      of      emotional      words,  \\n \\n  (comment      on      article      Lahaina,      Hawaii) [] but they still have not fixed the issue of neutral point of view.\\nIt is also evident in this  \\n \\n  article      that      the      writer’s      standpoint      is      biased      towards      Taylor      Swift.\\nOther      than      that,      it      did  \\n \\n  a      good      job      at      summarizing      key      points      and      putting      depth      into      this.\\n \\n \\n  unneutral      12      (comment      on      article      Speak      Now      (Taylor’s      Version))  \\n \\n  “The      film      was      also      featured      in      an      art      and      film      festival      hosted      by      The      California      Endowment,  \\n \\n  highlighting      the      power      of      stories      in      reshaping      narratives      about      communities.”\\nYes,      technically  \\n \\n  the      source      says      that,      but      it’s   \\n \\na      stretch      to      say      in      Wikipedia      voice      and      just      sounds      like  \\n \\n  non-neutral,      promotional      prose.\\n(comment      on      article      Gehraiyaan)  \\n \\n  Polling      from      America      shouldn’t      be      included      and      links      to      climate      change      shouldn’t      be  \\n \\n  made      unless      explicitly      connected      by      the      source.\\n(comment      on      article      Typhoon      Hinnamnor)  \\n \\n  Red      herring      fallacy,   \\n \\nu      Sourcing      seems      mostly      fine,      though      some      aren’t      directly      related      (Ex.      39,40).\\n \\n \\n  associating      unrelated      sources      (comment      on      article      Gehraiyaan)  \\n \\n  Here      is   \\n \\na      lengthy      digression      about      KISS,      not      necessary      because      the      article      on      the      band  \\n \\n  should      be      linked      to.\\n(comment      on      article      2022      AFL      Grand      Final)  \\n \\n  “One      study,      conducted      by      Sinéad      Griffin,   \\n \\na      physicist      at      the      Lawrence      Berkeley      National  \\n \\n  Laboratory,      provided      some      analysis      of      LK-99’s      abilities      using      supercomputer      simulations[20].”\\n \\n \\n  This      is      not      enough      information      about      the      analysis,      which      would      have      been      very      useful      in      the  \\n \\n  rr  \\n \\n.    \\n.\\n     article.\\n(comment      on      article      LK-99)  \\n \\n  Missing      important      information   \\n \\n6       Although      the      earthquake’s      immediate      aftermath      and      response      are      adequately      covered,      there  \\n \\n  could      be      more      about      the      long-term      socioeconomic      impact      and      recovery      processes.\\n \\n \\n  (comment      on      article      2022      West      Java      earthquake)  \\n \\n  Words      like      “now”      should      be      avoided      in      Wikipedia      articles      to      prevent      them      from      becoming  \\n \\n  dated      and      phrases      such      as,      “as      of      December      2023”      should      be      used      instead.\\n \\n \\n  Improper      handling      of   \\n \\n5      (comment      on      article      Cyclone      Batsirai)  \\n \\n  time-sensitive      information      “as      of      December      13”      doesn’t      specify   \\n \\na      year,      and      is      old      information  \\n \\n  (comment      on      article      2022      West      Java      earthquake) too      many      subsections      in      the      “Recovery      and      Rehabilitation”      section  \\n \\n  (comment      on      article      2022      West      Java      earthquake)'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 209, 'chunk_type': 'para'}, 'page_content': 'TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\n \\n \\n  Section      organization      problem   \\n \\n5   \\n \\nI      do      not      like      how      the      article      is      organized,      with      too      many      headers      cluttering      the      article,  \\n \\n  making      it      not      as      readable.\\nOther      than      that,      the      AI      did      great      work      on      the      piece.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 210, 'chunk_type': 'table'}, 'page_content': \"TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\n |       Select      a      key       8      v       Selected      Key:      8       Title:      Taylor      Hawkins      -      marshall.com       Snippets:       In      1995,      off      the      back      of      her      hit      album       ‘Jagged      Little      Pill’,      Canadian      American       superstar      Alanis      Morissette      recruited      him       to      be      her      touring      drummer      for      her      18       month      album      tour,      along      with      him       featuring      in      the      music      videos      for      “You       Oughta      Know’,      “All|      Really      Want”      and       “You      Learn’.      It      was      during      this      tour      that      he       met      his      musical      soul      mate,      Dave      Grohl.       The      tour      with      Alanis      ended      and      he\\\\\\\\\\\\'d      heard       that      Dave      and      Foo      Fighters      were      looking       for      anew      drummer,      so      Taylor      enquired.       Dave      initially      thought      that      Taylor      wouldn’t       be      interested      in      joining      as      Alanis       Morissette      was      much      bigger      than      the      Foo       Fighters      at      that      time,      but      Taylor      jumped      at       the      chance      due      to      his      overwhelming       desire      to      be      in      a      rock      band.      It      was      from       that      point      that      the      bromance      between       them      began      and      on      March      18th      1997       Taylor      was      announced      as      their      new       drummer.       Url:      https://marshall.com/live-for-       music/drum-icons/taylor-hawkins | Logout  \\n \\n  Select      an      option:  \\n \\n  (11,      ‘Taylor      Hawkins      [Quite      Richard}’)   \\n \\n¥       You      selected:      Taylor      Hawkins      [Quite      Richard]  \\n \\n  Taylor      Hawkins  \\n \\n  Oliver      Taylor      Hawkins      (1972-2022)      was      an      American      musician,      best      known      as      the      drummer      for      the      rock      band      Foo      Fighters[1].\\nBorn      in      Fort      Walton,      Texas,      Hawkins’      love      for      music      was      ignited      at   \\n \\na      young  \\n \\n  age,      particularly      after      watching   \\n \\na      Queen      concert      in      1982[2][3][5].\\nHe      kick-started      his      professional      career      as      the      drummer      for      Alanis      Morissette’s      band      during      her      world      tour      for      the      hit      album      ‘Jagged      Little  \\n \\n  Pill\\\\\\\\\\\\'[8][9].\\nHis      talents      were      recognized      by      Dave      Grohl,      who      invited      him      to      join      the      Foo      Fighters      in      1997,      marking      the      beginning      of      his      impactful      tenure      with      the      band{7][8].\\n \\n \\n  Hawkins      was      celebrated      for      his      versatile      drumming      style,      drawing      influence      from      renowned      drummers      like      Roger      Taylor,      Neil      Peart,      Phil      Collins,      Alex      Van      Halen,      and      Stewart      Copeland[14).\\nHis  \\n \\n  performances,      marked      by   \\n \\na      unique      energy      and      aggressive      style      of      play,      earned      him      recognition      as      one      of      the      top      rock      drummers      of      his      era[15].\\nApart      from      his      role      in      the      Foo      Fighters,      Hawkins\\\\\\\\\\\\'      passion  \\n \\n  for      music      saw      him      involved      in      numerous      side      projects      and      collaborations,      cementing      his      place      in      the      world      of      rock      music(10].\\n \\n \\n  Outside      of      his      professional      life,      Hawkins      was      known      for      his      resilience      and      dedication      to      his      family.\\nDespite      personal      struggles,      including   \\n \\na      near-fatal      drug      overdose      in      2001,      Hawkins      remained      committed  \\n \\n  to      his      musical      career{4][9].\\nHis      legacy      continues      to      inspire      musicians      and      fans      alike,      as      his      contributions      to      rock      music,      coupled      with      his   \\n \\ni       industry(13}.\\n \\n \\n  spirit,      made      him      an      icon      in      the      music  \\n \\n  Hawkins’      sudden      death      in      2022      while      on      tour      in      Bogotd,      Colombia,      sent      shockwaves      through      the      music      world[34].\\nTributes      poured      in      from      around      the      globe,      reflecting      the      respect      and      admiration  \\n \\n  Hawkins      had      garnered      during      his      lifetime[21][31].\\nHis      life      and      career      were      honored      at   \\n \\na      star-studded      tribute      concert      in      Los      Angeles,      attesting      to      the      enduring      impact      of      his      music[22].\\n | Early      Life      and      Background\\n |       Oliver      Taylor      Hawkins,      known      as      Taylor      Hawkins,      was      born      and      raised      in      Fort      Walton,      Texas[3].      His      family      moved      to      Laguna      Beach,      California      when      he      was      four      years      old[3].      He      has      two      younger      siblings,       a      brother      named      Jason,      and      a      sister      named      Heather[3].      Asa      child,      Hawkins      wa:      influenced      by      his      paternal      Josie      Hawkins,      who      had      grown      up      during      the      Great      Depression      and       lived      in      Jackson,      Mississippi(2].\\n |       During      his      high      school      days      at      Laguna      Beach      High      School,      from      where      he      graduated      in      1990,      he      became      friends      with      Jon      Davison,      who      later      became      the      lead      vocalist      of      the      band      Yes(2][3].      His      interest      in       music      was      nurtured      from      an      early      age,      particularly      after      watching      a      Queen      concert      in      1982      which      inspired      him      to      learn      to      play      the      drums(2][5].      He      noted      that      music      was      a      constant      presence      in      his      family       Taylor      Hawkins\\n\"}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 211, 'chunk_type': 'para'}, 'page_content': 'TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\n \\n \\n  Oliver      Taylor      Hawkins      (1972-2022)      was      an      American      musician,      best      known      as      the      drummer      for      the      rock  \\n \\n  band      Foo      Fighters[1].\\nBorn      in      Fort      Walton,      Texas,      Hawkins’      love      for      music      was      ignited      at   \\n \\na      young      age,  \\n \\n  particularly      after      watching   \\n \\na      Queen      concert      in      1982[2][3][5].\\nHe      kick-started      his      professional      career  \\n \\n  as      the      drummer      for      Alanis      Morissette’s      band      during      her      world      tour      for      the      hit      album      ’Jagged      Little  \\n \\n  Pill’[8]L[9].\\nHis      talents      were      recognized      by      Dave      Grohl,      who      invited      him      to      join      the      Foo      Fighters      in  \\n \\n  1997,      marking      the      beginning      of      his      impactful      tenure      with      the      band[7][8].'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 212, 'chunk_type': 'para'}, 'page_content': 'TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\n \\n \\n  Hawkins      was      celebrated      for      his      versatile      drumming      style,      drawing      influence      from      renowned      drummers      like  \\n \\n  Roger      Taylor,      Neil      Peart,      Phil      Collins,      Alex      Van      Halen,      and      Stewart      Copeland[14].\\nHis      performances,  \\n \\n  marked      by   \\n \\na      unique      energy      and      aggressive      style      of      play,      earned      him      recognition      as      one      of      the      top      rock  \\n \\n  drummers      of      his      era[15].\\nApart      from      his      role      in      the      Foo      Fighters,      Hawkins’      passion      for      music      saw  \\n \\n  him      involved      in      numerous      side      projects      and      collaborations,      cementing      his      place      in      the      world      of      rock  \\n \\n  music[10].'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 213, 'chunk_type': 'para'}, 'page_content': 'TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\n \\n \\n  Outside      of      his      professional      life,      Hawkins      was      known      for      his      resilience      and      dedication      to      his      family.\\n \\n \\n  Despite      personal      struggles,      including   \\n \\na      near-fatal      drug      overdose      in      2001,      Hawkins      remained      committed      to  \\n \\n  his      musical      career[4][9].\\nHis      legacy      continues      to      inspire      musicians      and      fans      alike,      as      his      contributions  \\n \\n  to      rock      music,      coupled      with      his      indomitable      spirit,      made      him      an      unforgettable      icon      in      the      music  \\n \\n  industry[13].'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 214, 'chunk_type': 'para'}, 'page_content': 'TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\n \\n \\n  Hawkins’      sudden      death      in      2022      while      on      tour      in      Bogota,      Colombia,      sent      shockwaves      through      the      music  \\n \\n  world[34].\\nTributes      poured      in      from      around      the      globe,      reflecting      the      respect      and      admiration      Hawkins  \\n \\n  had      garnered      during      his      lifetime[21][31].\\nHis      life      and      career      were      honored      at   \\n \\na      star-studded      tribute  \\n \\n  concert      in      Los      Angeles,      attesting      to      the      enduring      impact      of      his      music[22].'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 215, 'chunk_type': 'para'}, 'page_content': 'TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\n#\\n \\n   Early      Life      and      Background  \\n \\n  Oliver      Taylor      Hawkins,      known      as      Taylor      Hawkins,      was      born      and      raised      in      Fort      Walton,      Texas[3].\\nHis  \\n \\n  family      moved      to      Laguna      Beach,      California      when      he      was      four      years      old[3].\\nHe      has      two      younger      siblings,  \\n \\n  a      brother      named      Jason,      and   \\n \\na      sister      named      Heather[3].\\nAs   \\n \\na      child,      Hawkins      was      particularly      influenced  \\n \\n  by      his      paternal      grandmother,      Josie      Hawkins,      who      had      grown      up      during      the      Great      Depression      and      lived      in  \\n \\n  Jackson,      Mississippi[1].'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 216, 'chunk_type': 'para'}, 'page_content': 'TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\n \\n \\n  During      his      high      school      days      at      Laguna      Beach      High      School,      from      where      he      graduated      in      1990,      he      became  \\n \\n  friends      with      Jon      Davison,      who      later      became      the      lead      vocalist      of      the      band      Yes[2][3].\\nHis      interest      in  \\n \\n  music      was      nurtured      from      an      early      age,      particularly      after      watching   \\n \\na      Queen      concert      in      1982      which      inspired  \\n \\n  him      to      learn      to      play      the      drums[2][5].\\nHe      noted      that      music      was   \\n \\na      constant      presence      in      his      family      home[5].'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 217, 'chunk_type': 'para'}, 'page_content': 'TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\n \\n \\n  Despite      facing      certain      hardships      during      his      upbringing,      including      his      mother’s      struggles      with      \"demons\",  \\n \\n  Hawkins      pursued      his      musical      ambitions[4].\\nHe      credits      his      older      sister      Heather      for      taking      care      of      the  \\n \\n  family      during      difficult      times[4].'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 218, 'chunk_type': 'para'}, 'page_content': 'TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\n \\n \\n  His      first      major      musical      experience      came      from      playing      drums      for      Alanis      Morissette’s      album,      Jagged      Little  \\n \\n  Pill,      and      accompanying      her      on      the      subsequent      tour[3].\\nThis      marked      the      beginning      of      his      professional  \\n \\n  career      in      the      music      industry.'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 219, 'chunk_type': 'para'}, 'page_content': 'TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\n#\\n \\n   Career  \\n \\n  Taylor      Hawkins      began      his      professional      music      career      playing      in      Alanis      Morissette’s      band      during      her  \\n \\n  18-month      world      tour      in      support      of      the      hit      album      ’Jagged      Little      Pill’      from      1995      to      1997[8][9].\\nHis  \\n \\n  performances      not      only      in      the      tour      but      also      in      the      music      videos      for      “You      Oughta      Know”,      “All   \\n \\nI      Really      Want”  \\n \\n  and      “You      Learn”      introduced      him      to      the      world      of      rock      music      and      ultimately      led      to      his      meeting      with      Dave  \\n \\n  Grohl[8].\\nThroughout      this      time,      Hawkins      contributed      significantly      to      the      band’s      sound      and      performance,  \\n \\n  transforming      the      songs      from      their      original      drum      loop      format      to   \\n \\na      rock-band      vibe      that      resonated      with  \\n \\n  audiences[1][7].'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 220, 'chunk_type': 'para'}, 'page_content': 'TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\n \\n \\n  In      1997,      Hawkins      was      asked      by      Grohl      to      join      the      Foo      Fighters,      an      invitation      that      he      readily      accepted[7][8].\\n \\n \\n  At      the      time,      Grohl      thought      it      was   \\n \\na      long      shot      to      recruit      Hawkins      given      that      Morissette      was      at      the      height  \\n \\n  of      her      career,      but      Hawkins’      desire      to      be   \\n \\na      part      of   \\n \\na      rock      band      compelled      him      to      make      the      move[7].\\nThis  \\n \\n  marked      the      beginning      of      Hawkins’      tenure      as      the      drummer      of      the      Foo      Fighters,   \\n \\na      role      that      he      would      play  \\n \\n  until      his      passing[6][9].'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 221, 'chunk_type': 'para'}, 'page_content': 'TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\n \\n \\n  Apart      from      his      work      with      Morissette      and      the      Foo      Fighters,      Hawkins      had      an      array      of      other      musical  \\n \\n  experiences[10].\\nHe      drummed      for      Sass      Jordan      before      joining      Morissette’s      touring      band[10].\\nHe      was      part  \\n \\n  of      an      ad      hoc      drum      supergroup      called      SOS      Allstars      and      filled      the      void      for      Coheed      and      Cambria’s      2007  \\n \\n  album      after      their      drummer      Josh      Eppard      left      the      group[10].\\nIn      addition,      Hawkins      formed      his      own      side  \\n \\n  project,      the      Coattail      Riders,      in      2005,      through      which      he      recorded      his      own      music      and      took      the      project      on  \\n \\n  the      road,      performing      in      small      clubs      despite      the      Foo      Fighters’      arena-status[7].\\nHis      son,      Shane      Hawkins,  \\n \\n  has      since      taken      on      his      father’s      legacy,      joining      the      Foo      Fighters      for   \\n \\na      performance      during      the      Boston  \\n \\n  Calling      Music      Festival      in      2023[6].'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 222, 'chunk_type': 'para'}, 'page_content': 'TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\n#\\n \\n   Musical      Style      and      Influences  \\n \\n  Taylor      Hawkins      was   \\n \\na      profound      drummer,      with      his      musical      style      and      influences      spreading      across   \\n \\na      wide  \\n \\n  array      of      rock      genres[11].\\nKnown      for      his      passionate      fandom      of      groups      that      came      before      him,      Hawkins  \\n \\n  regularly      expressed      his      admiration      for      bands      like      Rush,      Genesis,      and      the      Police,      all      of      which      featured  \\n \\n  some      of      the      greatest      drummers      in      rock      history      like      Neil      Peart,      Phil      Collins,      and      Stewart      Copeland[11].'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 223, 'chunk_type': 'para'}, 'page_content': 'TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\nHe      was      heavily      influenced      by      his      love      for      classic      rock,      as      evidenced      by      his      performances,      where      he  \\n \\n  covered      songs      from      bands      like      Van      Halen[11].'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 224, 'chunk_type': 'para'}, 'page_content': 'TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\n \\n \\n  Hawkins      drew      influences      from   \\n \\na      variety      of      drumming      styles,      developing   \\n \\na      signature      style      inspired      by  \\n \\n  greats      like      Roger      Taylor,      Neil      Peart,      Phil      Collins,      Alex      Van      Halen,      and      Stewart      Copeland[14].\\nThis  \\n \\n  distinctive      style      and      influence      extended      to      his      drum      kit,      which      incorporated      elements      like      rototoms  \\n \\n  and      concert      toms[14].'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 225, 'chunk_type': 'para'}, 'page_content': 'TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\n \\n \\n  Beyond      his      influences,      Hawkins      had   \\n \\na      unique      energy      that      made      him      stand      out      as   \\n \\na      drummer.\\nHis      performances  \\n \\n  were      recognized      for      their      power,      and      he      was      known      for      his      enthusiastic      and      aggressive      style      of      play[15].\\n \\n \\n  This      earned      him      recognition      as      one      of      the      top      rock      drummers      of      his      time,      with      his      passion      for      music  \\n \\n  living      on      through      his      performances[14].'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 226, 'chunk_type': 'para'}, 'page_content': 'TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\n \\n \\n  Through      his      career,      Hawkins      left      an      indelible      mark      on      rock      music,      through      his      distinct      style,      passion,  \\n \\n  and      contributions      to      the      music      industry[13].\\nHis      love      for      music      and      dedication      to      his      craft      made      him  \\n \\n  an      unforgettable      icon      in      the      world      of      rock      music[13].'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 227, 'chunk_type': 'para'}, 'page_content': 'TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\n#\\n \\n   Personal      Life  \\n \\n  Taylor      Hawkins      married      Alison      Hawkins,      an      American      celebrity      and      entrepreneur,      in      2005[18].\\nThe      couple  \\n \\n  had      three      children,      Oliver,      Annabelle,      and      Everleigh[19].\\nHawkins’      commitment      to      his      family      was      evident;'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 228, 'chunk_type': 'para'}, 'page_content': 'TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\n \\n \\n  in      fact,      he      even      wrote   \\n \\na      song      for      his      middle      child,      Annabelle[9].'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 229, 'chunk_type': 'para'}, 'page_content': 'TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\n \\n \\n  In      his      personal      life,      Hawkins      had      also      struggled      with      drug      use,      which      nearly      claimed      his      life      in   \\n \\na      2001  \\n \\n  overdose[9][7][4].\\nHowever,      he      managed      to      overcome      this      challenge,      and      later      expressed      gratitude      for  \\n \\n  the      experience      as   \\n \\na      lesson      that      allowed      him      to      realize      the      destructive      path      he      was      on[7].'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 230, 'chunk_type': 'para'}, 'page_content': 'TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\n \\n \\n  Outside      of      his      main      role      in      the      Foo      Fighters,      Hawkins      also      pursued      various      side      projects      including      the  \\n \\n  Birds      of      Satan,      NHC,      and      Chevy      Metal.\\nHis      motivation      for      such      ventures      was   \\n \\na      constant      drive      to      create  \\n \\n  and      his      love      for      music[7].\\nHawkins      was      also      known      for      his      unabashed      fanboy      nature,      often      vocalizing  \\n \\n  his      admiration      for      fellow      musicians      and      his      heroes[7].'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 231, 'chunk_type': 'para'}, 'page_content': 'TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\n#\\n \\n   Legacy      and      Impact Taylor      Hawkins      was      known      for      his      raw      and      authentic      drumming      style,      described      as      \"courageous,      damaged  \\n \\n  and      unflinchingly      authentic”[20].\\nHis      work      with      the      Foo      Fighters,      as      well      as      his      various      collaborations n’      roll[10].\\n \\n \\n  ‘ and      side      projects,      made      him   \\n \\na      celebrated      figure      in      rock  \\n \\n  Hawkins’      death      in      2022      was      met      with      heartfelt      tributes      from      colleagues      and      fans      around      the      world.\\n \\n \\n  Notable      tributes      came      from      rock      legends      like      Roger      Taylor      of      Queen,      who      considered      Hawkins      as   \\n \\na      kind,  \\n \\n  brilliant      man      and      an      inspirational      mentor,      likening      his      death      to      \"losing   \\n \\na      younger      favourite      brother”[21].\\n \\n \\n  Similarly,      Led      Zeppelin’s      Jimmy      Page      admired      his      technique,      energy      and      spirited      enthusiasm[21].'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 232, 'chunk_type': 'para'}, 'page_content': 'TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\n \\n \\n  An      LA      tribute      concert      held      in      his      honor      included      guest      drummers      like      Lars      Ulrich      of      Metallica,      Travis  \\n \\n  Barker      of      blink-182,      and      Brad      Wilk      of      Rage      Against      the      Machine.\\nSingers      like      Miley      Cyrus      and      Alanis  \\n \\n  Morissette      also      performed      at      the      concert[22].'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 233, 'chunk_type': 'para'}, 'page_content': 'TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\n \\n \\n  Apart      from      his      music,      Taylor      Hawkins      also      contributed      to      charities      Music      Support      and      MusiCares,      both      of  \\n \\n  which      were      chosen      by      the      Hawkins      family[23].\\nHe      had      received      numerous      accolades      throughout      his      career,  \\n \\n  including      27      Grammy      nominations,      of      which      he      won      14[2].\\nIn      2021,      the      Foo      Fighters      were      inducted      into  \\n \\n  the      Rock      and      Roll      Hall      of      Fame[9].'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 234, 'chunk_type': 'para'}, 'page_content': 'TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\n#\\n \\n   Discography  \\n \\n  Taylor      Hawkins      also      led   \\n \\na      notable      music      career      through      his      own      side      projects      and      collaborations[10].\\n \\n \\n  Aside      from      his      work      with      the      Foo      Fighters,      Hawkins      formed      and      fronted      the      band      Taylor      Hawkins   \\n \\n&      The  \\n \\n  Coattail      Riders,   \\n \\na      project      which      originated      from      jamming      sessions      with      his      friend      Drew      Hester[10].'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 235, 'chunk_type': 'para'}, 'page_content': 'TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\n###      Taylor      Hawkins   \\n \\n&      The      Coattail      Riders  \\n \\n  Taylor      Hawkins   \\n \\n&      The      Coattail      Riders,   \\n \\na      band      formed      in      2004,      have      released      three      albums      and      their  \\n \\n  music      spans      genres      including      Hard      Rock,      Art      Rock,      and      Alternative      Rock[24][25][26].\\nThe      band      grew      from  \\n \\n  an      initial      casual      jamming      session,      gradually      evolving      into   \\n \\na      more      formal      arrangement      that      led      to      the  \\n \\n  production      of      record      albums.\\nNotably,      these      albums      featured      guest      appearances      by      renowned      musicians  \\n \\n  such      as      Dave      Grohl,      Queen’s      Brian      May      and      Roger      Taylor,      The      Cars’      Elliot      Easton,      Perry      Farrell,      and       Jon      Davison,      who      is   \\n \\na      school      friend      of      Hawkins’[10].'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 236, 'chunk_type': 'para'}, 'page_content': 'TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\n###      Red      Light      Fever  \\n \\n  Red      Light      Fever,      released      on      April      19,      2010,      was      the      band’s      first      album[29][30].\\nPrior      to      its      release,  \\n \\n  Hawkins      revealed      in      an      interview      that      the      album      had      completed      the      recording      and      production      stages,      but  \\n \\n  its      title      and      release      date      were      yet      to      be      determined[29].\\nRed      Light      Fever      was      recorded      at      the      Foo  \\n \\n  Fighters’      Studio      606      in      California      and      featured      guest      musicians      such      as      Brian      May      and      Roger      Taylor      of  \\n \\n  Queen,      Dave      Grohl      of      Foo      Fighters,      and      Elliot      Easton      of      The      Cars[29][30].'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 237, 'chunk_type': 'para'}, 'page_content': 'TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\n##      Get      the      Money  \\n \\n  Get      the      Money,      the      third      album      from      Taylor      Hawkins   \\n \\n&      The      Coattail      Riders,      was      released      on      November      8,  \\n \\n  2019[29].\\nThe      album’s      first      single,      \"Crossed      the      Line”,      released      on      October      15,      2019,      featured      Dave  \\n \\n  Grohl      and      Jon      Davison,      the      frontman      of      Yes[29].\\nThe      music      video      for      the      single      \"I      Really      Blew      It”      also  \\n \\n  featured      appearances      from      Grohl      and      Perry      Farrel1[29].'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 238, 'chunk_type': 'para'}, 'page_content': 'TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\n#\\n \\n   Collaborations      and      Guest      Appearances  \\n \\n  Throughout      his      career,      Taylor      Hawkins      collaborated      with      various      prominent      artists      and      bands.\\nThe  \\n \\n  Coattail      Riders’      albums      notably      featured      appearances      from      luminaries      such      as      Brian      May      and      Roger      Taylor  \\n \\n  of      Queen,      Chrissie      Hynde,      Nancy      Wilson      of      Heart,      Sex      Pistol      Steve      Jones      and      James      Gang’s      Joe      Walsh[28].'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 239, 'chunk_type': 'para'}, 'page_content': 'TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\n \\n \\n  Hawkins      also      fronted      another      group,      The      Birds      of      Satan,      which      evolved      from      his      heavy      rock      covers      band,  \\n \\n  Chevy      Metal[28].'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 240, 'chunk_type': 'para'}, 'page_content': 'TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\nDespite      his      diverse      musical      engagements,      Hawkins      always      maintained   \\n \\na      close      allegiance      with      the      Foo  \\n \\n  Fighters,      which      remained      the      center      of      his      music      life[7][28].'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 241, 'chunk_type': 'para'}, 'page_content': 'TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\n#\\n \\n   Tragic      Passing  \\n \\n  Taylor      Hawkins,      the      esteemed      drummer      of      the      alt-rock      band      Foo      Fighters,      passed      away      suddenly      on      March  \\n \\n  25,      2022,      while      on      tour      with      his      band      in      Bogota,      Colombia[34].\\nThe      official      cause      of      death      was      cardiac  \\n \\n  arrest,      though      inquiries      were      raised      concerning      the      presence      of      drugs      in      his      system      and      their      potential  \\n \\n  contribution      to      his      death[33][34].\\nOn      the      night      of      his      passing,      paramedics      were      called      to      the      Four  \\n \\n  Seasons      hotel      in      Bogota      due      to      reports      of      chest      pain      from      an      unnamed      guest,      later      revealed      to      be  \\n \\n  Hawkins[34].\\nUnfortunately,      resuscitation      efforts      were      unsuccessful,      and      Hawkins      was      declared      dead      at  \\n \\n  the      scene[34].'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 242, 'chunk_type': 'para'}, 'page_content': 'TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\n \\n \\n  The      news      of      Hawkins’      sudden      demise      was      announced      on      the      morning      of      March      25th,      2022,      which      left      the      music  \\n \\n  world      in      shock[32].\\nThe      band      confirmed      the      news      with   \\n \\na      short      statement,      expressing      their      devastation  \\n \\n  at      the      loss      of      Hawkins,      whose      \"musical      spirit      and      infectious      laughter”      would      live      on      forever[32].'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 243, 'chunk_type': 'para'}, 'page_content': 'TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\n \\n \\n  As   \\n \\na      result      of      Hawkins’      untimely      passing,      the      band      canceled      their      ongoing      South      American      tour[33].\\nThe  \\n \\n  festival      stage      at      the      Estéreo      Picnic      Festival,      where      the      Foo      Fighters      were      scheduled      to      perform      that  \\n \\n  night,      was      transformed      into   \\n \\na      candlelight      vigil      in      memory      of      Hawkins[33].'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 244, 'chunk_type': 'para'}, 'page_content': 'TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\n##      Tributes      and      Remembrances  \\n \\n  In      the      wake      of      Hawkins’      death,      tributes      from      fans      and      colleagues      alike      poured      in      from      around      the  \\n \\n  world[21][31].\\nAmong      the      many      paying      their      respects      were      legendary      rock      and      roll      musicians      like      Roger  \\n \\n  Taylor,      the      drummer      of      Queen,      who      Hawkins      credited      with      inspiring      his      own      career      behind      the      drum      set[21].\\n \\n \\n  In      heartfelt      social      media      posts,      Taylor      described      Hawkins      as      an      \"inspirational      mentor”      and   \\n \\na      \"kind  \\n \\n  brilliant      man\"[21],      while      Led      Zeppelin’s      Jimmy      Page      reminisced      about      sharing      the      stage      with      Hawkins  \\n \\n  and      praised      his      \"technique,      energy      and      spirited      enthusiasm\"[21].'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 245, 'chunk_type': 'para'}, 'page_content': 'TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\n \\n \\n  There      were      also      numerous      onstage      tributes      to      Hawkins.\\nNotably,      Miley      Cyrus      expressed      her      grief      and      sent  \\n \\n  peaceful      wishes      to      the      Foo      Fighters      and      the      Hawkins      family      during   \\n \\na      performance      at      Lollapalooza[31].'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 246, 'chunk_type': 'para'}, 'page_content': 'TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\n \\n \\n  Similarly,      Liam      Gallagher      of      Oasis      dedicated      one      of      the      band’s      biggest      hits      to      Hawkins      during   \\n \\na      concert  \\n \\n  at      the      Royal      Albert      Hall      in      London[31].'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 247, 'chunk_type': 'para'}, 'page_content': 'TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\nFans      gathered      outside      the      hotel      where      Hawkins      died,      lighting      candles,      leaving      flowers,      and      singing      the  \\n \\n  band’s      songs      in      his      honor[31].'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 248, 'chunk_type': 'para'}, 'page_content': 'TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\n \\n \\n  Hawkins’      life      and      career      were      celebrated      in   \\n \\na      star-studded      tribute      concert      in      Los      Angeles,      which      saw  \\n \\n  performances      from      over      5@      musicians,      including      his      former      bands      and      colleagues      from      Def      Leppard,      Queen,  \\n \\n  and      Foo      Fighters[22].'}, {'metadata': {'source': 'https://arxiv.org/pdf/2402.14207.pdf', 'chunk_number': 249, 'chunk_type': 'para'}, 'page_content': 'TMAWP      YN > Issue       Mentioned      Time       Example      Comments >       (comment      on      article      2022      Crimean      Bridge      explosion)\\nTable      12:      STORM’s      generated      article      for      “Taylor      Hawkins”.\\n“#’,      “##”      indicate      the      section      title      and      subsection      title  \\n \\n  respectively.\\nNumbers      in      brackets      indicate      the      cited      references.'}]\n"
     ]
    }
   ],
   "source": [
    "class Document:\n",
    "    def __init__(self, metadata, page_content):\n",
    "        self.metadata = metadata\n",
    "        self.page_content = page_content\n",
    "\n",
    "# List of Document objects\n",
    "documents =chunks\n",
    "\n",
    "# Convert to list of dictionaries\n",
    "documents_dicts = []\n",
    "for doc in documents:\n",
    "    doc_dict = {\n",
    "        'metadata': doc.metadata,\n",
    "        'page_content': doc.page_content\n",
    "    }\n",
    "    documents_dicts.append(doc_dict)\n",
    "\n",
    "# Print the result\n",
    "print(documents_dicts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Abstract\\n \\n \\n  For      evaluation,      we      curate      FreshWiki,   \\n \\na      dataset  \\n \\n  of      recent      high-quality      Wikipedia      articles,      and       formulate      outline      assessments      to      evaluate      the  \\n \\n  pre-writing      stage.\\nWe      further      gather      feedback  \\n \\n  from      experienced      Wikipedia      editors.\\nCom-  \\n \\n  pared      to      articles      generated      by      an      outline-  \\n \\n  driven      retrieval-augmented      baseline,      more      of  \\n \\n  STORM’;      articles      are      deemed      to      be      organized  \\n \\n  (by   \\n \\na      25%      absolute      increase)      and      broad      in      cov-  \\n \\n  erage      (by      10%).\\nThe      expert      feedback      also  \\n \\n  helps      identify      new      challenges      for      generating  \\n \\n  grounded      long      articles,      such      as      source      bias  \\n \\n  transfer      and      over-association      of      unrelated      facts.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_dicts[1]['page_content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
