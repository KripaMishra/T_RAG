{
  "id": 1,
  "title": "Textbook Title",
  "type": "book",
  "children": [
    {
      "id": 2,
      "title": "2.1      Architecture",
      "type": "section",
      "children": [],
      "content": "2.1      Architecture\nture  \n \n  BART      uses      the      standard      sequence-to-sequence      Trans-  \n \n  former      architecture      from      (Vaswani      et      al.,      2017),      ex-  \n \n  cept,      following      GPT,      that      we      modify      ReLU      activa-  \n \n  tion      functions      to      GeLUs      (Hendrycks   \n \n&      Gimpel,      2016)  \n \n  and      initialise      parameters      from      A/(0,0.02).\nFor      our  \n \n  base      model,      we      use   \n \n6      layers      in      the      encoder      and      de-  \n \n  coder,      and      for      our      large      model      we      use      12      layers      in  \n \n  each.\nThe      architecture      is      closely      related      to      that      used      in  \n \n  BERT,      with      the      following      differences:      (1)      each      layer      of  \n \n  the      decoder      additionally      performs      cross-attention      over  \n \n  the      final      hidden      layer      of      the      encoder      (as      in      the      trans-  \n \n  former      sequence-to-sequence      model);      and      (2)      BERT  \n \n  uses      an      additional      feed-forward      network      before      word-  \n \n  prediction,      which      BART      does      not.\nIn      total,      BART      con-  \n \n  tains      roughly      10%      more      parameters      than      the      equiva-  \n \n  lently      sized      BERT      model."
    },
    {
      "id": 3,
      "title": "2.2      Pre-training      BART",
      "type": "section",
      "children": [],
      "content": "2.2      Pre-training      BART\nBART  \n \n  BART      is      trained      by      corrupting      documents      and      then      op-  \n \n  timizing   \n \na      reconstruction      loss—the      cross-entropy      be-  \n \n  tween      the      decoder’s      output      and      the      original      document.\n \n \n  Unlike      existing      denoising      autoencoders,      which      are      tai-  \n \n  lored      to      specific      noising      schemes,      BART      allows      us      to  \n \n  apply      any      type      of      document      corruption.\nIn      the      extreme  \n \n  case,      where      all      information      about      the      source      is      lost,  \n \n  BART      is      equivalent      to   \n \na      language      model.\n \n \n  We      experiment      with      several      previously      proposed      and  \n \n  novel      transformations,      but      we      believe      there      is   \n \na      sig-  \n \n  nificant      potential      for      development      of      other      new      alter-  \n \n  natives.\nThe      transformations      we      used      are      summarized  \n \n  below,      and      examples      are      shown      in      Figure      2.\n \n \n  Token      Masking      Following      BERT      (Devlin      et      al.,  \n \n  2019),      random      tokens      are      sampled      and      replaced      with  \n \n  [MASK]      elements.\n \n \n  Token      Deletion      Random      tokens      are      deleted      from      the  \n \n  input.\nIn      contrast      to      token      masking,      the      model      must  \n \n  decide      which      positions      are      missing      inputs.\nToken      Masking"
    },
    {
      "id": 4,
      "title": "Token      Masking",
      "type": "section",
      "children": [],
      "content": "Token      Masking"
    },
    {
      "id": 5,
      "title": "C.DE.AB",
      "type": "section",
      "children": [],
      "content": "C.DE.AB\nToken      Deletion Text      Infilling\n \n \n  Text      Infilling   \n \nA      number      of      text      spans      are      sampled,  \n \n  with      span      lengths      drawn      from   \n \na      Poisson      distribution  \n \n  (A   \n \n=      3).\nEach      span      is      replaced      with   \n \na      single      [MASK]  \n \n  token.\nO-length      spans      correspond      to      the      insertion      of  \n \n  [MASK]      tokens.\nText      infilling      is      inspired      by      Span-  \n \n  BERT      (Joshi      et      al.,      2019),      but      SpanBERT      samples  \n \n  span      lengths      from   \n \na      different      (clamped      geometric)      dis-  \n \n  tribution,      and      replaces      each      span      with   \n \na      sequence      of  \n \n  [MASK]      tokens      of      exactly      the      same      length.\nText      infill-  \n \n  ing      teaches      the      model      to      predict      how      many      tokens      are  \n \n  missing      from   \n \na      span.\n \n \n  Sentence      Permutation   \n \nA      document      is      divided      into  \n \n  sentences      based      on      full      stops,      and      these      sentences      are  \n \n  shuffled      in   \n \na      random      order.\n \n \n  Document      Rotation   \n \nA      token      is      chosen      uniformly      at  \n \n  random,      and      the      document      is      rotated      so      that      it      begins  \n \n  with      that      token.\nThis      task      trains      the      model      to      identify  \n \n  the      start      of      the      document.\n3\n \n   Fine-tuning      BART The      representations      produced      by      BART      can      be      used      in  \n \n  several      ways      for      downstream      applications.\n3.1      Sequence      Classification      Tasks\nasks  \n \n  For      sequence      classification      tasks,      the      same      input      is      fed  \n \n  into      the      encoder      and      decoder,      and      the      final      hidden      state  \n \n  of      the      final      decoder      token      is      fed      into      new      multi-class  \n \n  linear      classifier.\nThis      approach      is      related      to      the      CLS  \n \n  token      in      BERT;      however      we      add      the      additional      token  \n \n  to      the      end      so      that      representation      for      the      token      in      the  \n \n  decoder      can      attend      to      decoder      states      from      the      complete  \n \n  input      (Figure      3a).\n3.2.      Token      Classification      Tasks\nasks  \n \n  For      token      classification      tasks,      such      as      answer      endpoint  \n \n  classification      for      SQUAD,      we      feed      the      complete      doc-  \n \n  ument      into      the      encoder      and      decoder,      and      use      the      top  \n \n  hidden      state      of      the      decoder      as   \n \na      representation      for      each  \n \n  word.\nThis      representation      is      used      to      classify      the      token.\n3.3      Sequence      Generation      Tasks\nasks  \n \n  Because      BART      has      an      autoregressive      decoder,      it      can      be  \n \n  directly      fine      tuned      for      sequence      generation      tasks      such  \n \n  as      abstractive      question      answering      and      summarization.\n \n \n  In      both      of      these      tasks,      information      is      copied      from      the  \n \n  input      but      manipulated,      which      is      closely      related      to      the  \n \n  denoising      pre-training      objective.\nHere,      the      encoder      in-  \n \n  put      is      the      input      sequence,      and      the      decoder      generates  \n \n  outputs      autoregressively.\n3.4      Machine      Translation\ntion  \n \n  We      also      explore      using      BART      to      improve      machine      trans-  \n \n  lation      decoders      for      translating      into      English.\nPrevious  \n \n  work      Edunov      et      al.\n(2019)      has      shown      that      models      can  \n \n  be      improved      by      incorporating      pre-trained      encoders,      but  \n \n  gains      from      using      pre-trained      language      models      in      de-  \n \n  coders      have      been      limited.\nWe      show      that      it      is      possible  \n \n  to      use      the      entire      BART      model      (both      encoder      and      de-  \n \n  coder)      as   \n \na      single      pretrained      decoder      for      machine      trans-  \n \n  lation,      by      adding   \n \na      new      set      of      encoder      parameters      that  \n \n  are      learned      from      bitext      (see      Figure      3b).\n \n \n  More      precisely,      we      replace      BART’s      encoder      embed-  \n \n  ding      layer      with   \n \na      new      randomly      initialized      encoder.\n \n \n  The      model      is      trained      end-to-end,      which      trains      the      new       encoder      to      map      foreign      words      into      an      input      that      BART  \n \n  can      de-noise      to      English.\nThe      new      encoder      can      use   \n \na       separate      vocabulary      from      the      original      BART      model.\n \n \n  We      train      the      source      encoder      in      two      steps,      in      both  \n \n  cases      backpropagating      the      cross-entropy      loss      from      the  \n \n  output      of      the      BART      model.\nIn      the      first      step,      we      freeze  \n \n  most      of      BART      parameters      and      only      update      the      ran-  \n \n  domly      initialized      source      encoder,      the      BART      positional  \n \n  embeddings,      and      the      self-attention      input      projection      ma-  \n \n  trix      of      BART’s      encoder      first      layer.\nIn      the      second      step,  \n \n  we      train      all      model      parameters      for   \n \na      small      number      of  \n \n  iterations.\n4\n \n   Comparing      Pre-training      Objectives  \n \n  BART      supports   \n \na      much      wider      range      of      noising      schemes  \n \n  during      pre-training      than      previous      work.\nWe      compare   \n \na       range      of      options      using      base-size      models      (6      encoder      and  \n \n  6      decoder      layers,      with   \n \na      hidden      size      of      768),      evaluated  \n \n  on   \n \na      representative      subset      of      the      tasks      we      will      consider  \n \n  for      the      full      large      scale      experiments      in      85.\n4.1      Comparison      Objectives\nives  \n \n  While      many      pre-training      objectives      have      been      pro-  \n \n  posed,      fair      comparisons      between      these      have      been      dif-  \n \n  ficult      to      perform,      at      least      in      part      due      to      differences      in  \n \n  training      data,      training      resources,      architectural      differ-  \n \n  ences      between      models,      and      fine-tuning      procedures.\nWe  \n \n  ABCDE  \n \n  weee  \n \n  Pre-trained      Pre-trained  \n \n  Craters      Joo      rezapes cise)      Te      TrET Pre-trained  \n \n  Encoder Pre-trained  \n \n  Decoder aa      AB  \n \n  trrrt  \n \n  tr      ttt  \n \n  <s>A      BCD  \n \n  Randomly  \n \n  Initialized      Encoder  \n \n  TFT  \n \n  aBpByoe sentation      from      the      final      output      is      used.\n \n \n  re-implement      strong      pre-training      approaches      recently  \n \n  proposed      for      discriminative      and      generation      tasks.\nWe  \n \n  aim,      as      much      as      possible,      to      control      for      differences      un-  \n \n  related      to      the      pre-training      objective.\nHowever,      we      do  \n \n  make      minor      changes      to      the      learning      rate      and      usage      of  \n \n  layer      normalisation      in      order      to      improve      performance  \n \n  (tuning      these      separately      for      each      objective).\nFor      refer-  \n \n  ence,      we      compare      our      implementations      with      published  \n \n  numbers      from      BERT,      which      was      also      trained      for      1M  \n \n  steps      on   \n \na      combination      of      books      and      Wikipedia      data.\n \n \n  We      compare      the      following      approaches:\n \n \n  Language      Model      Similarly      to      GPT      (Radford      et      al.,  \n \n  2018),      we      train   \n \na      left-to-right      Transformer      language  \n \n  model.\nThis      model      is      equivalent      to      the      BART      decoder,  \n \n  without      cross-attention.\n \n \n  Permuted      Language      Model      Based      on      XLNet      (Yang  \n \n  et      al.,      2019),      we      sample      1/6      of      the      tokens,      and      gener-  \n \n  ate      them      in   \n \na      random      order      autoregressively.\nFor      con-  \n \n  sistency      with      other      models,      we      do      not      implement      the  \n \n  relative      positional      embeddings      or      attention      across      seg-  \n \n  ments      from      XLNet.\n \n \n  Masked      Language      Model      Following      BERT      (Devlin  \n \n  et      al.,      2019),      we      replace      15%      of      tokens      with      [MASK]  \n \n  symbols,      and      train      the      model      to      independently      predict  \n \n  the      original      tokens.\n \n \n  Multitask      Masked      Language      Model      As      in      UniLM  \n \n  (Dong      et      al.,      2019),      we      train   \n \na      Masked      Language  \n \n  Model      with      additional      self-attention      masks.\nSelf      at-  \n \n  tention      masks      are      chosen      randomly      in      with      the      follow  \n \n  proportions:      1/6      left-to-right,      1/6      right-to-left,      1/3      un-  \n \n  masked,      and      1/3      with      the      first      50%      of      tokens      unmasked  \n \n  and   \n \na      left-to-right      mask      for      the      remainder.\n \n \n  Masked      Seq-to-Seq      Inspired      by      MASS      (Song      et      al.,  \n \n  2019),      we      mask   \n \na      span      containing      50%      of      tokens,  \n \n  and      train   \n \na      sequence      to      sequence      model      to      predict      the  \n \n  masked      tokens.\n \n \n  For      the      Permuted      LM,      Masked      LM      and      Multitask  \n \n  Masked      LM,      we      use      two-stream      attention      (Yang      et      al.,  \n \n  2019)      to      efficiently      compute      likelihoods      of      the      output  \n \n  part      of      the      sequence      (using   \n \na      diagonal      self-attention  \n \n  mask      on      the      output      to      predict      words      left-to-right).\n \n \n  We      experiment      with      (1)      treating      the      task      as   \n \na      stan-  \n \n  dard      sequence-to-sequence      problem,      where      the      source  \n \n  input      to      the      encoder      and      the      target      is      the      decoder      out-  \n \n  put,      or      (2)      adding      the      source      as      prefix      to      the      target      in  \n \n  the      decoder,      with   \n \na      loss      only      on      the      target      part      of      the  \n \n  sequence.\nWe      find      the      former      works      better      for      BART  \n \n  models,      and      the      latter      for      other      models.\n \n \n  To      most      directly      compare      our      models      on      their      ability  \n \n  to      model      their      fine-tuning      objective      (the      log      likelihood  \n \n  of      the      human      text),      we      report      perplexity      in      Table      1.\n4.2      Tasks\nasks  \n \n  SQuAD  \n \n  (Rajpurkar      et      al.,      2016)a      an      extractive      ques-  \n \n  tion      answering      task      on      Wikipedia      paragraphs.\nAnswers  \n \n  are      text      spans      extracted      from   \n \na      given      document      context.\n \n \n  Similar      to      BERT      (Devlin      et      al.,      2019),      we      use      concate-  \n \n  nated      question      and      context      as      input      to      the      encoder      of  \n \n  BART,      and      additionally      pass      them      to      the      decoder.\nThe  \n \n  model      includes      classifiers      to      predict      the      start      and      end  \n \n  indices      of      each      token.\n \n \n  MNLI      (Williams      et      al.,      2017),   \n \na      bitext      classification  \n \n  task      to      predict      whether      one      sentence      entails      another.\n \n \n  The      fine-tuned      model      concatenates      the      two      sentences  \n \n  with      appended      an      EOS      token,      and      passes      them      to      both  \n \n  the      BART      encoder      and      decoder.\nIn      contrast      to      BERT,  \n \n  the      representation      of      the      EOS      token      is      used      to      classify  \n \n  the      sentences      relations.\n \n \n  ELIS      (Fanetal.,      2019),   \n \na      long-form      abstractive      ques-  \n \n  tion      answering      dataset.\nModels      generate      answers      con-  \n \n  ditioned      on      the      concatenation      of   \n \na      question      and      sup-  \n \n  porting      documents.\nXSum_      (Narayan      et      al.,      2018),   \n \na      news      summarization  \n \n  dataset      with      highly      abstractive      summaries.\nConvAI2_      (Dinan      et      al.,      2019),   \n \na      dialogue      response  \n \n  generation      task,      conditioned      on      context      and   \n \na      persona.\n \n \n  CNN/DM_      (Hermann      et      al.,      2015),   \n \na      news      summa-  \n \n  rization      dataset.\nSummaries      here      are      typically      closely  \n \n  related      to      source      sentences.\n4.3      Results\nults Results      are      shown      in      Table      1.\nSeveral      trends      are      clear:\nModel      SQuAD      1.1      MNLI      ELIS      XSum   \n \n_      ConvAI2      CNN/DM  \n \n  Fl      Acc      PPL      PPL      PPL      PPL BERT      Base      (Devlin      et      al.,      2019)      88.5      84.3   \n \n-   \n \n-   \n \n-   \n \n-       Masked      Language      Model      90.0      83.5      24.77      7.87      12.59      7.06  \n \n  Masked      Seq2seq      87.0      82.1      23.40      6.80      11.43      6.19  \n \n  Language      Model      76.7      80.1      21.40      7.00      11.51      6.56  \n \n  Permuted      Language      Model      89.1      83.7      24.03      7.69      12.23      6.96  \n \n  Multitask      Masked      Language      Model      89.2      82.4      23.73      7.50      12.39      6.74  \n \n  BART      Base  \n \n  w/      Token      Masking      90.4      84.1      25.05      7.08      11.73      6.10  \n \n  w/      Token      Deletion      90.4      84.1      24.61      6.90      11.46      5.87  \n \n  w/      Text      Infilling      90.8      84.0      24.26      6.61      11.05      5.83  \n \n  w/      Document      Rotation      77.2      75.3      53.69      17.14      19.87      10.59  \n \n  w/      Sentence      Shuffling      85.4      81.5      41.87      10.93      16.67      7.89  \n \n  w/      Text      Infilling   \n \n+      Sentence      Shuffling      90.8      83.8      24.17      6.62      11.12      5.41\nconsistently      strong      performance.\n \n \n  Performance      of      pre-training      methods      varies      signifi-  \n \n  cantly      across      tasks      The      effectiveness      of      pre-training  \n \n  methods      is      highly      dependent      on      the      task.\nFor      exam-  \n \n  ple,   \n \na      simple      language      model      achieves      the      best      ELIS  \n \n  performance,      but      the      worst      SQUAD      results.\n \n \n  Token      masking      is      crucial      Pre-training      objectives  \n \n  based      on      rotating      documents      or      permuting      sentences  \n \n  perform      poorly      in      isolation.\nThe      successful      methods  \n \n  either      use      token      deletion      or      masking,      or      self-attention  \n \n  masks.\nDeletion      appears      to      outperform      masking      on  \n \n  generation      tasks.\n \n \n  Left-to-right      pre-training      improves      generation  \n \n  The      Masked      Language      Model      and      the      Permuted  \n \n  Language      Model      perform      less      well      than      others      on  \n \n  generation,      and      are      the      only      models      we      consider      that  \n \n  do      not      include      left-to-right      auto-regressive      language  \n \n  modelling      during      pre-training.\n \n \n  Bidirectional      encoders      are      crucial      for      SQUAD      As  \n \n  noted      in      previous      work      (Devlin      et      al.,      2019),      just  \n \n  left-to-right      decoder      performs      poorly      on      SQuAD,      be-  \n \n  cause      future      context      is      crucial      in      classification      deci-  \n \n  sions.\nHowever,      BART      achieves      similar      performance  \n \n  with      only      half      the      number      of      bidirectional      layers.\n \n \n  The      pre-training      objective      is      not      the      only      important  \n \n  factor      Our      Permuted      Language      Model      performs      less  \n \n  well      than      XLNet      (Yang      et      al.,      2019).\nSome      of      this      dif-  \n \n  ference      is      likely      due      to      not      including      other      architectural  \n \n  improvements,      such      as      relative-position      embeddings      or  \n \n  segment-level      recurrence.\n \n \n  Pure      language      models      perform      best      on      ELIS      The  \n \n  ELIS      dataset      is      an      outlier,      with      much      higher      perplex-  \n \n  ities      than      other      tasks,      and      is      the      only      generation      task  \n \n  where      other      models      outperform      BART.\n \n \nA      pure      lan-  \n \n  guage      model      performs      best,      suggesting      that      BART      is  \n \n  less      effective      when      the      output      is      only      loosely      con-  \n \n  strained      by      the      input.\n \n \n  BART      achieves      the      most      consistently      strong      perfor-  \n \n  mance.\nWith      the      exception      of      ELI5,      BART      models  \n \n  using      text-infilling      perform      well      on      all      tasks.\n5\n \n   Large-scale      Pre-training      Experiments  \n \n  Recent      work      has      shown      that      downstream      performance  \n \n  can      dramatically      improve      when      pre-training      is      scaled  \n \n  to      large      batch      sizes      (Yang      et      al.,      2019;      Liu      et      al.,      2019)  \n \n  and      corpora.\nTo      test      how      well      BART      performs      in      this  \n \n  regime,      and      to      create   \n \na      useful      model      for      downstream  \n \n  tasks,      we      trained      BART      using      the      same      scale      as      the  \n \n  RoBERTa      model.\n5.1      Experimental      Setup\netup  \n \n  We      pre-train   \n \na      large      model      with      12      layers      in      each      of      the  \n \n  encoder      and      decoder,      and   \n \na      hidden      size      of      1024.\nFol-  \n \n  lowing      RoBERTa      (Liu      et      al.,      2019),      we      use   \n \na      batch      size  \n \n  of      8000,      and      train      the      model      for      500000      steps.\nDocu-  \n \n  ments      are      tokenized      with      the      same      byte-pair      encoding  \n \n  as      GPT-2      (Radford      et      al.,      2019).\nBased      on      the      results      in  \n \n  Section      §4,      we      use   \n \na      combination      of      text      infilling      and  \n \n  sentence      permutation.\nWe      mask      30%      of      tokens      in      each  \n \n  document,      and      permute      all      sentences.\nAlthough      sen-  \n \n  tence      permutation      only      shows      significant      additive      gains SQuAD      1.1      SQuAD      2.0      MNLI      SST      QQP      QNLI      STS-B      RTE      MRPC      CoLA  \n \n  EM/F1      EM/F1      m/mm      Acc      Acc      Acc      Acc      Acc      Acc      Mcc\n \n \n  BERT      84.1/90.9      79.0/81.8      86.6/-      93.2      91.3      92.3      90.0      70.4      88.0      60.6  \n \n  UniLM      -/-      80.5/83.4      87.0/85.9      94.5   \n \n-      92.7   \n \n-      70.9   \n \n-      61.1  \n \n  XLNet      89.0/94.5      86.1/88.8      89.8/-      95.6      91.8      93.9      91.8      83.8      89.2      63.6  \n \n  RoBERTa      88.9/94.6      86.5/89.4      90.2/90.2      96.4      92.2      94.7      92.4      86.6      90.9      68.0  \n \n  BART      88.8/94.6      86.1/89.2      89.9/90.1      96.6      92.5      94.9      91.2      87.0      90.4      62.8 Table      2:      Results      for      large      models      on      SQUAD      and      GLUE      tasks.\nBART      performs      comparably      to      ROBERTa      and  \n \n  XLNet,      suggesting      that      BART’s      uni-directional      decoder      layers      do      not      reduce      performance      on      discriminative      tasks.\nCNN/DailyMail      XSum       RI      R2      RL      RI      R2      RL\n \n \n  Lead-3      40.42      17.62      36.67      16.30      1.60      11.95  \n \n  PTGEN      (See      et      al.,      2017)      36.44      15.66      33.42      29.70      9.21      23.24  \n \n  PTGEN+COV      (See      et      al.,      2017)      39.53      17.28      36.38      28.10      8.02      21.72  \n \n  UniLM      43.33      20.21      40.51   \n \n-   \n \n-   \n \n-       BERTSUMABS      (Liu   \n \n&      Lapata,      2019)      41.72      19.39      38.76      38.76      16.33      31.15  \n \n  BERTSUMEXTABS      (Liu   \n \n&      Lapata,      2019)      42.13      19.60      39.18      38.81      16.50      31.27 BART      44.16      21.28      40.90      45.14      22.27      37.25 Table      3:      Results      on      two      standard      summarization      datasets.\nBART      outperforms      previous      work      on      summarization      on\n \n \n  on      the      CNN/DM      summarization      dataset,      we      hypothe-      ConvAI2  \n \n  sised      that      larger      pre-trained      models      may      be      better      able      Valid      F1      Valid      PPL  \n \n  to   \n \n|      fi      this      task.\nTo      help      th      del      better      fit      th  \n \n  RE      aa      ae      Pai      Cae      Seq2Seq+      Attention      16.02      35.07  \n \n  data,      we      disabled      dropout      for      the      final      10%      of      training   \n \n.    \n.\n     Best      System      19.09      17.51  \n \n  steps.\nWe      use      the      same      pre-training      data      as      Liu      et      al.\n(2019),      consisting      of      160Gb      of      news,      books,      stories,  \n \n  and      web      text.\nBART      20.72      11.85 5.2      Discriminative      Tasks  \n \n  Table   \n \n2      compares      the      performance      of      BART      with      sev-  \n \n  eral      recent      approaches      on      the      well-studied      SQUAD      and  \n \n  GLUE      tasks      (Warstadt      et      al.,      2018;      Socher      et      al.,      2013;  \n \n  Dolan   \n \n&      Brockett,      2005;      Agirre      et      al.,      2007;      Williams  \n \n  et      al.,      2018;      Dagan      et      al.,      2006;      Levesque      et      al.,      2011).\n \n \n  Table      4:      BART      outperforms      previous      work      on      conver-  \n \n  sational      response      generation.\nPerplexities      are      renor-  \n \n  malized      based      on      official      tokenizer      for      ConvAI2.\n \n \n  The      most      directly      comparable      baseline      is      ROBERTa,  \n \n  which      was      pre-trained      with      the      same      resources,      but  \n \n  a      different      objective.\nOverall,      BART      performs      simi-  \n \n  larly,      with      only      small      differences      between      the      models  \n \n  on      most      tasks.\nsuggesting      that      BART’s      improvements  \n \n  on      generation      tasks      do      not      come      at      the      expense      of      clas-  \n \n  sification      performance.\n \n \n  Summarization      To      provide   \n \na      comparison      with      the  \n \n  state-of-the-art      in      summarization,      we      present      results  \n \n  on      two      summarization      datasets,      CNN/DailyMail      and  \n \n  XSum,      which      have      distinct      properties.\n \n \n  Summaries      in      the      CNN/DailyMail      tend      to      resemble  \n \n  source      sentences.\nExtractive      models      do      well      here,      and  \n \n  even      the      baseline      of      the      first-three      source      sentences      is  \n \n  highly      competitive.\nNevertheless,      BART      outperforms  \n \n  all      existing      work.\n5.3.      Generation      Tasks\nGeneration      Tasks\n |       We      also      experiment      with      several      text      generation      tasks.       BART      is      fine-tuned      as      a      standard      sequence-to-sequence       model      from      the      input      to      the      output      text.      During      fine-       tuning      we      use      a      label      smoothed      cross      entropy      loss       (Pereyra      et      al.,      2017),      with      the      smoothing      parameter       set      to      0.1.      During      generation,      we      set      beam      size      as      5,       remove      duplicated      trigrams      in      beam      search,      and      tuned       the      model      with      min-len,      max-len,      length      penalty      on      the       validation      set      (Fan      et      al.,      2017). | 6.0      points      on      all      ROUGE      metrics—trepresenting   \n \na      sig-  \n \n  nificant      advance      in      performance      on      this      problem.\nQual-  \n \n  itatively,      sample      quality      is      high      (see      86).\n \n \n  In      contrast,      XSum      is      highly      abstractive,      and      extrac-  \n \n  tive      models      perform      poorly.\nBART      outperforms      the  \n \n  best      previous      work,      which      leverages      BERT,      by      roughly  \n \n  Dialogue      We      evaluate      dialogue      response      generation  \n \n  on      CONVAI2      (Dinan      et      al.,      2019),      in      which      agents  \n \n  must      generate      responses      conditioned      on      both      the      pre-  \n \n  vious      context      and   \n \na      textually-specified      persona.\nBART  \n \n  outperforms      previous      work      on      two      automated      metrics.\n\nRl      R2      RL\n \n \n  Best      Extractive      23.55      3.1      17.5  \n \n  Language      Model      27.8      47      23.1  \n \n  Seq2Seq      28.3      5.1      22.8  \n \n  Seq2Seq      Multitask      28.9      54      23.1  \n \n  BART      30.6      6.2      24.3  \n \n  Table      5:      BART      achieves      state-of-the-art      results      on  \n \n  the      challenging      ELI5      abstractive      question      answering  \n \n  dataset.\nComparison      models      are      from      Fan      et      al.\n(2019)."
    },
    {
      "id": 6,
      "title": "Token      Deletion Text      Infilling",
      "type": "section",
      "children": [],
      "content": "Token      Deletion Text      Infilling\n \n \n  Text      Infilling   \n \nA      number      of      text      spans      are      sampled,  \n \n  with      span      lengths      drawn      from   \n \na      Poisson      distribution  \n \n  (A   \n \n=      3).\nEach      span      is      replaced      with   \n \na      single      [MASK]  \n \n  token.\nO-length      spans      correspond      to      the      insertion      of  \n \n  [MASK]      tokens.\nText      infilling      is      inspired      by      Span-  \n \n  BERT      (Joshi      et      al.,      2019),      but      SpanBERT      samples  \n \n  span      lengths      from   \n \na      different      (clamped      geometric)      dis-  \n \n  tribution,      and      replaces      each      span      with   \n \na      sequence      of  \n \n  [MASK]      tokens      of      exactly      the      same      length.\nText      infill-  \n \n  ing      teaches      the      model      to      predict      how      many      tokens      are  \n \n  missing      from   \n \na      span.\n \n \n  Sentence      Permutation   \n \nA      document      is      divided      into  \n \n  sentences      based      on      full      stops,      and      these      sentences      are  \n \n  shuffled      in   \n \na      random      order.\n \n \n  Document      Rotation   \n \nA      token      is      chosen      uniformly      at  \n \n  random,      and      the      document      is      rotated      so      that      it      begins  \n \n  with      that      token.\nThis      task      trains      the      model      to      identify  \n \n  the      start      of      the      document.\n3\n \n   Fine-tuning      BART The      representations      produced      by      BART      can      be      used      in  \n \n  several      ways      for      downstream      applications.\n3.1      Sequence      Classification      Tasks\nasks  \n \n  For      sequence      classification      tasks,      the      same      input      is      fed  \n \n  into      the      encoder      and      decoder,      and      the      final      hidden      state  \n \n  of      the      final      decoder      token      is      fed      into      new      multi-class  \n \n  linear      classifier.\nThis      approach      is      related      to      the      CLS  \n \n  token      in      BERT;      however      we      add      the      additional      token  \n \n  to      the      end      so      that      representation      for      the      token      in      the  \n \n  decoder      can      attend      to      decoder      states      from      the      complete  \n \n  input      (Figure      3a).\n3.2.      Token      Classification      Tasks\nasks  \n \n  For      token      classification      tasks,      such      as      answer      endpoint  \n \n  classification      for      SQUAD,      we      feed      the      complete      doc-  \n \n  ument      into      the      encoder      and      decoder,      and      use      the      top  \n \n  hidden      state      of      the      decoder      as   \n \na      representation      for      each  \n \n  word.\nThis      representation      is      used      to      classify      the      token.\n3.3      Sequence      Generation      Tasks\nasks  \n \n  Because      BART      has      an      autoregressive      decoder,      it      can      be  \n \n  directly      fine      tuned      for      sequence      generation      tasks      such  \n \n  as      abstractive      question      answering      and      summarization.\n \n \n  In      both      of      these      tasks,      information      is      copied      from      the  \n \n  input      but      manipulated,      which      is      closely      related      to      the  \n \n  denoising      pre-training      objective.\nHere,      the      encoder      in-  \n \n  put      is      the      input      sequence,      and      the      decoder      generates  \n \n  outputs      autoregressively.\n3.4      Machine      Translation\ntion  \n \n  We      also      explore      using      BART      to      improve      machine      trans-  \n \n  lation      decoders      for      translating      into      English.\nPrevious  \n \n  work      Edunov      et      al.\n(2019)      has      shown      that      models      can  \n \n  be      improved      by      incorporating      pre-trained      encoders,      but  \n \n  gains      from      using      pre-trained      language      models      in      de-  \n \n  coders      have      been      limited.\nWe      show      that      it      is      possible  \n \n  to      use      the      entire      BART      model      (both      encoder      and      de-  \n \n  coder)      as   \n \na      single      pretrained      decoder      for      machine      trans-  \n \n  lation,      by      adding   \n \na      new      set      of      encoder      parameters      that  \n \n  are      learned      from      bitext      (see      Figure      3b).\n \n \n  More      precisely,      we      replace      BART’s      encoder      embed-  \n \n  ding      layer      with   \n \na      new      randomly      initialized      encoder.\n \n \n  The      model      is      trained      end-to-end,      which      trains      the      new       encoder      to      map      foreign      words      into      an      input      that      BART  \n \n  can      de-noise      to      English.\nThe      new      encoder      can      use   \n \na       separate      vocabulary      from      the      original      BART      model.\n \n \n  We      train      the      source      encoder      in      two      steps,      in      both  \n \n  cases      backpropagating      the      cross-entropy      loss      from      the  \n \n  output      of      the      BART      model.\nIn      the      first      step,      we      freeze  \n \n  most      of      BART      parameters      and      only      update      the      ran-  \n \n  domly      initialized      source      encoder,      the      BART      positional  \n \n  embeddings,      and      the      self-attention      input      projection      ma-  \n \n  trix      of      BART’s      encoder      first      layer.\nIn      the      second      step,  \n \n  we      train      all      model      parameters      for   \n \na      small      number      of  \n \n  iterations.\n4\n \n   Comparing      Pre-training      Objectives  \n \n  BART      supports   \n \na      much      wider      range      of      noising      schemes  \n \n  during      pre-training      than      previous      work.\nWe      compare   \n \na       range      of      options      using      base-size      models      (6      encoder      and  \n \n  6      decoder      layers,      with   \n \na      hidden      size      of      768),      evaluated  \n \n  on   \n \na      representative      subset      of      the      tasks      we      will      consider  \n \n  for      the      full      large      scale      experiments      in      85.\n4.1      Comparison      Objectives\nives  \n \n  While      many      pre-training      objectives      have      been      pro-  \n \n  posed,      fair      comparisons      between      these      have      been      dif-  \n \n  ficult      to      perform,      at      least      in      part      due      to      differences      in  \n \n  training      data,      training      resources,      architectural      differ-  \n \n  ences      between      models,      and      fine-tuning      procedures.\nWe  \n \n  ABCDE  \n \n  weee  \n \n  Pre-trained      Pre-trained  \n \n  Craters      Joo      rezapes cise)      Te      TrET Pre-trained  \n \n  Encoder Pre-trained  \n \n  Decoder aa      AB  \n \n  trrrt  \n \n  tr      ttt  \n \n  <s>A      BCD  \n \n  Randomly  \n \n  Initialized      Encoder  \n \n  TFT  \n \n  aBpByoe sentation      from      the      final      output      is      used.\n \n \n  re-implement      strong      pre-training      approaches      recently  \n \n  proposed      for      discriminative      and      generation      tasks.\nWe  \n \n  aim,      as      much      as      possible,      to      control      for      differences      un-  \n \n  related      to      the      pre-training      objective.\nHowever,      we      do  \n \n  make      minor      changes      to      the      learning      rate      and      usage      of  \n \n  layer      normalisation      in      order      to      improve      performance  \n \n  (tuning      these      separately      for      each      objective).\nFor      refer-  \n \n  ence,      we      compare      our      implementations      with      published  \n \n  numbers      from      BERT,      which      was      also      trained      for      1M  \n \n  steps      on   \n \na      combination      of      books      and      Wikipedia      data.\n \n \n  We      compare      the      following      approaches:\n \n \n  Language      Model      Similarly      to      GPT      (Radford      et      al.,  \n \n  2018),      we      train   \n \na      left-to-right      Transformer      language  \n \n  model.\nThis      model      is      equivalent      to      the      BART      decoder,  \n \n  without      cross-attention.\n \n \n  Permuted      Language      Model      Based      on      XLNet      (Yang  \n \n  et      al.,      2019),      we      sample      1/6      of      the      tokens,      and      gener-  \n \n  ate      them      in   \n \na      random      order      autoregressively.\nFor      con-  \n \n  sistency      with      other      models,      we      do      not      implement      the  \n \n  relative      positional      embeddings      or      attention      across      seg-  \n \n  ments      from      XLNet.\n \n \n  Masked      Language      Model      Following      BERT      (Devlin  \n \n  et      al.,      2019),      we      replace      15%      of      tokens      with      [MASK]  \n \n  symbols,      and      train      the      model      to      independently      predict  \n \n  the      original      tokens.\n \n \n  Multitask      Masked      Language      Model      As      in      UniLM  \n \n  (Dong      et      al.,      2019),      we      train   \n \na      Masked      Language  \n \n  Model      with      additional      self-attention      masks.\nSelf      at-  \n \n  tention      masks      are      chosen      randomly      in      with      the      follow  \n \n  proportions:      1/6      left-to-right,      1/6      right-to-left,      1/3      un-  \n \n  masked,      and      1/3      with      the      first      50%      of      tokens      unmasked  \n \n  and   \n \na      left-to-right      mask      for      the      remainder.\n \n \n  Masked      Seq-to-Seq      Inspired      by      MASS      (Song      et      al.,  \n \n  2019),      we      mask   \n \na      span      containing      50%      of      tokens,  \n \n  and      train   \n \na      sequence      to      sequence      model      to      predict      the  \n \n  masked      tokens.\n \n \n  For      the      Permuted      LM,      Masked      LM      and      Multitask  \n \n  Masked      LM,      we      use      two-stream      attention      (Yang      et      al.,  \n \n  2019)      to      efficiently      compute      likelihoods      of      the      output  \n \n  part      of      the      sequence      (using   \n \na      diagonal      self-attention  \n \n  mask      on      the      output      to      predict      words      left-to-right).\n \n \n  We      experiment      with      (1)      treating      the      task      as   \n \na      stan-  \n \n  dard      sequence-to-sequence      problem,      where      the      source  \n \n  input      to      the      encoder      and      the      target      is      the      decoder      out-  \n \n  put,      or      (2)      adding      the      source      as      prefix      to      the      target      in  \n \n  the      decoder,      with   \n \na      loss      only      on      the      target      part      of      the  \n \n  sequence.\nWe      find      the      former      works      better      for      BART  \n \n  models,      and      the      latter      for      other      models.\n \n \n  To      most      directly      compare      our      models      on      their      ability  \n \n  to      model      their      fine-tuning      objective      (the      log      likelihood  \n \n  of      the      human      text),      we      report      perplexity      in      Table      1.\n4.2      Tasks\nasks  \n \n  SQuAD  \n \n  (Rajpurkar      et      al.,      2016)a      an      extractive      ques-  \n \n  tion      answering      task      on      Wikipedia      paragraphs.\nAnswers  \n \n  are      text      spans      extracted      from   \n \na      given      document      context.\n \n \n  Similar      to      BERT      (Devlin      et      al.,      2019),      we      use      concate-  \n \n  nated      question      and      context      as      input      to      the      encoder      of  \n \n  BART,      and      additionally      pass      them      to      the      decoder.\nThe  \n \n  model      includes      classifiers      to      predict      the      start      and      end  \n \n  indices      of      each      token.\n \n \n  MNLI      (Williams      et      al.,      2017),   \n \na      bitext      classification  \n \n  task      to      predict      whether      one      sentence      entails      another.\n \n \n  The      fine-tuned      model      concatenates      the      two      sentences  \n \n  with      appended      an      EOS      token,      and      passes      them      to      both  \n \n  the      BART      encoder      and      decoder.\nIn      contrast      to      BERT,  \n \n  the      representation      of      the      EOS      token      is      used      to      classify  \n \n  the      sentences      relations.\n \n \n  ELIS      (Fanetal.,      2019),   \n \na      long-form      abstractive      ques-  \n \n  tion      answering      dataset.\nModels      generate      answers      con-  \n \n  ditioned      on      the      concatenation      of   \n \na      question      and      sup-  \n \n  porting      documents.\nXSum_      (Narayan      et      al.,      2018),   \n \na      news      summarization  \n \n  dataset      with      highly      abstractive      summaries.\nConvAI2_      (Dinan      et      al.,      2019),   \n \na      dialogue      response  \n \n  generation      task,      conditioned      on      context      and   \n \na      persona.\n \n \n  CNN/DM_      (Hermann      et      al.,      2015),   \n \na      news      summa-  \n \n  rization      dataset.\nSummaries      here      are      typically      closely  \n \n  related      to      source      sentences.\n4.3      Results\nults Results      are      shown      in      Table      1.\nSeveral      trends      are      clear:\nModel      SQuAD      1.1      MNLI      ELIS      XSum   \n \n_      ConvAI2      CNN/DM  \n \n  Fl      Acc      PPL      PPL      PPL      PPL BERT      Base      (Devlin      et      al.,      2019)      88.5      84.3   \n \n-   \n \n-   \n \n-   \n \n-       Masked      Language      Model      90.0      83.5      24.77      7.87      12.59      7.06  \n \n  Masked      Seq2seq      87.0      82.1      23.40      6.80      11.43      6.19  \n \n  Language      Model      76.7      80.1      21.40      7.00      11.51      6.56  \n \n  Permuted      Language      Model      89.1      83.7      24.03      7.69      12.23      6.96  \n \n  Multitask      Masked      Language      Model      89.2      82.4      23.73      7.50      12.39      6.74  \n \n  BART      Base  \n \n  w/      Token      Masking      90.4      84.1      25.05      7.08      11.73      6.10  \n \n  w/      Token      Deletion      90.4      84.1      24.61      6.90      11.46      5.87  \n \n  w/      Text      Infilling      90.8      84.0      24.26      6.61      11.05      5.83  \n \n  w/      Document      Rotation      77.2      75.3      53.69      17.14      19.87      10.59  \n \n  w/      Sentence      Shuffling      85.4      81.5      41.87      10.93      16.67      7.89  \n \n  w/      Text      Infilling   \n \n+      Sentence      Shuffling      90.8      83.8      24.17      6.62      11.12      5.41\nconsistently      strong      performance.\n \n \n  Performance      of      pre-training      methods      varies      signifi-  \n \n  cantly      across      tasks      The      effectiveness      of      pre-training  \n \n  methods      is      highly      dependent      on      the      task.\nFor      exam-  \n \n  ple,   \n \na      simple      language      model      achieves      the      best      ELIS  \n \n  performance,      but      the      worst      SQUAD      results.\n \n \n  Token      masking      is      crucial      Pre-training      objectives  \n \n  based      on      rotating      documents      or      permuting      sentences  \n \n  perform      poorly      in      isolation.\nThe      successful      methods  \n \n  either      use      token      deletion      or      masking,      or      self-attention  \n \n  masks.\nDeletion      appears      to      outperform      masking      on  \n \n  generation      tasks.\n \n \n  Left-to-right      pre-training      improves      generation  \n \n  The      Masked      Language      Model      and      the      Permuted  \n \n  Language      Model      perform      less      well      than      others      on  \n \n  generation,      and      are      the      only      models      we      consider      that  \n \n  do      not      include      left-to-right      auto-regressive      language  \n \n  modelling      during      pre-training.\n \n \n  Bidirectional      encoders      are      crucial      for      SQUAD      As  \n \n  noted      in      previous      work      (Devlin      et      al.,      2019),      just  \n \n  left-to-right      decoder      performs      poorly      on      SQuAD,      be-  \n \n  cause      future      context      is      crucial      in      classification      deci-  \n \n  sions.\nHowever,      BART      achieves      similar      performance  \n \n  with      only      half      the      number      of      bidirectional      layers.\n \n \n  The      pre-training      objective      is      not      the      only      important  \n \n  factor      Our      Permuted      Language      Model      performs      less  \n \n  well      than      XLNet      (Yang      et      al.,      2019).\nSome      of      this      dif-  \n \n  ference      is      likely      due      to      not      including      other      architectural  \n \n  improvements,      such      as      relative-position      embeddings      or  \n \n  segment-level      recurrence.\n \n \n  Pure      language      models      perform      best      on      ELIS      The  \n \n  ELIS      dataset      is      an      outlier,      with      much      higher      perplex-  \n \n  ities      than      other      tasks,      and      is      the      only      generation      task  \n \n  where      other      models      outperform      BART.\n \n \nA      pure      lan-  \n \n  guage      model      performs      best,      suggesting      that      BART      is  \n \n  less      effective      when      the      output      is      only      loosely      con-  \n \n  strained      by      the      input.\n \n \n  BART      achieves      the      most      consistently      strong      perfor-  \n \n  mance.\nWith      the      exception      of      ELI5,      BART      models  \n \n  using      text-infilling      perform      well      on      all      tasks.\n5\n \n   Large-scale      Pre-training      Experiments  \n \n  Recent      work      has      shown      that      downstream      performance  \n \n  can      dramatically      improve      when      pre-training      is      scaled  \n \n  to      large      batch      sizes      (Yang      et      al.,      2019;      Liu      et      al.,      2019)  \n \n  and      corpora.\nTo      test      how      well      BART      performs      in      this  \n \n  regime,      and      to      create   \n \na      useful      model      for      downstream  \n \n  tasks,      we      trained      BART      using      the      same      scale      as      the  \n \n  RoBERTa      model.\n5.1      Experimental      Setup\netup  \n \n  We      pre-train   \n \na      large      model      with      12      layers      in      each      of      the  \n \n  encoder      and      decoder,      and   \n \na      hidden      size      of      1024.\nFol-  \n \n  lowing      RoBERTa      (Liu      et      al.,      2019),      we      use   \n \na      batch      size  \n \n  of      8000,      and      train      the      model      for      500000      steps.\nDocu-  \n \n  ments      are      tokenized      with      the      same      byte-pair      encoding  \n \n  as      GPT-2      (Radford      et      al.,      2019).\nBased      on      the      results      in  \n \n  Section      §4,      we      use   \n \na      combination      of      text      infilling      and  \n \n  sentence      permutation.\nWe      mask      30%      of      tokens      in      each  \n \n  document,      and      permute      all      sentences.\nAlthough      sen-  \n \n  tence      permutation      only      shows      significant      additive      gains SQuAD      1.1      SQuAD      2.0      MNLI      SST      QQP      QNLI      STS-B      RTE      MRPC      CoLA  \n \n  EM/F1      EM/F1      m/mm      Acc      Acc      Acc      Acc      Acc      Acc      Mcc\n \n \n  BERT      84.1/90.9      79.0/81.8      86.6/-      93.2      91.3      92.3      90.0      70.4      88.0      60.6  \n \n  UniLM      -/-      80.5/83.4      87.0/85.9      94.5   \n \n-      92.7   \n \n-      70.9   \n \n-      61.1  \n \n  XLNet      89.0/94.5      86.1/88.8      89.8/-      95.6      91.8      93.9      91.8      83.8      89.2      63.6  \n \n  RoBERTa      88.9/94.6      86.5/89.4      90.2/90.2      96.4      92.2      94.7      92.4      86.6      90.9      68.0  \n \n  BART      88.8/94.6      86.1/89.2      89.9/90.1      96.6      92.5      94.9      91.2      87.0      90.4      62.8 Table      2:      Results      for      large      models      on      SQUAD      and      GLUE      tasks.\nBART      performs      comparably      to      ROBERTa      and  \n \n  XLNet,      suggesting      that      BART’s      uni-directional      decoder      layers      do      not      reduce      performance      on      discriminative      tasks.\nCNN/DailyMail      XSum       RI      R2      RL      RI      R2      RL\n \n \n  Lead-3      40.42      17.62      36.67      16.30      1.60      11.95  \n \n  PTGEN      (See      et      al.,      2017)      36.44      15.66      33.42      29.70      9.21      23.24  \n \n  PTGEN+COV      (See      et      al.,      2017)      39.53      17.28      36.38      28.10      8.02      21.72  \n \n  UniLM      43.33      20.21      40.51   \n \n-   \n \n-   \n \n-       BERTSUMABS      (Liu   \n \n&      Lapata,      2019)      41.72      19.39      38.76      38.76      16.33      31.15  \n \n  BERTSUMEXTABS      (Liu   \n \n&      Lapata,      2019)      42.13      19.60      39.18      38.81      16.50      31.27 BART      44.16      21.28      40.90      45.14      22.27      37.25 Table      3:      Results      on      two      standard      summarization      datasets.\nBART      outperforms      previous      work      on      summarization      on\n \n \n  on      the      CNN/DM      summarization      dataset,      we      hypothe-      ConvAI2  \n \n  sised      that      larger      pre-trained      models      may      be      better      able      Valid      F1      Valid      PPL  \n \n  to   \n \n|      fi      this      task.\nTo      help      th      del      better      fit      th  \n \n  RE      aa      ae      Pai      Cae      Seq2Seq+      Attention      16.02      35.07  \n \n  data,      we      disabled      dropout      for      the      final      10%      of      training   \n \n.    \n.\n     Best      System      19.09      17.51  \n \n  steps.\nWe      use      the      same      pre-training      data      as      Liu      et      al.\n(2019),      consisting      of      160Gb      of      news,      books,      stories,  \n \n  and      web      text.\nBART      20.72      11.85 5.2      Discriminative      Tasks  \n \n  Table   \n \n2      compares      the      performance      of      BART      with      sev-  \n \n  eral      recent      approaches      on      the      well-studied      SQUAD      and  \n \n  GLUE      tasks      (Warstadt      et      al.,      2018;      Socher      et      al.,      2013;  \n \n  Dolan   \n \n&      Brockett,      2005;      Agirre      et      al.,      2007;      Williams  \n \n  et      al.,      2018;      Dagan      et      al.,      2006;      Levesque      et      al.,      2011).\n \n \n  Table      4:      BART      outperforms      previous      work      on      conver-  \n \n  sational      response      generation.\nPerplexities      are      renor-  \n \n  malized      based      on      official      tokenizer      for      ConvAI2.\n \n \n  The      most      directly      comparable      baseline      is      ROBERTa,  \n \n  which      was      pre-trained      with      the      same      resources,      but  \n \n  a      different      objective.\nOverall,      BART      performs      simi-  \n \n  larly,      with      only      small      differences      between      the      models  \n \n  on      most      tasks.\nsuggesting      that      BART’s      improvements  \n \n  on      generation      tasks      do      not      come      at      the      expense      of      clas-  \n \n  sification      performance.\n \n \n  Summarization      To      provide   \n \na      comparison      with      the  \n \n  state-of-the-art      in      summarization,      we      present      results  \n \n  on      two      summarization      datasets,      CNN/DailyMail      and  \n \n  XSum,      which      have      distinct      properties.\n \n \n  Summaries      in      the      CNN/DailyMail      tend      to      resemble  \n \n  source      sentences.\nExtractive      models      do      well      here,      and  \n \n  even      the      baseline      of      the      first-three      source      sentences      is  \n \n  highly      competitive.\nNevertheless,      BART      outperforms  \n \n  all      existing      work.\n5.3.      Generation      Tasks\nGeneration      Tasks\n |       We      also      experiment      with      several      text      generation      tasks.       BART      is      fine-tuned      as      a      standard      sequence-to-sequence       model      from      the      input      to      the      output      text.      During      fine-       tuning      we      use      a      label      smoothed      cross      entropy      loss       (Pereyra      et      al.,      2017),      with      the      smoothing      parameter       set      to      0.1.      During      generation,      we      set      beam      size      as      5,       remove      duplicated      trigrams      in      beam      search,      and      tuned       the      model      with      min-len,      max-len,      length      penalty      on      the       validation      set      (Fan      et      al.,      2017). | 6.0      points      on      all      ROUGE      metrics—trepresenting   \n \na      sig-  \n \n  nificant      advance      in      performance      on      this      problem.\nQual-  \n \n  itatively,      sample      quality      is      high      (see      86).\n \n \n  In      contrast,      XSum      is      highly      abstractive,      and      extrac-  \n \n  tive      models      perform      poorly.\nBART      outperforms      the  \n \n  best      previous      work,      which      leverages      BERT,      by      roughly  \n \n  Dialogue      We      evaluate      dialogue      response      generation  \n \n  on      CONVAI2      (Dinan      et      al.,      2019),      in      which      agents  \n \n  must      generate      responses      conditioned      on      both      the      pre-  \n \n  vious      context      and   \n \na      textually-specified      persona.\nBART  \n \n  outperforms      previous      work      on      two      automated      metrics.\n"
    },
    {
      "id": 7,
      "title": "3.1      Sequence      Classification      Tasks",
      "type": "section",
      "children": [],
      "content": "3.1      Sequence      Classification      Tasks\nasks  \n \n  For      sequence      classification      tasks,      the      same      input      is      fed  \n \n  into      the      encoder      and      decoder,      and      the      final      hidden      state  \n \n  of      the      final      decoder      token      is      fed      into      new      multi-class  \n \n  linear      classifier.\nThis      approach      is      related      to      the      CLS  \n \n  token      in      BERT;      however      we      add      the      additional      token  \n \n  to      the      end      so      that      representation      for      the      token      in      the  \n \n  decoder      can      attend      to      decoder      states      from      the      complete  \n \n  input      (Figure      3a)."
    },
    {
      "id": 8,
      "title": "3.2.      Token      Classification      Tasks",
      "type": "section",
      "children": [],
      "content": "3.2.      Token      Classification      Tasks\nasks  \n \n  For      token      classification      tasks,      such      as      answer      endpoint  \n \n  classification      for      SQUAD,      we      feed      the      complete      doc-  \n \n  ument      into      the      encoder      and      decoder,      and      use      the      top  \n \n  hidden      state      of      the      decoder      as   \n \na      representation      for      each  \n \n  word.\nThis      representation      is      used      to      classify      the      token."
    },
    {
      "id": 9,
      "title": "3.3      Sequence      Generation      Tasks",
      "type": "section",
      "children": [],
      "content": "3.3      Sequence      Generation      Tasks\nasks  \n \n  Because      BART      has      an      autoregressive      decoder,      it      can      be  \n \n  directly      fine      tuned      for      sequence      generation      tasks      such  \n \n  as      abstractive      question      answering      and      summarization.\n \n \n  In      both      of      these      tasks,      information      is      copied      from      the  \n \n  input      but      manipulated,      which      is      closely      related      to      the  \n \n  denoising      pre-training      objective.\nHere,      the      encoder      in-  \n \n  put      is      the      input      sequence,      and      the      decoder      generates  \n \n  outputs      autoregressively."
    },
    {
      "id": 10,
      "title": "3.4      Machine      Translation",
      "type": "section",
      "children": [],
      "content": "3.4      Machine      Translation\ntion  \n \n  We      also      explore      using      BART      to      improve      machine      trans-  \n \n  lation      decoders      for      translating      into      English.\nPrevious  \n \n  work      Edunov      et      al.\n(2019)      has      shown      that      models      can  \n \n  be      improved      by      incorporating      pre-trained      encoders,      but  \n \n  gains      from      using      pre-trained      language      models      in      de-  \n \n  coders      have      been      limited.\nWe      show      that      it      is      possible  \n \n  to      use      the      entire      BART      model      (both      encoder      and      de-  \n \n  coder)      as   \n \na      single      pretrained      decoder      for      machine      trans-  \n \n  lation,      by      adding   \n \na      new      set      of      encoder      parameters      that  \n \n  are      learned      from      bitext      (see      Figure      3b).\n \n \n  More      precisely,      we      replace      BART’s      encoder      embed-  \n \n  ding      layer      with   \n \na      new      randomly      initialized      encoder.\n \n \n  The      model      is      trained      end-to-end,      which      trains      the      new       encoder      to      map      foreign      words      into      an      input      that      BART  \n \n  can      de-noise      to      English.\nThe      new      encoder      can      use   \n \na       separate      vocabulary      from      the      original      BART      model.\n \n \n  We      train      the      source      encoder      in      two      steps,      in      both  \n \n  cases      backpropagating      the      cross-entropy      loss      from      the  \n \n  output      of      the      BART      model.\nIn      the      first      step,      we      freeze  \n \n  most      of      BART      parameters      and      only      update      the      ran-  \n \n  domly      initialized      source      encoder,      the      BART      positional  \n \n  embeddings,      and      the      self-attention      input      projection      ma-  \n \n  trix      of      BART’s      encoder      first      layer.\nIn      the      second      step,  \n \n  we      train      all      model      parameters      for   \n \na      small      number      of  \n \n  iterations.\n4\n \n   Comparing      Pre-training      Objectives  \n \n  BART      supports   \n \na      much      wider      range      of      noising      schemes  \n \n  during      pre-training      than      previous      work.\nWe      compare   \n \na       range      of      options      using      base-size      models      (6      encoder      and  \n \n  6      decoder      layers,      with   \n \na      hidden      size      of      768),      evaluated  \n \n  on   \n \na      representative      subset      of      the      tasks      we      will      consider  \n \n  for      the      full      large      scale      experiments      in      85.\n4.1      Comparison      Objectives\nives  \n \n  While      many      pre-training      objectives      have      been      pro-  \n \n  posed,      fair      comparisons      between      these      have      been      dif-  \n \n  ficult      to      perform,      at      least      in      part      due      to      differences      in  \n \n  training      data,      training      resources,      architectural      differ-  \n \n  ences      between      models,      and      fine-tuning      procedures.\nWe  \n \n  ABCDE  \n \n  weee  \n \n  Pre-trained      Pre-trained  \n \n  Craters      Joo      rezapes cise)      Te      TrET Pre-trained  \n \n  Encoder Pre-trained  \n \n  Decoder aa      AB  \n \n  trrrt  \n \n  tr      ttt  \n \n  <s>A      BCD  \n \n  Randomly  \n \n  Initialized      Encoder  \n \n  TFT  \n \n  aBpByoe sentation      from      the      final      output      is      used.\n \n \n  re-implement      strong      pre-training      approaches      recently  \n \n  proposed      for      discriminative      and      generation      tasks.\nWe  \n \n  aim,      as      much      as      possible,      to      control      for      differences      un-  \n \n  related      to      the      pre-training      objective.\nHowever,      we      do  \n \n  make      minor      changes      to      the      learning      rate      and      usage      of  \n \n  layer      normalisation      in      order      to      improve      performance  \n \n  (tuning      these      separately      for      each      objective).\nFor      refer-  \n \n  ence,      we      compare      our      implementations      with      published  \n \n  numbers      from      BERT,      which      was      also      trained      for      1M  \n \n  steps      on   \n \na      combination      of      books      and      Wikipedia      data.\n \n \n  We      compare      the      following      approaches:\n \n \n  Language      Model      Similarly      to      GPT      (Radford      et      al.,  \n \n  2018),      we      train   \n \na      left-to-right      Transformer      language  \n \n  model.\nThis      model      is      equivalent      to      the      BART      decoder,  \n \n  without      cross-attention.\n \n \n  Permuted      Language      Model      Based      on      XLNet      (Yang  \n \n  et      al.,      2019),      we      sample      1/6      of      the      tokens,      and      gener-  \n \n  ate      them      in   \n \na      random      order      autoregressively.\nFor      con-  \n \n  sistency      with      other      models,      we      do      not      implement      the  \n \n  relative      positional      embeddings      or      attention      across      seg-  \n \n  ments      from      XLNet.\n \n \n  Masked      Language      Model      Following      BERT      (Devlin  \n \n  et      al.,      2019),      we      replace      15%      of      tokens      with      [MASK]  \n \n  symbols,      and      train      the      model      to      independently      predict  \n \n  the      original      tokens.\n \n \n  Multitask      Masked      Language      Model      As      in      UniLM  \n \n  (Dong      et      al.,      2019),      we      train   \n \na      Masked      Language  \n \n  Model      with      additional      self-attention      masks.\nSelf      at-  \n \n  tention      masks      are      chosen      randomly      in      with      the      follow  \n \n  proportions:      1/6      left-to-right,      1/6      right-to-left,      1/3      un-  \n \n  masked,      and      1/3      with      the      first      50%      of      tokens      unmasked  \n \n  and   \n \na      left-to-right      mask      for      the      remainder.\n \n \n  Masked      Seq-to-Seq      Inspired      by      MASS      (Song      et      al.,  \n \n  2019),      we      mask   \n \na      span      containing      50%      of      tokens,  \n \n  and      train   \n \na      sequence      to      sequence      model      to      predict      the  \n \n  masked      tokens.\n \n \n  For      the      Permuted      LM,      Masked      LM      and      Multitask  \n \n  Masked      LM,      we      use      two-stream      attention      (Yang      et      al.,  \n \n  2019)      to      efficiently      compute      likelihoods      of      the      output  \n \n  part      of      the      sequence      (using   \n \na      diagonal      self-attention  \n \n  mask      on      the      output      to      predict      words      left-to-right).\n \n \n  We      experiment      with      (1)      treating      the      task      as   \n \na      stan-  \n \n  dard      sequence-to-sequence      problem,      where      the      source  \n \n  input      to      the      encoder      and      the      target      is      the      decoder      out-  \n \n  put,      or      (2)      adding      the      source      as      prefix      to      the      target      in  \n \n  the      decoder,      with   \n \na      loss      only      on      the      target      part      of      the  \n \n  sequence.\nWe      find      the      former      works      better      for      BART  \n \n  models,      and      the      latter      for      other      models.\n \n \n  To      most      directly      compare      our      models      on      their      ability  \n \n  to      model      their      fine-tuning      objective      (the      log      likelihood  \n \n  of      the      human      text),      we      report      perplexity      in      Table      1.\n4.2      Tasks\nasks  \n \n  SQuAD  \n \n  (Rajpurkar      et      al.,      2016)a      an      extractive      ques-  \n \n  tion      answering      task      on      Wikipedia      paragraphs.\nAnswers  \n \n  are      text      spans      extracted      from   \n \na      given      document      context.\n \n \n  Similar      to      BERT      (Devlin      et      al.,      2019),      we      use      concate-  \n \n  nated      question      and      context      as      input      to      the      encoder      of  \n \n  BART,      and      additionally      pass      them      to      the      decoder.\nThe  \n \n  model      includes      classifiers      to      predict      the      start      and      end  \n \n  indices      of      each      token.\n \n \n  MNLI      (Williams      et      al.,      2017),   \n \na      bitext      classification  \n \n  task      to      predict      whether      one      sentence      entails      another.\n \n \n  The      fine-tuned      model      concatenates      the      two      sentences  \n \n  with      appended      an      EOS      token,      and      passes      them      to      both  \n \n  the      BART      encoder      and      decoder.\nIn      contrast      to      BERT,  \n \n  the      representation      of      the      EOS      token      is      used      to      classify  \n \n  the      sentences      relations.\n \n \n  ELIS      (Fanetal.,      2019),   \n \na      long-form      abstractive      ques-  \n \n  tion      answering      dataset.\nModels      generate      answers      con-  \n \n  ditioned      on      the      concatenation      of   \n \na      question      and      sup-  \n \n  porting      documents.\nXSum_      (Narayan      et      al.,      2018),   \n \na      news      summarization  \n \n  dataset      with      highly      abstractive      summaries.\nConvAI2_      (Dinan      et      al.,      2019),   \n \na      dialogue      response  \n \n  generation      task,      conditioned      on      context      and   \n \na      persona.\n \n \n  CNN/DM_      (Hermann      et      al.,      2015),   \n \na      news      summa-  \n \n  rization      dataset.\nSummaries      here      are      typically      closely  \n \n  related      to      source      sentences.\n4.3      Results\nults Results      are      shown      in      Table      1.\nSeveral      trends      are      clear:\nModel      SQuAD      1.1      MNLI      ELIS      XSum   \n \n_      ConvAI2      CNN/DM  \n \n  Fl      Acc      PPL      PPL      PPL      PPL BERT      Base      (Devlin      et      al.,      2019)      88.5      84.3   \n \n-   \n \n-   \n \n-   \n \n-       Masked      Language      Model      90.0      83.5      24.77      7.87      12.59      7.06  \n \n  Masked      Seq2seq      87.0      82.1      23.40      6.80      11.43      6.19  \n \n  Language      Model      76.7      80.1      21.40      7.00      11.51      6.56  \n \n  Permuted      Language      Model      89.1      83.7      24.03      7.69      12.23      6.96  \n \n  Multitask      Masked      Language      Model      89.2      82.4      23.73      7.50      12.39      6.74  \n \n  BART      Base  \n \n  w/      Token      Masking      90.4      84.1      25.05      7.08      11.73      6.10  \n \n  w/      Token      Deletion      90.4      84.1      24.61      6.90      11.46      5.87  \n \n  w/      Text      Infilling      90.8      84.0      24.26      6.61      11.05      5.83  \n \n  w/      Document      Rotation      77.2      75.3      53.69      17.14      19.87      10.59  \n \n  w/      Sentence      Shuffling      85.4      81.5      41.87      10.93      16.67      7.89  \n \n  w/      Text      Infilling   \n \n+      Sentence      Shuffling      90.8      83.8      24.17      6.62      11.12      5.41\nconsistently      strong      performance.\n \n \n  Performance      of      pre-training      methods      varies      signifi-  \n \n  cantly      across      tasks      The      effectiveness      of      pre-training  \n \n  methods      is      highly      dependent      on      the      task.\nFor      exam-  \n \n  ple,   \n \na      simple      language      model      achieves      the      best      ELIS  \n \n  performance,      but      the      worst      SQUAD      results.\n \n \n  Token      masking      is      crucial      Pre-training      objectives  \n \n  based      on      rotating      documents      or      permuting      sentences  \n \n  perform      poorly      in      isolation.\nThe      successful      methods  \n \n  either      use      token      deletion      or      masking,      or      self-attention  \n \n  masks.\nDeletion      appears      to      outperform      masking      on  \n \n  generation      tasks.\n \n \n  Left-to-right      pre-training      improves      generation  \n \n  The      Masked      Language      Model      and      the      Permuted  \n \n  Language      Model      perform      less      well      than      others      on  \n \n  generation,      and      are      the      only      models      we      consider      that  \n \n  do      not      include      left-to-right      auto-regressive      language  \n \n  modelling      during      pre-training.\n \n \n  Bidirectional      encoders      are      crucial      for      SQUAD      As  \n \n  noted      in      previous      work      (Devlin      et      al.,      2019),      just  \n \n  left-to-right      decoder      performs      poorly      on      SQuAD,      be-  \n \n  cause      future      context      is      crucial      in      classification      deci-  \n \n  sions.\nHowever,      BART      achieves      similar      performance  \n \n  with      only      half      the      number      of      bidirectional      layers.\n \n \n  The      pre-training      objective      is      not      the      only      important  \n \n  factor      Our      Permuted      Language      Model      performs      less  \n \n  well      than      XLNet      (Yang      et      al.,      2019).\nSome      of      this      dif-  \n \n  ference      is      likely      due      to      not      including      other      architectural  \n \n  improvements,      such      as      relative-position      embeddings      or  \n \n  segment-level      recurrence.\n \n \n  Pure      language      models      perform      best      on      ELIS      The  \n \n  ELIS      dataset      is      an      outlier,      with      much      higher      perplex-  \n \n  ities      than      other      tasks,      and      is      the      only      generation      task  \n \n  where      other      models      outperform      BART.\n \n \nA      pure      lan-  \n \n  guage      model      performs      best,      suggesting      that      BART      is  \n \n  less      effective      when      the      output      is      only      loosely      con-  \n \n  strained      by      the      input.\n \n \n  BART      achieves      the      most      consistently      strong      perfor-  \n \n  mance.\nWith      the      exception      of      ELI5,      BART      models  \n \n  using      text-infilling      perform      well      on      all      tasks.\n5\n \n   Large-scale      Pre-training      Experiments  \n \n  Recent      work      has      shown      that      downstream      performance  \n \n  can      dramatically      improve      when      pre-training      is      scaled  \n \n  to      large      batch      sizes      (Yang      et      al.,      2019;      Liu      et      al.,      2019)  \n \n  and      corpora.\nTo      test      how      well      BART      performs      in      this  \n \n  regime,      and      to      create   \n \na      useful      model      for      downstream  \n \n  tasks,      we      trained      BART      using      the      same      scale      as      the  \n \n  RoBERTa      model.\n5.1      Experimental      Setup\netup  \n \n  We      pre-train   \n \na      large      model      with      12      layers      in      each      of      the  \n \n  encoder      and      decoder,      and   \n \na      hidden      size      of      1024.\nFol-  \n \n  lowing      RoBERTa      (Liu      et      al.,      2019),      we      use   \n \na      batch      size  \n \n  of      8000,      and      train      the      model      for      500000      steps.\nDocu-  \n \n  ments      are      tokenized      with      the      same      byte-pair      encoding  \n \n  as      GPT-2      (Radford      et      al.,      2019).\nBased      on      the      results      in  \n \n  Section      §4,      we      use   \n \na      combination      of      text      infilling      and  \n \n  sentence      permutation.\nWe      mask      30%      of      tokens      in      each  \n \n  document,      and      permute      all      sentences.\nAlthough      sen-  \n \n  tence      permutation      only      shows      significant      additive      gains SQuAD      1.1      SQuAD      2.0      MNLI      SST      QQP      QNLI      STS-B      RTE      MRPC      CoLA  \n \n  EM/F1      EM/F1      m/mm      Acc      Acc      Acc      Acc      Acc      Acc      Mcc\n \n \n  BERT      84.1/90.9      79.0/81.8      86.6/-      93.2      91.3      92.3      90.0      70.4      88.0      60.6  \n \n  UniLM      -/-      80.5/83.4      87.0/85.9      94.5   \n \n-      92.7   \n \n-      70.9   \n \n-      61.1  \n \n  XLNet      89.0/94.5      86.1/88.8      89.8/-      95.6      91.8      93.9      91.8      83.8      89.2      63.6  \n \n  RoBERTa      88.9/94.6      86.5/89.4      90.2/90.2      96.4      92.2      94.7      92.4      86.6      90.9      68.0  \n \n  BART      88.8/94.6      86.1/89.2      89.9/90.1      96.6      92.5      94.9      91.2      87.0      90.4      62.8 Table      2:      Results      for      large      models      on      SQUAD      and      GLUE      tasks.\nBART      performs      comparably      to      ROBERTa      and  \n \n  XLNet,      suggesting      that      BART’s      uni-directional      decoder      layers      do      not      reduce      performance      on      discriminative      tasks."
    },
    {
      "id": 11,
      "title": "4.1      Comparison      Objectives",
      "type": "section",
      "children": [],
      "content": "4.1      Comparison      Objectives\nives  \n \n  While      many      pre-training      objectives      have      been      pro-  \n \n  posed,      fair      comparisons      between      these      have      been      dif-  \n \n  ficult      to      perform,      at      least      in      part      due      to      differences      in  \n \n  training      data,      training      resources,      architectural      differ-  \n \n  ences      between      models,      and      fine-tuning      procedures.\nWe  \n \n  ABCDE  \n \n  weee  \n \n  Pre-trained      Pre-trained  \n \n  Craters      Joo      rezapes cise)      Te      TrET Pre-trained  \n \n  Encoder Pre-trained  \n \n  Decoder aa      AB  \n \n  trrrt  \n \n  tr      ttt  \n \n  <s>A      BCD  \n \n  Randomly  \n \n  Initialized      Encoder  \n \n  TFT  \n \n  aBpByoe sentation      from      the      final      output      is      used.\n \n \n  re-implement      strong      pre-training      approaches      recently  \n \n  proposed      for      discriminative      and      generation      tasks.\nWe  \n \n  aim,      as      much      as      possible,      to      control      for      differences      un-  \n \n  related      to      the      pre-training      objective.\nHowever,      we      do  \n \n  make      minor      changes      to      the      learning      rate      and      usage      of  \n \n  layer      normalisation      in      order      to      improve      performance  \n \n  (tuning      these      separately      for      each      objective).\nFor      refer-  \n \n  ence,      we      compare      our      implementations      with      published  \n \n  numbers      from      BERT,      which      was      also      trained      for      1M  \n \n  steps      on   \n \na      combination      of      books      and      Wikipedia      data.\n \n \n  We      compare      the      following      approaches:\n \n \n  Language      Model      Similarly      to      GPT      (Radford      et      al.,  \n \n  2018),      we      train   \n \na      left-to-right      Transformer      language  \n \n  model.\nThis      model      is      equivalent      to      the      BART      decoder,  \n \n  without      cross-attention.\n \n \n  Permuted      Language      Model      Based      on      XLNet      (Yang  \n \n  et      al.,      2019),      we      sample      1/6      of      the      tokens,      and      gener-  \n \n  ate      them      in   \n \na      random      order      autoregressively.\nFor      con-  \n \n  sistency      with      other      models,      we      do      not      implement      the  \n \n  relative      positional      embeddings      or      attention      across      seg-  \n \n  ments      from      XLNet.\n \n \n  Masked      Language      Model      Following      BERT      (Devlin  \n \n  et      al.,      2019),      we      replace      15%      of      tokens      with      [MASK]  \n \n  symbols,      and      train      the      model      to      independently      predict  \n \n  the      original      tokens.\n \n \n  Multitask      Masked      Language      Model      As      in      UniLM  \n \n  (Dong      et      al.,      2019),      we      train   \n \na      Masked      Language  \n \n  Model      with      additional      self-attention      masks.\nSelf      at-  \n \n  tention      masks      are      chosen      randomly      in      with      the      follow  \n \n  proportions:      1/6      left-to-right,      1/6      right-to-left,      1/3      un-  \n \n  masked,      and      1/3      with      the      first      50%      of      tokens      unmasked  \n \n  and   \n \na      left-to-right      mask      for      the      remainder.\n \n \n  Masked      Seq-to-Seq      Inspired      by      MASS      (Song      et      al.,  \n \n  2019),      we      mask   \n \na      span      containing      50%      of      tokens,  \n \n  and      train   \n \na      sequence      to      sequence      model      to      predict      the  \n \n  masked      tokens.\n \n \n  For      the      Permuted      LM,      Masked      LM      and      Multitask  \n \n  Masked      LM,      we      use      two-stream      attention      (Yang      et      al.,  \n \n  2019)      to      efficiently      compute      likelihoods      of      the      output  \n \n  part      of      the      sequence      (using   \n \na      diagonal      self-attention  \n \n  mask      on      the      output      to      predict      words      left-to-right).\n \n \n  We      experiment      with      (1)      treating      the      task      as   \n \na      stan-  \n \n  dard      sequence-to-sequence      problem,      where      the      source  \n \n  input      to      the      encoder      and      the      target      is      the      decoder      out-  \n \n  put,      or      (2)      adding      the      source      as      prefix      to      the      target      in  \n \n  the      decoder,      with   \n \na      loss      only      on      the      target      part      of      the  \n \n  sequence.\nWe      find      the      former      works      better      for      BART  \n \n  models,      and      the      latter      for      other      models.\n \n \n  To      most      directly      compare      our      models      on      their      ability  \n \n  to      model      their      fine-tuning      objective      (the      log      likelihood  \n \n  of      the      human      text),      we      report      perplexity      in      Table      1."
    },
    {
      "id": 12,
      "title": "4.2      Tasks",
      "type": "section",
      "children": [],
      "content": "4.2      Tasks\nasks  \n \n  SQuAD  \n \n  (Rajpurkar      et      al.,      2016)a      an      extractive      ques-  \n \n  tion      answering      task      on      Wikipedia      paragraphs.\nAnswers  \n \n  are      text      spans      extracted      from   \n \na      given      document      context.\n \n \n  Similar      to      BERT      (Devlin      et      al.,      2019),      we      use      concate-  \n \n  nated      question      and      context      as      input      to      the      encoder      of  \n \n  BART,      and      additionally      pass      them      to      the      decoder.\nThe  \n \n  model      includes      classifiers      to      predict      the      start      and      end  \n \n  indices      of      each      token.\n \n \n  MNLI      (Williams      et      al.,      2017),   \n \na      bitext      classification  \n \n  task      to      predict      whether      one      sentence      entails      another.\n \n \n  The      fine-tuned      model      concatenates      the      two      sentences  \n \n  with      appended      an      EOS      token,      and      passes      them      to      both  \n \n  the      BART      encoder      and      decoder.\nIn      contrast      to      BERT,  \n \n  the      representation      of      the      EOS      token      is      used      to      classify  \n \n  the      sentences      relations.\n \n \n  ELIS      (Fanetal.,      2019),   \n \na      long-form      abstractive      ques-  \n \n  tion      answering      dataset.\nModels      generate      answers      con-  \n \n  ditioned      on      the      concatenation      of   \n \na      question      and      sup-  \n \n  porting      documents.\nXSum_      (Narayan      et      al.,      2018),   \n \na      news      summarization  \n \n  dataset      with      highly      abstractive      summaries.\nConvAI2_      (Dinan      et      al.,      2019),   \n \na      dialogue      response  \n \n  generation      task,      conditioned      on      context      and   \n \na      persona.\n \n \n  CNN/DM_      (Hermann      et      al.,      2015),   \n \na      news      summa-  \n \n  rization      dataset.\nSummaries      here      are      typically      closely  \n \n  related      to      source      sentences."
    },
    {
      "id": 13,
      "title": "4.3      Results",
      "type": "section",
      "children": [],
      "content": "4.3      Results\nults Results      are      shown      in      Table      1.\nSeveral      trends      are      clear:\nModel      SQuAD      1.1      MNLI      ELIS      XSum   \n \n_      ConvAI2      CNN/DM  \n \n  Fl      Acc      PPL      PPL      PPL      PPL BERT      Base      (Devlin      et      al.,      2019)      88.5      84.3   \n \n-   \n \n-   \n \n-   \n \n-       Masked      Language      Model      90.0      83.5      24.77      7.87      12.59      7.06  \n \n  Masked      Seq2seq      87.0      82.1      23.40      6.80      11.43      6.19  \n \n  Language      Model      76.7      80.1      21.40      7.00      11.51      6.56  \n \n  Permuted      Language      Model      89.1      83.7      24.03      7.69      12.23      6.96  \n \n  Multitask      Masked      Language      Model      89.2      82.4      23.73      7.50      12.39      6.74  \n \n  BART      Base  \n \n  w/      Token      Masking      90.4      84.1      25.05      7.08      11.73      6.10  \n \n  w/      Token      Deletion      90.4      84.1      24.61      6.90      11.46      5.87  \n \n  w/      Text      Infilling      90.8      84.0      24.26      6.61      11.05      5.83  \n \n  w/      Document      Rotation      77.2      75.3      53.69      17.14      19.87      10.59  \n \n  w/      Sentence      Shuffling      85.4      81.5      41.87      10.93      16.67      7.89  \n \n  w/      Text      Infilling   \n \n+      Sentence      Shuffling      90.8      83.8      24.17      6.62      11.12      5.41\nconsistently      strong      performance.\n \n \n  Performance      of      pre-training      methods      varies      signifi-  \n \n  cantly      across      tasks      The      effectiveness      of      pre-training  \n \n  methods      is      highly      dependent      on      the      task.\nFor      exam-  \n \n  ple,   \n \na      simple      language      model      achieves      the      best      ELIS  \n \n  performance,      but      the      worst      SQUAD      results.\n \n \n  Token      masking      is      crucial      Pre-training      objectives  \n \n  based      on      rotating      documents      or      permuting      sentences  \n \n  perform      poorly      in      isolation.\nThe      successful      methods  \n \n  either      use      token      deletion      or      masking,      or      self-attention  \n \n  masks.\nDeletion      appears      to      outperform      masking      on  \n \n  generation      tasks.\n \n \n  Left-to-right      pre-training      improves      generation  \n \n  The      Masked      Language      Model      and      the      Permuted  \n \n  Language      Model      perform      less      well      than      others      on  \n \n  generation,      and      are      the      only      models      we      consider      that  \n \n  do      not      include      left-to-right      auto-regressive      language  \n \n  modelling      during      pre-training.\n \n \n  Bidirectional      encoders      are      crucial      for      SQUAD      As  \n \n  noted      in      previous      work      (Devlin      et      al.,      2019),      just  \n \n  left-to-right      decoder      performs      poorly      on      SQuAD,      be-  \n \n  cause      future      context      is      crucial      in      classification      deci-  \n \n  sions.\nHowever,      BART      achieves      similar      performance  \n \n  with      only      half      the      number      of      bidirectional      layers.\n \n \n  The      pre-training      objective      is      not      the      only      important  \n \n  factor      Our      Permuted      Language      Model      performs      less  \n \n  well      than      XLNet      (Yang      et      al.,      2019).\nSome      of      this      dif-  \n \n  ference      is      likely      due      to      not      including      other      architectural  \n \n  improvements,      such      as      relative-position      embeddings      or  \n \n  segment-level      recurrence.\n \n \n  Pure      language      models      perform      best      on      ELIS      The  \n \n  ELIS      dataset      is      an      outlier,      with      much      higher      perplex-  \n \n  ities      than      other      tasks,      and      is      the      only      generation      task  \n \n  where      other      models      outperform      BART.\n \n \nA      pure      lan-  \n \n  guage      model      performs      best,      suggesting      that      BART      is  \n \n  less      effective      when      the      output      is      only      loosely      con-  \n \n  strained      by      the      input.\n \n \n  BART      achieves      the      most      consistently      strong      perfor-  \n \n  mance.\nWith      the      exception      of      ELI5,      BART      models  \n \n  using      text-infilling      perform      well      on      all      tasks.\n5\n \n   Large-scale      Pre-training      Experiments  \n \n  Recent      work      has      shown      that      downstream      performance  \n \n  can      dramatically      improve      when      pre-training      is      scaled  \n \n  to      large      batch      sizes      (Yang      et      al.,      2019;      Liu      et      al.,      2019)  \n \n  and      corpora.\nTo      test      how      well      BART      performs      in      this  \n \n  regime,      and      to      create   \n \na      useful      model      for      downstream  \n \n  tasks,      we      trained      BART      using      the      same      scale      as      the  \n \n  RoBERTa      model."
    },
    {
      "id": 14,
      "title": "5.1      Experimental      Setup",
      "type": "section",
      "children": [],
      "content": "5.1      Experimental      Setup\netup  \n \n  We      pre-train   \n \na      large      model      with      12      layers      in      each      of      the  \n \n  encoder      and      decoder,      and   \n \na      hidden      size      of      1024.\nFol-  \n \n  lowing      RoBERTa      (Liu      et      al.,      2019),      we      use   \n \na      batch      size  \n \n  of      8000,      and      train      the      model      for      500000      steps.\nDocu-  \n \n  ments      are      tokenized      with      the      same      byte-pair      encoding  \n \n  as      GPT-2      (Radford      et      al.,      2019).\nBased      on      the      results      in  \n \n  Section      §4,      we      use   \n \na      combination      of      text      infilling      and  \n \n  sentence      permutation.\nWe      mask      30%      of      tokens      in      each  \n \n  document,      and      permute      all      sentences.\nAlthough      sen-  \n \n  tence      permutation      only      shows      significant      additive      gains SQuAD      1.1      SQuAD      2.0      MNLI      SST      QQP      QNLI      STS-B      RTE      MRPC      CoLA  \n \n  EM/F1      EM/F1      m/mm      Acc      Acc      Acc      Acc      Acc      Acc      Mcc\n \n \n  BERT      84.1/90.9      79.0/81.8      86.6/-      93.2      91.3      92.3      90.0      70.4      88.0      60.6  \n \n  UniLM      -/-      80.5/83.4      87.0/85.9      94.5   \n \n-      92.7   \n \n-      70.9   \n \n-      61.1  \n \n  XLNet      89.0/94.5      86.1/88.8      89.8/-      95.6      91.8      93.9      91.8      83.8      89.2      63.6  \n \n  RoBERTa      88.9/94.6      86.5/89.4      90.2/90.2      96.4      92.2      94.7      92.4      86.6      90.9      68.0  \n \n  BART      88.8/94.6      86.1/89.2      89.9/90.1      96.6      92.5      94.9      91.2      87.0      90.4      62.8 Table      2:      Results      for      large      models      on      SQUAD      and      GLUE      tasks.\nBART      performs      comparably      to      ROBERTa      and  \n \n  XLNet,      suggesting      that      BART’s      uni-directional      decoder      layers      do      not      reduce      performance      on      discriminative      tasks."
    },
    {
      "id": 15,
      "title": "CNN/DailyMail      XSum       RI      R2      RL      RI      R2      RL",
      "type": "section",
      "children": [],
      "content": "CNN/DailyMail      XSum       RI      R2      RL      RI      R2      RL\n \n \n  Lead-3      40.42      17.62      36.67      16.30      1.60      11.95  \n \n  PTGEN      (See      et      al.,      2017)      36.44      15.66      33.42      29.70      9.21      23.24  \n \n  PTGEN+COV      (See      et      al.,      2017)      39.53      17.28      36.38      28.10      8.02      21.72  \n \n  UniLM      43.33      20.21      40.51   \n \n-   \n \n-   \n \n-       BERTSUMABS      (Liu   \n \n&      Lapata,      2019)      41.72      19.39      38.76      38.76      16.33      31.15  \n \n  BERTSUMEXTABS      (Liu   \n \n&      Lapata,      2019)      42.13      19.60      39.18      38.81      16.50      31.27 BART      44.16      21.28      40.90      45.14      22.27      37.25 Table      3:      Results      on      two      standard      summarization      datasets.\nBART      outperforms      previous      work      on      summarization      on\n \n \n  on      the      CNN/DM      summarization      dataset,      we      hypothe-      ConvAI2  \n \n  sised      that      larger      pre-trained      models      may      be      better      able      Valid      F1      Valid      PPL  \n \n  to   \n \n|      fi      this      task.\nTo      help      th      del      better      fit      th  \n \n  RE      aa      ae      Pai      Cae      Seq2Seq+      Attention      16.02      35.07  \n \n  data,      we      disabled      dropout      for      the      final      10%      of      training   \n \n.    \n.\n     Best      System      19.09      17.51  \n \n  steps.\nWe      use      the      same      pre-training      data      as      Liu      et      al.\n(2019),      consisting      of      160Gb      of      news,      books,      stories,  \n \n  and      web      text.\nBART      20.72      11.85 5.2      Discriminative      Tasks  \n \n  Table   \n \n2      compares      the      performance      of      BART      with      sev-  \n \n  eral      recent      approaches      on      the      well-studied      SQUAD      and  \n \n  GLUE      tasks      (Warstadt      et      al.,      2018;      Socher      et      al.,      2013;  \n \n  Dolan   \n \n&      Brockett,      2005;      Agirre      et      al.,      2007;      Williams  \n \n  et      al.,      2018;      Dagan      et      al.,      2006;      Levesque      et      al.,      2011).\n \n \n  Table      4:      BART      outperforms      previous      work      on      conver-  \n \n  sational      response      generation.\nPerplexities      are      renor-  \n \n  malized      based      on      official      tokenizer      for      ConvAI2.\n \n \n  The      most      directly      comparable      baseline      is      ROBERTa,  \n \n  which      was      pre-trained      with      the      same      resources,      but  \n \n  a      different      objective.\nOverall,      BART      performs      simi-  \n \n  larly,      with      only      small      differences      between      the      models  \n \n  on      most      tasks.\nsuggesting      that      BART’s      improvements  \n \n  on      generation      tasks      do      not      come      at      the      expense      of      clas-  \n \n  sification      performance.\n \n \n  Summarization      To      provide   \n \na      comparison      with      the  \n \n  state-of-the-art      in      summarization,      we      present      results  \n \n  on      two      summarization      datasets,      CNN/DailyMail      and  \n \n  XSum,      which      have      distinct      properties.\n \n \n  Summaries      in      the      CNN/DailyMail      tend      to      resemble  \n \n  source      sentences.\nExtractive      models      do      well      here,      and  \n \n  even      the      baseline      of      the      first-three      source      sentences      is  \n \n  highly      competitive.\nNevertheless,      BART      outperforms  \n \n  all      existing      work.\n5.3.      Generation      Tasks\nGeneration      Tasks\n |       We      also      experiment      with      several      text      generation      tasks.       BART      is      fine-tuned      as      a      standard      sequence-to-sequence       model      from      the      input      to      the      output      text.      During      fine-       tuning      we      use      a      label      smoothed      cross      entropy      loss       (Pereyra      et      al.,      2017),      with      the      smoothing      parameter       set      to      0.1.      During      generation,      we      set      beam      size      as      5,       remove      duplicated      trigrams      in      beam      search,      and      tuned       the      model      with      min-len,      max-len,      length      penalty      on      the       validation      set      (Fan      et      al.,      2017). | 6.0      points      on      all      ROUGE      metrics—trepresenting   \n \na      sig-  \n \n  nificant      advance      in      performance      on      this      problem.\nQual-  \n \n  itatively,      sample      quality      is      high      (see      86).\n \n \n  In      contrast,      XSum      is      highly      abstractive,      and      extrac-  \n \n  tive      models      perform      poorly.\nBART      outperforms      the  \n \n  best      previous      work,      which      leverages      BERT,      by      roughly  \n \n  Dialogue      We      evaluate      dialogue      response      generation  \n \n  on      CONVAI2      (Dinan      et      al.,      2019),      in      which      agents  \n \n  must      generate      responses      conditioned      on      both      the      pre-  \n \n  vious      context      and   \n \na      textually-specified      persona.\nBART  \n \n  outperforms      previous      work      on      two      automated      metrics.\n"
    },
    {
      "id": 16,
      "title": "5.3.      Generation      Tasks",
      "type": "section",
      "children": [],
      "content": "5.3.      Generation      Tasks\nGeneration      Tasks\n |       We      also      experiment      with      several      text      generation      tasks.       BART      is      fine-tuned      as      a      standard      sequence-to-sequence       model      from      the      input      to      the      output      text.      During      fine-       tuning      we      use      a      label      smoothed      cross      entropy      loss       (Pereyra      et      al.,      2017),      with      the      smoothing      parameter       set      to      0.1.      During      generation,      we      set      beam      size      as      5,       remove      duplicated      trigrams      in      beam      search,      and      tuned       the      model      with      min-len,      max-len,      length      penalty      on      the       validation      set      (Fan      et      al.,      2017). | 6.0      points      on      all      ROUGE      metrics—trepresenting   \n \na      sig-  \n \n  nificant      advance      in      performance      on      this      problem.\nQual-  \n \n  itatively,      sample      quality      is      high      (see      86).\n \n \n  In      contrast,      XSum      is      highly      abstractive,      and      extrac-  \n \n  tive      models      perform      poorly.\nBART      outperforms      the  \n \n  best      previous      work,      which      leverages      BERT,      by      roughly  \n \n  Dialogue      We      evaluate      dialogue      response      generation  \n \n  on      CONVAI2      (Dinan      et      al.,      2019),      in      which      agents  \n \n  must      generate      responses      conditioned      on      both      the      pre-  \n \n  vious      context      and   \n \na      textually-specified      persona.\nBART  \n \n  outperforms      previous      work      on      two      automated      metrics.\n"
    },
    {
      "id": 17,
      "title": "Rl      R2      RL",
      "type": "section",
      "children": [],
      "content": "Rl      R2      RL\n \n \n  Best      Extractive      23.55      3.1      17.5  \n \n  Language      Model      27.8      47      23.1  \n \n  Seq2Seq      28.3      5.1      22.8  \n \n  Seq2Seq      Multitask      28.9      54      23.1  \n \n  BART      30.6      6.2      24.3  \n \n  Table      5:      BART      achieves      state-of-the-art      results      on  \n \n  the      challenging      ELI5      abstractive      question      answering  \n \n  dataset.\nComparison      models      are      from      Fan      et      al.\n(2019)."
    },
    {
      "id": 18,
      "title": "RO-EN",
      "type": "section",
      "children": [],
      "content": "RO-EN\nBaseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96\n \n \n  Table      6:      The      performance      (BLEU)      of      baseline      and  \n \n  BART      on      WMT’16      RO-EN      augmented      with      back-  \n \n  translation      data.\nBART      improves      over   \n \na      strong      back-  \n \n  translation      (BT)      baseline      by      using      monolingual      English  \n \n  pre-training.\n \n \n  Abstractive      QA      We      use      the      recently      proposed      ELIS  \n \n  dataset      to      test      the      model’s      ability      to      generate      long      free-  \n \n  form      answers.\nWe      find      BART      outperforms      the      best      pre-  \n \n  vious      work      by      1.2      ROUGE-L,      but      the      dataset      remains  \n \n  a      challenging,      because      answers      are      only      weakly      speci-  \n \n  fied      by      the      question.\n5.4      Translation\ntion  \n \n  We      also      evaluated      performance      on      WMT16      Romanian-  \n \n  English,      augmented      with      back-translation      data  \n \n  from      Sennrich      et      al.\n(2016).\nWe      use   \n \na      6-layer  \n \n  transformer      source      encoder      to      map      Romanian      into  \n \n  a      representation      that      BART      is      able      to      de-noise      into  \n \n  English,      following      the      approach      introduced      in      83.4.  \n \n  Experiment      results      are      presented      in      Table      6.      We  \n \n  compare      our      results      against   \n \na      baseline      Transformer  \n \n  architecture      (Vaswani      et      al.,      2017)      with      Transformer-  \n \n  large      settings      (the      baseline      row).\nWe      show      the  \n \n  performance      of      both      steps      of      our      model      in      the      fixed  \n \n  BART      and      tuned      BART      rows.\nFor      each      row      we  \n \n  experiment      on      the      original      WMT16      Romanian-English  \n \n  augmented      with      back-translation      data.\nWe      use   \n \na       beam      width      of   \n \n5      and   \n \na      length      penalty      of   \n \na   \n \n=      1.  \n \n  Preliminary      results      suggested      that      our      approach      was  \n \n  less      effective      without      back-translation      data,      and      prone  \n \n  to      overfitting—future      work      should      explore      additional  \n \n  regularization      techniques.\n6\n \n   Qualitative      Analysis  \n \n  BART      shows      large      improvements      on      summarization  \n \n  metrics,      of      up      to   \n \n6      points      over      the      prior      state-of-the-art.\n \n \n  To      understand      BART’s      performance      beyond      automated  \n \n  metrics,      we      analyse      its      generations      qualitatively.\n \n \n  Table   \n \n7      shows      example      summaries      generated      by  \n \n  BART.\nExamples      are      taken      from      WikiNews      articles  \n \n  published      after      the      creation      of      the      pre-training      corpus,  \n \n  to      eliminate      the      possibility      of      the      events      described      be-  \n \n  ing      present      in      the      model’s      training      data.\nFollowing  \n \n  Narayan      et      al.\n(2018),      we      remove      the      first      sentence      of  \n \n  the      article      prior      to      summarizing      it,      so      there      is      no      easy  \n \n  extractive      summary      of      the      document.\n \n \n  Unsurprisingly,      model      output      is      fluent      and      grammat-  \n \n  ical      English.\nHowever,      model      output      is      also      highly      ab-  \n \n  stractive,      with      few      phrases      copied      from      the      input.\nThe  \n \n  output      is      also      generally      factually      accurate,      and      inte-  \n \n  grates      supporting      evidence      from      across      the      input      doc-  \n \n  ument      with      background      knowledge      (for      example,      cor-  \n \n  rectly      completing      names,      or      inferring      that      PG&E      oper-  \n \n  ates      in      California).\nIn      the      first      example,      inferring      that  \n \n  fish      are      protecting      reefs      from      global      warming      requires  \n \n  non-trivial      inference      from      the      text.\nHowever,      the      claim  \n \n  that      the      work      was      published      in      Science      is      not      supported  \n \n  by      the      source.\n \n \n  These      samples      demonstrate      that      the      BART      pretrain-  \n \n  ing      has      learned   \n \na      strong      combination      of      natural      lan-  \n \n  guage      understanding      and      generation.\n7\n \n   Related      Work  \n \n  Early      methods      for      pretraining      were      based      on      language  \n \n  models.\nGPT      (Radford      et      al.,      2018)      only      models      left-  \n \n  ward      context,      which      is      problematic      for      some      tasks.\n \n \n  ELMo      (Peters      et      al.,      2018)      concatenates      left-only      and  \n \n  right-only      representations,      but      does      not      pre-train      inter-  \n \n  actions      between      these      features.\nRadford      et      al.\n(2019)  \n \n  demonstrated      that      very      large      language      models      can      act       as      unsupervised      multitask      models.\n \n \n  MASS      (Song      et      al.,      2019)      is      perhaps      the      most      similar  \n \n  model      to      BART.\nAn      input      sequence      where   \n \na      contiguous  \n \n  span      of      tokens      is      masked      is      mapped      to   \n \na      sequence      con-  \n \n  sisting      of      the      missing      tokens.\nMASS      is      less      effective  \n \n  for      discriminative      tasks,      because      disjoint      sets      of      tokens  \n \n  are      fed      into      the      encoder      and      decoder.\n \n \n  BERT      (Devlin      et      al.,      2019)      introduced      masked      lan-  \n \n  guage      modelling,      which      allows      pre-training      to      learn      in-  \n \n  teractions      between      left      and      right      context      words.\nRe-  \n \n  cent      work      has      shown      that      very      strong      performance      can  \n \n  be      achieved      by      training      for      longer      (Liu      et      al.,      2019),  \n \n  by      tying      parameters      across      layers      (Lan      et      al.,      2019),  \n \n  and      by      masking      spans      instead      of      words      (Joshi      et      al.,  \n \n  2019).\nPredictions      are      not      made      auto-regressively,      re-  \n \n  ducing      the      effectiveness      of      BERT      for      generation      tasks.\n \n \n  UniLM      (Dong      et      al.,      2019)      fine-tunes      BERT      with      an  \n \n  ensemble      of      masks,      some      of      which      allow      only      leftward  \n \n  context.\nLike      BART,      this      allows      UniLM      to      be      used      for  \n \n  both      generative      and      discriminative      tasks.\n \n \nA      difference  \n \n  is      that      UniLM      predictions      are      conditionally      indepen-  \n \n  dent,      whereas      BART’s      are      autoregressive.\nBART      re-  \n \n  duces      the      mismatch      between      pre-training      and      genera-  \n \n  tion      tasks,      because      the      decoder      is      always      trained      on      un-  \n \n  corrupted      context.\n \n \n  XL-Net      (Yang      et      al.,      2019)      extends      BERT      by      pre-\n |       Source      Document      (abbreviated) |       BART      Summary\n |       The      researchers      examined      three      types      of      coral      in      reefs      off      the coast of Fiji The researchers found when fish were plentiful,       they      would      eat      algae      and      seaweed      off      the      corals,      which      appeared       to      leave      them      more      resistant      to      the      bacterium      Vibrio      coralliilyti-       cus,      a      bacterium      associated      with      bleaching.      The      researchers      sug-       gested      the      algae,      like      warming      temperatures,      might      render      the       corals’      chemical      defenses      less      effective,      and      the      fish      were      pro-       tecting      the      coral      by      removing      the      algae. |       Fisheries      off      the      coast      of      Fiji      are      protect-       ing      coral      reefs      from      the      effects      of      global       warming,      according      to      a      study      in      the      jour-       nal      Science.\n |       Sacoolas,      who      has      immunity      as      a      diplomat’s      wife,      was      involved in a traffic collision Prime Minister Johnson was questioned       about      the      case      while      speaking      to      the      press      at      a      hospital      in      Wat- ford. He said, “I hope that Anne Sacoolas will come back       if      we      can’t      resolve      it      then      of      course      I      will      be      raising      it      myself       personally      with      the      White      House.” |       Boris      Johnson      has      said      he      will      raise      the      is-       sue      of      US      diplomat      Anne      Sacoolas’      diplo-       matic      immunity      with      the      White      House.\n |       According      to      Syrian      state      media,      government      forces      began      de- ploying into previously SDF controlled territory yesterday.       On      October      6,      US      President      Donald      Trump      and      Turkish      Presi-       dent      Recep      Tayyip      Erdoan      spoke      on      the      phone.      Then      both      na-       tions      issued      statements      speaking      of      an      imminent      incursion      into northeast Syria On Wednesday, Turkey began a military       offensive      with      airstrikes      followed      by      a      ground      invasion. |       Syrian      government      forces      have      entered       territory      held      by      the      US-backed      Syrian       Democratic      Forces      (SDF)      in      response      to       Turkey’s      incursion      into      the      region.\n |       This      is      the      first      time      anyone      has      been      recorded      to      run      a      full       marathon      of      42.195      kilometers      (approximately      26      miles)      under       this      pursued      landmark      time.      It      was      not,      however,      an      officially       sanctioned      world      record,      as      it      was      not      an      ’open      race”      of      the       IAAF.      His      time      was      1      hour      59      minutes      40.2      seconds.      Kipchoge       ran      in      Vienna,      Austria.      It      was      an      event      specifically      designed      to       help      Kipchoge      break      the      two      hour      barrier. |       Kenyan      runner      Eliud      Kipchoge      has      run      a       marathon      in      less      than      two      hours.\n |       PG&E      stated      it      scheduled      the      blackouts      in      response      to      forecasts       for      high      winds      amid      dry      conditions.      The      aim      is      to      reduce      the      risk       of      wildfires.      Nearly      800      thousand      customers      were      scheduled      to       be      affected      by      the      shutoffs      which      were      expected      to      last      through       at      least      midday      tomorrow. |       Power      has      been      turned      off      to      millions      of       customers      in      California      as      part      of      a      power       shutoff      plan.\n\n \n \n  dicting      masked      tokens      auto-regressively      in   \n \na      permuted  \n \n  order.\nThis      objective      allows      predictions      to      condition      on  \n \n  both      left      and      right      context.\nIn      contrast,      the      BART      de-  \n \n  coder      works      left-to-right      during      pre-training,      matching  \n \n  the      setting      during      generation.\n \n \n  Several      papers      have      explored      using      pre-trained      rep-  \n \n  resentations      to      improve      machine      translation.\nThe  \n \n  largest      improvements      have      come      from      pre-training      on  \n \n  both      source      and      target      languages      (Song      et      al.,      2019;  \n \n  Lample   \n \n&      Conneau,      2019),      but      this      requires      pre-  \n \n  training      on      all      languages      of      interest.\nOther      work      has  \n \n  shown      that      encoders      can      be      improved      using      pre-trained  \n \n  representations      (Edunov      et      al.,      2019),      but      gains      in      de-  \n \n  coders      are      more      limited.\nWe      show      how      BART      can      be  \n \n  used      to      improve      machine      translation      decoders.\n8\n \n   Conclusions  \n \n  We      introduced      BART,   \n \na      pre-training      approach      that  \n \n  learns      to      map      corrupted      documents      to      the      original.\n \n \n  BART      achieves      similar      performance      to      ROBERTa      on  \n \n  discriminative      tasks,      while      achieving      new      state-of-the-  \n \n  art      results      on   \n \na      number      of      text      generation      tasks.\nFu-  \n \n  ture      work      should      explore      new      methods      for      corrupting  \n \n  documents      for      pre-training,      perhaps      tailoring      them      to  \n \n  specific      end      tasks.\nReferences\n \n \n  Eneko      Agirre,      Llu’is      M‘arquez,      and      Richard      Wicen-  \n \n  towski      (eds.).\nProceedings      of      the      Fourth      Interna-  \n \n  tional      Workshop      on      Semantic      Evaluations      (SemEval-  \n \n  2007).\nAssociation      for      Computational      Linguistics,  \n \n  Prague,      Czech      Republic,      June      2007.\n \n \n  Ido      Dagan,      Oren      Glickman,      and      Bernardo      Magnini.\n \n \n  The      PASCAL      recognising      textual      entailment      chal-  \n \n  lenge.\nIn      Machine      learning      challenges.\nevaluat-  \n \n  ing      predictive      uncertainty,      visual      object      classifica-  \n \n  tion,      and      recognising      tectual      entailment,      pp.\n177—  \n \n  190.\nSpringer,      2006.\n \n \n  Jacob      Devlin,      Ming-Wei      Chang,      Kenton      Lee,      and  \n \n  Kristina      Toutanova.\nBERT:      Pre-training      of      deep  \n \n  bidirectional      transformers      for      language      understand-  \n \n  ing.\nIn      Proceedings      of      the      2019      Conference      of      the  \n \n  North      American      Chapter      of      the      Association      for      Com-  \n \n  putational      Linguistics:      Human      Language      Technolo-  \n \n  gies,      Volume   \n \nI      (Long      and      Short      Papers),      pp.\n4171-  \n \n  4186,      Minneapolis,      Minnesota,      June      2019.\nAssocia-  \n \n  tion      for      Computational      Linguistics.\ndoi:      10.18653/  \n \n  vI/N19-1423.\nURL      https://www.aclweb.\n \n \n  org/anthology/N19-1423.\n \n \n  Emily      Dinan,      Varvara      Logacheva,      Valentin      Malykh,  \n \n  Alexander      Miller,      Kurt      Shuster,      Jack      Urbanek,  \n \n  Douwe      Kiela,      Arthur      Szlam,      Iulian      Serban,      Ryan  \n \n  Lowe,      et      al.\nThe      second      conversational      in-  \n \n  telligence      challenge      (convai2).\narXiv      preprint  \n \n  arXiv:      1902.00098,      2019.\n \n \n  William   \n \nB      Dolan      and      Chris      Brockett.\nAutomatically  \n \n  constructing   \n \na      corpus      of      sentential      paraphrases.\nIn  \n \n  Proceedings      of      the      International      Workshop      on      Para-  \n \n  phrasing,      2005.\n \n \n  Li      Dong,      Nan      Yang,      Wenhui      Wang,      Furu      Wei,      Xi-  \n \n  aodong      Liu,      Yu      Wang,      Jianfeng      Gao,      Ming      Zhou,  \n \n  and      Hsiao-Wuen      Hon.\nUnified      language      model      pre-  \n \n  training      for      natural      language      understanding      and      gen-  \n \n  eration.\narXiv      preprint      arXiv:      1905.03197,      2019.\n \n \n  Sergey      Edunov,      Alexei      Baevski,      and      Michael      Auli.\n \n \n  Pre-trained      language      model      representations      for      lan-  \n \n  guage      generation.\nIn      Proceedings      of      the      2019      Con-  \n \n  ference      of      the      North      American      Chapter      of      the      Asso-  \n \n  ciation      for      Computational      Linguistics:      Human      Lan-  \n \n  guage      Technologies,      Volume   \n \n1      (Long      and      Short      Pa-  \n \n  pers),      2019.\n \n \n  Angela      Fan,      David      Grangier,      and      Michael      Auli.\nCon-  \n \n  trollable      abstractive      summarization.\narXiv      preprint  \n \n  arXiv:      1711.05217,      2017.\n \n \n  Angela      Fan,      Yacine      Jernite,      Ethan      Perez,      David  \n \n  Grangier,      Jason      Weston,      and      Michael      Auli.\nEli5:  \n \n  Long      form      question      answering.\narXiv      preprint  \n \n  arXiv:      1907.09190,      2019.\n \n \n  Dan      Hendrycks      and      Kevin      Gimpel.\nGaussian      error      lin-  \n \n  ear      units      (gelus).\narXiv      preprint      arXiv:      1606.08415,  \n \n  2016.\n \n \n  Karl      Moritz      Hermann,      Tomas      Kocisky,      Edward  \n \n  Grefenstette,      Lasse      Espeholt,      Will      Kay,      Mustafa      Su-  \n \n  leyman,      and      Phil      Blunsom.\nTeaching      machines      to  \n \n  read      and      comprehend.\nIn      Advances      in      neural      infor-  \n \n  mation      processing      systems,      pp.\n1693-1701,      2015.\n \n \n  Mandar      Joshi,      Danqi      Chen,      Yinhan      Liu,      Daniel   \n \nS      Weld,  \n \n  Luke      Zettlemoyer,      and      Omer      Levy.\nSpanbert:      Im-  \n \n  proving      pre-training      by      representing      and      predicting  \n \n  spans.\narXiv      preprint      arXiv:      1907.10529,      2019.\n \n \n  Guillaume      Lample      and      Alexis      Conneau.\n \n \n—      Cross-  \n \n  lingual      language      model      pretraining.\narXiv      preprint  \n \n  arXiv:      1901.07291,      2019.\n \n \n  Zhenzhong      Lan,      Mingda      Chen,      Sebastian      Goodman,  \n \n  Kevin      Gimpel,      Piyush      Sharma,      and      Radu      Sori-  \n \n  cut.\nAlbert:   \n \nA      lite      bert      for      self-supervised      learn-  \n \n  ing      of      language      representations.\narXiv      preprint  \n \n  arXiv:      1909.11942,      2019.\n \n \n  Hector   \n \nJ      Levesque,      Ernest      Davis,      and      Leora      Morgen-  \n \n  stern.\nThe      Winograd      schema      challenge.\nIn      AAAI  \n \n  Spring      Symposium:      Logical      Formalizations      of      Com-  \n \n  monsense      Reasoning,      volume      46,      pp.\n47,      2011.\n \n \n  Yang      Liu      and      Mirella      Lapata.\n \n \n  tion      with      pretrained      encoders.\n \n \n  arXiv:      1908.08345,      2019.  \n \n  Text      summariza-  \n \n  arXiv      preprint  \n \n  Yinhan      Liu,      Myle      Ott,      Naman      Goyal,      Jingfei      Du,      Man-  \n \n  dar      Joshi,      Dangi      Chen,      Omer      Levy,      Mike      Lewis,  \n \n  Luke      Zettlemoyer,      and      Veselin      Stoyanov.\nRoberta:\n \n \n \nA      robustly      optimized      bert      pretraining      approach.\n \n \n  arXiv      preprint      arXiv:      1907.11692,      2019.\n \n \n  Tomas      Mikolov,      Kai      Chen,      Greg      Corrado,      and      Jeffrey  \n \n  Dean.\nEfficient      estimation      of      word      representations  \n \n  in      vector      space.\narXiv      preprint      arXiv:1301.3781,  \n \n  2013.\n \n \n  Shashi      Narayan,      Shay   \n \nB      Cohen,      and      Mirella      Lapata.\n \n \n  Don’t      give      me      the      details,      just      the      summary!\ntopic-  \n \n  aware      convolutional      neural      networks      for      extreme  \n \n  summarization.\narXiv      preprint      arXiv:      1808.08745,  \n \n  2018.\n \n \n  Gabriel      Pereyra,      George      Tucker,      Jan      Chorowski,  \n \n  Lukasz      Kaiser,      and      Geoffrey      Hinton.\nRegularizing  \n \n  neural      networks      by      penalizing      confident      output      dis-  \n \n  tributions.\narXiv      preprint      arXiv:      1701.06548,      2017.\n \n \n  Matthew      E      Peters,      Mark      Neumann,      Mohit      Iyyer,      Matt  \n \n  Gardner,      Christopher      Clark,      Kenton      Lee,      and      Luke  \n \n  Zettlemoyer.\nDeep      contextualized      word      representa-  \n \n  tions.\narXiv      preprint      arXiv:      1802.05365,      2018.\n \n \n  Alec      Radford,      Karthik      Narasimhan,      Tim      Salimans,  \n \n  and      Ilya      Sutskever.\nImproving      language      un-  \n \n  derstanding      by      generative      pre-training.\nURL  \n \n  https://s3-us-west-2.\n \n \n|      amazonaws.\n \n \n—      com/openai-  \n \n  assets/researchcovers/languageunsupervised/language  \n \n  understanding      paper.\npdf,      2018.\n \n \n  Alec      Radford,      Jeffrey      Wu,      Rewon      Child,      David      Luan,  \n \n  Dario      Amodei,      and      Ilya      Sutskever.\nLanguage      mod-  \n \n  els      are      unsupervised      multitask      learners.\nOpenAI  \n \n  Blog,      1(8),      2019.\n \n \n  Pranav      Rajpurkar,      Jian      Zhang,      Konstantin      Lopyrev,  \n \n  and      Percy      Liang.\nSquad:      100,000+      questions      for  \n \n  machine      comprehension      of      text.\narXiv      preprint  \n \n  arXiv:      1606.05250,      2016.\n \n \n  Abigail      See,      Peter   \n \nJ      Liu,      and      Christopher   \n \nD       Manning.\nGet      to      the      point:      Summarization  \n \n  with      pointer-generator      networks.\narXiv      preprint  \n \n  arXiv:      1704.04368,      2017.\n \n \n  Rico      Sennrich,      Barry      Haddow,      and      Alexandra      Birch.\n \n \n  Edinburgh      neural      machine      translation      systems      for  \n \n  WMT      16.\nIn      Proceedings      of      the      First      Conference  \n \n  on      Machine      Translation:      Volume      2,      Shared      Task      Pa-  \n \n  pers,      2016.\n \n \n  Richard      Socher,      Alex      Perelygin,      Jean      Wu,      Jason  \n \n  Chuang,      Christopher   \n \nD      Manning,      Andrew      Ng,      and  \n \n  Christopher      Potts.\nRecursive      deep      models      for      se-  \n \n  mantic      compositionality      over   \n \na      sentiment      treebank.\n \n \n  In      Proceedings      of      EMNLP,      pp.\n1631-1642,      2013.\n \n \n  Kaitao      Song,      Xu      Tan,      Tao      Qin,      Jianfeng      Lu,      and      Tie-  \n \n  Yan      Liu.\nMass:      Masked      sequence      to      sequence      pre-  \n \n  training      for      language      generation.\nIn      International  \n \n  Conference      on      Machine      Learning,      2019.\n \n \n  Ashish      Vaswani,      Noam      Shazeer,      Niki      Parmar,      Jakob  \n \n  Uszkoreit,      Llion      Jones,      Aidan      N      Gomez,      Lukasz  \n \n  Kaiser,      and      Ilia      Polosukhin.\nAttention      is      all      you  \n \n  need.\nIn      Advances      in      neural      information      processing  \n \n  systems,      pp.\n5998-6008,      2017.\n \n \n  Alex      Wang,      Amanpreet      Singh,      Julian      Michael,      Felix  \n \n  Hill,      Omer      Levy,      and      Samuel      R      Bowman.\nGlue:\n \n \n \nA      multi-task      benchmark      and      analysis      platform      for  \n \n  natural      language      understanding.\narXiv      preprint  \n \n  arXiv:      1804.07461,      2018.\n \n \n  Alex      Warstadt,      Amanpreet      Singh,      and      Samuel      R.  \n \n  Bowman.\nNeural      network      acceptability      judgments.\n \n \n  arXiv      preprint      1805.12471,      2018.\n \n \n  Adina      Williams,      Nikita      Nangia,      and      Samuel      R      Bow-  \n \n  man.\n \n \nA   \n \n_      broad-coverage      challenge      corpus      for  \n \n  sentence      understanding      through      inference.\narXiv  \n \n  preprint      arXiv:      1704.05426,      2017.\n \n \n  Adina      Williams,      Nikita      Nangia,      and      Samuel      R.      Bow-  \n \n  man.\n \n \nA      broad-coverage      challenge      corpus      for      sen-  \n \n  tence      understanding      through      inference.\nIn      Proceed-  \n \n  ings      of      NAACL-HLT,      2018.\n \n \n  Zhilin      Yang,      Zihang      Dai,      Yiming      Yang,      Jaime  \n \n  Carbonell,      Ruslan      Salakhutdinov,      and      Quoc   \n \nV       Le.\nXlInet:      Generalized      autoregressive      pretrain-  \n \n  ing      for      language      understanding.\narXiv      preprint  \n \n  arXiv:      1906.08237,      2019."
    },
    {
      "id": 19,
      "title": "Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96",
      "type": "section",
      "children": [],
      "content": "Baseline      36.80 Fixed      BART      36.29 Tuned      BART      37.96\n \n \n  Table      6:      The      performance      (BLEU)      of      baseline      and  \n \n  BART      on      WMT’16      RO-EN      augmented      with      back-  \n \n  translation      data.\nBART      improves      over   \n \na      strong      back-  \n \n  translation      (BT)      baseline      by      using      monolingual      English  \n \n  pre-training.\n \n \n  Abstractive      QA      We      use      the      recently      proposed      ELIS  \n \n  dataset      to      test      the      model’s      ability      to      generate      long      free-  \n \n  form      answers.\nWe      find      BART      outperforms      the      best      pre-  \n \n  vious      work      by      1.2      ROUGE-L,      but      the      dataset      remains  \n \n  a      challenging,      because      answers      are      only      weakly      speci-  \n \n  fied      by      the      question.\n5.4      Translation\ntion  \n \n  We      also      evaluated      performance      on      WMT16      Romanian-  \n \n  English,      augmented      with      back-translation      data  \n \n  from      Sennrich      et      al.\n(2016).\nWe      use   \n \na      6-layer  \n \n  transformer      source      encoder      to      map      Romanian      into  \n \n  a      representation      that      BART      is      able      to      de-noise      into  \n \n  English,      following      the      approach      introduced      in      83.4.  \n \n  Experiment      results      are      presented      in      Table      6.      We  \n \n  compare      our      results      against   \n \na      baseline      Transformer  \n \n  architecture      (Vaswani      et      al.,      2017)      with      Transformer-  \n \n  large      settings      (the      baseline      row).\nWe      show      the  \n \n  performance      of      both      steps      of      our      model      in      the      fixed  \n \n  BART      and      tuned      BART      rows.\nFor      each      row      we  \n \n  experiment      on      the      original      WMT16      Romanian-English  \n \n  augmented      with      back-translation      data.\nWe      use   \n \na       beam      width      of   \n \n5      and   \n \na      length      penalty      of   \n \na   \n \n=      1.  \n \n  Preliminary      results      suggested      that      our      approach      was  \n \n  less      effective      without      back-translation      data,      and      prone  \n \n  to      overfitting—future      work      should      explore      additional  \n \n  regularization      techniques.\n6\n \n   Qualitative      Analysis  \n \n  BART      shows      large      improvements      on      summarization  \n \n  metrics,      of      up      to   \n \n6      points      over      the      prior      state-of-the-art.\n \n \n  To      understand      BART’s      performance      beyond      automated  \n \n  metrics,      we      analyse      its      generations      qualitatively.\n \n \n  Table   \n \n7      shows      example      summaries      generated      by  \n \n  BART.\nExamples      are      taken      from      WikiNews      articles  \n \n  published      after      the      creation      of      the      pre-training      corpus,  \n \n  to      eliminate      the      possibility      of      the      events      described      be-  \n \n  ing      present      in      the      model’s      training      data.\nFollowing  \n \n  Narayan      et      al.\n(2018),      we      remove      the      first      sentence      of  \n \n  the      article      prior      to      summarizing      it,      so      there      is      no      easy  \n \n  extractive      summary      of      the      document.\n \n \n  Unsurprisingly,      model      output      is      fluent      and      grammat-  \n \n  ical      English.\nHowever,      model      output      is      also      highly      ab-  \n \n  stractive,      with      few      phrases      copied      from      the      input.\nThe  \n \n  output      is      also      generally      factually      accurate,      and      inte-  \n \n  grates      supporting      evidence      from      across      the      input      doc-  \n \n  ument      with      background      knowledge      (for      example,      cor-  \n \n  rectly      completing      names,      or      inferring      that      PG&E      oper-  \n \n  ates      in      California).\nIn      the      first      example,      inferring      that  \n \n  fish      are      protecting      reefs      from      global      warming      requires  \n \n  non-trivial      inference      from      the      text.\nHowever,      the      claim  \n \n  that      the      work      was      published      in      Science      is      not      supported  \n \n  by      the      source.\n \n \n  These      samples      demonstrate      that      the      BART      pretrain-  \n \n  ing      has      learned   \n \na      strong      combination      of      natural      lan-  \n \n  guage      understanding      and      generation.\n7\n \n   Related      Work  \n \n  Early      methods      for      pretraining      were      based      on      language  \n \n  models.\nGPT      (Radford      et      al.,      2018)      only      models      left-  \n \n  ward      context,      which      is      problematic      for      some      tasks.\n \n \n  ELMo      (Peters      et      al.,      2018)      concatenates      left-only      and  \n \n  right-only      representations,      but      does      not      pre-train      inter-  \n \n  actions      between      these      features.\nRadford      et      al.\n(2019)  \n \n  demonstrated      that      very      large      language      models      can      act       as      unsupervised      multitask      models.\n \n \n  MASS      (Song      et      al.,      2019)      is      perhaps      the      most      similar  \n \n  model      to      BART.\nAn      input      sequence      where   \n \na      contiguous  \n \n  span      of      tokens      is      masked      is      mapped      to   \n \na      sequence      con-  \n \n  sisting      of      the      missing      tokens.\nMASS      is      less      effective  \n \n  for      discriminative      tasks,      because      disjoint      sets      of      tokens  \n \n  are      fed      into      the      encoder      and      decoder.\n \n \n  BERT      (Devlin      et      al.,      2019)      introduced      masked      lan-  \n \n  guage      modelling,      which      allows      pre-training      to      learn      in-  \n \n  teractions      between      left      and      right      context      words.\nRe-  \n \n  cent      work      has      shown      that      very      strong      performance      can  \n \n  be      achieved      by      training      for      longer      (Liu      et      al.,      2019),  \n \n  by      tying      parameters      across      layers      (Lan      et      al.,      2019),  \n \n  and      by      masking      spans      instead      of      words      (Joshi      et      al.,  \n \n  2019).\nPredictions      are      not      made      auto-regressively,      re-  \n \n  ducing      the      effectiveness      of      BERT      for      generation      tasks.\n \n \n  UniLM      (Dong      et      al.,      2019)      fine-tunes      BERT      with      an  \n \n  ensemble      of      masks,      some      of      which      allow      only      leftward  \n \n  context.\nLike      BART,      this      allows      UniLM      to      be      used      for  \n \n  both      generative      and      discriminative      tasks.\n \n \nA      difference  \n \n  is      that      UniLM      predictions      are      conditionally      indepen-  \n \n  dent,      whereas      BART’s      are      autoregressive.\nBART      re-  \n \n  duces      the      mismatch      between      pre-training      and      genera-  \n \n  tion      tasks,      because      the      decoder      is      always      trained      on      un-  \n \n  corrupted      context.\n \n \n  XL-Net      (Yang      et      al.,      2019)      extends      BERT      by      pre-\n |       Source      Document      (abbreviated) |       BART      Summary\n |       The      researchers      examined      three      types      of      coral      in      reefs      off      the coast of Fiji The researchers found when fish were plentiful,       they      would      eat      algae      and      seaweed      off      the      corals,      which      appeared       to      leave      them      more      resistant      to      the      bacterium      Vibrio      coralliilyti-       cus,      a      bacterium      associated      with      bleaching.      The      researchers      sug-       gested      the      algae,      like      warming      temperatures,      might      render      the       corals’      chemical      defenses      less      effective,      and      the      fish      were      pro-       tecting      the      coral      by      removing      the      algae. |       Fisheries      off      the      coast      of      Fiji      are      protect-       ing      coral      reefs      from      the      effects      of      global       warming,      according      to      a      study      in      the      jour-       nal      Science.\n |       Sacoolas,      who      has      immunity      as      a      diplomat’s      wife,      was      involved in a traffic collision Prime Minister Johnson was questioned       about      the      case      while      speaking      to      the      press      at      a      hospital      in      Wat- ford. He said, “I hope that Anne Sacoolas will come back       if      we      can’t      resolve      it      then      of      course      I      will      be      raising      it      myself       personally      with      the      White      House.” |       Boris      Johnson      has      said      he      will      raise      the      is-       sue      of      US      diplomat      Anne      Sacoolas’      diplo-       matic      immunity      with      the      White      House.\n |       According      to      Syrian      state      media,      government      forces      began      de- ploying into previously SDF controlled territory yesterday.       On      October      6,      US      President      Donald      Trump      and      Turkish      Presi-       dent      Recep      Tayyip      Erdoan      spoke      on      the      phone.      Then      both      na-       tions      issued      statements      speaking      of      an      imminent      incursion      into northeast Syria On Wednesday, Turkey began a military       offensive      with      airstrikes      followed      by      a      ground      invasion. |       Syrian      government      forces      have      entered       territory      held      by      the      US-backed      Syrian       Democratic      Forces      (SDF)      in      response      to       Turkey’s      incursion      into      the      region.\n |       This      is      the      first      time      anyone      has      been      recorded      to      run      a      full       marathon      of      42.195      kilometers      (approximately      26      miles)      under       this      pursued      landmark      time.      It      was      not,      however,      an      officially       sanctioned      world      record,      as      it      was      not      an      ’open      race”      of      the       IAAF.      His      time      was      1      hour      59      minutes      40.2      seconds.      Kipchoge       ran      in      Vienna,      Austria.      It      was      an      event      specifically      designed      to       help      Kipchoge      break      the      two      hour      barrier. |       Kenyan      runner      Eliud      Kipchoge      has      run      a       marathon      in      less      than      two      hours.\n |       PG&E      stated      it      scheduled      the      blackouts      in      response      to      forecasts       for      high      winds      amid      dry      conditions.      The      aim      is      to      reduce      the      risk       of      wildfires.      Nearly      800      thousand      customers      were      scheduled      to       be      affected      by      the      shutoffs      which      were      expected      to      last      through       at      least      midday      tomorrow. |       Power      has      been      turned      off      to      millions      of       customers      in      California      as      part      of      a      power       shutoff      plan.\n\n \n \n  dicting      masked      tokens      auto-regressively      in   \n \na      permuted  \n \n  order.\nThis      objective      allows      predictions      to      condition      on  \n \n  both      left      and      right      context.\nIn      contrast,      the      BART      de-  \n \n  coder      works      left-to-right      during      pre-training,      matching  \n \n  the      setting      during      generation.\n \n \n  Several      papers      have      explored      using      pre-trained      rep-  \n \n  resentations      to      improve      machine      translation.\nThe  \n \n  largest      improvements      have      come      from      pre-training      on  \n \n  both      source      and      target      languages      (Song      et      al.,      2019;  \n \n  Lample   \n \n&      Conneau,      2019),      but      this      requires      pre-  \n \n  training      on      all      languages      of      interest.\nOther      work      has  \n \n  shown      that      encoders      can      be      improved      using      pre-trained  \n \n  representations      (Edunov      et      al.,      2019),      but      gains      in      de-  \n \n  coders      are      more      limited.\nWe      show      how      BART      can      be  \n \n  used      to      improve      machine      translation      decoders.\n8\n \n   Conclusions  \n \n  We      introduced      BART,   \n \na      pre-training      approach      that  \n \n  learns      to      map      corrupted      documents      to      the      original.\n \n \n  BART      achieves      similar      performance      to      ROBERTa      on  \n \n  discriminative      tasks,      while      achieving      new      state-of-the-  \n \n  art      results      on   \n \na      number      of      text      generation      tasks.\nFu-  \n \n  ture      work      should      explore      new      methods      for      corrupting  \n \n  documents      for      pre-training,      perhaps      tailoring      them      to  \n \n  specific      end      tasks."
    },
    {
      "id": 20,
      "title": "5.4      Translation",
      "type": "section",
      "children": [],
      "content": "5.4      Translation\ntion  \n \n  We      also      evaluated      performance      on      WMT16      Romanian-  \n \n  English,      augmented      with      back-translation      data  \n \n  from      Sennrich      et      al.\n(2016).\nWe      use   \n \na      6-layer  \n \n  transformer      source      encoder      to      map      Romanian      into  \n \n  a      representation      that      BART      is      able      to      de-noise      into  \n \n  English,      following      the      approach      introduced      in      83.4.  \n \n  Experiment      results      are      presented      in      Table      6.      We  \n \n  compare      our      results      against   \n \na      baseline      Transformer  \n \n  architecture      (Vaswani      et      al.,      2017)      with      Transformer-  \n \n  large      settings      (the      baseline      row).\nWe      show      the  \n \n  performance      of      both      steps      of      our      model      in      the      fixed  \n \n  BART      and      tuned      BART      rows.\nFor      each      row      we  \n \n  experiment      on      the      original      WMT16      Romanian-English  \n \n  augmented      with      back-translation      data.\nWe      use   \n \na       beam      width      of   \n \n5      and   \n \na      length      penalty      of   \n \na   \n \n=      1.  \n \n  Preliminary      results      suggested      that      our      approach      was  \n \n  less      effective      without      back-translation      data,      and      prone  \n \n  to      overfitting—future      work      should      explore      additional  \n \n  regularization      techniques.\n6\n \n   Qualitative      Analysis  \n \n  BART      shows      large      improvements      on      summarization  \n \n  metrics,      of      up      to   \n \n6      points      over      the      prior      state-of-the-art.\n \n \n  To      understand      BART’s      performance      beyond      automated  \n \n  metrics,      we      analyse      its      generations      qualitatively.\n \n \n  Table   \n \n7      shows      example      summaries      generated      by  \n \n  BART.\nExamples      are      taken      from      WikiNews      articles  \n \n  published      after      the      creation      of      the      pre-training      corpus,  \n \n  to      eliminate      the      possibility      of      the      events      described      be-  \n \n  ing      present      in      the      model’s      training      data.\nFollowing  \n \n  Narayan      et      al.\n(2018),      we      remove      the      first      sentence      of  \n \n  the      article      prior      to      summarizing      it,      so      there      is      no      easy  \n \n  extractive      summary      of      the      document.\n \n \n  Unsurprisingly,      model      output      is      fluent      and      grammat-  \n \n  ical      English.\nHowever,      model      output      is      also      highly      ab-  \n \n  stractive,      with      few      phrases      copied      from      the      input.\nThe  \n \n  output      is      also      generally      factually      accurate,      and      inte-  \n \n  grates      supporting      evidence      from      across      the      input      doc-  \n \n  ument      with      background      knowledge      (for      example,      cor-  \n \n  rectly      completing      names,      or      inferring      that      PG&E      oper-  \n \n  ates      in      California).\nIn      the      first      example,      inferring      that  \n \n  fish      are      protecting      reefs      from      global      warming      requires  \n \n  non-trivial      inference      from      the      text.\nHowever,      the      claim  \n \n  that      the      work      was      published      in      Science      is      not      supported  \n \n  by      the      source.\n \n \n  These      samples      demonstrate      that      the      BART      pretrain-  \n \n  ing      has      learned   \n \na      strong      combination      of      natural      lan-  \n \n  guage      understanding      and      generation.\n7\n \n   Related      Work  \n \n  Early      methods      for      pretraining      were      based      on      language  \n \n  models.\nGPT      (Radford      et      al.,      2018)      only      models      left-  \n \n  ward      context,      which      is      problematic      for      some      tasks.\n \n \n  ELMo      (Peters      et      al.,      2018)      concatenates      left-only      and  \n \n  right-only      representations,      but      does      not      pre-train      inter-  \n \n  actions      between      these      features.\nRadford      et      al.\n(2019)  \n \n  demonstrated      that      very      large      language      models      can      act       as      unsupervised      multitask      models.\n \n \n  MASS      (Song      et      al.,      2019)      is      perhaps      the      most      similar  \n \n  model      to      BART.\nAn      input      sequence      where   \n \na      contiguous  \n \n  span      of      tokens      is      masked      is      mapped      to   \n \na      sequence      con-  \n \n  sisting      of      the      missing      tokens.\nMASS      is      less      effective  \n \n  for      discriminative      tasks,      because      disjoint      sets      of      tokens  \n \n  are      fed      into      the      encoder      and      decoder.\n \n \n  BERT      (Devlin      et      al.,      2019)      introduced      masked      lan-  \n \n  guage      modelling,      which      allows      pre-training      to      learn      in-  \n \n  teractions      between      left      and      right      context      words.\nRe-  \n \n  cent      work      has      shown      that      very      strong      performance      can  \n \n  be      achieved      by      training      for      longer      (Liu      et      al.,      2019),  \n \n  by      tying      parameters      across      layers      (Lan      et      al.,      2019),  \n \n  and      by      masking      spans      instead      of      words      (Joshi      et      al.,  \n \n  2019).\nPredictions      are      not      made      auto-regressively,      re-  \n \n  ducing      the      effectiveness      of      BERT      for      generation      tasks.\n \n \n  UniLM      (Dong      et      al.,      2019)      fine-tunes      BERT      with      an  \n \n  ensemble      of      masks,      some      of      which      allow      only      leftward  \n \n  context.\nLike      BART,      this      allows      UniLM      to      be      used      for  \n \n  both      generative      and      discriminative      tasks.\n \n \nA      difference  \n \n  is      that      UniLM      predictions      are      conditionally      indepen-  \n \n  dent,      whereas      BART’s      are      autoregressive.\nBART      re-  \n \n  duces      the      mismatch      between      pre-training      and      genera-  \n \n  tion      tasks,      because      the      decoder      is      always      trained      on      un-  \n \n  corrupted      context.\n \n \n  XL-Net      (Yang      et      al.,      2019)      extends      BERT      by      pre-\n |       Source      Document      (abbreviated) |       BART      Summary\n |       The      researchers      examined      three      types      of      coral      in      reefs      off      the coast of Fiji The researchers found when fish were plentiful,       they      would      eat      algae      and      seaweed      off      the      corals,      which      appeared       to      leave      them      more      resistant      to      the      bacterium      Vibrio      coralliilyti-       cus,      a      bacterium      associated      with      bleaching.      The      researchers      sug-       gested      the      algae,      like      warming      temperatures,      might      render      the       corals’      chemical      defenses      less      effective,      and      the      fish      were      pro-       tecting      the      coral      by      removing      the      algae. |       Fisheries      off      the      coast      of      Fiji      are      protect-       ing      coral      reefs      from      the      effects      of      global       warming,      according      to      a      study      in      the      jour-       nal      Science.\n |       Sacoolas,      who      has      immunity      as      a      diplomat’s      wife,      was      involved in a traffic collision Prime Minister Johnson was questioned       about      the      case      while      speaking      to      the      press      at      a      hospital      in      Wat- ford. He said, “I hope that Anne Sacoolas will come back       if      we      can’t      resolve      it      then      of      course      I      will      be      raising      it      myself       personally      with      the      White      House.” |       Boris      Johnson      has      said      he      will      raise      the      is-       sue      of      US      diplomat      Anne      Sacoolas’      diplo-       matic      immunity      with      the      White      House.\n |       According      to      Syrian      state      media,      government      forces      began      de- ploying into previously SDF controlled territory yesterday.       On      October      6,      US      President      Donald      Trump      and      Turkish      Presi-       dent      Recep      Tayyip      Erdoan      spoke      on      the      phone.      Then      both      na-       tions      issued      statements      speaking      of      an      imminent      incursion      into northeast Syria On Wednesday, Turkey began a military       offensive      with      airstrikes      followed      by      a      ground      invasion. |       Syrian      government      forces      have      entered       territory      held      by      the      US-backed      Syrian       Democratic      Forces      (SDF)      in      response      to       Turkey’s      incursion      into      the      region.\n |       This      is      the      first      time      anyone      has      been      recorded      to      run      a      full       marathon      of      42.195      kilometers      (approximately      26      miles)      under       this      pursued      landmark      time.      It      was      not,      however,      an      officially       sanctioned      world      record,      as      it      was      not      an      ’open      race”      of      the       IAAF.      His      time      was      1      hour      59      minutes      40.2      seconds.      Kipchoge       ran      in      Vienna,      Austria.      It      was      an      event      specifically      designed      to       help      Kipchoge      break      the      two      hour      barrier. |       Kenyan      runner      Eliud      Kipchoge      has      run      a       marathon      in      less      than      two      hours.\n |       PG&E      stated      it      scheduled      the      blackouts      in      response      to      forecasts       for      high      winds      amid      dry      conditions.      The      aim      is      to      reduce      the      risk       of      wildfires.      Nearly      800      thousand      customers      were      scheduled      to       be      affected      by      the      shutoffs      which      were      expected      to      last      through       at      least      midday      tomorrow. |       Power      has      been      turned      off      to      millions      of       customers      in      California      as      part      of      a      power       shutoff      plan.\n\n \n \n  dicting      masked      tokens      auto-regressively      in   \n \na      permuted  \n \n  order.\nThis      objective      allows      predictions      to      condition      on  \n \n  both      left      and      right      context.\nIn      contrast,      the      BART      de-  \n \n  coder      works      left-to-right      during      pre-training,      matching  \n \n  the      setting      during      generation.\n \n \n  Several      papers      have      explored      using      pre-trained      rep-  \n \n  resentations      to      improve      machine      translation.\nThe  \n \n  largest      improvements      have      come      from      pre-training      on  \n \n  both      source      and      target      languages      (Song      et      al.,      2019;  \n \n  Lample   \n \n&      Conneau,      2019),      but      this      requires      pre-  \n \n  training      on      all      languages      of      interest.\nOther      work      has  \n \n  shown      that      encoders      can      be      improved      using      pre-trained  \n \n  representations      (Edunov      et      al.,      2019),      but      gains      in      de-  \n \n  coders      are      more      limited.\nWe      show      how      BART      can      be  \n \n  used      to      improve      machine      translation      decoders.\n8\n \n   Conclusions  \n \n  We      introduced      BART,   \n \na      pre-training      approach      that  \n \n  learns      to      map      corrupted      documents      to      the      original.\n \n \n  BART      achieves      similar      performance      to      ROBERTa      on  \n \n  discriminative      tasks,      while      achieving      new      state-of-the-  \n \n  art      results      on   \n \na      number      of      text      generation      tasks.\nFu-  \n \n  ture      work      should      explore      new      methods      for      corrupting  \n \n  documents      for      pre-training,      perhaps      tailoring      them      to  \n \n  specific      end      tasks."
    },
    {
      "id": 21,
      "title": "References",
      "type": "section",
      "children": [],
      "content": "References\n \n \n  Eneko      Agirre,      Llu’is      M‘arquez,      and      Richard      Wicen-  \n \n  towski      (eds.).\nProceedings      of      the      Fourth      Interna-  \n \n  tional      Workshop      on      Semantic      Evaluations      (SemEval-  \n \n  2007).\nAssociation      for      Computational      Linguistics,  \n \n  Prague,      Czech      Republic,      June      2007.\n \n \n  Ido      Dagan,      Oren      Glickman,      and      Bernardo      Magnini.\n \n \n  The      PASCAL      recognising      textual      entailment      chal-  \n \n  lenge.\nIn      Machine      learning      challenges.\nevaluat-  \n \n  ing      predictive      uncertainty,      visual      object      classifica-  \n \n  tion,      and      recognising      tectual      entailment,      pp.\n177—  \n \n  190.\nSpringer,      2006.\n \n \n  Jacob      Devlin,      Ming-Wei      Chang,      Kenton      Lee,      and  \n \n  Kristina      Toutanova.\nBERT:      Pre-training      of      deep  \n \n  bidirectional      transformers      for      language      understand-  \n \n  ing.\nIn      Proceedings      of      the      2019      Conference      of      the  \n \n  North      American      Chapter      of      the      Association      for      Com-  \n \n  putational      Linguistics:      Human      Language      Technolo-  \n \n  gies,      Volume   \n \nI      (Long      and      Short      Papers),      pp.\n4171-  \n \n  4186,      Minneapolis,      Minnesota,      June      2019.\nAssocia-  \n \n  tion      for      Computational      Linguistics.\ndoi:      10.18653/  \n \n  vI/N19-1423.\nURL      https://www.aclweb.\n \n \n  org/anthology/N19-1423.\n \n \n  Emily      Dinan,      Varvara      Logacheva,      Valentin      Malykh,  \n \n  Alexander      Miller,      Kurt      Shuster,      Jack      Urbanek,  \n \n  Douwe      Kiela,      Arthur      Szlam,      Iulian      Serban,      Ryan  \n \n  Lowe,      et      al.\nThe      second      conversational      in-  \n \n  telligence      challenge      (convai2).\narXiv      preprint  \n \n  arXiv:      1902.00098,      2019.\n \n \n  William   \n \nB      Dolan      and      Chris      Brockett.\nAutomatically  \n \n  constructing   \n \na      corpus      of      sentential      paraphrases.\nIn  \n \n  Proceedings      of      the      International      Workshop      on      Para-  \n \n  phrasing,      2005.\n \n \n  Li      Dong,      Nan      Yang,      Wenhui      Wang,      Furu      Wei,      Xi-  \n \n  aodong      Liu,      Yu      Wang,      Jianfeng      Gao,      Ming      Zhou,  \n \n  and      Hsiao-Wuen      Hon.\nUnified      language      model      pre-  \n \n  training      for      natural      language      understanding      and      gen-  \n \n  eration.\narXiv      preprint      arXiv:      1905.03197,      2019.\n \n \n  Sergey      Edunov,      Alexei      Baevski,      and      Michael      Auli.\n \n \n  Pre-trained      language      model      representations      for      lan-  \n \n  guage      generation.\nIn      Proceedings      of      the      2019      Con-  \n \n  ference      of      the      North      American      Chapter      of      the      Asso-  \n \n  ciation      for      Computational      Linguistics:      Human      Lan-  \n \n  guage      Technologies,      Volume   \n \n1      (Long      and      Short      Pa-  \n \n  pers),      2019.\n \n \n  Angela      Fan,      David      Grangier,      and      Michael      Auli.\nCon-  \n \n  trollable      abstractive      summarization.\narXiv      preprint  \n \n  arXiv:      1711.05217,      2017.\n \n \n  Angela      Fan,      Yacine      Jernite,      Ethan      Perez,      David  \n \n  Grangier,      Jason      Weston,      and      Michael      Auli.\nEli5:  \n \n  Long      form      question      answering.\narXiv      preprint  \n \n  arXiv:      1907.09190,      2019.\n \n \n  Dan      Hendrycks      and      Kevin      Gimpel.\nGaussian      error      lin-  \n \n  ear      units      (gelus).\narXiv      preprint      arXiv:      1606.08415,  \n \n  2016.\n \n \n  Karl      Moritz      Hermann,      Tomas      Kocisky,      Edward  \n \n  Grefenstette,      Lasse      Espeholt,      Will      Kay,      Mustafa      Su-  \n \n  leyman,      and      Phil      Blunsom.\nTeaching      machines      to  \n \n  read      and      comprehend.\nIn      Advances      in      neural      infor-  \n \n  mation      processing      systems,      pp.\n1693-1701,      2015.\n \n \n  Mandar      Joshi,      Danqi      Chen,      Yinhan      Liu,      Daniel   \n \nS      Weld,  \n \n  Luke      Zettlemoyer,      and      Omer      Levy.\nSpanbert:      Im-  \n \n  proving      pre-training      by      representing      and      predicting  \n \n  spans.\narXiv      preprint      arXiv:      1907.10529,      2019.\n \n \n  Guillaume      Lample      and      Alexis      Conneau.\n \n \n—      Cross-  \n \n  lingual      language      model      pretraining.\narXiv      preprint  \n \n  arXiv:      1901.07291,      2019.\n \n \n  Zhenzhong      Lan,      Mingda      Chen,      Sebastian      Goodman,  \n \n  Kevin      Gimpel,      Piyush      Sharma,      and      Radu      Sori-  \n \n  cut.\nAlbert:   \n \nA      lite      bert      for      self-supervised      learn-  \n \n  ing      of      language      representations.\narXiv      preprint  \n \n  arXiv:      1909.11942,      2019.\n \n \n  Hector   \n \nJ      Levesque,      Ernest      Davis,      and      Leora      Morgen-  \n \n  stern.\nThe      Winograd      schema      challenge.\nIn      AAAI  \n \n  Spring      Symposium:      Logical      Formalizations      of      Com-  \n \n  monsense      Reasoning,      volume      46,      pp.\n47,      2011.\n \n \n  Yang      Liu      and      Mirella      Lapata.\n \n \n  tion      with      pretrained      encoders.\n \n \n  arXiv:      1908.08345,      2019.  \n \n  Text      summariza-  \n \n  arXiv      preprint  \n \n  Yinhan      Liu,      Myle      Ott,      Naman      Goyal,      Jingfei      Du,      Man-  \n \n  dar      Joshi,      Dangi      Chen,      Omer      Levy,      Mike      Lewis,  \n \n  Luke      Zettlemoyer,      and      Veselin      Stoyanov.\nRoberta:\n \n \n \nA      robustly      optimized      bert      pretraining      approach.\n \n \n  arXiv      preprint      arXiv:      1907.11692,      2019.\n \n \n  Tomas      Mikolov,      Kai      Chen,      Greg      Corrado,      and      Jeffrey  \n \n  Dean.\nEfficient      estimation      of      word      representations  \n \n  in      vector      space.\narXiv      preprint      arXiv:1301.3781,  \n \n  2013.\n \n \n  Shashi      Narayan,      Shay   \n \nB      Cohen,      and      Mirella      Lapata.\n \n \n  Don’t      give      me      the      details,      just      the      summary!\ntopic-  \n \n  aware      convolutional      neural      networks      for      extreme  \n \n  summarization.\narXiv      preprint      arXiv:      1808.08745,  \n \n  2018.\n \n \n  Gabriel      Pereyra,      George      Tucker,      Jan      Chorowski,  \n \n  Lukasz      Kaiser,      and      Geoffrey      Hinton.\nRegularizing  \n \n  neural      networks      by      penalizing      confident      output      dis-  \n \n  tributions.\narXiv      preprint      arXiv:      1701.06548,      2017.\n \n \n  Matthew      E      Peters,      Mark      Neumann,      Mohit      Iyyer,      Matt  \n \n  Gardner,      Christopher      Clark,      Kenton      Lee,      and      Luke  \n \n  Zettlemoyer.\nDeep      contextualized      word      representa-  \n \n  tions.\narXiv      preprint      arXiv:      1802.05365,      2018.\n \n \n  Alec      Radford,      Karthik      Narasimhan,      Tim      Salimans,  \n \n  and      Ilya      Sutskever.\nImproving      language      un-  \n \n  derstanding      by      generative      pre-training.\nURL  \n \n  https://s3-us-west-2.\n \n \n|      amazonaws.\n \n \n—      com/openai-  \n \n  assets/researchcovers/languageunsupervised/language  \n \n  understanding      paper.\npdf,      2018.\n \n \n  Alec      Radford,      Jeffrey      Wu,      Rewon      Child,      David      Luan,  \n \n  Dario      Amodei,      and      Ilya      Sutskever.\nLanguage      mod-  \n \n  els      are      unsupervised      multitask      learners.\nOpenAI  \n \n  Blog,      1(8),      2019.\n \n \n  Pranav      Rajpurkar,      Jian      Zhang,      Konstantin      Lopyrev,  \n \n  and      Percy      Liang.\nSquad:      100,000+      questions      for  \n \n  machine      comprehension      of      text.\narXiv      preprint  \n \n  arXiv:      1606.05250,      2016.\n \n \n  Abigail      See,      Peter   \n \nJ      Liu,      and      Christopher   \n \nD       Manning.\nGet      to      the      point:      Summarization  \n \n  with      pointer-generator      networks.\narXiv      preprint  \n \n  arXiv:      1704.04368,      2017.\n \n \n  Rico      Sennrich,      Barry      Haddow,      and      Alexandra      Birch.\n \n \n  Edinburgh      neural      machine      translation      systems      for  \n \n  WMT      16.\nIn      Proceedings      of      the      First      Conference  \n \n  on      Machine      Translation:      Volume      2,      Shared      Task      Pa-  \n \n  pers,      2016.\n \n \n  Richard      Socher,      Alex      Perelygin,      Jean      Wu,      Jason  \n \n  Chuang,      Christopher   \n \nD      Manning,      Andrew      Ng,      and  \n \n  Christopher      Potts.\nRecursive      deep      models      for      se-  \n \n  mantic      compositionality      over   \n \na      sentiment      treebank.\n \n \n  In      Proceedings      of      EMNLP,      pp.\n1631-1642,      2013.\n \n \n  Kaitao      Song,      Xu      Tan,      Tao      Qin,      Jianfeng      Lu,      and      Tie-  \n \n  Yan      Liu.\nMass:      Masked      sequence      to      sequence      pre-  \n \n  training      for      language      generation.\nIn      International  \n \n  Conference      on      Machine      Learning,      2019.\n \n \n  Ashish      Vaswani,      Noam      Shazeer,      Niki      Parmar,      Jakob  \n \n  Uszkoreit,      Llion      Jones,      Aidan      N      Gomez,      Lukasz  \n \n  Kaiser,      and      Ilia      Polosukhin.\nAttention      is      all      you  \n \n  need.\nIn      Advances      in      neural      information      processing  \n \n  systems,      pp.\n5998-6008,      2017.\n \n \n  Alex      Wang,      Amanpreet      Singh,      Julian      Michael,      Felix  \n \n  Hill,      Omer      Levy,      and      Samuel      R      Bowman.\nGlue:\n \n \n \nA      multi-task      benchmark      and      analysis      platform      for  \n \n  natural      language      understanding.\narXiv      preprint  \n \n  arXiv:      1804.07461,      2018.\n \n \n  Alex      Warstadt,      Amanpreet      Singh,      and      Samuel      R.  \n \n  Bowman.\nNeural      network      acceptability      judgments.\n \n \n  arXiv      preprint      1805.12471,      2018.\n \n \n  Adina      Williams,      Nikita      Nangia,      and      Samuel      R      Bow-  \n \n  man.\n \n \nA   \n \n_      broad-coverage      challenge      corpus      for  \n \n  sentence      understanding      through      inference.\narXiv  \n \n  preprint      arXiv:      1704.05426,      2017.\n \n \n  Adina      Williams,      Nikita      Nangia,      and      Samuel      R.      Bow-  \n \n  man.\n \n \nA      broad-coverage      challenge      corpus      for      sen-  \n \n  tence      understanding      through      inference.\nIn      Proceed-  \n \n  ings      of      NAACL-HLT,      2018.\n \n \n  Zhilin      Yang,      Zihang      Dai,      Yiming      Yang,      Jaime  \n \n  Carbonell,      Ruslan      Salakhutdinov,      and      Quoc   \n \nV       Le.\nXlInet:      Generalized      autoregressive      pretrain-  \n \n  ing      for      language      understanding.\narXiv      preprint  \n \n  arXiv:      1906.08237,      2019."
    }
  ],
  "content": ""
}